"import re
from thefuck.utils import replace_argument
from thefuck.specific.git import git_support

@git_support
def match(command):
    return ('push' in command.script_parts
            and 'set-upstream' in command.output)

def _get_upstream_option_index(command_parts):
    if '--set-upstream' in command_parts:
        return command_parts.index('--set-upstream')
    elif '-u' in command_parts:
        return command_parts.index('-u')
    else:
        return None

@git_support
def get_new_command(command):
    # If --set-upstream or -u are passed, remove it and its argument. This is
    # because the remaining arguments are concatenated onto the command suggested
    # by git, which includes --set-upstream and its argument
    command_parts = command.script_parts[:]
    upstream_option_index = _get_upstream_option_index(command_parts)
    if upstream_option_index is not None:
        command_parts.pop(upstream_option_index)
        # In case of `git push -u` we don't have next argument:
        if len(command_parts) > upstream_option_index:
            command_parts.pop(upstream_option_index)
    else:
        # the only non-qualified permitted options are the repository and refspec; git's
        # suggestion include them, so they won't be lost, but would be duplicated otherwise.
        push_idx = command_parts.index('push') + 1
        while len(command_parts) > push_idx and command_parts[len(command_parts) - 1][0] != '-':
            command_parts.pop(len(command_parts) - 1)
    arguments = re.findall(r'git push (.*)', command.output)[0].replace(""'"", r""\'"").strip()
    return replace_argument("" "".join(command_parts), 'push',
                            'push {}'.format(arguments))",[8]
"#!/usr/bin/env python
# -*- coding: utf-8 -*-
""""""
cookiecutter.hooks
------------------
Functions for discovering and executing various cookiecutter hooks.
""""""
import io
import logging
import os
import subprocess
import sys
import tempfile
from jinja2 import Template
from cookiecutter import utils

_HOOKS = [
    'pre_gen_project',
    'post_gen_project',
    # TODO: other hooks should be listed here
]
EXIT_SUCCESS = 0

def find_hooks():
    """"""
    Must be called with the project template as the current working directory.
    Returns a dict of all hook scripts provided.
    Dict's key will be the hook/script's name, without extension, while
    values will be the absolute path to the script.
    Missing scripts will not be included in the returned dict.
    """"""
    hooks_dir = 'hooks'
    r = {}
    logging.debug('hooks_dir is {0}'.format(hooks_dir))
    if not os.path.isdir(hooks_dir):
        logging.debug('No hooks/ dir in template_dir')
        return r
    for f in os.listdir(hooks_dir):
        basename = os.path.splitext(os.path.basename(f))[0]
        if basename in _HOOKS:
            r[basename] = os.path.abspath(os.path.join(hooks_dir, f))
    return r

def run_script(script_path, cwd='.'):
    """"""
    Executes a script from a working directory.
    :param script_path: Absolute path to the script to run.
    :param cwd: The directory to run the script from.
    """"""
    run_thru_shell = sys.platform.startswith('win')
    if script_path.endswith('.py'):
        script_command = [sys.executable, script_path]
    else:
        script_command = [script_path]
    utils.make_executable(script_path)
    proc = subprocess.Popen(
        script_command,
        shell=run_thru_shell,
        cwd=cwd
    )
    return proc.wait()

def run_script_with_context(script_path, cwd, context):
    """"""
    Executes a script after rendering with it Jinja.
    :param script_path: Absolute path to the script to run.
    :param cwd: The directory to run the script from.
    :param context: Cookiecutter project template context.
    """"""
    _, extension = os.path.splitext(script_path)
    contents = io.open(script_path, 'r', encoding='utf-8').read()
    with tempfile.NamedTemporaryFile(
        delete=False,
        mode='w',
        suffix=extension
    ) as temp:
        temp.write(Template(contents).render(**context))
    return run_script(temp.name, cwd)

def run_hook(hook_name, project_dir, context):
    """"""
    Try to find and execute a hook from the specified project directory.
    :param hook_name: The hook to execute.
    :param project_dir: The directory to execute the script from.
    :param context: Cookiecutter project context.
    """"""
    script = find_hooks().get(hook_name)
    if script is None:
        logging.debug('No hooks found')
        return EXIT_SUCCESS
    return run_script_with_context(script, project_dir, context)","[71, 93, 107]"
"    def _getitem_iterable(self, key, axis: int):
        """"""
        Index current object with an an iterable collection of keys.
        Parameters
        ----------
        key : iterable
            Targeted labels.
        axis: int
            Dimension on which the indexing is being made.
        Raises
        ------
        KeyError
            If no key was found. Will change in the future to raise if not all
            keys were found.
        Returns
        -------
        scalar, DataFrame, or Series: indexed value(s).
        """"""
        # we assume that not com.is_bool_indexer(key), as that is
        #  handled before we get here.
        self._validate_key(key, axis)
        # A collection of keys
        keyarr, indexer = self._get_listlike_indexer(key, axis, raise_missing=False)
        return self.obj._reindex_with_indexers(
            {axis: [keyarr, indexer]}, copy=True, allow_dups=True
        )
    def _getitem_tuple(self, tup: Tuple):
        try:
            return self._getitem_lowerdim(tup)
        except IndexingError:
            pass
        # no multi-index, so validate all of the indexers
        self._has_valid_tuple(tup)
        # ugly hack for GH #836
        if self._multi_take_opportunity(tup):
            return self._multi_take(tup)
        return self._getitem_tuple_same_dim(tup)
    def _get_label(self, label, axis: int):
        # GH#5667 this will fail if the label is not present in the axis.
        return self.obj.xs(label, axis=axis)
    def _handle_lowerdim_multi_index_axis0(self, tup: Tuple):
        # we have an axis0 multi-index, handle or raise
        axis = self.axis or 0
        try:
            # fast path for series or for tup devoid of slices
            return self._get_label(tup, axis=axis)
        except TypeError:
            # slices are unhashable
            pass
        except KeyError as ek:
            # raise KeyError if number of indexers match
            # else IndexingError will be raised
            if len(tup) <= self.obj.index.nlevels and len(tup) > self.ndim:
                raise ek
        return None
    def _getitem_axis(self, key, axis: int):
        key = item_from_zerodim(key)
        if is_iterator(key):
            key = list(key)
        labels = self.obj._get_axis(axis)
        key = labels._get_partial_string_timestamp_match_key(key)
        if isinstance(key, slice):
            self._validate_key(key, axis)
            return self._get_slice_axis(key, axis=axis)
        elif com.is_bool_indexer(key):
            return self._getbool_axis(key, axis=axis)
        elif is_list_like_indexer(key):
            # convert various list-like indexers
            # to a list of keys
            # we will use the *values* of the object
            # and NOT the index if its a PandasObject
            if isinstance(labels, ABCMultiIndex):
                if isinstance(key, (ABCSeries, np.ndarray)) and key.ndim <= 1:
                    # Series, or 0,1 ndim ndarray
                    # GH 14730
                    key = list(key)
                elif isinstance(key, ABCDataFrame):
                    # GH 15438
                    raise NotImplementedError(
                        ""Indexing a MultiIndex with a ""
                        ""DataFrame key is not ""
                        ""implemented""
                    )
                elif hasattr(key, ""ndim"") and key.ndim > 1:
                    raise NotImplementedError(
                        ""Indexing a MultiIndex with a ""
                        ""multidimensional key is not ""
                        ""implemented""
                    )
                if (
                    not isinstance(key, tuple)
                    and len(key)
                    and not isinstance(key[0], tuple)
                ):
                    key = tuple([key])
            # an iterable multi-selection
            if not (isinstance(key, tuple) and isinstance(labels, ABCMultiIndex)):
                if hasattr(key, ""ndim"") and key.ndim > 1:
                    raise ValueError(""Cannot index with multidimensional key"")
                return self._getitem_iterable(key, axis=axis)
            # nested tuple slicing
            if is_nested_tuple(key, labels):
                locs = labels.get_locs(key)
                indexer = [slice(None)] * self.ndim
                indexer[axis] = locs","[87, 88, 89, 92, 93, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113]"
"""""""
This module implements the FormRequest class which is a more convenient class
(than Request) to generate Requests based on form data.
See documentation in docs/topics/request-response.rst
""""""
from six.moves.urllib.parse import urljoin, urlencode
import lxml.html
from parsel.selector import create_root_node
import six
from scrapy.http.request import Request
from scrapy.utils.python import to_bytes, is_listlike
from scrapy.utils.response import get_base_url

class FormRequest(Request):
    def __init__(self, *args, **kwargs):
        formdata = kwargs.pop('formdata', None)
        if formdata and kwargs.get('method') is None:
            kwargs['method'] = 'POST'
        super(FormRequest, self).__init__(*args, **kwargs)
        if formdata:
            items = formdata.items() if isinstance(formdata, dict) else formdata
            querystr = _urlencode(items, self.encoding)
            if self.method == 'POST':
                self.headers.setdefault(b'Content-Type', b'application/x-www-form-urlencoded')
                self._set_body(querystr)
            else:
                self._set_url(self.url + ('&' if '?' in self.url else '?') + querystr)
    @classmethod
    def from_response(cls, response, formname=None, formid=None, formnumber=0, formdata=None,
                      clickdata=None, dont_click=False, formxpath=None, formcss=None, **kwargs):
        kwargs.setdefault('encoding', response.encoding)
        if formcss is not None:
            from parsel.csstranslator import HTMLTranslator
            formxpath = HTMLTranslator().css_to_xpath(formcss)
        form = _get_form(response, formname, formid, formnumber, formxpath)
        formdata = _get_inputs(form, formdata, dont_click, clickdata, response)
        url = _get_form_url(form, kwargs.pop('url', None))
        method = kwargs.pop('method', form.method)
        return cls(url=url, method=method, formdata=formdata, **kwargs)

def _get_form_url(form, url):
    if url is None:
        return urljoin(form.base_url, form.action)
    return urljoin(form.base_url, url)

def _urlencode(seq, enc):
    values = [(to_bytes(k, enc), to_bytes(v, enc))
              for k, vs in seq
              for v in (vs if is_listlike(vs) else [vs])]
    return urlencode(values, doseq=1)

def _get_form(response, formname, formid, formnumber, formxpath):
    """"""Find the form element """"""
    root = create_root_node(response.text, lxml.html.HTMLParser,
                            base_url=get_base_url(response))
    forms = root.xpath('//form')
    if not forms:
        raise ValueError(""No <form> element found in %s"" % response)
    if formname is not None:
        f = root.xpath('//form[@name=""%s""]' % formname)
        if f:
            return f[0]
    if formid is not None:
        f = root.xpath('//form[@id=""%s""]' % formid)
        if f:
            return f[0]
    # Get form element from xpath, if not found, go up
    if formxpath is not None:
        nodes = root.xpath(formxpath)
        if nodes:
            el = nodes[0]
            while True:
                if el.tag == 'form':
                    return el
                el = el.getparent()
                if el is None:
                    break
        encoded = formxpath if six.PY3 else formxpath.encode('unicode_escape')
        raise ValueError('No <form> element found with %s' % encoded)
    # If we get here, it means that either formname was None
    # or invalid
    if formnumber is not None:
        try:
            form = forms[formnumber]
        except IndexError:
            raise IndexError(""Form number %d not found in %s"" %
                             (formnumber, response))
        else:
            return form

def _get_inputs(form, formdata, dont_click, clickdata, response):
    try:
        formdata = dict(formdata or ())
    except (ValueError, TypeError):
        raise ValueError('formdata should be a dict or iterable of tuples')
    inputs = form.xpath('descendant::textarea'
                        '|descendant::select'
                        '|descendant::input[not(@type) or @type['
                        ' not(re:test(., ""^(?:submit|image|reset)$"", ""i""))'
                        ' and (../@checked or'
                        '  not(re:test(., ""^(?:checkbox|radio)$"", ""i"")))]]',
                        namespaces={
                            ""re"": ""http://exslt.org/regular-expressions""})
    values = [(k, u'' if v is None else v)
              for k, v in (_value(e) for e in inputs)
              if k and k not in formdata]
    if not dont_click:","[10, 53]"
"        flag = '--' + dest.replace('_', '-')
        description = []
        description.append('%s.%s' % (task_name, param_name))
        if glob:
            description.append('for all instances of class %s' % task_name)
        elif self.description:
            description.append(self.description)
        if self.has_value:
            description.append("" [default: %s]"" % (self.value,))
        if self.is_list:
            action = ""append""
        elif self.is_bool:
            action = ""store_true""
        else:
            action = ""store""
        if optparse:
            f = parser.add_option
        else:
            f = parser.add_argument
        f(flag,
          help=' '.join(description),
          action=action,
          dest=dest)
    def parse_from_args(self, param_name, task_name, args, params):
        # Note: modifies arguments
        dest = self.parser_dest(param_name, task_name, glob=False)
        if dest is not None:
            value = getattr(args, dest, None)
            params[param_name] = self.parse_from_input(param_name, value)
    def set_global_from_args(self, param_name, task_name, args, is_without_section=False):
        # Note: side effects
        dest = self.parser_dest(param_name, task_name, glob=True, is_without_section=is_without_section)
        if dest is not None:
            value = getattr(args, dest, None)
            if value:
                self.set_global(self.parse_from_input(param_name, value))
            else:  # either False (bools) or None (everything else)
                self.reset_global()

class DateHourParameter(Parameter):
    """"""
    Parameter whose value is a :py:class:`~datetime.datetime` specified to the hour.
    A DateHourParameter is a `ISO 8601 <http://en.wikipedia.org/wiki/ISO_8601>`_ formatted
    date and time specified to the hour. For example, ``2013-07-10T19`` specifies July 10, 2013 at
    19:00.
    """"""
    date_format = '%Y-%m-%dT%H'  # ISO 8601 is to use 'T'
    def parse(self, s):
        """"""
        Parses a string to a :py:class:`~datetime.datetime` using the format string ``%Y-%m-%dT%H``.
        """"""
        # TODO(erikbern): we should probably use an internal class for arbitary
        # time intervals (similar to date_interval). Or what do you think?
        return datetime.datetime.strptime(s, self.date_format)
    def serialize(self, dt):
        """"""
        Converts the datetime to a string usnig the format string ``%Y-%m-%dT%H``.
        """"""
        if dt is None:
            return str(dt)
        return dt.strftime(self.date_format)

class DateMinuteParameter(DateHourParameter):
    """"""
    Parameter whose value is a :py:class:`~datetime.datetime` specified to the minute.
    A DateMinuteParameter is a `ISO 8601 <http://en.wikipedia.org/wiki/ISO_8601>`_ formatted
    date and time specified to the minute. For example, ``2013-07-10T19H07`` specifies July 10, 2013 at
    19:07.
    """"""
    date_format = '%Y-%m-%dT%HH%M'  # ISO 8601 is to use 'T' and 'H'

class DateParameter(Parameter):
    """"""
    Parameter whose value is a :py:class:`~datetime.date`.
    A DateParameter is a Date string formatted ``YYYY-MM-DD``. For example, ``2013-07-10`` specifies
    July 10, 2013.
    """"""
    def parse(self, s):
        """"""Parses a date string formatted as ``YYYY-MM-DD``.""""""
        return datetime.date(*map(int, s.split('-')))

class IntParameter(Parameter):
    """"""
    Parameter whose value is an ``int``.
    """"""
    def parse(self, s):
        """"""
        Parses an ``int`` from the string using ``int()``.
        """"""
        return int(s)

class FloatParameter(Parameter):
    """"""
    Parameter whose value is a ``float``.
    """"""
    def parse(self, s):
        """"""
        Parses a ``float`` from the string using ``float()``.
        """"""
        return float(s)

class BoolParameter(Parameter):
    """"""
    A Parameter whose value is a ``bool``.
    """"""
    def __init__(self, *args, **kwargs):","[8, 9, 31, 39]"
"    exclude: Union[SetIntStr, DictIntStrAny] = set(),
    by_alias: bool = True,
    skip_defaults: bool = None,
    exclude_unset: bool = False,
    include_none: bool = True,
    custom_encoder: dict = {},
    sqlalchemy_safe: bool = True,
) -> Any:
    if skip_defaults is not None:
        logger.warning(  # pragma: nocover
            ""skip_defaults in jsonable_encoder has been deprecated in favor of ""
            ""exclude_unset to keep in line with Pydantic v1, support for it will be ""
            ""removed soon.""
        )
    if include is not None and not isinstance(include, set):
        include = set(include)
    if exclude is not None and not isinstance(exclude, set):
        exclude = set(exclude)
    if isinstance(obj, BaseModel):
        encoder = getattr(obj.Config, ""json_encoders"", {})
        if custom_encoder:
            encoder.update(custom_encoder)
        if PYDANTIC_1:
            obj_dict = obj.dict(
                include=include,
                exclude=exclude,
                by_alias=by_alias,
                exclude_unset=bool(exclude_unset or skip_defaults),
            )
        else:  # pragma: nocover
            obj_dict = obj.dict(
                include=include,
                exclude=exclude,
                by_alias=by_alias,
                skip_defaults=bool(exclude_unset or skip_defaults),
            )
        return jsonable_encoder(
            obj_dict,
            include_none=include_none,
            custom_encoder=encoder,
            sqlalchemy_safe=sqlalchemy_safe,
        )
    if isinstance(obj, Enum):
        return obj.value
    if isinstance(obj, PurePath):
        return str(obj)
    if isinstance(obj, (str, int, float, type(None))):
        return obj
    if isinstance(obj, dict):
        encoded_dict = {}
        for key, value in obj.items():
            if (
                (
                    not sqlalchemy_safe
                    or (not isinstance(key, str))
                    or (not key.startswith(""_sa""))
                )
                and (value is not None or include_none)
                and ((include and key in include) or key not in exclude)
            ):
                encoded_key = jsonable_encoder(
                    key,
                    by_alias=by_alias,
                    exclude_unset=exclude_unset,
                    include_none=include_none,
                    custom_encoder=custom_encoder,
                    sqlalchemy_safe=sqlalchemy_safe,
                )
                encoded_value = jsonable_encoder(
                    value,
                    by_alias=by_alias,
                    exclude_unset=exclude_unset,
                    include_none=include_none,
                    custom_encoder=custom_encoder,
                    sqlalchemy_safe=sqlalchemy_safe,
                )
                encoded_dict[encoded_key] = encoded_value
        return encoded_dict
    if isinstance(obj, (list, set, frozenset, GeneratorType, tuple)):
        encoded_list = []
        for item in obj:
            encoded_list.append(
                jsonable_encoder(
                    item,
                    include=include,
                    exclude=exclude,
                    by_alias=by_alias,
                    exclude_unset=exclude_unset,
                    include_none=include_none,
                    custom_encoder=custom_encoder,
                    sqlalchemy_safe=sqlalchemy_safe,
                )
            )
        return encoded_list
    if custom_encoder:
        if type(obj) in custom_encoder:
            return custom_encoder[type(obj)](obj)
        else:
            for encoder_type, encoder in custom_encoder.items():
                if isinstance(obj, encoder_type):
                    return encoder(obj)
    if type(obj) in ENCODERS_BY_TYPE:
        return ENCODERS_BY_TYPE[type(obj)](obj)
    for encoder, classes_tuple in encoders_by_class_tuples.items():
        if isinstance(obj, classes_tuple):
            return encoder(obj)
    errors: List[Exception] = []
    try:
        data = dict(obj)
    except Exception as e:
        errors.append(e)
        try:
            data = vars(obj)
        except Exception as e:
            errors.append(e)
            raise ValueError(errors)
    return jsonable_encoder(
        data,
        by_alias=by_alias,
        exclude_unset=exclude_unset,
        include_none=include_none,
        custom_encoder=custom_encoder,
        sqlalchemy_safe=sqlalchemy_safe,","[4, 38, 57, 64, 72, 88, 123]"
"        Returns
        -------
        bound : Period or object
        Notes
        -----
        Value of `side` parameter should be validated in caller.
        """"""
        assert kind in [""ix"", ""loc"", ""getitem""]
        if isinstance(label, datetime):
            return Period(label, freq=self.freq)
        elif isinstance(label, str):
            try:
                _, parsed, reso = parse_time_string(label, self.freq)
                bounds = self._parsed_string_to_bounds(reso, parsed)
                return bounds[0 if side == ""left"" else 1]
            except ValueError:
                # string cannot be parsed as datetime-like
                # TODO: we need tests for this case
                raise KeyError(label)
        elif is_integer(label) or is_float(label):
            self._invalid_indexer(""slice"", label)
        return label
    def _parsed_string_to_bounds(self, reso, parsed):
        if reso == ""year"":
            t1 = Period(year=parsed.year, freq=""A"")
        elif reso == ""month"":
            t1 = Period(year=parsed.year, month=parsed.month, freq=""M"")
        elif reso == ""quarter"":
            q = (parsed.month - 1) // 3 + 1
            t1 = Period(year=parsed.year, quarter=q, freq=""Q-DEC"")
        elif reso == ""day"":
            t1 = Period(year=parsed.year, month=parsed.month, day=parsed.day, freq=""D"")
        elif reso == ""hour"":
            t1 = Period(
                year=parsed.year,
                month=parsed.month,
                day=parsed.day,
                hour=parsed.hour,
                freq=""H"",
            )
        elif reso == ""minute"":
            t1 = Period(
                year=parsed.year,
                month=parsed.month,
                day=parsed.day,
                hour=parsed.hour,
                minute=parsed.minute,
                freq=""T"",
            )
        elif reso == ""second"":
            t1 = Period(
                year=parsed.year,
                month=parsed.month,
                day=parsed.day,
                hour=parsed.hour,
                minute=parsed.minute,
                second=parsed.second,
                freq=""S"",
            )
        else:
            raise KeyError(reso)
        return (t1.asfreq(self.freq, how=""start""), t1.asfreq(self.freq, how=""end""))
    def _get_string_slice(self, key):
        if not self.is_monotonic:
            raise ValueError(""Partial indexing only valid for ordered time series"")
        key, parsed, reso = parse_time_string(key, self.freq)
        grp = resolution.Resolution.get_freq_group(reso)
        freqn = resolution.get_freq_group(self.freq)
        if reso in [""day"", ""hour"", ""minute"", ""second""] and not grp < freqn:
            raise KeyError(key)
        t1, t2 = self._parsed_string_to_bounds(reso, parsed)
        return slice(
            self.searchsorted(t1.ordinal, side=""left""),
            self.searchsorted(t2.ordinal, side=""right""),
        )
    def _convert_tolerance(self, tolerance, target):
        tolerance = DatetimeIndexOpsMixin._convert_tolerance(self, tolerance, target)
        if target.size != tolerance.size and tolerance.size > 1:
            raise ValueError(""list-like tolerance size must match target index size"")
        return self._maybe_convert_timedelta(tolerance)
    def insert(self, loc, item):
        if not isinstance(item, Period) or self.freq != item.freq:
            return self.astype(object).insert(loc, item)
        idx = np.concatenate(
            (self[:loc].asi8, np.array([item.ordinal]), self[loc:].asi8)
        )
        return self._shallow_copy(idx)
    def join(self, other, how=""left"", level=None, return_indexers=False, sort=False):
        """"""
        See Index.join
        """"""
        self._assert_can_do_setop(other)
        if not isinstance(other, PeriodIndex):
            return self.astype(object).join(
                other, how=how, level=level, return_indexers=return_indexers, sort=sort
            )
        result = Int64Index.join(
            self,
            other,
            how=how,
            level=level,
            return_indexers=return_indexers,
            sort=sort,
        )
        if return_indexers:
            result, lidx, ridx = result
            return self._apply_meta(result), lidx, ridx
        return self._apply_meta(result)
    # ------------------------------------------------------------------------
    # Set Operation Methods","[81, 82]"
"""""""Built-in metrics.
""""""
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function
import six
from . import backend as K
from .losses import mean_squared_error
from .losses import mean_absolute_error
from .losses import mean_absolute_percentage_error
from .losses import mean_squared_logarithmic_error
from .losses import hinge
from .losses import logcosh
from .losses import squared_hinge
from .losses import categorical_crossentropy
from .losses import sparse_categorical_crossentropy
from .losses import binary_crossentropy
from .losses import kullback_leibler_divergence
from .losses import poisson
from .losses import cosine_proximity
from .utils.generic_utils import deserialize_keras_object
from .utils.generic_utils import serialize_keras_object

def binary_accuracy(y_true, y_pred):
    return K.mean(K.equal(y_true, K.round(y_pred)), axis=-1)

def categorical_accuracy(y_true, y_pred):
    return K.cast(K.equal(K.argmax(y_true, axis=-1),
                          K.argmax(y_pred, axis=-1)),
                  K.floatx())

def sparse_categorical_accuracy(y_true, y_pred):
    return K.cast(K.equal(K.max(y_true, axis=-1),
                          K.cast(K.argmax(y_pred, axis=-1), K.floatx())),
                  K.floatx())

def top_k_categorical_accuracy(y_true, y_pred, k=5):
    return K.mean(K.in_top_k(y_pred, K.argmax(y_true, axis=-1), k), axis=-1)

def sparse_top_k_categorical_accuracy(y_true, y_pred, k=5):
    return K.mean(K.in_top_k(y_pred, K.cast(K.max(y_true, axis=-1), 'int32'), k), axis=-1)

# Aliases
mse = MSE = mean_squared_error
mae = MAE = mean_absolute_error
mape = MAPE = mean_absolute_percentage_error
msle = MSLE = mean_squared_logarithmic_error
cosine = cosine_proximity

def serialize(metric):
    return serialize_keras_object(metric)

def deserialize(config, custom_objects=None):
    return deserialize_keras_object(config,
                                    module_objects=globals(),
                                    custom_objects=custom_objects,
                                    printable_module_name='metric function')

def get(identifier):
    if isinstance(identifier, dict):
        config = {'class_name': str(identifier), 'config': {}}
        return deserialize(config)
    elif isinstance(identifier, six.string_types):
        return deserialize(str(identifier))
    elif callable(identifier):
        return identifier
    else:
        raise ValueError('Could not interpret '
                         'metric function identifier:', identifier)",[36]
"            new_node_index = node_conversion_map[node_key]
            tensor_index = self._input_coordinates[i][2]
            model_inputs.append([layer.name, new_node_index, tensor_index])
        config['input_layers'] = model_inputs
        model_outputs = []
        for i in range(len(self._output_layers)):
            layer = self._output_layers[i]
            node_index = self._output_coordinates[i][1]
            node_key = self._node_key(layer, node_index)
            if node_key not in self._network_nodes:
                continue
            new_node_index = node_conversion_map[node_key]
            tensor_index = self._output_coordinates[i][2]
            model_outputs.append([layer.name, new_node_index, tensor_index])
        config['output_layers'] = model_outputs
        return copy.deepcopy(config)
    @classmethod
    def from_config(cls, config, custom_objects=None):
        """"""Instantiates a Model from its config (output of `get_config()`).
        # Arguments
            config: Model config dictionary.
            custom_objects: Optional dictionary mapping names
                (strings) to custom classes or functions to be
                considered during deserialization.
        # Returns
            A model instance.
        # Raises
            ValueError: In case of improperly formatted config dict.
        """"""
        # Layer instances created during
        # the graph reconstruction process
        created_layers = {}
        # Dictionary mapping layer instances to
        # node data that specifies a layer call.
        # It acts as a queue that maintains any unprocessed
        # layer call until it becomes possible to process it
        # (i.e. until the input tensors to the call all exist).
        unprocessed_nodes = {}
        def add_unprocessed_node(layer, node_data):
            if layer not in unprocessed_nodes:
                unprocessed_nodes[layer] = [node_data]
            else:
                unprocessed_nodes[layer].append(node_data)
        def process_node(layer, node_data):
            input_tensors = []
            for input_data in node_data:
                inbound_layer_name = input_data[0]
                inbound_node_index = input_data[1]
                inbound_tensor_index = input_data[2]
                if len(input_data) == 3:
                    kwargs = {}
                elif len(input_data) == 4:
                    kwargs = input_data[3]
                else:
                    raise ValueError('Improperly formatted model config.')
                inbound_layer = created_layers[inbound_layer_name]
                if len(inbound_layer._inbound_nodes) <= inbound_node_index:
                    add_unprocessed_node(layer, node_data)
                    return
                inbound_node = inbound_layer._inbound_nodes[inbound_node_index]
                input_tensors.append(
                    inbound_node.output_tensors[inbound_tensor_index])
            # Call layer on its inputs, thus creating the node
            # and building the layer if needed.
            if input_tensors:
                layer(unpack_singleton(input_tensors), **kwargs)
        def process_layer(layer_data):
            """"""Deserializes a layer, then call it on appropriate inputs.
            # Arguments
                layer_data: layer config dict.
            # Raises
                ValueError: In case of improperly formatted `layer_data` dict.
            """"""
            layer_name = layer_data['name']
            # Instantiate layer.
            from ..layers import deserialize as deserialize_layer
            layer = deserialize_layer(layer_data,
                                      custom_objects=custom_objects)
            created_layers[layer_name] = layer
            # Gather layer inputs.
            inbound_nodes_data = layer_data['inbound_nodes']
            for node_data in inbound_nodes_data:
                # We don't process nodes (i.e. make layer calls)
                # on the fly because the inbound node may not yet exist,
                # in case of layer shared at different topological depths
                # (e.g. a model such as A(B(A(B(x)))))
                add_unprocessed_node(layer, node_data)
        # First, we create all layers and enqueue nodes to be processed
        for layer_data in config['layers']:
            process_layer(layer_data)
        # Then we process nodes in order of layer depth.
        # Nodes that cannot yet be processed (if the inbound node
        # does not yet exist) are re-enqueued, and the process
        # is repeated until all nodes are processed.
        while unprocessed_nodes:
            for layer_data in config['layers']:
                layer = created_layers[layer_data['name']]
                if layer in unprocessed_nodes:
                    for node_data in unprocessed_nodes.pop(layer):
                        process_node(layer, node_data)
        name = config.get('name')
        input_tensors = []
        output_tensors = []
        for layer_data in config['input_layers']:
            layer_name, node_index, tensor_index = layer_data
            assert layer_name in created_layers
            layer = created_layers[layer_name]
            layer_output_tensors = layer._inbound_nodes[node_index].output_tensors
            input_tensors.append(layer_output_tensors[tensor_index])
        for layer_data in config['output_layers']:
            layer_name, node_index, tensor_index = layer_data","[65, 66, 113, 114]"
"        stddev: Standard deviation of the values.
        dtype: String, dtype of returned tensor.
        seed: Integer, random seed.
    # Returns
        A tensor.
    """"""
    if dtype is None:
        dtype = floatx()
    if seed is None:
        seed = np.random.randint(10e6)
    return tf.truncated_normal(shape, mean, stddev, dtype=dtype, seed=seed)

# CTC
# TensorFlow has a native implementation, but it uses sparse tensors
# and therefore requires a wrapper for Keras. The functions below convert
# dense to sparse tensors and also wraps up the beam search code that is
# in TensorFlow's CTC implementation

def ctc_label_dense_to_sparse(labels, label_lengths):
    """"""Converts CTC labels from dense to sparse.
    # Arguments
        labels: dense CTC labels.
        label_lengths: length of the labels.
    # Returns
        A sparse tensor representation of the labels.
    """"""
    label_shape = tf.shape(labels)
    num_batches_tns = tf.stack([label_shape[0]])
    max_num_labels_tns = tf.stack([label_shape[1]])
    def range_less_than(_, current_input):
        return tf.expand_dims(tf.range(label_shape[1]), 0) < tf.fill(
            max_num_labels_tns, current_input)
    init = tf.cast(tf.fill([1, label_shape[1]], 0), tf.bool)
    dense_mask = functional_ops.scan(range_less_than, label_lengths,
                                     initializer=init, parallel_iterations=1)
    dense_mask = dense_mask[:, 0, :]
    label_array = tf.reshape(tf.tile(tf.range(label_shape[1]), num_batches_tns),
                             label_shape)
    label_ind = tf.boolean_mask(label_array, dense_mask)
    batch_array = tf.transpose(tf.reshape(tf.tile(tf.range(label_shape[0]),
                                                  max_num_labels_tns), reverse(label_shape, 0)))
    batch_ind = tf.boolean_mask(batch_array, dense_mask)
    indices = tf.transpose(tf.reshape(concatenate([batch_ind, label_ind], axis=0), [2, -1]))
    vals_sparse = tf.gather_nd(labels, indices)
    return tf.SparseTensor(tf.to_int64(indices), vals_sparse, tf.to_int64(label_shape))

def ctc_batch_cost(y_true, y_pred, input_length, label_length):
    """"""Runs CTC loss algorithm on each batch element.
    # Arguments
        y_true: tensor `(samples, max_string_length)`
            containing the truth labels.
        y_pred: tensor `(samples, time_steps, num_categories)`
            containing the prediction, or output of the softmax.
        input_length: tensor `(samples, 1)` containing the sequence length for
            each batch item in `y_pred`.
        label_length: tensor `(samples, 1)` containing the sequence length for
            each batch item in `y_true`.
    # Returns
        Tensor with shape (samples,1) containing the
            CTC loss of each element.
    """"""
    label_length = tf.to_int32(tf.squeeze(label_length))
    input_length = tf.to_int32(tf.squeeze(input_length))
    sparse_labels = tf.to_int32(ctc_label_dense_to_sparse(y_true, label_length))
    y_pred = tf.log(tf.transpose(y_pred, perm=[1, 0, 2]) + epsilon())
    return tf.expand_dims(ctc.ctc_loss(inputs=y_pred,
                                       labels=sparse_labels,
                                       sequence_length=input_length), 1)

def ctc_decode(y_pred, input_length, greedy=True, beam_width=100,
               top_paths=1):
    """"""Decodes the output of a softmax.
    Can use either greedy search (also known as best path)
    or a constrained dictionary search.
    # Arguments
        y_pred: tensor `(samples, time_steps, num_categories)`
            containing the prediction, or output of the softmax.
        input_length: tensor `(samples, )` containing the sequence length for
            each batch item in `y_pred`.
        greedy: perform much faster best-path search if `true`.
            This does not use a dictionary.
        beam_width: if `greedy` is `false`: a beam search decoder will be used
            with a beam of this width.
        top_paths: if `greedy` is `false`,
            how many of the most probable paths will be returned.
    # Returns
        Tuple:
            List: if `greedy` is `true`, returns a list of one element that
                contains the decoded sequence.
                If `false`, returns the `top_paths` most probable
                decoded sequences.
                Important: blank labels are returned as `-1`.
            Tensor `(top_paths, )` that contains
                the log probability of each decoded sequence.
    """"""
    y_pred = tf.log(tf.transpose(y_pred, perm=[1, 0, 2]) + epsilon())
    input_length = tf.to_int32(input_length)
    if greedy:
        (decoded, log_prob) = ctc.ctc_greedy_decoder(
            inputs=y_pred,
            sequence_length=input_length)
    else:
        (decoded, log_prob) = ctc.ctc_beam_search_decoder(
            inputs=y_pred,
            sequence_length=input_length, beam_width=beam_width,
            top_paths=top_paths)","[75, 76]"
"    from pandas import Index
    return Index(tz_results, name=name)

def _convert_listlike_datetimes(
    arg,
    format,
    name=None,
    tz=None,
    unit=None,
    errors=None,
    infer_datetime_format=None,
    dayfirst=None,
    yearfirst=None,
    exact=None,
):
    """"""
    Helper function for to_datetime. Performs the conversions of 1D listlike
    of dates
    Parameters
    ----------
    arg : list, tuple, ndarray, Series, Index
        date to be parced
    name : object
        None or string for the Index name
    tz : object
        None or 'utc'
    unit : string
        None or string of the frequency of the passed data
    errors : string
        error handing behaviors from to_datetime, 'raise', 'coerce', 'ignore'
    infer_datetime_format : boolean
        inferring format behavior from to_datetime
    dayfirst : boolean
        dayfirst parsing behavior from to_datetime
    yearfirst : boolean
        yearfirst parsing behavior from to_datetime
    exact : boolean
        exact format matching behavior from to_datetime
    Returns
    -------
    Index-like of parsed dates
    """"""
    from pandas import DatetimeIndex
    from pandas.core.arrays import DatetimeArray
    from pandas.core.arrays.datetimes import (
        maybe_convert_dtype,
        objects_to_datetime64ns,
    )
    if isinstance(arg, (list, tuple)):
        arg = np.array(arg, dtype=""O"")
    # these are shortcutable
    if is_datetime64tz_dtype(arg):
        if not isinstance(arg, (DatetimeArray, DatetimeIndex)):
            return DatetimeIndex(arg, tz=tz, name=name)
        if tz == ""utc"":
            arg = arg.tz_convert(None).tz_localize(tz)
        return arg
    elif is_datetime64_ns_dtype(arg):
        if not isinstance(arg, (DatetimeArray, DatetimeIndex)):
            try:
                return DatetimeIndex(arg, tz=tz, name=name)
            except ValueError:
                pass
        elif tz:
            # DatetimeArray, DatetimeIndex
            return arg.tz_localize(tz)
        return arg
    elif unit is not None:
        if format is not None:
            raise ValueError(""cannot specify both format and unit"")
        arg = getattr(arg, ""values"", arg)
        result, tz_parsed = tslib.array_with_unit_to_datetime(arg, unit, errors=errors)
        if errors == ""ignore"":
            from pandas import Index
            result = Index(result, name=name)
        else:
            result = DatetimeIndex(result, name=name)
        # GH 23758: We may still need to localize the result with tz
        # GH 25546: Apply tz_parsed first (from arg), then tz (from caller)
        # result will be naive but in UTC
        try:
            result = result.tz_localize(""UTC"").tz_convert(tz_parsed)
        except AttributeError:
            # Regular Index from 'ignore' path
            return result
        if tz is not None:
            if result.tz is None:
                result = result.tz_localize(tz)
            else:
                result = result.tz_convert(tz)
        return result
    elif getattr(arg, ""ndim"", 1) > 1:
        raise TypeError(
            ""arg must be a string, datetime, list, tuple, 1-d array, or Series""
        )
    # warn if passing timedelta64, raise for PeriodDtype
    # NB: this must come after unit transformation
    orig_arg = arg
    arg, _ = maybe_convert_dtype(arg, copy=False)
    arg = ensure_object(arg)
    require_iso8601 = False
    if infer_datetime_format and format is None:
        format = _guess_datetime_format_for_array(arg, dayfirst=dayfirst)
    if format is not None:
        # There is a special fast-path for iso8601 formatted
        # datetime strings, so in those cases don't use the inferred
        # format because this path makes process slower in this
        # special case
        format_is_iso8601 = _format_is_iso(format)
        if format_is_iso8601:
            require_iso8601 = not infer_datetime_format
            format = None
","[79, 80]"
"                        STANDALONE_COMMENT,
                        hidden_value,
                        prefix=prefix[:previous_consumed] + ""\n"" * comment.newlines,
                    ),
                )
                return True
            previous_consumed = comment.consumed
    return False

def generate_ignored_nodes(leaf: Leaf) -> Iterator[LN]:
    """"""Starting from the container of `leaf`, generate all leaves until `# fmt: on`.
    Stops at the end of the block.
    """"""
    container: Optional[LN] = container_of(leaf)
    while container is not None and container.type != token.ENDMARKER:
        is_fmt_on = False
        for comment in list_comments(container.prefix, is_endmarker=False):
            if comment.value in FMT_ON:
                is_fmt_on = True
            elif comment.value in FMT_OFF:
                is_fmt_on = False
        if is_fmt_on:
            return
        yield container
        container = container.next_sibling

def maybe_make_parens_invisible_in_atom(node: LN, parent: LN) -> bool:
    """"""If it's safe, make the parens in the atom `node` invisible, recursively.
    Additionally, remove repeated, adjacent invisible parens from the atom `node`
    as they are redundant.
    Returns whether the node should itself be wrapped in invisible parentheses.
    """"""
    if (
        node.type != syms.atom
        or is_empty_tuple(node)
        or is_one_tuple(node)
        or (is_yield(node) and parent.type != syms.expr_stmt)
        or max_delimiter_priority_in_atom(node) >= COMMA_PRIORITY
    ):
        return False
    first = node.children[0]
    last = node.children[-1]
    if first.type == token.LPAR and last.type == token.RPAR:
        middle = node.children[1]
        # make parentheses invisible
        first.value = """"  # type: ignore
        last.value = """"  # type: ignore
        maybe_make_parens_invisible_in_atom(middle, parent=parent)
        if is_atom_with_invisible_parens(middle):
            # Strip the invisible parens from `middle` by replacing
            # it with the child in-between the invisible parens
            middle.replace(middle.children[1])
        return False
    return True

def is_atom_with_invisible_parens(node: LN) -> bool:
    """"""Given a `LN`, determines whether it's an atom `node` with invisible
    parens. Useful in dedupe-ing and normalizing parens.
    """"""
    if isinstance(node, Leaf) or node.type != syms.atom:
        return False
    first, last = node.children[0], node.children[-1]
    return (
        isinstance(first, Leaf)
        and first.type == token.LPAR
        and first.value == """"
        and isinstance(last, Leaf)
        and last.type == token.RPAR
        and last.value == """"
    )

def is_empty_tuple(node: LN) -> bool:
    """"""Return True if `node` holds an empty tuple.""""""
    return (
        node.type == syms.atom
        and len(node.children) == 2
        and node.children[0].type == token.LPAR
        and node.children[1].type == token.RPAR
    )

def unwrap_singleton_parenthesis(node: LN) -> Optional[LN]:
    """"""Returns `wrapped` if `node` is of the shape ( wrapped ).
    Parenthesis can be optional. Returns None otherwise""""""
    if len(node.children) != 3:
        return None
    lpar, wrapped, rpar = node.children
    if not (lpar.type == token.LPAR and rpar.type == token.RPAR):
        return None
    return wrapped

def wrap_in_parentheses(parent: Node, child: LN, *, visible: bool = True) -> None:
    """"""Wrap `child` in parentheses.
    This replaces `child` with an atom holding the parentheses and the old
    child.  That requires moving the prefix.
    If `visible` is False, the leaves will be valueless (and thus invisible).
    """"""
    lpar = Leaf(token.LPAR, ""("" if visible else """")
    rpar = Leaf(token.RPAR, "")"" if visible else """")
    prefix = child.prefix
    child.prefix = """"
    index = child.remove() or 0
    new_child = Node(syms.atom, [lpar, child, rpar])
    new_child.prefix = prefix
    parent.insert_child(index, new_child)","[19, 20, 21, 22, 23, 24, 25, 28, 30]"
"                                        segment_number += 1
                                    segment_time += segment_d
                        elif 'segment_urls' in representation_ms_info and 's' in representation_ms_info:
                            # No media template
                            # Example: https://www.youtube.com/watch?v=iXZV5uAYMJI
                            # or any YouTube dashsegments video
                            fragments = []
                            segment_index = 0
                            timescale = representation_ms_info['timescale']
                            for s in representation_ms_info['s']:
                                duration = float_or_none(s['d'], timescale)
                                for r in range(s.get('r', 0) + 1):
                                    segment_uri = representation_ms_info['segment_urls'][segment_index]
                                    fragments.append({
                                        location_key(segment_uri): segment_uri,
                                        'duration': duration,
                                    })
                                    segment_index += 1
                            representation_ms_info['fragments'] = fragments
                        elif 'segment_urls' in representation_ms_info:
                            # Segment URLs with no SegmentTimeline
                            # Example: https://www.seznam.cz/zpravy/clanek/cesko-zasahne-vitr-o-sile-vichrice-muze-byt-i-zivotu-nebezpecny-39091
                            # https://github.com/rg3/youtube-dl/pull/14844
                            fragments = []
                            segment_duration = float_or_none(
                                representation_ms_info['segment_duration'],
                                representation_ms_info['timescale']) if 'segment_duration' in representation_ms_info else None
                            for segment_url in representation_ms_info['segment_urls']:
                                fragment = {
                                    location_key(segment_url): segment_url,
                                }
                                if segment_duration:
                                    fragment['duration'] = segment_duration
                                fragments.append(fragment)
                            representation_ms_info['fragments'] = fragments
                        # NB: MPD manifest may contain direct URLs to unfragmented media.
                        # No fragments key is present in this case.
                        if 'fragments' in representation_ms_info:
                            f.update({
                                'fragment_base_url': base_url,
                                'fragments': [],
                                'protocol': 'http_dash_segments',
                            })
                            if 'initialization_url' in representation_ms_info:
                                initialization_url = representation_ms_info['initialization_url']
                                if not f.get('url'):
                                    f['url'] = initialization_url
                                f['fragments'].append({location_key(initialization_url): initialization_url})
                            f['fragments'].extend(representation_ms_info['fragments'])
                        try:
                            existing_format = next(
                                fo for fo in formats
                                if fo['format_id'] == representation_id)
                        except StopIteration:
                            full_info = formats_dict.get(representation_id, {}).copy()
                            full_info.update(f)
                            formats.append(full_info)
                        else:
                            existing_format.update(f)
                    else:
                        self.report_warning('Unknown MIME type %s in DASH manifest' % mime_type)
        return formats
    def _extract_ism_formats(self, ism_url, video_id, ism_id=None, note=None, errnote=None, fatal=True):
        res = self._download_webpage_handle(
            ism_url, video_id,
            note=note or 'Downloading ISM manifest',
            errnote=errnote or 'Failed to download ISM manifest',
            fatal=fatal)
        if res is False:
            return []
        ism, urlh = res
        return self._parse_ism_formats(
            compat_etree_fromstring(ism.encode('utf-8')), urlh.geturl(), ism_id)
    def _parse_ism_formats(self, ism_doc, ism_url, ism_id=None):
        """"""
        Parse formats from ISM manifest.
        References:
         1. [MS-SSTR]: Smooth Streaming Protocol,
            https://msdn.microsoft.com/en-us/library/ff469518.aspx
        """"""
        if ism_doc.get('IsLive') == 'TRUE' or ism_doc.find('Protection') is not None:
            return []
        duration = int(ism_doc.attrib['Duration'])
        timescale = int_or_none(ism_doc.get('TimeScale')) or 10000000
        formats = []
        for stream in ism_doc.findall('StreamIndex'):
            stream_type = stream.get('Type')
            if stream_type not in ('video', 'audio'):
                continue
            url_pattern = stream.attrib['Url']
            stream_timescale = int_or_none(stream.get('TimeScale')) or timescale
            stream_name = stream.get('Name')
            for track in stream.findall('QualityLevel'):
                fourcc = track.get('FourCC')
                # TODO: add support for WVC1 and WMAP
                if fourcc not in ('H264', 'AVC1', 'AACL'):
                    self.report_warning('%s is not a supported codec' % fourcc)
                    continue
                tbr = int(track.attrib['Bitrate']) // 1000
                # [1] does not mention Width and Height attributes. However,
                # they're often present while MaxWidth and MaxHeight are
                # missing, so should be used as fallbacks
                width = int_or_none(track.get('MaxWidth') or track.get('Width'))
                height = int_or_none(track.get('MaxHeight') or track.get('Height'))
                sampling_rate = int_or_none(track.get('SamplingRate'))
                track_url_pattern = re.sub(r'{[Bb]itrate}', track.attrib['Bitrate'], url_pattern)
                track_url_pattern = compat_urlparse.urljoin(ism_url, track_url_pattern)
                fragments = []
                fragment_ctx = {
                    'time': 0,
                }
                stream_fragments = stream.findall('c')
                for stream_fragment_index, stream_fragment in enumerate(stream_fragments):
                    fragment_ctx['time'] = int_or_none(stream_fragment.get('t')) or fragment_ctx['time']
                    fragment_repeat = int_or_none(stream_fragment.get('r')) or 1
                    fragment_ctx['duration'] = int_or_none(stream_fragment.get('d'))
                    if not fragment_ctx['duration']:
                        try:
                            next_fragment_time = int(stream_fragment[stream_fragment_index + 1].attrib['t'])
                        except IndexError:","[49, 50, 51, 52, 53, 54, 55, 56, 57, 58]"
"        :type name: string
        :param default: the value to return if no setting is found
        :type default: any
        """"""
        value = self.get(name, default or {})
        if isinstance(value, six.string_types):
            value = json.loads(value)
        return dict(value)
    def _getcomposite(self, name):
        # DO NOT USE THIS FUNCTION IN YOUR CUSTOM PROJECTS
        # It's for internal use in the transition away from the _BASE settings
        # and will be removed along with _BASE support in a future release
        basename = name + ""_BASE""
        if basename in self:
            warnings.warn('_BASE settings are deprecated.',
                          category=ScrapyDeprecationWarning)
            compsett = BaseSettings(self[name + ""_BASE""], priority='default')
            compsett.update(self[name])
            return compsett
        else:
            return self[name]
    def getpriority(self, name):
        """"""
        Return the current numerical priority value of a setting, or ``None`` if
        the given ``name`` does not exist.
        :param name: the setting name
        :type name: string
        """"""
        prio = None
        if name in self:
            prio = self.attributes[name].priority
        return prio
    def maxpriority(self):
        """"""
        Return the numerical value of the highest priority present throughout
        all settings, or the numerical value for ``default`` from
        :attr:`~scrapy.settings.SETTINGS_PRIORITIES` if there are no settings
        stored.
        """"""
        if len(self) > 0:
            return max(self.getpriority(name) for name in self)
        else:
            return get_settings_priority('default')
    def __setitem__(self, name, value):
        self.set(name, value)
    def set(self, name, value, priority='project'):
        """"""
        Store a key/value attribute with a given priority.
        Settings should be populated *before* configuring the Crawler object
        (through the :meth:`~scrapy.crawler.Crawler.configure` method),
        otherwise they won't have any effect.
        :param name: the setting name
        :type name: string
        :param value: the value to associate with the setting
        :type value: any
        :param priority: the priority of the setting. Should be a key of
            :attr:`~scrapy.settings.SETTINGS_PRIORITIES` or an integer
        :type priority: string or int
        """"""
        self._assert_mutability()
        priority = get_settings_priority(priority)
        if name not in self:
            if isinstance(value, SettingsAttribute):
                self.attributes[name] = value
            else:
                self.attributes[name] = SettingsAttribute(value, priority)
        else:
            self.attributes[name].set(value, priority)
    def setdict(self, values, priority='project'):
        self.update(values, priority)
    def setmodule(self, module, priority='project'):
        """"""
        Store settings from a module with a given priority.
        This is a helper function that calls
        :meth:`~scrapy.settings.BaseSettings.set` for every globally declared
        uppercase variable of ``module`` with the provided ``priority``.
        :param module: the module or the path of the module
        :type module: module object or string
        :param priority: the priority of the settings. Should be a key of
            :attr:`~scrapy.settings.SETTINGS_PRIORITIES` or an integer
        :type priority: string or int
        """"""
        self._assert_mutability()
        if isinstance(module, six.string_types):
            module = import_module(module)
        for key in dir(module):
            if key.isupper():
                self.set(key, getattr(module, key), priority)
    def update(self, values, priority='project'):
        """"""
        Store key/value pairs with a given priority.
        This is a helper function that calls
        :meth:`~scrapy.settings.BaseSettings.set` for every item of ``values``
        with the provided ``priority``.
        If ``values`` is a string, it is assumed to be JSON-encoded and parsed
        into a dict with ``json.loads()`` first. If it is a
        :class:`~scrapy.settings.BaseSettings` instance, the per-key priorities
        will be used and the ``priority`` parameter ignored. This allows
        inserting/updating settings with different priorities with a single
        command.
        :param values: the settings names and values
        :type values: dict or string or :class:`~scrapy.settings.BaseSettings`
        :param priority: the priority of the settings. Should be a key of
            :attr:`~scrapy.settings.SETTINGS_PRIORITIES` or an integer
        :type priority: string or int
        """"""","[18, 19, 21, 22]"
"from __future__ import print_function
import sys
import optparse
import cProfile
import inspect
import pkg_resources
import scrapy
from scrapy.crawler import CrawlerProcess
from scrapy.xlib import lsprofcalltree
from scrapy.commands import ScrapyCommand
from scrapy.exceptions import UsageError
from scrapy.utils.misc import walk_modules
from scrapy.utils.project import inside_project, get_project_settings
from scrapy.settings.deprecated import check_deprecated_settings
def _iter_command_classes(module_name):
    # TODO: add `name` attribute to commands and and merge this function with
    # scrapy.utils.spider.iter_spider_classes
    for module in walk_modules(module_name):
        for obj in vars(module).itervalues():
            if inspect.isclass(obj) and \
               issubclass(obj, ScrapyCommand) and \
               obj.__module__ == module.__name__:
                yield obj
def _get_commands_from_module(module, inproject):
    d = {}
    for cmd in _iter_command_classes(module):
        if inproject or not cmd.requires_project:
            cmdname = cmd.__module__.split('.')[-1]
            d[cmdname] = cmd()
    return d
def _get_commands_from_entry_points(inproject, group='scrapy.commands'):
    cmds = {}
    for entry_point in pkg_resources.iter_entry_points(group):
        obj = entry_point.load()
        if inspect.isclass(obj):
            cmds[entry_point.name] = obj()
        else:
            raise Exception(""Invalid entry point %s"" % entry_point.name)
    return cmds
def _get_commands_dict(settings, inproject):
    cmds = _get_commands_from_module('scrapy.commands', inproject)
    cmds.update(_get_commands_from_entry_points(inproject))
    cmds_module = settings['COMMANDS_MODULE']
    if cmds_module:
        cmds.update(_get_commands_from_module(cmds_module, inproject))
    return cmds
def _pop_command_name(argv):
    i = 0
    for arg in argv[1:]:
        if not arg.startswith('-'):
            del argv[i]
            return arg
        i += 1
def _print_header(settings, inproject):
    if inproject:
        print(""Scrapy %s - project: %s\n"" % (scrapy.__version__, \
            settings['BOT_NAME']))
    else:
        print(""Scrapy %s - no active project\n"" % scrapy.__version__)
def _print_commands(settings, inproject):
    _print_header(settings, inproject)
    print(""Usage:"")
    print(""  scrapy <command> [options] [args]\n"")
    print(""Available commands:"")
    cmds = _get_commands_dict(settings, inproject)
    for cmdname, cmdclass in sorted(cmds.items()):
        print(""  %-13s %s"" % (cmdname, cmdclass.short_desc()))
    if not inproject:
        print()
        print(""  [ more ]      More commands available when run from project directory"")
    print()
    print('Use ""scrapy <command> -h"" to see more info about a command')
def _print_unknown_command(settings, cmdname, inproject):
    _print_header(settings, inproject)
    print(""Unknown command: %s\n"" % cmdname)
    print('Use ""scrapy"" to see available commands')
def _run_print_help(parser, func, *a, **kw):
    try:
        func(*a, **kw)
    except UsageError as e:
        if str(e):
            parser.error(str(e))
        if e.print_help:
            parser.print_help()
        sys.exit(2)
def execute(argv=None, settings=None):
    if argv is None:
        argv = sys.argv
    # --- backwards compatibility for scrapy.conf.settings singleton ---
    if settings is None and 'scrapy.conf' in sys.modules:
        from scrapy import conf
        if hasattr(conf, 'settings'):
            settings = conf.settings
    # ------------------------------------------------------------------
    if settings is None:
        settings = get_project_settings()
    check_deprecated_settings(settings)
    # --- backwards compatibility for scrapy.conf.settings singleton ---
    import warnings
    from scrapy.exceptions import ScrapyDeprecationWarning
    with warnings.catch_warnings():
        warnings.simplefilter(""ignore"", ScrapyDeprecationWarning)
        from scrapy import conf
        conf.settings = settings
    # ------------------------------------------------------------------
    inproject = inside_project()
    cmds = _get_commands_dict(settings, inproject)
    cmdname = _pop_command_name(argv)
    parser = optparse.OptionParser(formatter=optparse.TitledHelpFormatter(), \
        conflict_handler='resolve')
    if not cmdname:
        _print_commands(settings, inproject)","[20, 22, 23]"
"    def axis_date(self, tz=None):
        """"""
        Sets up axis ticks and labels treating data along this axis as dates.
        Parameters
        ----------
        tz : tzinfo or str or None
            The timezone used to create date labels.
        """"""
        # By providing a sample datetime instance with the desired timezone,
        # the registered converter can be selected, and the ""units"" attribute,
        # which is the timezone, can be set.
        if isinstance(tz, str):
            import dateutil.tz
            tz = dateutil.tz.gettz(tz)
        self.update_units(datetime.datetime(2009, 1, 1, 0, 0, 0, 0, tz))
    def get_tick_space(self):
        """"""Return the estimated number of ticks that can fit on the axis.""""""
        # Must be overridden in the subclass
        raise NotImplementedError()
    def _get_ticks_position(self):
        """"""
        Helper for `XAxis.get_ticks_position` and `YAxis.get_ticks_position`.
        Check the visibility of tick1line, label1, tick2line, and label2 on
        the first major and the first minor ticks, and return
        - 1 if only tick1line and label1 are visible (which corresponds to
          ""bottom"" for the x-axis and ""left"" for the y-axis);
        - 2 if only tick2line and label2 are visible (which corresponds to
          ""top"" for the x-axis and ""right"" for the y-axis);
        - ""default"" if only tick1line, tick2line and label1 are visible;
        - ""unknown"" otherwise.
        """"""
        major = self.majorTicks[0]
        minor = self.minorTicks[0]
        if all(tick.tick1line.get_visible()
               and not tick.tick2line.get_visible()
               and tick.label1.get_visible()
               and not tick.label2.get_visible()
               for tick in [major, minor]):
            return 1
        elif all(tick.tick2line.get_visible()
                 and not tick.tick1line.get_visible()
                 and tick.label2.get_visible()
                 and not tick.label1.get_visible()
                 for tick in [major, minor]):
            return 2
        elif all(tick.tick1line.get_visible()
                 and tick.tick2line.get_visible()
                 and tick.label1.get_visible()
                 and not tick.label2.get_visible()
                 for tick in [major, minor]):
            return ""default""
        else:
            return ""unknown""
    def get_label_position(self):
        """"""
        Return the label position (top or bottom)
        """"""
        return self.label_position
    def set_label_position(self, position):
        """"""
        Set the label position (top or bottom)
        Parameters
        ----------
        position : {'top', 'bottom'}
        """"""
        raise NotImplementedError()
    def get_minpos(self):
        raise NotImplementedError()

def _make_getset_interval(method_name, lim_name, attr_name):
    """"""
    Helper to generate ``get_{data,view}_interval`` and
    ``set_{data,view}_interval`` implementations.
    """"""
    def getter(self):
        # docstring inherited.
        return getattr(getattr(self.axes, lim_name), attr_name)
    def setter(self, vmin, vmax, ignore=False):
        # docstring inherited.
        if ignore:
            setattr(getattr(self.axes, lim_name), attr_name, (vmin, vmax))
        else:
            oldmin, oldmax = getter(self)
            if oldmin < oldmax:
                setter(self, min(vmin, vmax, oldmin), max(vmin, vmax, oldmax),
                       ignore=True)
            else:
                setter(self, max(vmin, vmax, oldmax), min(vmin, vmax, oldmin),
                       ignore=True)
        self.stale = True
    getter.__name__ = f""get_{method_name}_interval""
    setter.__name__ = f""set_{method_name}_interval""
    return getter, setter

class XAxis(Axis):
    __name__ = 'xaxis'
    axis_name = 'x'
    def contains(self, mouseevent):
        """"""Test whether the mouse event occurred in the x axis.
        """"""
        if self._contains is not None:
            return self._contains(self, mouseevent)
        x, y = mouseevent.x, mouseevent.y
        try:
            trans = self.axes.transAxes.inverted()
            xaxes, yaxes = trans.transform_point((x, y))
        except ValueError:
            return False, {}
        l, b = self.axes.transAxes.transform_point((0, 0))
        r, t = self.axes.transAxes.transform_point((1, 1))",[99]
"        Cast values to specified type
        Parameters
        ----------
        values : ndarray
        cast_type : string or np.dtype
           dtype to cast values to
        column : string
            column name - used only for error reporting
        Returns
        -------
        converted : ndarray
        """"""
        if is_categorical_dtype(cast_type):
            known_cats = (
                isinstance(cast_type, CategoricalDtype)
                and cast_type.categories is not None
            )
            if not is_object_dtype(values) and not known_cats:
                # XXX this is for consistency with
                # c-parser which parses all categories
                # as strings
                values = astype_nansafe(values, str)
            cats = Index(values).unique().dropna()
            values = Categorical._from_inferred_categories(
                cats, cats.get_indexer(values), cast_type, true_values=self.true_values
            )
        # use the EA's implementation of casting
        elif is_extension_array_dtype(cast_type):
            # ensure cast_type is an actual dtype and not a string
            cast_type = pandas_dtype(cast_type)
            array_type = cast_type.construct_array_type()
            try:
                return array_type._from_sequence_of_strings(values, dtype=cast_type)
            except NotImplementedError:
                raise NotImplementedError(
                    f""Extension Array: {array_type} must implement ""
                    ""_from_sequence_of_strings in order to be used in parser methods""
                )
        else:
            try:
                values = astype_nansafe(values, cast_type, copy=True, skipna=True)
            except ValueError:
                raise ValueError(
                    f""Unable to convert column {column} to type {cast_type}""
                )
        return values
    def _do_date_conversions(self, names, data):
        # returns data, columns
        if self.parse_dates is not None:
            data, names = _process_date_conversion(
                data,
                self._date_conv,
                self.parse_dates,
                self.index_col,
                self.index_names,
                names,
                keep_date_col=self.keep_date_col,
            )
        return names, data

class CParserWrapper(ParserBase):
    """"""
    """"""
    def __init__(self, src, **kwds):
        self.kwds = kwds
        kwds = kwds.copy()
        ParserBase.__init__(self, kwds)
        encoding = kwds.get(""encoding"")
        if kwds.get(""compression"") is None and encoding:
            if isinstance(src, str):
                src = open(src, ""rb"")
                self.handles.append(src)
            # Handle the file object with universal line mode enabled.
            # We will handle the newline character ourselves later on.
            if isinstance(src, BufferedIOBase):
                src = TextIOWrapper(src, encoding=encoding, newline="""")
            kwds[""encoding""] = ""utf-8""
        # #2442
        kwds[""allow_leading_cols""] = self.index_col is not False
        # GH20529, validate usecol arg before TextReader
        self.usecols, self.usecols_dtype = _validate_usecols_arg(kwds[""usecols""])
        kwds[""usecols""] = self.usecols
        self._reader = parsers.TextReader(src, **kwds)
        self.unnamed_cols = self._reader.unnamed_cols
        passed_names = self.names is None
        if self._reader.header is None:
            self.names = None
        else:
            if len(self._reader.header) > 1:
                # we have a multi index in the columns
                (
                    self.names,
                    self.index_names,
                    self.col_names,
                    passed_names,
                ) = self._extract_multi_indexer_columns(
                    self._reader.header, self.index_names, self.col_names, passed_names
                )
            else:
                self.names = list(self._reader.header[0])
        if self.names is None:
            if self.prefix:
                self.names = [",[91]
"        ids, _, ngroups = self.grouper.group_info
        sorter = get_group_index_sorter(ids, ngroups)
        ids, count = ids[sorter], len(ids)
        if count == 0:
            return np.empty(0, dtype=np.int64)
        run = np.r_[True, ids[:-1] != ids[1:]]
        rep = np.diff(np.r_[np.nonzero(run)[0], count])
        out = (~run).cumsum()
        if ascending:
            out -= np.repeat(out[run], rep)
        else:
            out = np.repeat(out[np.r_[run[1:], True]], rep) - out
        rev = np.empty(count, dtype=np.intp)
        rev[sorter] = np.arange(count, dtype=np.intp)
        return out[rev].astype(np.int64, copy=False)
    def _try_cast(self, result, obj, numeric_only: bool = False):
        """"""
        Try to cast the result to our obj original type,
        we may have roundtripped through object in the mean-time.
        If numeric_only is True, then only try to cast numerics
        and not datetimelikes.
        """"""
        if obj.ndim > 1:
            dtype = obj._values.dtype
        else:
            dtype = obj.dtype
        if not is_scalar(result):
            if is_extension_array_dtype(dtype) and dtype.kind != ""M"":
                # The function can return something of any type, so check
                #  if the type is compatible with the calling EA.
                # datetime64tz is handled correctly in agg_series,
                #  so is excluded here.
                # return the same type (Series) as our caller
                cls = dtype.construct_array_type()
                result = try_cast_to_ea(cls, result, dtype=dtype)
            elif numeric_only and is_numeric_dtype(dtype) or not numeric_only:
                result = maybe_downcast_to_dtype(result, dtype)
        return result
    def _transform_should_cast(self, func_nm: str) -> bool:
        """"""
        Parameters
        ----------
        func_nm: str
            The name of the aggregation function being performed
        Returns
        -------
        bool
            Whether transform should attempt to cast the result of aggregation
        """"""
        return (self.size().fillna(0) > 0).any() and (
            func_nm not in base.cython_cast_blacklist
        )
    def _cython_transform(self, how: str, numeric_only: bool = True, **kwargs):
        output: Dict[base.OutputKey, np.ndarray] = {}
        for idx, obj in enumerate(self._iterate_slices()):
            name = obj.name
            is_numeric = is_numeric_dtype(obj.dtype)
            if numeric_only and not is_numeric:
                continue
            try:
                result, _ = self.grouper.transform(obj.values, how, **kwargs)
            except NotImplementedError:
                continue
            if self._transform_should_cast(how):
                result = self._try_cast(result, obj)
            key = base.OutputKey(label=name, position=idx)
            output[key] = result
        if len(output) == 0:
            raise DataError(""No numeric types to aggregate"")
        return self._wrap_transformed_output(output)
    def _wrap_aggregated_output(self, output: Mapping[base.OutputKey, np.ndarray]):
        raise AbstractMethodError(self)
    def _wrap_transformed_output(self, output: Mapping[base.OutputKey, np.ndarray]):
        raise AbstractMethodError(self)
    def _wrap_applied_output(self, keys, values, not_indexed_same: bool = False):
        raise AbstractMethodError(self)
    def _cython_agg_general(
        self, how: str, alt=None, numeric_only: bool = True, min_count: int = -1
    ):
        output: Dict[base.OutputKey, Union[np.ndarray, DatetimeArray]] = {}
        # Ideally we would be able to enumerate self._iterate_slices and use
        # the index from enumeration as the key of output, but ohlc in particular
        # returns a (n x 4) array. Output requires 1D ndarrays as values, so we
        # need to slice that up into 1D arrays
        idx = 0
        for obj in self._iterate_slices():
            name = obj.name
            is_numeric = is_numeric_dtype(obj.dtype)
            if numeric_only and not is_numeric:
                continue
            result, agg_names = self.grouper.aggregate(
                obj._values, how, min_count=min_count
            )
            if agg_names:
                # e.g. ohlc
                assert len(agg_names) == result.shape[1]
                for result_column, result_name in zip(result.T, agg_names):
                    key = base.OutputKey(label=result_name, position=idx)
                    output[key] = self._try_cast(result_column, obj)
                    idx += 1
            else:
                assert result.ndim == 1
                key = base.OutputKey(label=name, position=idx)","[42, 43]"
"from fastapi.exceptions import RequestValidationError
from starlette.exceptions import HTTPException
from starlette.requests import Request
from starlette.responses import JSONResponse
from starlette.status import HTTP_422_UNPROCESSABLE_ENTITY

async def http_exception_handler(request: Request, exc: HTTPException) -> JSONResponse:
    headers = getattr(exc, ""headers"", None)
    if headers:
        return JSONResponse(
            {""detail"": exc.detail}, status_code=exc.status_code, headers=headers
        )
    else:
        return JSONResponse({""detail"": exc.detail}, status_code=exc.status_code)

async def request_validation_exception_handler(
    request: Request, exc: RequestValidationError
) -> JSONResponse:
    return JSONResponse(
        status_code=HTTP_422_UNPROCESSABLE_ENTITY, content={""detail"": exc.errors()}
    )",[21]
"import six
import json
import copy
import warnings
from collections import MutableMapping
from importlib import import_module
from scrapy.utils.deprecate import create_deprecated_class
from scrapy.exceptions import ScrapyDeprecationWarning
from . import default_settings

SETTINGS_PRIORITIES = {
    'default': 0,
    'command': 10,
    'project': 20,
    'spider': 30,
    'cmdline': 40,
}

def get_settings_priority(priority):
    """"""
    Small helper function that looks up a given string priority in the
    :attr:`~scrapy.settings.SETTINGS_PRIORITIES` dictionary and returns its
    numerical value, or directly returns a given numerical priority.
    """"""
    if isinstance(priority, six.string_types):
        return SETTINGS_PRIORITIES[priority]
    else:
        return priority

class SettingsAttribute(object):
    """"""Class for storing data related to settings attributes.
    This class is intended for internal usage, you should try Settings class
    for settings configuration, not this one.
    """"""
    def __init__(self, value, priority):
        self.value = value
        if isinstance(self.value, BaseSettings):
            self.priority = max(self.value.maxpriority(), priority)
        else:
            self.priority = priority
    def set(self, value, priority):
        """"""Sets value if priority is higher or equal than current priority.""""""
        if isinstance(self.value, BaseSettings):
            # Ignore self.priority if self.value has per-key priorities
            self.value.update(value, priority)
            self.priority = max(self.value.maxpriority(), priority)
        else:
            if priority >= self.priority:
                self.value = value
                self.priority = priority
    def __str__(self):
        return ""<SettingsAttribute value={self.value!r} "" \
               ""priority={self.priority}>"".format(self=self)
    __repr__ = __str__

class BaseSettings(MutableMapping):
    """"""
    Instances of this class behave like dictionaries, but store priorities
    along with their ``(key, value)`` pairs, and can be frozen (i.e. marked
    immutable).
    Key-value entries can be passed on initialization with the ``values``
    argument, and they would take the ``priority`` level (unless ``values`` is
    already an instance of :class:`~scrapy.settings.BaseSettings`, in which
    case the existing priority levels will be kept).  If the ``priority``
    argument is a string, the priority name will be looked up in
    :attr:`~scrapy.settings.SETTINGS_PRIORITIES`. Otherwise, a specific integer
    should be provided.
    Once the object is created, new settings can be loaded or updated with the
    :meth:`~scrapy.settings.BaseSettings.set` method, and can be accessed with
    the square bracket notation of dictionaries, or with the
    :meth:`~scrapy.settings.BaseSettings.get` method of the instance and its
    value conversion variants. When requesting a stored key, the value with the
    highest priority will be retrieved.
    """"""
    def __init__(self, values=None, priority='project'):
        self.frozen = False
        self.attributes = {}
        self.update(values, priority)
    def __getitem__(self, opt_name):
        value = None
        if opt_name in self:
            value = self.attributes[opt_name].value
        return value
    def __contains__(self, name):
        return name in self.attributes
    def get(self, name, default=None):
        """"""
        Get a setting value without affecting its original type.
        :param name: the setting name
        :type name: string
        :param default: the value to return if no setting is found
        :type default: any
        """"""
        return self[name] if self[name] is not None else default
    def getbool(self, name, default=False):
        """"""
        Get a setting value as a boolean.
        
        ``1``, ``'1'``, and ``True`` return ``True``, while ``0``, ``'0'``,
        ``False`` and ``None`` return ``False``. 
        For example, settings populated through environment variables set to
        ``'0'`` will return ``False`` when using this method.
        :param name: the setting name
        :type name: string","[118, 120]"
"            dirs[:] = [d for d in dirs if not any(r.match(d) for r in skeleton_ignore_re)]
            for f in files:
                filename, ext = os.path.splitext(f)
                if any(r.match(os.path.join(rel_root, f)) for r in skeleton_ignore_re):
                    continue
                elif galaxy_type == 'collection' and own_skeleton and rel_root == '.' and f == 'galaxy.yml.j2':
                    # Special use case for galaxy.yml.j2 in our own default collection skeleton. We build the options
                    # dynamically which requires special options to be set.
                    # The templated data's keys must match the key name but the inject data contains collection_name
                    # instead of name. We just make a copy and change the key back to name for this file.
                    template_data = inject_data.copy()
                    template_data['name'] = template_data.pop('collection_name')
                    meta_value = GalaxyCLI._get_skeleton_galaxy_yml(os.path.join(root, rel_root, f), template_data)
                    b_dest_file = to_bytes(os.path.join(obj_path, rel_root, filename), errors='surrogate_or_strict')
                    with open(b_dest_file, 'wb') as galaxy_obj:
                        galaxy_obj.write(to_bytes(meta_value, errors='surrogate_or_strict'))
                elif ext == "".j2"" and not in_templates_dir:
                    src_template = os.path.join(rel_root, f)
                    dest_file = os.path.join(obj_path, rel_root, filename)
                    template_env.get_template(src_template).stream(inject_data).dump(dest_file, encoding='utf-8')
                else:
                    f_rel_path = os.path.relpath(os.path.join(root, f), obj_skeleton)
                    shutil.copyfile(os.path.join(root, f), os.path.join(obj_path, f_rel_path))
            for d in dirs:
                b_dir_path = to_bytes(os.path.join(obj_path, rel_root, d), errors='surrogate_or_strict')
                if not os.path.exists(b_dir_path):
                    os.makedirs(b_dir_path)
        display.display(""- %s %s was created successfully"" % (galaxy_type.title(), obj_name))
    def execute_info(self):
        """"""
        prints out detailed information about an installed role as well as info available from the galaxy API.
        """"""
        roles_path = context.CLIARGS['roles_path']
        data = ''
        for role in context.CLIARGS['args']:
            role_info = {'path': roles_path}
            gr = GalaxyRole(self.galaxy, self.api, role)
            install_info = gr.install_info
            if install_info:
                if 'version' in install_info:
                    install_info['installed_version'] = install_info['version']
                    del install_info['version']
                role_info.update(install_info)
            remote_data = False
            if not context.CLIARGS['offline']:
                remote_data = self.api.lookup_role_by_name(role, False)
            if remote_data:
                role_info.update(remote_data)
            if gr.metadata:
                role_info.update(gr.metadata)
            req = RoleRequirement()
            role_spec = req.role_yaml_parse({'role': role})
            if role_spec:
                role_info.update(role_spec)
            data = self._display_role_info(role_info)
            # FIXME: This is broken in both 1.9 and 2.0 as
            # _display_role_info() always returns something
            if not data:
                data = u""\n- the role %s was not found"" % role
        self.pager(data)
    def execute_install(self):
        """"""
        Install one or more roles(``ansible-galaxy role install``), or one or more collections(``ansible-galaxy collection install``).
        You can pass in a list (roles or collections) or use the file
        option listed below (these are mutually exclusive). If you pass in a list, it
        can be a name (which will be downloaded via the galaxy API and github), or it can be a local tar archive file.
        """"""
        if context.CLIARGS['type'] == 'collection':
            collections = context.CLIARGS['args']
            force = context.CLIARGS['force']
            output_path = context.CLIARGS['collections_path']
            ignore_certs = context.CLIARGS['ignore_certs']
            ignore_errors = context.CLIARGS['ignore_errors']
            requirements_file = context.CLIARGS['requirements']
            no_deps = context.CLIARGS['no_deps']
            force_deps = context.CLIARGS['force_with_deps']
            if collections and requirements_file:
                raise AnsibleError(""The positional collection_name arg and --requirements-file are mutually exclusive."")
            elif not collections and not requirements_file:
                raise AnsibleError(""You must specify a collection name or a requirements file."")
            if requirements_file:
                requirements_file = GalaxyCLI._resolve_path(requirements_file)
                requirements = self._parse_requirements_file(requirements_file, allow_old_format=False)['collections']
            else:
                requirements = []
                for collection_input in collections:
                    name, dummy, requirement = collection_input.partition(':')
                    requirements.append((name, requirement or '*', None))
            output_path = GalaxyCLI._resolve_path(output_path)
            collections_path = C.COLLECTIONS_PATHS
            if len([p for p in collections_path if p.startswith(output_path)]) == 0:
                display.warning(""The specified collections path '%s' is not part of the configured Ansible ""
                                ""collections paths '%s'. The installed collection won't be picked up in an Ansible ""
                                ""run."" % (to_text(output_path), to_text("":"".join(collections_path))))
            if os.path.split(output_path)[1] != 'ansible_collections':
                output_path = os.path.join(output_path, 'ansible_collections')
            b_output_path = to_bytes(output_path, errors='surrogate_or_strict')
            if not os.path.exists(b_output_path):
                os.makedirs(b_output_path)
            install_collections(requirements, output_path, self.api_servers, (not ignore_certs), ignore_errors,
                                no_deps, force, force_deps)
",[106]
"                if not (
                    is_integer_dtype(inference)
                    and interpolation in {""linear"", ""midpoint""}
                ):
                    vals = vals.astype(inference)
            return vals
        if is_scalar(q):
            return self._get_cythonized_result(
                ""group_quantile"",
                aggregate=True,
                needs_values=True,
                needs_mask=True,
                cython_dtype=np.dtype(np.float64),
                pre_processing=pre_processor,
                post_processing=post_processor,
                q=q,
                interpolation=interpolation,
            )
        else:
            results = [
                self._get_cythonized_result(
                    ""group_quantile"",
                    aggregate=True,
                    needs_values=True,
                    needs_mask=True,
                    cython_dtype=np.dtype(np.float64),
                    pre_processing=pre_processor,
                    post_processing=post_processor,
                    q=qi,
                    interpolation=interpolation,
                )
                for qi in q
            ]
            result = concat(results, axis=0, keys=q)
            # fix levels to place quantiles on the inside
            # TODO(GH-10710): Ideally, we could write this as
            #  >>> result.stack(0).loc[pd.IndexSlice[:, ..., q], :]
            #  but this hits https://github.com/pandas-dev/pandas/issues/10710
            #  which doesn't reorder the list-like `q` on the inner level.
            order = np.roll(list(range(result.index.nlevels)), -1)
            result = result.reorder_levels(order)
            result = result.reindex(q, level=-1)
            # fix order.
            hi = len(q) * self.ngroups
            arr = np.arange(0, hi, self.ngroups)
            arrays = []
            for i in range(self.ngroups):
                arr2 = arr + i
                arrays.append(arr2)
            indices = np.concatenate(arrays)
            assert len(indices) == len(result)
            return result.take(indices)
    @Substitution(name=""groupby"")
    def ngroup(self, ascending: bool = True):
        """"""
        Number each group from 0 to the number of groups - 1.
        This is the enumerative complement of cumcount.  Note that the
        numbers given to the groups match the order in which the groups
        would be seen when iterating over the groupby object, not the
        order they are first observed.
        Parameters
        ----------
        ascending : bool, default True
            If False, number in reverse, from number of group - 1 to 0.
        Returns
        -------
        Series
            Unique numbers for each group.
        See Also
        --------
        .cumcount : Number the rows in each group.
        Examples
        --------
        >>> df = pd.DataFrame({""A"": list(""aaabba"")})
        >>> df
           A
        0  a
        1  a
        2  a
        3  b
        4  b
        5  a
        >>> df.groupby('A').ngroup()
        0    0
        1    0
        2    0
        3    1
        4    1
        5    0
        dtype: int64
        >>> df.groupby('A').ngroup(ascending=False)
        0    1
        1    1
        2    1
        3    0
        4    0
        5    1
        dtype: int64
        >>> df.groupby([""A"", [1,1,2,3,2,1]]).ngroup()
        0    0
        1    0
        2    1
        3    3
        4    2
        5    0
        dtype: int64
        """"""
        with _group_selection_context(self):
            index = self._selected_obj.index
            result = Series(self.grouper.group_info[0], index)
            if not ascending:
                result = self.ngroups - 1 - result
            return result
","[41, 42, 43, 46, 47, 48, 50, 51, 52, 54, 55]"
"from datetime import date, datetime, timedelta
import functools
import inspect
import re
from typing import Any, List
import warnings
import numpy as np
from pandas._libs import NaT, Timestamp, lib, tslib
import pandas._libs.internals as libinternals
from pandas._libs.tslibs import Timedelta, conversion
from pandas._libs.tslibs.timezones import tz_compare
from pandas.util._validators import validate_bool_kwarg
from pandas.core.dtypes.cast import (
    astype_nansafe,
    find_common_type,
    infer_dtype_from,
    infer_dtype_from_scalar,
    maybe_downcast_numeric,
    maybe_downcast_to_dtype,
    maybe_infer_dtype_type,
    maybe_promote,
    maybe_upcast,
    soft_convert_objects,
)
from pandas.core.dtypes.common import (
    _NS_DTYPE,
    _TD_DTYPE,
    ensure_platform_int,
    is_bool_dtype,
    is_categorical,
    is_categorical_dtype,
    is_datetime64_dtype,
    is_datetime64tz_dtype,
    is_dtype_equal,
    is_extension_array_dtype,
    is_extension_type,
    is_float_dtype,
    is_integer,
    is_integer_dtype,
    is_interval_dtype,
    is_list_like,
    is_object_dtype,
    is_period_dtype,
    is_re,
    is_re_compilable,
    is_sparse,
    is_timedelta64_dtype,
    pandas_dtype,
)
from pandas.core.dtypes.concat import concat_categorical, concat_datetime
from pandas.core.dtypes.dtypes import CategoricalDtype, ExtensionDtype
from pandas.core.dtypes.generic import (
    ABCDataFrame,
    ABCDatetimeIndex,
    ABCExtensionArray,
    ABCPandasArray,
    ABCSeries,
)
from pandas.core.dtypes.missing import (
    _isna_compat,
    array_equivalent,
    is_valid_nat_for_dtype,
    isna,
    notna,
)
import pandas.core.algorithms as algos
from pandas.core.arrays import Categorical, DatetimeArray, PandasDtype, TimedeltaArray
from pandas.core.base import PandasObject
import pandas.core.common as com
from pandas.core.construction import extract_array
from pandas.core.indexers import (
    check_setitem_lengths,
    is_empty_indexer,
    is_scalar_indexer,
)
import pandas.core.missing as missing
from pandas.core.nanops import nanpercentile
from pandas.io.formats.printing import pprint_thing

class Block(PandasObject):
    """"""
    Canonical n-dimensional unit of homogeneous dtype contained in a pandas
    data structure
    Index-ignorant; let the container take care of that
    """"""
    __slots__ = [""_mgr_locs"", ""values"", ""ndim""]
    is_numeric = False
    is_float = False
    is_integer = False
    is_complex = False
    is_datetime = False
    is_datetimetz = False
    is_timedelta = False
    is_bool = False
    is_object = False
    is_categorical = False
    is_extension = False
    _can_hold_na = False
    _can_consolidate = True
    _verify_integrity = True
    _validate_ndim = True
    _ftype = ""dense""
    _concatenator = staticmethod(np.concatenate)
    def __init__(self, values, placement, ndim=None):
        self.ndim = self._check_ndim(values, ndim)
        self.mgr_locs = placement
        self.values = values
        if self._validate_ndim and self.ndim and len(self.mgr_locs) != len(self.values):
            raise ValueError(
                ""Wrong number of items passed {val}, placement implies ""
                ""{mgr}"".format(val=len(self.values), mgr=len(self.mgr_locs))
            )
    def _check_ndim(self, values, ndim):
        """"""
        ndim inference and validation.
",[9]
"            **kwargs: dictionary arguments
                Legal arguments are the arguments
                of `Sequential.predict_classes`.
        # Returns
            preds: array-like, shape `(n_samples,)`
                Class predictions.
        """"""
        kwargs = self.filter_sk_params(Sequential.predict_classes, kwargs)
        proba = self.model.predict(x, **kwargs)
        if proba.shape[-1] > 1:
            classes = proba.argmax(axis=-1)
        else:
            classes = (proba > 0.5).astype('int32')
        return self.classes_[classes]
    def predict_proba(self, x, **kwargs):
        """"""Returns class probability estimates for the given test data.
        # Arguments
            x: array-like, shape `(n_samples, n_features)`
                Test samples where `n_samples` is the number of samples
                and `n_features` is the number of features.
            **kwargs: dictionary arguments
                Legal arguments are the arguments
                of `Sequential.predict_classes`.
        # Returns
            proba: array-like, shape `(n_samples, n_outputs)`
                Class probability estimates.
                In the case of binary classification,
                to match the scikit-learn API,
                will return an array of shape `(n_samples, 2)`
                (instead of `(n_sample, 1)` as in Keras).
        """"""
        kwargs = self.filter_sk_params(Sequential.predict_proba, kwargs)
        probs = self.model.predict(x, **kwargs)
        # check if binary classification
        if probs.shape[1] == 1:
            # first column is probability of class 0 and second is of class 1
            probs = np.hstack([1 - probs, probs])
        return probs
    def score(self, x, y, **kwargs):
        """"""Returns the mean accuracy on the given test data and labels.
        # Arguments
            x: array-like, shape `(n_samples, n_features)`
                Test samples where `n_samples` is the number of samples
                and `n_features` is the number of features.
            y: array-like, shape `(n_samples,)` or `(n_samples, n_outputs)`
                True labels for `x`.
            **kwargs: dictionary arguments
                Legal arguments are the arguments of `Sequential.evaluate`.
        # Returns
            score: float
                Mean accuracy of predictions on `x` wrt. `y`.
        # Raises
            ValueError: If the underlying model isn't configured to
                compute accuracy. You should pass `metrics=[""accuracy""]` to
                the `.compile()` method of the model.
        """"""
        y = np.searchsorted(self.classes_, y)
        kwargs = self.filter_sk_params(Sequential.evaluate, kwargs)
        loss_name = self.model.loss
        if hasattr(loss_name, '__name__'):
            loss_name = loss_name.__name__
        if loss_name == 'categorical_crossentropy' and len(y.shape) != 2:
            y = to_categorical(y)
        outputs = self.model.evaluate(x, y, **kwargs)
        outputs = to_list(outputs)
        for name, output in zip(self.model.metrics_names, outputs):
            if name == 'acc':
                return output
        raise ValueError('The model is not configured to compute accuracy. '
                         'You should pass `metrics=[""accuracy""]` to '
                         'the `model.compile()` method.')

class KerasRegressor(BaseWrapper):
    """"""Implementation of the scikit-learn regressor API for Keras.
    """"""
    def predict(self, x, **kwargs):
        """"""Returns predictions for the given test data.
        # Arguments
            x: array-like, shape `(n_samples, n_features)`
                Test samples where `n_samples` is the number of samples
                and `n_features` is the number of features.
            **kwargs: dictionary arguments
                Legal arguments are the arguments of `Sequential.predict`.
        # Returns
            preds: array-like, shape `(n_samples,)`
                Predictions.
        """"""
        kwargs = self.filter_sk_params(Sequential.predict, kwargs)
        return np.squeeze(self.model.predict(x, **kwargs))
    def score(self, x, y, **kwargs):
        """"""Returns the mean loss on the given test data and labels.
        # Arguments
            x: array-like, shape `(n_samples, n_features)`
                Test samples where `n_samples` is the number of samples
                and `n_features` is the number of features.
            y: array-like, shape `(n_samples,)`
                True labels for `x`.
            **kwargs: dictionary arguments
                Legal arguments are the arguments of `Sequential.evaluate`.
        # Returns
            score: float
                Mean accuracy of predictions on `x` wrt. `y`.
        """"""
        kwargs = self.filter_sk_params(Sequential.evaluate, kwargs)
        loss = self.model.evaluate(x, y, **kwargs)
        if isinstance(loss, list):
            return -loss[0]",[104]
"        y = np.array(y)
    if is_integer_dtype(y.dtype):
        if (y == 0).any():
            # GH#7325, mask and nans must be broadcastable (also: GH#9308)
            # Raveling and then reshaping makes np.putmask faster
            mask = ((y == 0) & ~np.isnan(result)).ravel()
            shape = result.shape
            result = result.astype(""float64"", copy=False).ravel()
            np.putmask(result, mask, np.nan)
            result = result.reshape(shape)
    return result

def mask_zero_div_zero(x, y, result):
    """"""
    Set results of 0 / 0 or 0 // 0 to np.nan, regardless of the dtypes
    of the numerator or the denominator.
    Parameters
    ----------
    x : ndarray
    y : ndarray
    result : ndarray
    Returns
    -------
    filled_result : ndarray
    Examples
    --------
    >>> x = np.array([1, 0, -1], dtype=np.int64)
    >>> y = 0       # int 0; numpy behavior is different with float
    >>> result = x / y
    >>> result      # raw numpy result does not fill division by zero
    array([0, 0, 0])
    >>> mask_zero_div_zero(x, y, result)
    array([ inf,  nan, -inf])
    """"""
    if not isinstance(result, np.ndarray):
        # FIXME: SparseArray would raise TypeError with np.putmask
        return result
    if is_scalar(y):
        y = np.array(y)
    zmask = y == 0
    if isinstance(zmask, bool):
        # FIXME: numpy did not evaluate pointwise, seen in docs build
        return result
    if zmask.any():
        shape = result.shape
        # Flip sign if necessary for -0.0
        zneg_mask = zmask & np.signbit(y)
        zpos_mask = zmask & ~zneg_mask
        nan_mask = (zmask & (x == 0)).ravel()
        with np.errstate(invalid=""ignore""):
            neginf_mask = ((zpos_mask & (x < 0)) | (zneg_mask & (x > 0))).ravel()
            posinf_mask = ((zpos_mask & (x > 0)) | (zneg_mask & (x < 0))).ravel()
        if nan_mask.any() or neginf_mask.any() or posinf_mask.any():
            # Fill negative/0 with -inf, positive/0 with +inf, 0/0 with NaN
            result = result.astype(""float64"", copy=False).ravel()
            np.putmask(result, nan_mask, np.nan)
            np.putmask(result, posinf_mask, np.inf)
            np.putmask(result, neginf_mask, -np.inf)
            result = result.reshape(shape)
    return result

def dispatch_fill_zeros(op, left, right, result):
    """"""
    Call fill_zeros with the appropriate fill value depending on the operation,
    with special logic for divmod and rdivmod.
    Parameters
    ----------
    op : function (operator.add, operator.div, ...)
    left : object (np.ndarray for non-reversed ops)
    right : object (np.ndarray for reversed ops)
    result : ndarray
    Returns
    -------
    result : np.ndarray
    Notes
    -----
    For divmod and rdivmod, the `result` parameter and returned `result`
    is a 2-tuple of ndarray objects.
    """"""
    if op is divmod:
        result = (
            mask_zero_div_zero(left, right, result[0]),
            fill_zeros(result[1], left, right),
        )
    elif op is rdivmod:
        result = (
            mask_zero_div_zero(right, left, result[0]),
            fill_zeros(result[1], right, left),
        )
    elif op is operator.floordiv:
        # Note: no need to do this for truediv; in py3 numpy behaves the way
        #  we want.
        result = mask_zero_div_zero(left, right, result)
    elif op is rfloordiv:
        # Note: no need to do this for rtruediv; in py3 numpy behaves the way
        #  we want.
        result = mask_zero_div_zero(right, left, result)
    elif op is operator.mod:
        result = fill_zeros(result, left, right)
    elif op is rmod:
        result = fill_zeros(result, right, left)","[59, 65, 67, 68, 72, 73, 74, 75, 76, 78]"
"    Parameters
    ----------
    df : DataFrame or Styler
    na_rep: na representation
    float_format : string, default None
            Format string for floating point numbers
    cols : sequence, optional
        Columns to write
    header : boolean or list of string, default True
        Write out column names. If a list of string is given it is
        assumed to be aliases for the column names
    index : boolean, default True
        output row names (index)
    index_label : string or sequence, default None
            Column label for index column(s) if desired. If None is given, and
            `header` and `index` are True, then the index names are used. A
            sequence should be given if the DataFrame uses MultiIndex.
    merge_cells : boolean, default False
            Format MultiIndex and Hierarchical Rows as merged cells.
    inf_rep : string, default `'inf'`
        representation for np.inf values (which aren't representable in Excel)
        A `'-'` sign will be added in front of -inf.
    style_converter : callable, optional
        This translates Styler styles (CSS) into ExcelWriter styles.
        Defaults to ``CSSToExcelConverter()``.
        It should have signature css_declarations string -> excel style.
        This is only called for body cells.
    """"""
    max_rows = 2 ** 20
    max_cols = 2 ** 14
    def __init__(
        self,
        df,
        na_rep: str = """",
        float_format: Optional[str] = None,
        cols: Optional[Sequence[Label]] = None,
        header: Union[Sequence[Label], bool] = True,
        index: bool = True,
        index_label: Optional[Union[Label, Sequence[Label]]] = None,
        merge_cells: bool = False,
        inf_rep: str = ""inf"",
        style_converter: Optional[Callable] = None,
    ):
        self.rowcounter = 0
        self.na_rep = na_rep
        if hasattr(df, ""render""):
            self.styler = df
            df = df.data
            if style_converter is None:
                style_converter = CSSToExcelConverter()
            self.style_converter = style_converter
        else:
            self.styler = None
        self.df = df
        if cols is not None:
            # all missing, raise
            if not len(Index(cols) & df.columns):
                raise KeyError(""passes columns are not ALL present dataframe"")
            if len(Index(cols) & df.columns) != len(cols):
                # Deprecated in GH#17295, enforced in 1.0.0
                raise KeyError(""Not all names specified in 'columns' are found"")
            self.df = df
        self.columns = self.df.columns
        self.float_format = float_format
        self.index = index
        self.index_label = index_label
        self.header = header
        self.merge_cells = merge_cells
        self.inf_rep = inf_rep
    @property
    def header_style(self):
        return {
            ""font"": {""bold"": True},
            ""borders"": {
                ""top"": ""thin"",
                ""right"": ""thin"",
                ""bottom"": ""thin"",
                ""left"": ""thin"",
            },
            ""alignment"": {""horizontal"": ""center"", ""vertical"": ""top""},
        }
    def _format_value(self, val):
        if is_scalar(val) and missing.isna(val):
            val = self.na_rep
        elif is_float(val):
            if missing.isposinf_scalar(val):
                val = self.inf_rep
            elif missing.isneginf_scalar(val):
                val = f""-{self.inf_rep}""
            elif self.float_format is not None:
                val = float(self.float_format % val)
        if getattr(val, ""tzinfo"", None) is not None:
            raise ValueError(
                ""Excel does not support datetimes with ""
                ""timezones. Please ensure that datetimes ""
                ""are timezone unaware before writing to Excel.""
            )
        return val
    def _format_header_mi(self):
        if self.columns.nlevels > 1:
            if not self.index:
                raise NotImplementedError(
                    ""Writing to Excel with MultiIndex columns and no ""
                    ""index ('index'=False) is not yet implemented.""
                )
        has_aliases = isinstance(self.header, (tuple, list, np.ndarray, ABCIndex))
        if not (has_aliases or self.header):
            return
        columns = self.columns
        level_strs = columns.format(
            sparsify=self.merge_cells, adjoin=False, names=False
        )
        level_lengths = get_level_lengths(level_strs)
        coloffset = 0
        lnum = 0
",[66]
"            freq = attribs[""freq""]
            if freq is not None and not is_period_dtype(self):
                if freq.n > 0 and not ascending:
                    freq = freq * -1
                elif freq.n < 0 and ascending:
                    freq = freq * -1
            attribs[""freq""] = freq
            if not ascending:
                sorted_values = sorted_values[::-1]
            return self._simple_new(sorted_values, **attribs)
    @Appender(_index_shared_docs[""take""] % _index_doc_kwargs)
    def take(self, indices, axis=0, allow_fill=True, fill_value=None, **kwargs):
        nv.validate_take(tuple(), kwargs)
        indices = ensure_int64(indices)
        maybe_slice = lib.maybe_indices_to_slice(indices, len(self))
        if isinstance(maybe_slice, slice):
            return self[maybe_slice]
        taken = ExtensionIndex.take(
            self, indices, axis, allow_fill, fill_value, **kwargs
        )
        # keep freq in PeriodArray/Index, reset otherwise
        freq = self.freq if is_period_dtype(self) else None
        assert taken.freq == freq, (taken.freq, freq, taken)
        return self._shallow_copy(taken, freq=freq)
    _can_hold_na = True
    _na_value = NaT
    """"""The expected NA value to use with this index.""""""
    def _convert_tolerance(self, tolerance, target):
        tolerance = np.asarray(to_timedelta(tolerance).to_numpy())
        if target.size != tolerance.size and tolerance.size > 1:
            raise ValueError(""list-like tolerance size must match target index size"")
        return tolerance
    def tolist(self) -> List:
        """"""
        Return a list of the underlying data.
        """"""
        return list(self.astype(object))
    def min(self, axis=None, skipna=True, *args, **kwargs):
        """"""
        Return the minimum value of the Index or minimum along
        an axis.
        See Also
        --------
        numpy.ndarray.min
        Series.min : Return the minimum value in a Series.
        """"""
        nv.validate_min(args, kwargs)
        nv.validate_minmax_axis(axis)
        if not len(self):
            return self._na_value
        i8 = self.asi8
        try:
            # quick check
            if len(i8) and self.is_monotonic:
                if i8[0] != iNaT:
                    return self._box_func(i8[0])
            if self.hasnans:
                if skipna:
                    min_stamp = self[~self._isnan].asi8.min()
                else:
                    return self._na_value
            else:
                min_stamp = i8.min()
            return self._box_func(min_stamp)
        except ValueError:
            return self._na_value
    def argmin(self, axis=None, skipna=True, *args, **kwargs):
        """"""
        Returns the indices of the minimum values along an axis.
        See `numpy.ndarray.argmin` for more information on the
        `axis` parameter.
        See Also
        --------
        numpy.ndarray.argmin
        """"""
        nv.validate_argmin(args, kwargs)
        nv.validate_minmax_axis(axis)
        i8 = self.asi8
        if self.hasnans:
            mask = self._isnan
            if mask.all() or not skipna:
                return -1
            i8 = i8.copy()
            i8[mask] = np.iinfo(""int64"").max
        return i8.argmin()
    def max(self, axis=None, skipna=True, *args, **kwargs):
        """"""
        Return the maximum value of the Index or maximum along
        an axis.
        See Also
        --------
        numpy.ndarray.max
        Series.max : Return the maximum value in a Series.
        """"""
        nv.validate_max(args, kwargs)
        nv.validate_minmax_axis(axis)
        if not len(self):
            return self._na_value
        i8 = self.asi8
        try:
            # quick check
            if len(i8) and self.is_monotonic:","[23, 28, 29, 30, 31]"
"""""""Functions for generating a project from a project template.""""""
import fnmatch
import json
import logging
import os
import shutil
from collections import OrderedDict
from binaryornot.check import is_binary
from jinja2 import FileSystemLoader
from jinja2.exceptions import TemplateSyntaxError, UndefinedError
from cookiecutter.environment import StrictEnvironment
from cookiecutter.exceptions import (
    ContextDecodingException,
    FailedHookException,
    NonTemplatedInputDirException,
    OutputDirExistsException,
    UndefinedVariableInTemplate,
)
from cookiecutter.find import find_template
from cookiecutter.hooks import run_hook
from cookiecutter.utils import make_sure_path_exists, rmtree, work_in
logger = logging.getLogger(__name__)

def is_copy_only_path(path, context):
    """"""Check whether the given `path` should only be copied and not rendered.
    Returns True if `path` matches a pattern in the given `context` dict,
    otherwise False.
    :param path: A file-system path referring to a file or dir that
        should be rendered or just copied.
    :param context: cookiecutter context.
    """"""
    try:
        for dont_render in context['cookiecutter']['_copy_without_render']:
            if fnmatch.fnmatch(path, dont_render):
                return True
    except KeyError:
        return False
    return False

def apply_overwrites_to_context(context, overwrite_context):
    """"""Modify the given context in place based on the overwrite_context.""""""
    for variable, overwrite in overwrite_context.items():
        if variable not in context:
            # Do not include variables which are not used in the template
            continue
        context_value = context[variable]
        if isinstance(context_value, list):
            # We are dealing with a choice variable
            if overwrite in context_value:
                # This overwrite is actually valid for the given context
                # Let's set it as default (by definition first item in list)
                # see ``cookiecutter.prompt.prompt_choice_for_config``
                context_value.remove(overwrite)
                context_value.insert(0, overwrite)
        else:
            # Simply overwrite the value for this variable
            context[variable] = overwrite

def generate_context(
    context_file='cookiecutter.json', default_context=None, extra_context=None
):
    """"""Generate the context for a Cookiecutter project template.
    Loads the JSON file as a Python object, with key being the JSON filename.
    :param context_file: JSON file containing key/value pairs for populating
        the cookiecutter's variables.
    :param default_context: Dictionary containing config to take into account.
    :param extra_context: Dictionary containing configuration overrides
    """"""
    context = OrderedDict([])
    try:
        with open(context_file) as file_handle:
            obj = json.load(file_handle, object_pairs_hook=OrderedDict)
    except ValueError as e:
        # JSON decoding error.  Let's throw a new exception that is more
        # friendly for the developer or user.
        full_fpath = os.path.abspath(context_file)
        json_exc_message = str(e)
        our_exc_message = (
            'JSON decoding error while loading ""{0}"".  Decoding'
            ' error details: ""{1}""'.format(full_fpath, json_exc_message)
        )
        raise ContextDecodingException(our_exc_message)
    # Add the Python object to the context dictionary
    file_name = os.path.split(context_file)[1]
    file_stem = file_name.split('.')[0]
    context[file_stem] = obj
    # Overwrite context variable defaults with the default context from the
    # user's global config, if available
    if default_context:
        apply_overwrites_to_context(obj, default_context)
    if extra_context:
        apply_overwrites_to_context(obj, extra_context)
    logger.debug('Context generated is %s', context)
    return context

def generate_file(project_dir, infile, context, env, skip_if_file_exists=False):
    """"""Render filename of infile as name of outfile, handle infile correctly.
    Dealing with infile appropriately:
        a. If infile is a binary file, copy it over without rendering.
        b. If infile is a text file, render its contents and write the
           rendered infile to outfile.
    Precondition:
        When calling `generate_file()`, the root template dir must be the
        current working directory. Using `utils.work_in()` is the recommended
        way to perform this directory change.",[84]
"            Upper x-coordinate of the span, in data units.
        ymin : float, default: 0
            Lower y-coordinate of the span, in y-axis units (0-1).
        ymax : float, default: 1
            Upper y-coordinate of the span, in y-axis units (0-1).
        Returns
        -------
        `~matplotlib.patches.Polygon`
            Vertical span (rectangle) from (xmin, ymin) to (xmax, ymax).
        Other Parameters
        ----------------
        **kwargs : `~matplotlib.patches.Polygon` properties
        %(Polygon)s
        See Also
        --------
        axhspan : Add a horizontal span across the axes.
        Examples
        --------
        Draw a vertical, green, translucent rectangle from x = 1.25 to
        x = 1.55 that spans the yrange of the axes.
        >>> axvspan(1.25, 1.55, facecolor='g', alpha=0.5)
        """"""
        trans = self.get_xaxis_transform(which='grid')
        # process the unit information
        self._process_unit_info([xmin, xmax], [ymin, ymax], kwargs=kwargs)
        # first we need to strip away the units
        xmin, xmax = self.convert_xunits([xmin, xmax])
        ymin, ymax = self.convert_yunits([ymin, ymax])
        verts = [(xmin, ymin), (xmin, ymax), (xmax, ymax), (xmax, ymin)]
        p = mpatches.Polygon(verts, **kwargs)
        p.set_transform(trans)
        self.add_patch(p)
        self._request_autoscale_view(scaley=False)
        return p
    @_preprocess_data(replace_names=[""y"", ""xmin"", ""xmax"", ""colors""],
                      label_namer=""y"")
    def hlines(self, y, xmin, xmax, colors='k', linestyles='solid',
               label='', **kwargs):
        """"""
        Plot horizontal lines at each *y* from *xmin* to *xmax*.
        Parameters
        ----------
        y : float or array-like
            y-indexes where to plot the lines.
        xmin, xmax : float or array-like
            Respective beginning and end of each line. If scalars are
            provided, all lines will have same length.
        colors : list of colors, default: 'k'
        linestyles : {'solid', 'dashed', 'dashdot', 'dotted'}, optional
        label : str, default: ''
        Returns
        -------
        `~matplotlib.collections.LineCollection`
        Other Parameters
        ----------------
        **kwargs :  `~matplotlib.collections.LineCollection` properties.
        See Also
        --------
        vlines : vertical lines
        axhline: horizontal line across the axes
        """"""
        # We do the conversion first since not all unitized data is uniform
        # process the unit information
        self._process_unit_info([xmin, xmax], y, kwargs=kwargs)
        y = self.convert_yunits(y)
        xmin = self.convert_xunits(xmin)
        xmax = self.convert_xunits(xmax)
        if not np.iterable(y):
            y = [y]
        if not np.iterable(xmin):
            xmin = [xmin]
        if not np.iterable(xmax):
            xmax = [xmax]
        # Create and combine masked_arrays from input
        y, xmin, xmax = cbook._combine_masks(y, xmin, xmax)
        y = np.ravel(y)
        xmin = np.ravel(xmin)
        xmax = np.ravel(xmax)
        masked_verts = np.ma.empty((len(y), 2, 2))
        masked_verts[:, 0, 0] = xmin
        masked_verts[:, 0, 1] = y
        masked_verts[:, 1, 0] = xmax
        masked_verts[:, 1, 1] = y
        lines = mcoll.LineCollection(masked_verts, colors=colors,
                                     linestyles=linestyles, label=label)
        self.add_collection(lines, autolim=False)
        lines.update(kwargs)
        if len(y) > 0:
            minx = min(xmin.min(), xmax.min())
            maxx = max(xmin.max(), xmax.max())
            miny = y.min()
            maxy = y.max()
            corners = (minx, miny), (maxx, maxy)
            self.update_datalim(corners)
            self._request_autoscale_view()
        return lines
    @_preprocess_data(replace_names=[""x"", ""ymin"", ""ymax"", ""colors""],
                      label_namer=""x"")","[47, 61, 127]"
"""""""
SQL-style merge routines
""""""
import copy
from functools import partial
import string
import warnings
import numpy as np
from pandas._libs import hashtable as libhashtable, lib
import pandas._libs.join as libjoin
from pandas.errors import MergeError
from pandas.util._decorators import Appender, Substitution
from pandas.core.dtypes.common import (
    ensure_float64,
    ensure_int64,
    ensure_object,
    is_array_like,
    is_bool,
    is_bool_dtype,
    is_categorical_dtype,
    is_datetime64_dtype,
    is_datetime64tz_dtype,
    is_datetimelike,
    is_dtype_equal,
    is_extension_array_dtype,
    is_float_dtype,
    is_int64_dtype,
    is_integer,
    is_integer_dtype,
    is_list_like,
    is_number,
    is_numeric_dtype,
    is_object_dtype,
    needs_i8_conversion,
)
from pandas.core.dtypes.missing import isnull, na_value_for_dtype
from pandas import Categorical, DataFrame, Index, MultiIndex, Series, Timedelta
import pandas.core.algorithms as algos
from pandas.core.arrays.categorical import _recode_for_categories
import pandas.core.common as com
from pandas.core.frame import _merge_doc
from pandas.core.internals import _transform_index, concatenate_block_managers
import pandas.core.sorting as sorting
from pandas.core.sorting import is_int64_overflow_possible

@Substitution(""\nleft : DataFrame"")
@Appender(_merge_doc, indents=0)
def merge(
    left,
    right,
    how=""inner"",
    on=None,
    left_on=None,
    right_on=None,
    left_index=False,
    right_index=False,
    sort=False,
    suffixes=(""_x"", ""_y""),
    copy=True,
    indicator=False,
    validate=None,
):
    op = _MergeOperation(
        left,
        right,
        how=how,
        on=on,
        left_on=left_on,
        right_on=right_on,
        left_index=left_index,
        right_index=right_index,
        sort=sort,
        suffixes=suffixes,
        copy=copy,
        indicator=indicator,
        validate=validate,
    )
    return op.get_result()

if __debug__:
    merge.__doc__ = _merge_doc % ""\nleft : DataFrame""

def _groupby_and_merge(by, on, left, right, _merge_pieces, check_duplicates=True):
    """"""
    groupby & merge; we are always performing a left-by type operation
    Parameters
    ----------
    by: field to group
    on: duplicates field
    left: left frame
    right: right frame
    _merge_pieces: function for merging
    check_duplicates: boolean, default True
        should we check & clean duplicates
    """"""
    pieces = []
    if not isinstance(by, (list, tuple)):
        by = [by]
    lby = left.groupby(by, sort=False)
    # if we can groupby the rhs
    # then we can get vastly better perf
    try:
        # we will check & remove duplicates if indicated
        if check_duplicates:
            if on is None:
                on = []
            elif not isinstance(on, (list, tuple)):
                on = [on]
            if right.duplicated(by + on).any():
                right = right.drop_duplicates(by + on, keep=""last"")
        rby = right.groupby(by, sort=False)
    except KeyError:
        rby = None",[24]
"""""""
Expressions
-----------
Offer fast expression evaluation through numexpr
""""""
import warnings
import numpy as np
from pandas._config import get_option
from pandas._libs.lib import values_from_object
from pandas.core.dtypes.generic import ABCDataFrame
from pandas.core.computation.check import _NUMEXPR_INSTALLED
if _NUMEXPR_INSTALLED:
    import numexpr as ne
_TEST_MODE = None
_TEST_RESULT = None
_USE_NUMEXPR = _NUMEXPR_INSTALLED
_evaluate = None
_where = None
# the set of dtypes that we will allow pass to numexpr
_ALLOWED_DTYPES = {
    ""evaluate"": {""int64"", ""int32"", ""float64"", ""float32"", ""bool""},
    ""where"": {""int64"", ""float64"", ""bool""},
}
# the minimum prod shape that we will use numexpr
_MIN_ELEMENTS = 10000

def set_use_numexpr(v=True):
    # set/unset to use numexpr
    global _USE_NUMEXPR
    if _NUMEXPR_INSTALLED:
        _USE_NUMEXPR = v
    # choose what we are going to do
    global _evaluate, _where
    if not _USE_NUMEXPR:
        _evaluate = _evaluate_standard
        _where = _where_standard
    else:
        _evaluate = _evaluate_numexpr
        _where = _where_numexpr

def set_numexpr_threads(n=None):
    # if we are using numexpr, set the threads to n
    # otherwise reset
    if _NUMEXPR_INSTALLED and _USE_NUMEXPR:
        if n is None:
            n = ne.detect_number_of_cores()
        ne.set_num_threads(n)

def _evaluate_standard(op, op_str, a, b, **eval_kwargs):
    """""" standard evaluation """"""
    if _TEST_MODE:
        _store_test_result(False)
    with np.errstate(all=""ignore""):
        return op(a, b)

def _can_use_numexpr(op, op_str, a, b, dtype_check):
    """""" return a boolean if we WILL be using numexpr """"""
    if op_str is not None:
        # required min elements (otherwise we are adding overhead)
        if np.prod(a.shape) > _MIN_ELEMENTS:
            # check for dtype compatibility
            dtypes = set()
            for o in [a, b]:
                if hasattr(o, ""dtypes""):
                    s = o.dtypes.value_counts()
                    if len(s) > 1:
                        return False
                    dtypes |= set(s.index.astype(str))
                elif isinstance(o, np.ndarray):
                    dtypes |= {o.dtype.name}
            # allowed are a superset
            if not len(dtypes) or _ALLOWED_DTYPES[dtype_check] >= dtypes:
                return True
    return False

def _evaluate_numexpr(op, op_str, a, b, truediv=True, reversed=False, **eval_kwargs):
    result = None
    if _can_use_numexpr(op, op_str, a, b, ""evaluate""):
        if reversed:
            # we were originally called by a reversed op method
            a, b = b, a
        a_value = getattr(a, ""values"", a)
        b_value = getattr(b, ""values"", b)
        try:
            result = ne.evaluate(
                ""a_value {op} b_value"".format(op=op_str),
                local_dict={""a_value"": a_value, ""b_value"": b_value},
                casting=""safe"",
                truediv=truediv,
                **eval_kwargs
            )
        except ValueError as detail:
            if ""unknown type object"" in str(detail):
                pass
    if _TEST_MODE:
        _store_test_result(result is not None)
    if result is None:
        result = _evaluate_standard(op, op_str, a, b)
    return result
","[78, 82, 87]"
"        if url.startswith(BOM_UTF8):
            url = url[len(BOM_UTF8):]
        url = url.strip()
        if url.startswith(('#', ';', ']')):
            return False
        return url
    with contextlib.closing(batch_fd) as fd:
        return [url for url in map(fixup, fd) if url]

def urlencode_postdata(*args, **kargs):
    return compat_urllib_parse.urlencode(*args, **kargs).encode('ascii')

try:
    etree_iter = xml.etree.ElementTree.Element.iter
except AttributeError:  # Python <=2.6
    etree_iter = lambda n: n.findall('.//*')

def parse_xml(s):
    class TreeBuilder(xml.etree.ElementTree.TreeBuilder):
        def doctype(self, name, pubid, system):
            pass  # Ignore doctypes
    parser = xml.etree.ElementTree.XMLParser(target=TreeBuilder())
    kwargs = {'parser': parser} if sys.version_info >= (2, 7) else {}
    tree = xml.etree.ElementTree.XML(s.encode('utf-8'), **kwargs)
    # Fix up XML parser in Python 2.x
    if sys.version_info < (3, 0):
        for n in etree_iter(tree):
            if n.text is not None:
                if not isinstance(n.text, compat_str):
                    n.text = n.text.decode('utf-8')
    return tree

US_RATINGS = {
    'G': 0,
    'PG': 10,
    'PG-13': 13,
    'R': 16,
    'NC': 18,
}

def parse_age_limit(s):
    if s is None:
        return None
    m = re.match(r'^(?P<age>\d{1,2})\+?$', s)
    return int(m.group('age')) if m else US_RATINGS.get(s, None)

def strip_jsonp(code):
    return re.sub(
        r'(?s)^[a-zA-Z0-9_]+\s*\(\s*(.*)\);?\s*?(?://[^\n]*)*$', r'\1', code)

def js_to_json(code):
    def fix_kv(m):
        v = m.group(0)
        if v in ('true', 'false', 'null'):
            return v
        if v.startswith('""'):
            return v
        if v.startswith(""'""):
            v = v[1:-1]
            v = re.sub(r""\\\\|\\'|\"""", lambda m: {
                '\\\\': '\\\\',
                ""\\'"": ""'"",
                '""': '\\""',
            }[m.group(0)], v)
        return '""%s""' % v
    res = re.sub(r'''(?x)
        ""(?:[^""\\]*(?:\\\\|\\"")?)*""|
        '(?:[^'\\]*(?:\\\\|\\')?)*'|
        [a-zA-Z_][.a-zA-Z_0-9]*
        ''', fix_kv, code)
    res = re.sub(r',(\s*\])', lambda m: m.group(1), res)
    return res

def qualities(quality_ids):
    """""" Get a numeric quality value out of a list of possible values """"""
    def q(qid):
        try:
            return quality_ids.index(qid)
        except ValueError:
            return -1
    return q

DEFAULT_OUTTMPL = '%(title)s-%(id)s.%(ext)s'

def limit_length(s, length):
    """""" Add ellipses to overly long strings """"""
    if s is None:
        return None
    ELLIPSES = '...'
    if len(s) > length:
        return s[:length - len(ELLIPSES)] + ELLIPSES
    return s

def version_tuple(v):
    return tuple(int(e) for e in re.split(r'[-.]', v))

def is_outdated_version(version, limit, assume_new=True):
    if not version:
        return not assume_new
    try:
        return version_tuple(version) < version_tuple(limit)
    except ValueError:
        return not assume_new

def ytdl_is_updateable():
    """""" Returns if youtube-dl can be updated with -U """"""
    from zipimport import zipimporter
    return isinstance(globals().get('__loader__'), zipimporter) or hasattr(sys, 'frozen')
","[76, 77]"
"    0  NaN   NaN   NaN
    1  3.0   9.0  15.0
    2  5.0  11.0  17.0
    >>> df.rolling(2).agg({""A"": ""sum"", ""B"": ""min""})
         A    B
    0  NaN  NaN
    1  3.0  4.0
    2  5.0  5.0
    """"""
    )
    @Substitution(
        see_also=_agg_see_also_doc,
        examples=_agg_examples_doc,
        versionadded="""",
        klass=""Series/Dataframe"",
        axis="""",
    )
    @Appender(_shared_docs[""aggregate""])
    def aggregate(self, func, *args, **kwargs):
        return super().aggregate(func, *args, **kwargs)
    agg = aggregate
    @Substitution(name=""rolling"")
    @Appender(_shared_docs[""count""])
    def count(self):
        # different impl for freq counting
        if self.is_freq_type:
            window_func = self._get_roll_func(""roll_count"")
            return self._apply(window_func, center=self.center, name=""count"")
        return super().count()
    @Substitution(name=""rolling"")
    @Appender(_shared_docs[""apply""])
    def apply(
        self,
        func,
        raw=False,
        engine=""cython"",
        engine_kwargs=None,
        args=None,
        kwargs=None,
    ):
        return super().apply(
            func,
            raw=raw,
            engine=engine,
            engine_kwargs=engine_kwargs,
            args=args,
            kwargs=kwargs,
        )
    @Substitution(name=""rolling"")
    @Appender(_shared_docs[""sum""])
    def sum(self, *args, **kwargs):
        nv.validate_rolling_func(""sum"", args, kwargs)
        return super().sum(*args, **kwargs)
    @Substitution(name=""rolling"")
    @Appender(_doc_template)
    @Appender(_shared_docs[""max""])
    def max(self, *args, **kwargs):
        nv.validate_rolling_func(""max"", args, kwargs)
        return super().max(*args, **kwargs)
    @Substitution(name=""rolling"")
    @Appender(_shared_docs[""min""])
    def min(self, *args, **kwargs):
        nv.validate_rolling_func(""min"", args, kwargs)
        return super().min(*args, **kwargs)
    @Substitution(name=""rolling"")
    @Appender(_shared_docs[""mean""])
    def mean(self, *args, **kwargs):
        nv.validate_rolling_func(""mean"", args, kwargs)
        return super().mean(*args, **kwargs)
    @Substitution(name=""rolling"")
    @Appender(_shared_docs[""median""])
    def median(self, **kwargs):
        return super().median(**kwargs)
    @Substitution(name=""rolling"", versionadded="""")
    @Appender(_shared_docs[""std""])
    def std(self, ddof=1, *args, **kwargs):
        nv.validate_rolling_func(""std"", args, kwargs)
        return super().std(ddof=ddof, **kwargs)
    @Substitution(name=""rolling"", versionadded="""")
    @Appender(_shared_docs[""var""])
    def var(self, ddof=1, *args, **kwargs):
        nv.validate_rolling_func(""var"", args, kwargs)
        return super().var(ddof=ddof, **kwargs)
    @Substitution(name=""rolling"")
    @Appender(_doc_template)
    @Appender(_shared_docs[""skew""])
    def skew(self, **kwargs):
        return super().skew(**kwargs)
    _agg_doc = dedent(
        """"""
    Examples
    --------
    The example below will show a rolling calculation with a window size of
    four matching the equivalent function call using `scipy.stats`.
    >>> arr = [1, 2, 3, 4, 999]
    >>> import scipy.stats
    >>> print(f""{scipy.stats.kurtosis(arr[:-1], bias=False):.6f}"")
    -1.200000
    >>> print(f""{scipy.stats.kurtosis(arr[1:], bias=False):.6f}"")
    3.999946
    >>> s = pd.Series(arr)
    >>> s.rolling(4).kurt()
    0         NaN
    1         NaN
    2         NaN
    3   -1.200000
    4    3.999946
    dtype: float64
    """"""",[30]
"                self._group_index = CategoricalIndex(
                    Categorical.from_codes(
                        codes=codes, categories=categories, ordered=self.grouper.ordered
                    )
                )
            # we are done
            if isinstance(self.grouper, Grouping):
                self.grouper = self.grouper.grouper
            # no level passed
            elif not isinstance(
                self.grouper, (Series, Index, ExtensionArray, np.ndarray)
            ):
                if getattr(self.grouper, ""ndim"", 1) != 1:
                    t = self.name or str(type(self.grouper))
                    raise ValueError(""Grouper for '{}' not 1-dimensional"".format(t))
                self.grouper = self.index.map(self.grouper)
                if not (
                    hasattr(self.grouper, ""__len__"")
                    and len(self.grouper) == len(self.index)
                ):
                    errmsg = (
                        ""Grouper result violates len(labels) == ""
                        ""len(data)\nresult: %s"" % pprint_thing(self.grouper)
                    )
                    self.grouper = None  # Try for sanity
                    raise AssertionError(errmsg)
        # if we have a date/time-like grouper, make sure that we have
        # Timestamps like
        if getattr(self.grouper, ""dtype"", None) is not None:
            if is_datetime64_dtype(self.grouper):
                self.grouper = self.grouper.astype(""datetime64[ns]"")
            elif is_timedelta64_dtype(self.grouper):
                self.grouper = self.grouper.astype(""timedelta64[ns]"")
    def __repr__(self):
        return ""Grouping({0})"".format(self.name)
    def __iter__(self):
        return iter(self.indices)
    _labels = None
    _group_index = None
    @property
    def ngroups(self):
        return len(self.group_index)
    @cache_readonly
    def indices(self):
        # we have a list of groupers
        if isinstance(self.grouper, BaseGrouper):
            return self.grouper.indices
        values = ensure_categorical(self.grouper)
        return values._reverse_indexer()
    @property
    def labels(self):
        if self._labels is None:
            self._make_labels()
        return self._labels
    @cache_readonly
    def result_index(self):
        if self.all_grouper is not None:
            return recode_from_groupby(self.all_grouper, self.sort, self.group_index)
        return self.group_index
    @property
    def group_index(self):
        if self._group_index is None:
            self._make_labels()
        return self._group_index
    def _make_labels(self):
        if self._labels is None or self._group_index is None:
            # we have a list of groupers
            if isinstance(self.grouper, BaseGrouper):
                labels = self.grouper.label_info
                uniques = self.grouper.result_index
            else:
                labels, uniques = algorithms.factorize(self.grouper, sort=self.sort)
                uniques = Index(uniques, name=self.name)
            self._labels = labels
            self._group_index = uniques
    @cache_readonly
    def groups(self):
        return self.index.groupby(Categorical.from_codes(self.labels, self.group_index))

def _get_grouper(
    obj: NDFrame,
    key=None,
    axis=0,
    level=None,
    sort=True,
    observed=False,
    mutated=False,
    validate=True,
):
    """"""
    create and return a BaseGrouper, which is an internal
    mapping of how to create the grouper indexers.
    This may be composed of multiple Grouping objects, indicating
    multiple groupers
    Groupers are ultimately index mappings. They can originate as:
    index mappings, keys to columns, functions, or Groupers
    Groupers enable local references to axis,level,sort, while
    the passed in axis, level, and sort are 'global'.
    This routine tries to figure out what the passing in references
    are and then creates a Grouping for each one, combined into
    a BaseGrouper.
    If observed & we have a categorical grouper, only show the observed
    values
    If validate, then check for key/level overlaps
    """"""",[3]
"            return {
                'vcodec': vcodec,
                'acodec': acodec,
            }
        elif len(splited_codecs) == 1:
            return {
                'vcodec': 'none',
                'acodec': vcodec,
            }
    else:
        return {
            'vcodec': vcodec or 'none',
            'acodec': acodec or 'none',
        }
    return {}

def urlhandle_detect_ext(url_handle):
    getheader = url_handle.headers.get
    cd = getheader('Content-Disposition')
    if cd:
        m = re.match(r'attachment;\s*filename=""(?P<filename>[^""]+)""', cd)
        if m:
            e = determine_ext(m.group('filename'), default_ext=None)
            if e:
                return e
    return mimetype2ext(getheader('Content-Type'))

def encode_data_uri(data, mime_type):
    return 'data:%s;base64,%s' % (mime_type, base64.b64encode(data).decode('ascii'))

def age_restricted(content_limit, age_limit):
    """""" Returns True iff the content should be blocked """"""
    if age_limit is None:  # No limit set
        return False
    if content_limit is None:
        return False  # Content available for everyone
    return age_limit < content_limit

def is_html(first_bytes):
    """""" Detect whether a file contains HTML by examining its first bytes. """"""
    BOMS = [
        (b'\xef\xbb\xbf', 'utf-8'),
        (b'\x00\x00\xfe\xff', 'utf-32-be'),
        (b'\xff\xfe\x00\x00', 'utf-32-le'),
        (b'\xff\xfe', 'utf-16-le'),
        (b'\xfe\xff', 'utf-16-be'),
    ]
    for bom, enc in BOMS:
        if first_bytes.startswith(bom):
            s = first_bytes[len(bom):].decode(enc, 'replace')
            break
    else:
        s = first_bytes.decode('utf-8', 'replace')
    return re.match(r'^\s*<', s)

def determine_protocol(info_dict):
    protocol = info_dict.get('protocol')
    if protocol is not None:
        return protocol
    url = info_dict['url']
    if url.startswith('rtmp'):
        return 'rtmp'
    elif url.startswith('mms'):
        return 'mms'
    elif url.startswith('rtsp'):
        return 'rtsp'
    ext = determine_ext(url)
    if ext == 'm3u8':
        return 'm3u8'
    elif ext == 'f4m':
        return 'f4m'
    return compat_urllib_parse_urlparse(url).scheme

def render_table(header_row, data):
    """""" Render a list of rows, each as a list of values """"""
    table = [header_row] + data
    max_lens = [max(len(compat_str(v)) for v in col) for col in zip(*table)]
    format_str = ' '.join('%-' + compat_str(ml + 1) + 's' for ml in max_lens[:-1]) + '%s'
    return '\n'.join(format_str % tuple(row) for row in table)

def _match_one(filter_part, dct):
    COMPARISON_OPERATORS = {
        '<': operator.lt,
        '<=': operator.le,
        '>': operator.gt,
        '>=': operator.ge,
        '=': operator.eq,
        '!=': operator.ne,
    }
    operator_rex = re.compile(r'''(?x)\s*
        (?P<key>[a-z_]+)
        \s*(?P<op>%s)(?P<none_inclusive>\s*\?)?\s*
        (?:
            (?P<intval>[0-9.]+(?:[kKmMgGtTpPeEzZyY]i?[Bb]?)?)|
            (?P<strval>(?![0-9.])[a-z0-9A-Z]*)
        )
        \s*$
        ''' % '|'.join(map(re.escape, COMPARISON_OPERATORS.keys())))
    m = operator_rex.search(filter_part)
    if m:
        op = COMPARISON_OPERATORS[m.group('op')]
        if m.group('strval') is not None:
            if m.group('op') not in ('=', '!='):
                raise ValueError(
                    'Operator %s does not support string values!' % m.group('op'))
            comparison_value = m.group('strval')
        else:
            try:
                comparison_value = int(m.group('intval'))
            except ValueError:
                comparison_value = parse_filesize(m.group('intval'))
                if comparison_value is None:","[116, 120]"
"# -*- coding: utf-8 -*-
""""""
cookiecutter.prompt
---------------------
Functions for prompting the user for project info.
""""""
from collections import OrderedDict
import json
import click
from past.builtins import basestring
from future.utils import iteritems
from jinja2.exceptions import UndefinedError
from .exceptions import UndefinedVariableInTemplate
from .environment import StrictEnvironment

def read_user_variable(var_name, default_value):
    """"""Prompt the user for the given variable and return the entered value
    or the given default.
    :param str var_name: Variable of the context to query the user
    :param default_value: Value that will be returned if no input happens
    """"""
    # Please see http://click.pocoo.org/4/api/#click.prompt
    return click.prompt(var_name, default=default_value)

def read_user_yes_no(question, default_value):
    """"""Prompt the user to reply with 'yes' or 'no' (or equivalent values).
    Note:
      Possible choices are 'true', '1', 'yes', 'y' or 'false', '0', 'no', 'n'
    :param str question: Question to the user
    :param default_value: Value that will be returned if no input happens
    """"""
    # Please see http://click.pocoo.org/4/api/#click.prompt
    return click.prompt(
        question,
        default=default_value,
        type=click.BOOL
    )

def read_repo_password(question):
    """"""Prompt the user to enter a password
    :param str question: Question to the user
    """"""
    # Please see http://click.pocoo.org/4/api/#click.prompt
    return click.prompt(question, hide_input=True)

def read_user_choice(var_name, options):
    """"""Prompt the user to choose from several options for the given variable.
    The first item will be returned if no input happens.
    :param str var_name: Variable as specified in the context
    :param list options: Sequence of options that are available to select from
    :return: Exactly one item of ``options`` that has been chosen by the user
    """"""
    # Please see http://click.pocoo.org/4/api/#click.prompt
    if not isinstance(options, list):
        raise TypeError
    if not options:
        raise ValueError
    choice_map = OrderedDict(
        (u'{}'.format(i), value) for i, value in enumerate(options, 1)
    )
    choices = choice_map.keys()
    default = u'1'
    choice_lines = [u'{} - {}'.format(*c) for c in choice_map.items()]
    prompt = u'\n'.join((
        u'Select {}:'.format(var_name),
        u'\n'.join(choice_lines),
        u'Choose from {}'.format(u', '.join(choices))
    ))
    user_choice = click.prompt(
        prompt, type=click.Choice(choices), default=default
    )
    return choice_map[user_choice]

def process_json(user_value):
    try:
        user_dict = json.loads(
            user_value,
            object_pairs_hook=OrderedDict,
        )
    except Exception:
        # Leave it up to click to ask the user again
        raise click.UsageError('Unable to decode to JSON.')
    if not isinstance(user_dict, dict):
        # Leave it up to click to ask the user again
        raise click.UsageError('Requires JSON dict.')
    return user_dict

def read_user_dict(var_name, default_value):
    """"""Prompt the user to provide a dictionary of data.
    :param str var_name: Variable as specified in the context
    :param default_value: Value that will be returned if no input is provided
    :return: A Python dictionary to use in the context.
    """"""
    # Please see http://click.pocoo.org/4/api/#click.prompt
    if not isinstance(default_value, dict):
        raise TypeError
    default_display = 'default'
    user_value = click.prompt(
        var_name,",[90]
"""""""
This module contains general purpose URL functions not found in the standard
library.
Some of the functions that used to be imported from this module have been moved
to the w3lib.url module. Always import those from there instead.
""""""
import posixpath
import re
from six.moves.urllib.parse import (ParseResult, urlunparse, urldefrag,
                                    urlparse, parse_qsl, urlencode,
                                    unquote)
# scrapy.utils.url was moved to w3lib.url and import * ensures this
# move doesn't break old code
from w3lib.url import *
from w3lib.url import _safe_chars
from scrapy.utils.python import to_native_str

def url_is_from_any_domain(url, domains):
    """"""Return True if the url belongs to any of the given domains""""""
    host = parse_url(url).netloc.lower()
    if not host:
        return False
    domains = [d.lower() for d in domains]
    return any((host == d) or (host.endswith('.%s' % d)) for d in domains)

def url_is_from_spider(url, spider):
    """"""Return True if the url belongs to the given spider""""""
    return url_is_from_any_domain(url,
        [spider.name] + list(getattr(spider, 'allowed_domains', [])))

def url_has_any_extension(url, extensions):
    return posixpath.splitext(parse_url(url).path)[1].lower() in extensions

def canonicalize_url(url, keep_blank_values=True, keep_fragments=False,
                     encoding=None):
    """"""Canonicalize the given url by applying the following procedures:
    - sort query arguments, first by key, then by value
    - percent encode paths and query arguments. non-ASCII characters are
      percent-encoded using UTF-8 (RFC-3986)
    - normalize all spaces (in query arguments) '+' (plus symbol)
    - normalize percent encodings case (%2f -> %2F)
    - remove query arguments with blank values (unless keep_blank_values is True)
    - remove fragments (unless keep_fragments is True)
    The url passed can be a str or unicode, while the url returned is always a
    str.
    For examples see the tests in tests/test_utils_url.py
    """"""
    scheme, netloc, path, params, query, fragment = parse_url(url)
    keyvals = parse_qsl(query, keep_blank_values)
    keyvals.sort()
    query = urlencode(keyvals)
    # XXX: copied from w3lib.url.safe_url_string to add encoding argument
    # path = to_native_str(path, encoding)
    # path = moves.urllib.parse.quote(path, _safe_chars, encoding='latin1') or '/'
    path = safe_url_string(_unquotepath(path)) or '/'
    fragment = '' if not keep_fragments else fragment
    return urlunparse((scheme, netloc.lower(), path, params, query, fragment))

def _unquotepath(path):
    for reserved in ('2f', '2F', '3f', '3F'):
        path = path.replace('%' + reserved, '%25' + reserved.upper())
    return unquote(path)

def parse_url(url, encoding=None):
    """"""Return urlparsed url from the given argument (which could be an already
    parsed url)
    """"""
    if isinstance(url, ParseResult):
        return url
    return urlparse(to_native_str(url, encoding))

def escape_ajax(url):
    """"""
    Return the crawleable url according to:
    http://code.google.com/web/ajaxcrawling/docs/getting-started.html
    >>> escape_ajax(""www.example.com/ajax.html#!key=value"")
    'www.example.com/ajax.html?_escaped_fragment_=key%3Dvalue'
    >>> escape_ajax(""www.example.com/ajax.html?k1=v1&k2=v2#!key=value"")
    'www.example.com/ajax.html?k1=v1&k2=v2&_escaped_fragment_=key%3Dvalue'
    >>> escape_ajax(""www.example.com/ajax.html?#!key=value"")
    'www.example.com/ajax.html?_escaped_fragment_=key%3Dvalue'
    >>> escape_ajax(""www.example.com/ajax.html#!"")
    'www.example.com/ajax.html?_escaped_fragment_='
    URLs that are not ""AJAX crawlable"" (according to Google) returned as-is:
    >>> escape_ajax(""www.example.com/ajax.html#key=value"")
    'www.example.com/ajax.html#key=value'
    >>> escape_ajax(""www.example.com/ajax.html#"")
    'www.example.com/ajax.html#'
    >>> escape_ajax(""www.example.com/ajax.html"")
    'www.example.com/ajax.html'
    """"""
    defrag, frag = urldefrag(url)
    if not frag.startswith('!'):
        return url
    return add_or_replace_parameter(defrag, '_escaped_fragment_', frag[1:])

def add_http_if_no_scheme(url):
    """"""Add http as the default scheme if it is missing from the url.""""""
    match = re.match(r""^\w+://"", url, flags=re.I)
    if not match:
        parts = urlparse(url)
        scheme = ""http:"" if parts.netloc else ""http://""
        url = scheme + url
    return url

def guess_scheme(url):","[11, 17, 44, 45, 48, 49, 51, 52, 57, 58, 66, 74, 83]"
"import os
from ..conf import settings
from ..utils import memoize
from .generic import Generic

class Bash(Generic):
    def app_alias(self, fuck):
        alias = ""TF_ALIAS={0}"" \
                "" alias {0}='PYTHONIOENCODING=utf-8"" \
                "" TF_CMD=$(TF_SHELL_ALIASES=$(alias) thefuck $(fc -ln -1)) && "" \
                "" eval $TF_CMD"".format(fuck)
        if settings.alter_history:
            return alias + "" && history -s $TF_CMD'""
        else:
            return alias + ""'""
    def _parse_alias(self, alias):
        name, value = alias.replace('alias ', '', 1).split('=', 1)
        if value[0] == value[-1] == '""' or value[0] == value[-1] == ""'"":
            value = value[1:-1]
        return name, value
    @memoize
    def get_aliases(self):
        raw_aliases = os.environ.get('TF_SHELL_ALIASES', '').split('\n')
        return dict(self._parse_alias(alias)
                    for alias in raw_aliases if alias and '=' in alias)
    def _get_history_file_name(self):
        return os.environ.get(""HISTFILE"",
                              os.path.expanduser('~/.bash_history'))
    def _get_history_line(self, command_script):
        return u'{}\n'.format(command_script)
    def how_to_configure(self):
        if os.path.join(os.path.expanduser('~'), '.bashrc'):
            config = '~/.bashrc'
        elif os.path.join(os.path.expanduser('~'), '.bash_profile'):
            config = '~/.bashrc'
        else:
            config = 'bash config'
        return 'eval $(thefuck --alias)', config","[8, 9, 10]"
"        """"""
        Concatenate two or more Series.
        Parameters
        ----------
        to_append : Series or list/tuple of Series
            Series to append with self.
        ignore_index : bool, default False
            If True, do not use the index labels.
        verify_integrity : bool, default False
            If True, raise Exception on creating index with duplicates.
        Returns
        -------
        Series
            Concatenated Series.
        See Also
        --------
        concat : General function to concatenate DataFrame or Series objects.
        Notes
        -----
        Iteratively appending to a Series can be more computationally intensive
        than a single concatenate. A better solution is to append values to a
        list and then concatenate the list with the original Series all at
        once.
        Examples
        --------
        >>> s1 = pd.Series([1, 2, 3])
        >>> s2 = pd.Series([4, 5, 6])
        >>> s3 = pd.Series([4, 5, 6], index=[3, 4, 5])
        >>> s1.append(s2)
        0    1
        1    2
        2    3
        0    4
        1    5
        2    6
        dtype: int64
        >>> s1.append(s3)
        0    1
        1    2
        2    3
        3    4
        4    5
        5    6
        dtype: int64
        With `ignore_index` set to True:
        >>> s1.append(s2, ignore_index=True)
        0    1
        1    2
        2    3
        3    4
        4    5
        5    6
        dtype: int64
        With `verify_integrity` set to True:
        >>> s1.append(s2, verify_integrity=True)
        Traceback (most recent call last):
        ...
        ValueError: Indexes have overlapping values: [0, 1, 2]
        """"""
        from pandas.core.reshape.concat import concat
        if isinstance(to_append, (list, tuple)):
            to_concat = [self] + to_append
        else:
            to_concat = [self, to_append]
        return concat(
            to_concat, ignore_index=ignore_index, verify_integrity=verify_integrity
        )
    def _binop(self, other, func, level=None, fill_value=None):
        """"""
        Perform generic binary operation with optional fill value.
        Parameters
        ----------
        other : Series
        func : binary operator
        fill_value : float or object
            Value to substitute for NA/null values. If both Series are NA in a
            location, the result will be NA regardless of the passed fill value
        level : int or level name, default None
            Broadcast across a level, matching Index values on the
            passed MultiIndex level
        Returns
        -------
        Series
        """"""
        if not isinstance(other, Series):
            raise AssertionError(""Other operand must be Series"")
        new_index = self.index
        this = self
        if not self.index.equals(other.index):
            this, other = self.align(other, level=level, join=""outer"", copy=False)
            new_index = this.index
        this_vals, other_vals = ops.fill_binop(this.values, other.values, fill_value)
        with np.errstate(all=""ignore""):
            result = func(this_vals, other_vals)
        name = ops.get_op_result_name(self, other)
        if func.__name__ in [""divmod"", ""rdivmod""]:
            ret = ops._construct_divmod_result(self, result, new_index, name)
        else:
            ret = ops._construct_result(self, result, new_index, name)
        return ret
    def combine(self, other, func, fill_value=None):
        """"""
        Combine the Series with a Series or scalar according to `func`.
        Combine the Series and `other` using `func` to perform elementwise
        selection for combined Series.",[72]
"                contracts.append(self.contracts[name](method, *args))
        return contracts
    def from_spider(self, spider, results):
        requests = []
        for method in self.tested_methods_from_spidercls(type(spider)):
            bound_method = spider.__getattribute__(method)
            requests.append(self.from_method(bound_method, results))
        return requests
    def from_method(self, method, results):
        contracts = self.extract_contracts(method)
        if contracts:
            # calculate request args
            args, kwargs = get_spec(Request.__init__)
            kwargs['callback'] = method
            for contract in contracts:
                kwargs = contract.adjust_request_args(kwargs)
            # create and prepare request
            args.remove('self')
            if set(args).issubset(set(kwargs)):
                request = Request(**kwargs)
                # execute pre and post hooks in order
                for contract in reversed(contracts):
                    request = contract.add_pre_hook(request, results)
                for contract in contracts:
                    request = contract.add_post_hook(request, results)
                self._clean_req(request, method, results)
                return request
    def _clean_req(self, request, method, results):
        """""" stop the request from returning objects and records any errors """"""
        cb = request.callback
        @wraps(cb)
        def cb_wrapper(response):
            try:
                output = cb(response)
                output = list(iterate_spider_output(output))
            except:
                case = _create_testcase(method, 'callback')
                results.addError(case, sys.exc_info())
        def eb_wrapper(failure):
            case = _create_testcase(method, 'errback')
            exc_info = failure.value, failure.type, failure.getTracebackObject()
            results.addError(case, exc_info)
        request.callback = cb_wrapper
        request.errback = eb_wrapper

class Contract(object):
    """""" Abstract class for contracts """"""
    def __init__(self, method, *args):
        self.testcase_pre = _create_testcase(method, '@%s pre-hook' % self.name)
        self.testcase_post = _create_testcase(method, '@%s post-hook' % self.name)
        self.args = args
    def add_pre_hook(self, request, results):
        if hasattr(self, 'pre_process'):
            cb = request.callback
            @wraps(cb)
            def wrapper(response):
                try:
                    results.startTest(self.testcase_pre)
                    self.pre_process(response)
                    results.stopTest(self.testcase_pre)
                except AssertionError:
                    results.addFailure(self.testcase_pre, sys.exc_info())
                except Exception:
                    results.addError(self.testcase_pre, sys.exc_info())
                else:
                    results.addSuccess(self.testcase_pre)
                finally:
                    return list(iterate_spider_output(cb(response)))
            request.callback = wrapper
        return request
    def add_post_hook(self, request, results):
        if hasattr(self, 'post_process'):
            cb = request.callback
            @wraps(cb)
            def wrapper(response):
                output = list(iterate_spider_output(cb(response)))
                try:
                    results.startTest(self.testcase_post)
                    self.post_process(output)
                    results.stopTest(self.testcase_post)
                except AssertionError:
                    results.addFailure(self.testcase_post, sys.exc_info())
                except Exception:
                    results.addError(self.testcase_post, sys.exc_info())
                else:
                    results.addSuccess(self.testcase_post)
                finally:
                    return output
            request.callback = wrapper
        return request
    def adjust_request_args(self, args):
        return args

def _create_testcase(method, desc):
    spider = method.__self__.name
    class ContractTestCase(TestCase):
        def __str__(_self):
            return ""[%s] %s (%s)"" % (spider, method.__name__, desc)
    name = '%s_%s' % (spider, method.__name__)
    setattr(ContractTestCase, name, lambda x: x)",[51]
"        """"""
        if freq is not None and freq != self.freq:
            if isinstance(freq, str):
                freq = frequencies.to_offset(freq)
            offset = periods * freq
            result = self + offset
            return result
        if periods == 0:
            # immutable so OK
            return self.copy()
        if self.freq is None:
            raise NullFrequencyError(""Cannot shift with no freq"")
        start = self[0] + periods * self.freq
        end = self[-1] + periods * self.freq
        # Note: in the DatetimeTZ case, _generate_range will infer the
        #  appropriate timezone from `start` and `end`, so tz does not need
        #  to be passed explicitly.
        return self._generate_range(start=start, end=end, periods=None, freq=self.freq)
    def __add__(self, other):
        other = lib.item_from_zerodim(other)
        if isinstance(other, (ABCSeries, ABCDataFrame)):
            return NotImplemented
        # scalar others
        elif other is NaT:
            result = self._add_nat()
        elif isinstance(other, (Tick, timedelta, np.timedelta64)):
            result = self._add_delta(other)
        elif isinstance(other, DateOffset):
            # specifically _not_ a Tick
            result = self._add_offset(other)
        elif isinstance(other, (datetime, np.datetime64)):
            result = self._add_datetimelike_scalar(other)
        elif lib.is_integer(other):
            # This check must come after the check for np.timedelta64
            # as is_integer returns True for these
            if not is_period_dtype(self):
                maybe_integer_op_deprecated(self)
            result = self._time_shift(other)
        # array-like others
        elif is_timedelta64_dtype(other):
            # TimedeltaIndex, ndarray[timedelta64]
            result = self._add_delta(other)
        elif is_offsetlike(other):
            # Array/Index of DateOffset objects
            result = self._addsub_offset_array(other, operator.add)
        elif is_datetime64_dtype(other) or is_datetime64tz_dtype(other):
            # DatetimeIndex, ndarray[datetime64]
            return self._add_datetime_arraylike(other)
        elif is_integer_dtype(other):
            if not is_period_dtype(self):
                maybe_integer_op_deprecated(self)
            result = self._addsub_int_array(other, operator.add)
        elif is_float_dtype(other):
            # Explicitly catch invalid dtypes
            raise TypeError(
                ""cannot add {dtype}-dtype to {cls}"".format(
                    dtype=other.dtype, cls=type(self).__name__
                )
            )
        elif is_period_dtype(other):
            # if self is a TimedeltaArray and other is a PeriodArray with
            #  a timedelta-like (i.e. Tick) freq, this operation is valid.
            #  Defer to the PeriodArray implementation.
            # In remaining cases, this will end up raising TypeError.
            return NotImplemented
        elif is_extension_array_dtype(other):
            # Categorical op will raise; defer explicitly
            return NotImplemented
        else:  # pragma: no cover
            return NotImplemented
        if is_timedelta64_dtype(result) and isinstance(result, np.ndarray):
            from pandas.core.arrays import TimedeltaArray
            # TODO: infer freq?
            return TimedeltaArray(result)
        return result
    def __radd__(self, other):
        # alias for __add__
        return self.__add__(other)
    def __sub__(self, other):
        other = lib.item_from_zerodim(other)
        if isinstance(other, (ABCSeries, ABCDataFrame)):
            return NotImplemented
        # scalar others
        elif other is NaT:
            result = self._sub_nat()
        elif isinstance(other, (Tick, timedelta, np.timedelta64)):
            result = self._add_delta(-other)
        elif isinstance(other, DateOffset):
            # specifically _not_ a Tick
            result = self._add_offset(-other)
        elif isinstance(other, (datetime, np.datetime64)):
            result = self._sub_datetimelike_scalar(other)
        elif lib.is_integer(other):
            # This check must come after the check for np.timedelta64
            # as is_integer returns True for these
            if not is_period_dtype(self):
                maybe_integer_op_deprecated(self)
            result = self._time_shift(-other)
        elif isinstance(other, Period):
            result = self._sub_period(other)
        # array-like others
        elif is_timedelta64_dtype(other):
            # TimedeltaIndex, ndarray[timedelta64]
            result = self._add_delta(-other)
        elif is_offsetlike(other):
            # Array/Index of DateOffset objects
            result = self._addsub_offset_array(other, operator.sub)
        elif is_datetime64_dtype(other) or is_datetime64tz_dtype(other):
            # DatetimeIndex, ndarray[datetime64]
            result = self._sub_datetime_arraylike(other)
        elif is_period_dtype(other):
            # PeriodIndex
            result = self._sub_period_array(other)","[25, 91]"
"        return self._selected_obj[mask]
    def _reindex_output(self, output):
        """"""
        If we have categorical groupers, then we might want to make sure that
        we have a fully re-indexed output to the levels. This means expanding
        the output space to accommodate all values in the cartesian product of
        our groups, regardless of whether they were observed in the data or
        not. This will expand the output space if there are missing groups.
        The method returns early without modifying the input if the number of
        groupings is less than 2, self.observed == True or none of the groupers
        are categorical.
        Parameters
        ----------
        output: Series or DataFrame
            Object resulting from grouping and applying an operation.
        Returns
        -------
        Series or DataFrame
            Object (potentially) re-indexed to include all possible groups.
        """"""
        groupings = self.grouper.groupings
        if groupings is None:
            return output
        elif len(groupings) == 1:
            return output
        # if we only care about the observed values
        # we are done
        elif self.observed:
            return output
        # reindexing only applies to a Categorical grouper
        elif not any(
            isinstance(ping.grouper, (Categorical, CategoricalIndex))
            for ping in groupings
        ):
            return output
        levels_list = [ping.group_index for ping in groupings]
        index, _ = MultiIndex.from_product(
            levels_list, names=self.grouper.names
        ).sortlevel()
        if self.as_index:
            d = {self.obj._get_axis_name(self.axis): index, ""copy"": False}
            return output.reindex(**d)
        # GH 13204
        # Here, the categorical in-axis groupers, which need to be fully
        # expanded, are columns in `output`. An idea is to do:
        # output = output.set_index(self.grouper.names)
        #                .reindex(index).reset_index()
        # but special care has to be taken because of possible not-in-axis
        # groupers.
        # So, we manually select and drop the in-axis grouper columns,
        # reindex `output`, and then reset the in-axis grouper columns.
        # Select in-axis groupers
        in_axis_grps = (
            (i, ping.name) for (i, ping) in enumerate(groupings) if ping.in_axis
        )
        g_nums, g_names = zip(*in_axis_grps)
        output = output.drop(labels=list(g_names), axis=1)
        # Set a temp index and reindex (possibly expanding)
        output = output.set_index(self.grouper.result_index).reindex(index, copy=False)
        # Reset in-axis grouper columns
        # (using level numbers `g_nums` because level names may not be unique)
        output = output.reset_index(level=g_nums)
        return output.reset_index(drop=True)

GroupBy._add_numeric_operations()

@Appender(GroupBy.__doc__)
def get_groupby(
    obj: NDFrame,
    by=None,
    axis: int = 0,
    level=None,
    grouper: ""Optional[ops.BaseGrouper]"" = None,
    exclusions=None,
    selection=None,
    as_index: bool = True,
    sort: bool = True,
    group_keys: bool = True,
    squeeze: bool = False,
    observed: bool = False,
    mutated: bool = False,
):
    if isinstance(obj, Series):
        from pandas.core.groupby.generic import SeriesGroupBy
        klass = (
            SeriesGroupBy
        )  # type: Union[Type[""SeriesGroupBy""], Type[""DataFrameGroupBy""]]
    elif isinstance(obj, DataFrame):
        from pandas.core.groupby.generic import DataFrameGroupBy
        klass = DataFrameGroupBy
    else:
        raise TypeError(""invalid type: {obj}"".format(obj=obj))
    return klass(
        obj=obj,
        keys=by,
        axis=axis,
        level=level,
        grouper=grouper,
        exclusions=exclusions,
        selection=selection,
        as_index=as_index,
        sort=sort,
        group_keys=group_keys,
        squeeze=squeeze,
        observed=observed,
        mutated=mutated,","[2, 16, 48, 70]"
"        for field in fields:
            fieldmap[field] = user.get(field)
        fieldmap.update({""access_token"": session[""access_token""], ""session_expires"": session.get(""expires"")})
        future.set_result(fieldmap)
    @_auth_return_future
    def facebook_request(self, path, callback, access_token=None,
                         post_args=None, **args):
        """"""Fetches the given relative API path, e.g., ""/btaylor/picture""
        If the request is a POST, ``post_args`` should be provided. Query
        string arguments should be given as keyword arguments.
        An introduction to the Facebook Graph API can be found at
        http://developers.facebook.com/docs/api
        Many methods require an OAuth access token which you can
        obtain through `~OAuth2Mixin.authorize_redirect` and
        `get_authenticated_user`. The user returned through that
        process includes an ``access_token`` attribute that can be
        used to make authenticated requests via this method.
        Example usage:
        ..testcode::
            class MainHandler(tornado.web.RequestHandler,
                              tornado.auth.FacebookGraphMixin):
                @tornado.web.authenticated
                @tornado.gen.coroutine
                def get(self):
                    new_entry = yield self.facebook_request(
                        ""/me/feed"",
                        post_args={""message"": ""I am posting from my Tornado application!""},
                        access_token=self.current_user[""access_token""])
                    if not new_entry:
                        # Call failed; perhaps missing permission?
                        yield self.authorize_redirect()
                        return
                    self.finish(""Posted a message!"")
        .. testoutput::
           :hide:
        The given path is relative to ``self._FACEBOOK_BASE_URL``,
        by default ""https://graph.facebook.com"".
        This method is a wrapper around `OAuth2Mixin.oauth2_request`;
        the only difference is that this method takes a relative path,
        while ``oauth2_request`` takes a complete url.
        .. versionchanged:: 3.1
           Added the ability to override ``self._FACEBOOK_BASE_URL``.
        """"""
        url = self._FACEBOOK_BASE_URL + path
        return self.oauth2_request(url, callback, access_token,
                                   post_args, **args)

def _oauth_signature(consumer_token, method, url, parameters={}, token=None):
    """"""Calculates the HMAC-SHA1 OAuth signature for the given request.
    See http://oauth.net/core/1.0/#signing_process
    """"""
    parts = urlparse.urlparse(url)
    scheme, netloc, path = parts[:3]
    normalized_url = scheme.lower() + ""://"" + netloc.lower() + path
    base_elems = []
    base_elems.append(method.upper())
    base_elems.append(normalized_url)
    base_elems.append(""&"".join(""%s=%s"" % (k, _oauth_escape(str(v)))
                               for k, v in sorted(parameters.items())))
    base_string = ""&"".join(_oauth_escape(e) for e in base_elems)
    key_elems = [escape.utf8(consumer_token[""secret""])]
    key_elems.append(escape.utf8(token[""secret""] if token else """"))
    key = b""&"".join(key_elems)
    hash = hmac.new(key, escape.utf8(base_string), hashlib.sha1)
    return binascii.b2a_base64(hash.digest())[:-1]

def _oauth10a_signature(consumer_token, method, url, parameters={}, token=None):
    """"""Calculates the HMAC-SHA1 OAuth 1.0a signature for the given request.
    See http://oauth.net/core/1.0a/#signing_process
    """"""
    parts = urlparse.urlparse(url)
    scheme, netloc, path = parts[:3]
    normalized_url = scheme.lower() + ""://"" + netloc.lower() + path
    base_elems = []
    base_elems.append(method.upper())
    base_elems.append(normalized_url)
    base_elems.append(""&"".join(""%s=%s"" % (k, _oauth_escape(str(v)))
                               for k, v in sorted(parameters.items())))
    base_string = ""&"".join(_oauth_escape(e) for e in base_elems)
    key_elems = [escape.utf8(urllib_parse.quote(consumer_token[""secret""], safe='~'))]
    key_elems.append(escape.utf8(urllib_parse.quote(token[""secret""], safe='~') if token else """"))
    key = b""&"".join(key_elems)
    hash = hmac.new(key, escape.utf8(base_string), hashlib.sha1)
    return binascii.b2a_base64(hash.digest())[:-1]

def _oauth_escape(val):
    if isinstance(val, unicode_type):
        val = val.encode(""utf-8"")
    return urllib_parse.quote(val, safe=""~"")

def _oauth_parse_response(body):
    # I can't find an officially-defined encoding for oauth responses and
    # have never seen anyone use non-ascii.  Leave the response in a byte
    # string for python 2, and use utf8 on python 3.
    body = escape.native_str(body)
    p = urlparse.parse_qs(body, keep_blank_values=False)
    token = dict(key=p[""oauth_token""][0], secret=p[""oauth_token_secret""][0])
    # Add the extra parameters the Provider included to the token
    special = (""oauth_token"", ""oauth_token_secret"")
    token.update((k, p[k][0]) for k in p if k not in special)","[57, 58]"
"            # Every time we add an attribute to the Worker or Task class, this
            # code needs to be updated
            # Compatibility since 2014-06-02
            for k, v in six.iteritems(self._active_workers):
                if isinstance(v, float):
                    self._active_workers[k] = Worker(worker_id=k, last_active=v)
            # Compatibility since 2015-05-28
            if any(not hasattr(w, 'tasks') for k, w in six.iteritems(self._active_workers)):
                # If you load from an old format where Workers don't contain tasks.
                for k, worker in six.iteritems(self._active_workers):
                    worker.tasks = set()
                for task in six.itervalues(self._tasks):
                    for worker_id in task.workers:
                        self._active_workers[worker_id].tasks.add(task)
            # Compatibility since 2015-04-28
            if any(not hasattr(t, 'disable_hard_timeout') for t in six.itervalues(self._tasks)):
                for t in six.itervalues(self._tasks):
                    t.disable_hard_timeout = None
        else:
            logger.info(""No prior state file exists at %s. Starting with clean slate"", self._state_path)
    def get_active_tasks(self, status=None):
        if status:
            for task in six.itervalues(self._status_tasks[status]):
                yield task
        else:
            for task in six.itervalues(self._tasks):
                yield task
    def get_running_tasks(self):
        return six.itervalues(self._status_tasks[RUNNING])
    def get_pending_tasks(self):
        return itertools.chain.from_iterable(six.itervalues(self._status_tasks[status])
                                             for status in [PENDING, RUNNING])
    def num_pending_tasks(self):
        """"""
        Return how many tasks are PENDING + RUNNING. O(1).
        """"""
        return len(self._status_tasks[PENDING]) + len(self._status_tasks[RUNNING])
    def get_task(self, task_id, default=None, setdefault=None):
        if setdefault:
            task = self._tasks.setdefault(task_id, setdefault)
            self._status_tasks[task.status][task.id] = task
            return task
        else:
            return self._tasks.get(task_id, default)
    def has_task(self, task_id):
        return task_id in self._tasks
    def re_enable(self, task, config=None):
        task.scheduler_disable_time = None
        task.failures.clear()
        if config:
            self.set_status(task, FAILED, config)
            task.failures.clear()
    def set_status(self, task, new_status, config=None):
        if new_status == FAILED:
            assert config is not None
        if new_status == DISABLED and task.status == RUNNING:
            return
        if task.status == DISABLED:
            if new_status == DONE:
                self.re_enable(task)
            # don't allow workers to override a scheduler disable
            elif task.scheduler_disable_time is not None:
                return
        if new_status == FAILED and task.can_disable():
            task.add_failure()
            if task.has_excessive_failures():
                task.scheduler_disable_time = time.time()
                new_status = DISABLED
                notifications.send_error_email(
                    'Luigi Scheduler: DISABLED {task} due to excessive failures'.format(task=task.id),
                    '{task} failed {failures} times in the last {window} seconds, so it is being '
                    'disabled for {persist} seconds'.format(
                        failures=config.disable_failures,
                        task=task.id,
                        window=config.disable_window,
                        persist=config.disable_persist,
                    ))
        elif new_status == DISABLED:
            task.scheduler_disable_time = None
        self._status_tasks[task.status].pop(task.id)
        self._status_tasks[new_status][task.id] = task
        task.status = new_status
    def fail_dead_worker_task(self, task, config, assistants):
        # If a running worker disconnects, tag all its jobs as FAILED and subject it to the same retry logic
        if task.status == RUNNING and task.worker_running and task.worker_running not in task.stakeholders | assistants:
            logger.info(""Task %r is marked as running by disconnected worker %r -> marking as ""
                        ""FAILED with retry delay of %rs"", task.id, task.worker_running,
                        config.retry_delay)
            task.worker_running = None
            self.set_status(task, FAILED, config)
            task.retry = time.time() + config.retry_delay
    def prune(self, task, config):
        remove = False
        # Mark tasks with no remaining active stakeholders for deletion
        if not task.stakeholders:
            if task.remove is None:
                logger.info(""Task %r has stakeholders %r but none remain connected -> will remove ""
                            ""task in %s seconds"", task.id, task.stakeholders, config.remove_delay)
                task.remove = time.time() + config.remove_delay
        # Re-enable task after the disable time expires
        if task.status == DISABLED and task.scheduler_disable_time is not None:
            if time.time() - fix_time(task.scheduler_disable_time) > config.disable_persist:
                self.re_enable(task, config)
        # Remove tasks that have no stakeholders
        if task.remove and time.time() > task.remove:
            logger.info(""Removing task %r (no connected stakeholders)"", task.id)",[78]
"def dict_get(d, key_or_keys, default=None, skip_false_values=True):
    if isinstance(key_or_keys, (list, tuple)):
        for key in key_or_keys:
            if key not in d or d[key] is None or skip_false_values and not d[key]:
                continue
            return d[key]
        return default
    return d.get(key_or_keys, default)

def try_get(src, getter, expected_type=None):
    if not isinstance(getter, (list, tuple)):
        getter = [getter]
    for get in getter:
        try:
            v = get(src)
        except (AttributeError, KeyError, TypeError, IndexError):
            pass
        else:
            if expected_type is None or isinstance(v, expected_type):
                return v

def encode_compat_str(string, encoding=preferredencoding(), errors='strict'):
    return string if isinstance(string, compat_str) else compat_str(string, encoding, errors)

US_RATINGS = {
    'G': 0,
    'PG': 10,
    'PG-13': 13,
    'R': 16,
    'NC': 18,
}

TV_PARENTAL_GUIDELINES = {
    'TV-Y': 0,
    'TV-Y7': 7,
    'TV-G': 0,
    'TV-PG': 0,
    'TV-14': 14,
    'TV-MA': 17,
}

def parse_age_limit(s):
    if type(s) == int:
        return s if 0 <= s <= 21 else None
    if not isinstance(s, compat_basestring):
        return None
    m = re.match(r'^(?P<age>\d{1,2})\+?$', s)
    if m:
        return int(m.group('age'))
    if s in US_RATINGS:
        return US_RATINGS[s]
    return TV_PARENTAL_GUIDELINES.get(s)

def strip_jsonp(code):
    return re.sub(
        r'''(?sx)^
            (?:window\.)?(?P<func_name>[a-zA-Z0-9_.$]+)
            (?:\s*&&\s*(?P=func_name))?
            \s*\(\s*(?P<callback_data>.*)\);?
            \s*?(?://[^\n]*)*$''',
        r'\g<callback_data>', code)

def js_to_json(code):
    COMMENT_RE = r'/\*(?:(?!\*/).)*?\*/|//[^\n]*'
    SKIP_RE = r'\s*(?:{comment})?\s*'.format(comment=COMMENT_RE)
    INTEGER_TABLE = (
        (r'(?s)^(0[xX][0-9a-fA-F]+){skip}:?$'.format(skip=SKIP_RE), 16),
        (r'(?s)^(0+[0-7]+){skip}:?$'.format(skip=SKIP_RE), 8),
    )
    def fix_kv(m):
        v = m.group(0)
        if v in ('true', 'false', 'null'):
            return v
        elif v.startswith('/*') or v.startswith('//') or v == ',':
            return """"
        if v[0] in (""'"", '""'):
            v = re.sub(r'(?s)\\.|""', lambda m: {
                '""': '\\""',
                ""\\'"": ""'"",
                '\\\n': '',
                '\\x': '\\u00',
            }.get(m.group(0), m.group(0)), v[1:-1])
        for regex, base in INTEGER_TABLE:
            im = re.match(regex, v)
            if im:
                i = int(im.group(1), base)
                return '""%d"":' % i if v.endswith(':') else '%d' % i
        return '""%s""' % v
    return re.sub(r'''(?sx)
        ""(?:[^""\\]*(?:\\\\|\\['""nurtbfx/\n]))*[^""\\]*""|
        '(?:[^'\\]*(?:\\\\|\\['""nurtbfx/\n]))*[^'\\]*'|
        {comment}|,(?={skip}[\]}}])|
        [a-zA-Z_][.a-zA-Z_0-9]*|
        \b(?:0[xX][0-9a-fA-F]+|0+[0-7]+)(?:{skip}:)?|
        [0-9]+(?={skip}:)
        '''.format(comment=COMMENT_RE, skip=SKIP_RE), fix_kv, code)

def qualities(quality_ids):
    """""" Get a numeric quality value out of a list of possible values """"""
    def q(qid):
        try:
            return quality_ids.index(qid)
        except ValueError:
            return -1
    return q

DEFAULT_OUTTMPL = '%(title)s-%(id)s.%(ext)s'

def limit_length(s, length):
    """""" Add ellipses to overly long strings """"""
    if s is None:
        return None",[104]
"    }
    operator_rex = re.compile(r'''(?x)\s*
        (?P<op>%s)\s*(?P<key>[a-z_]+)
        \s*$
        ''' % '|'.join(map(re.escape, UNARY_OPERATORS.keys())))
    m = operator_rex.search(filter_part)
    if m:
        op = UNARY_OPERATORS[m.group('op')]
        actual_value = dct.get(m.group('key'))
        return op(actual_value)
    raise ValueError('Invalid filter part %r' % filter_part)

def match_str(filter_str, dct):
    """""" Filter a dictionary with a simple string syntax. Returns True (=passes filter) or false """"""
    return all(
        _match_one(filter_part, dct) for filter_part in filter_str.split('&'))

def match_filter_func(filter_str):
    def _match_func(info_dict):
        if match_str(filter_str, info_dict):
            return None
        else:
            video_title = info_dict.get('title', info_dict.get('id', 'video'))
            return '%s does not pass filter %s, skipping ..' % (video_title, filter_str)
    return _match_func

def parse_dfxp_time_expr(time_expr):
    if not time_expr:
        return 0.0
    mobj = re.match(r'^(?P<time_offset>\d+(?:\.\d+)?)s?$', time_expr)
    if mobj:
        return float(mobj.group('time_offset'))
    mobj = re.match(r'^(\d+):(\d\d):(\d\d(?:\.\d+)?)$', time_expr)
    if mobj:
        return 3600 * int(mobj.group(1)) + 60 * int(mobj.group(2)) + float(mobj.group(3))

def srt_subtitles_timecode(seconds):
    return '%02d:%02d:%02d,%03d' % (seconds / 3600, (seconds % 3600) / 60, seconds % 60, (seconds % 1) * 1000)

def dfxp2srt(dfxp_data):
    _x = functools.partial(xpath_with_ns, ns_map={
        'ttml': 'http://www.w3.org/ns/ttml',
        'ttaf1': 'http://www.w3.org/2006/10/ttaf1',
    })
    def parse_node(node):
        str_or_empty = functools.partial(str_or_none, default='')
        out = str_or_empty(node.text)
        for child in node:
            if child.tag in (_x('ttml:br'), _x('ttaf1:br'), 'br'):
                out += '\n' + str_or_empty(child.tail)
            elif child.tag in (_x('ttml:span'), _x('ttaf1:span'), 'span'):
                out += str_or_empty(parse_node(child))
            else:
                out += str_or_empty(xml.etree.ElementTree.tostring(child))
        return out
    dfxp = compat_etree_fromstring(dfxp_data.encode('utf-8'))
    out = []
    paras = dfxp.findall(_x('.//ttml:p')) or dfxp.findall(_x('.//ttaf1:p')) or dfxp.findall('.//p')
    if not paras:
        raise ValueError('Invalid dfxp/TTML subtitle')
    for para, index in zip(paras, itertools.count(1)):
        begin_time = parse_dfxp_time_expr(para.attrib['begin'])
        end_time = parse_dfxp_time_expr(para.attrib.get('end'))
        if not end_time:
            end_time = begin_time + parse_dfxp_time_expr(para.attrib['dur'])
        out.append('%d\n%s --> %s\n%s\n\n' % (
            index,
            srt_subtitles_timecode(begin_time),
            srt_subtitles_timecode(end_time),
            parse_node(para)))
    return ''.join(out)

def cli_option(params, command_option, param):
    param = params.get(param)
    return [command_option, param] if param is not None else []

def cli_bool_option(params, command_option, param, true_value='true', false_value='false', separator=None):
    param = params.get(param)
    assert isinstance(param, bool)
    if separator:
        return [command_option + separator + (true_value if param else false_value)]
    return [command_option, true_value if param else false_value]

def cli_valueless_option(params, command_option, param, expected_value=True):
    param = params.get(param)
    return [command_option] if param == expected_value else []

def cli_configuration_args(params, param, default=[]):
    ex_args = params.get(param)
    if ex_args is None:
        return default
    assert isinstance(ex_args, list)
    return ex_args

class ISO639Utils(object):
    # See http://www.loc.gov/standards/iso639-2/ISO-639-2_utf-8.txt
    _lang_map = {
        'aa': 'aar',
        'ab': 'abk',
        'ae': 'ave',
        'af': 'afr',
        'ak': 'aka',
        'am': 'amh',
        'an': 'arg',
        'ar': 'ara',","[33, 77, 80]"
"            for layer_name, tensor in embeddings.items():
                embedding = config.embeddings.add()
                embedding.tensor_name = tensor.name
                if layer_name in embeddings_metadata:
                    embedding.metadata_path = embeddings_metadata[layer_name]
            projector.visualize_embeddings(self.writer, config)
    def on_epoch_end(self, epoch, logs=None):
        logs = logs or {}
        if not self.validation_data and self.histogram_freq:
            raise ValueError('If printing histograms, validation_data must be '
                             'provided, and cannot be a generator.')
        if self.validation_data and self.histogram_freq:
            if epoch % self.histogram_freq == 0:
                val_data = self.validation_data
                tensors = (self.model.inputs +
                           self.model.targets +
                           self.model.sample_weights)
                if self.model.uses_learning_phase:
                    tensors += [K.learning_phase()]
                assert len(val_data) == len(tensors)
                val_size = val_data[0].shape[0]
                i = 0
                while i < val_size:
                    step = min(self.batch_size, val_size - i)
                    if self.model.uses_learning_phase:
                        # do not slice the learning phase
                        batch_val = [x[i:i + step] for x in val_data[:-1]]
                        batch_val.append(val_data[-1])
                    else:
                        batch_val = [x[i:i + step] for x in val_data]
                    assert len(batch_val) == len(tensors)
                    feed_dict = dict(zip(tensors, batch_val))
                    result = self.sess.run([self.merged], feed_dict=feed_dict)
                    summary_str = result[0]
                    self.writer.add_summary(summary_str, epoch)
                    i += self.batch_size
        if self.embeddings_freq and self.embeddings_ckpt_path:
            if epoch % self.embeddings_freq == 0:
                self.saver.save(self.sess,
                                self.embeddings_ckpt_path,
                                epoch)
        for name, value in logs.items():
            if name in ['batch', 'size']:
                continue
            summary = tf.Summary()
            summary_value = summary.value.add()
            summary_value.simple_value = value.item()
            summary_value.tag = name
            self.writer.add_summary(summary, epoch)
        self.writer.flush()
    def on_train_end(self, _):
        self.writer.close()

class ReduceLROnPlateau(Callback):
    """"""Reduce learning rate when a metric has stopped improving.
    Models often benefit from reducing the learning rate by a factor
    of 2-10 once learning stagnates. This callback monitors a
    quantity and if no improvement is seen for a 'patience' number
    of epochs, the learning rate is reduced.
    # Example
    ```python
    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,
                                  patience=5, min_lr=0.001)
    model.fit(X_train, Y_train, callbacks=[reduce_lr])
    ```
    # Arguments
        monitor: quantity to be monitored.
        factor: factor by which the learning rate will
            be reduced. new_lr = lr * factor
        patience: number of epochs with no improvement
            after which learning rate will be reduced.
        verbose: int. 0: quiet, 1: update messages.
        mode: one of {auto, min, max}. In `min` mode,
            lr will be reduced when the quantity
            monitored has stopped decreasing; in `max`
            mode it will be reduced when the quantity
            monitored has stopped increasing; in `auto`
            mode, the direction is automatically inferred
            from the name of the monitored quantity.
        epsilon: threshold for measuring the new optimum,
            to only focus on significant changes.
        cooldown: number of epochs to wait before resuming
            normal operation after lr has been reduced.
        min_lr: lower bound on the learning rate.
    """"""
    def __init__(self, monitor='val_loss', factor=0.1, patience=10,
                 verbose=0, mode='auto', epsilon=1e-4, cooldown=0, min_lr=0):
        super(ReduceLROnPlateau, self).__init__()
        self.monitor = monitor
        if factor >= 1.0:
            raise ValueError('ReduceLROnPlateau '
                             'does not support a factor >= 1.0.')
        self.factor = factor
        self.min_lr = min_lr
        self.epsilon = epsilon
        self.patience = patience
        self.verbose = verbose
        self.cooldown = cooldown
        self.cooldown_counter = 0  # Cooldown counter.
        self.wait = 0
        self.best = 0
        self.mode = mode
        self.monitor_op = None
        self._reset()
    def _reset(self):
        """"""Resets wait counter and cooldown counter.
        """"""
        if self.mode not in ['auto', 'min', 'max']:
            warnings.warn('Learning Rate Plateau Reducing mode %s is unknown, '","[94, 102, 111]"
"            )
        if convert:
            blocks = [b.convert(numeric=False, copy=not inplace) for b in blocks]
        return blocks
    def _replace_single(self, *args, **kwargs):
        """""" no-op on a non-ObjectBlock """"""
        return self if kwargs[""inplace""] else self.copy()
    def setitem(self, indexer, value):
        """"""
        Set the value inplace, returning a a maybe different typed block.
        Parameters
        ----------
        indexer : tuple, list-like, array-like, slice
            The subset of self.values to set
        value : object
            The value being set
        Returns
        -------
        Block
        Notes
        -----
        `indexer` is a direct slice/positional indexer. `value` must
        be a compatible shape.
        """"""
        transpose = self.ndim == 2
        # coerce None values, if appropriate
        if value is None:
            if self.is_numeric:
                value = np.nan
        # coerce if block dtype can store value
        values = self.values
        if self._can_hold_element(value):
            # We only get here for non-Extension Blocks, so _try_coerce_args
            #  is only relevant for DatetimeBlock and TimedeltaBlock
            if lib.is_scalar(value):
                value = convert_scalar(values, value)
        else:
            # current dtype cannot store value, coerce to common dtype
            find_dtype = False
            if hasattr(value, ""dtype""):
                dtype = value.dtype
                find_dtype = True
            elif lib.is_scalar(value) and not isna(value):
                dtype, _ = infer_dtype_from_scalar(value, pandas_dtype=True)
                find_dtype = True
            if find_dtype:
                dtype = find_common_type([values.dtype, dtype])
                if not is_dtype_equal(self.dtype, dtype):
                    b = self.astype(dtype)
                    return b.setitem(indexer, value)
        # value must be storeable at this moment
        if is_extension_array_dtype(getattr(value, ""dtype"", None)):
            # We need to be careful not to allow through strings that
            #  can be parsed to EADtypes
            arr_value = value
        else:
            arr_value = np.array(value)
        # cast the values to a type that can hold nan (if necessary)
        if not self._can_hold_element(value):
            dtype, _ = maybe_promote(arr_value.dtype)
            values = values.astype(dtype)
        if transpose:
            values = values.T
        # length checking
        check_setitem_lengths(indexer, value, values)
        if is_empty_indexer(indexer, arr_value):
            # GH#8669 empty indexers
            pass
        elif is_scalar_indexer(indexer, arr_value):
            # setting a single element for each dim and with a rhs that could
            #  be e.g. a list; see GH#6043
            values[indexer] = value
        # if we are an exact match (ex-broadcasting),
        # then use the resultant dtype
        elif (
            len(arr_value.shape)
            and arr_value.shape[0] == values.shape[0]
            and arr_value.size == values.size
        ):
            values[indexer] = value
            try:
                values = values.astype(arr_value.dtype)
            except ValueError:
                pass
        # set
        else:
            values[indexer] = value
        if transpose:
            values = values.T
        block = self.make_block(values)
        return block
    def putmask(self, mask, new, align=True, inplace=False, axis=0, transpose=False):
        """""" putmask the data to the block; it is possible that we may create a
        new dtype of block
        return the resulting block(s)
        Parameters
        ----------
        mask  : the condition to respect
        new : a ndarray/object
        align : boolean, perform alignment on other/cond, default is True
        inplace : perform inplace modification, default is False
        axis : int
        transpose : boolean
            Set to True if self is stored with axes reversed","[80, 93, 94, 95]"
"""""""
This module implements a class which returns the appropriate Response class
based on different criteria.
""""""
from __future__ import absolute_import
from mimetypes import MimeTypes
from pkgutil import get_data
from io import StringIO
import six
from scrapy.http import Response
from scrapy.utils.misc import load_object
from scrapy.utils.python import isbinarytext, to_bytes, to_native_str

class ResponseTypes(object):
    CLASSES = {
        'text/html': 'scrapy.http.HtmlResponse',
        'application/atom+xml': 'scrapy.http.XmlResponse',
        'application/rdf+xml': 'scrapy.http.XmlResponse',
        'application/rss+xml': 'scrapy.http.XmlResponse',
        'application/xhtml+xml': 'scrapy.http.HtmlResponse',
        'application/vnd.wap.xhtml+xml': 'scrapy.http.HtmlResponse',
        'application/xml': 'scrapy.http.XmlResponse',
        'application/json': 'scrapy.http.TextResponse',
        'application/x-json': 'scrapy.http.TextResponse',
        'application/javascript': 'scrapy.http.TextResponse',
        'application/x-javascript': 'scrapy.http.TextResponse',
        'text/xml': 'scrapy.http.XmlResponse',
        'text/*': 'scrapy.http.TextResponse',
    }
    def __init__(self):
        self.classes = {}
        self.mimetypes = MimeTypes()
        mimedata = get_data('scrapy', 'mime.types').decode('utf8')
        self.mimetypes.readfp(StringIO(mimedata))
        for mimetype, cls in six.iteritems(self.CLASSES):
            self.classes[mimetype] = load_object(cls)
    def from_mimetype(self, mimetype):
        """"""Return the most appropriate Response class for the given mimetype""""""
        if mimetype is None:
            return Response
        elif mimetype in self.classes:
            return self.classes[mimetype]
        else:
            basetype = ""%s/*"" % mimetype.split('/')[0]
            return self.classes.get(basetype, Response)
    def from_content_type(self, content_type, content_encoding=None):
        """"""Return the most appropriate Response class from an HTTP Content-Type
        header """"""
        if content_encoding:
            return Response
        mimetype = to_native_str(content_type).split(';')[0].strip().lower()
        return self.from_mimetype(mimetype)
    def from_content_disposition(self, content_disposition):
        try:
            filename = to_native_str(content_disposition).split(';')[1].split('=')[1]
            filename = filename.strip('""\'')
            return self.from_filename(filename)
        except IndexError:
            return Response
    def from_headers(self, headers):
        """"""Return the most appropriate Response class by looking at the HTTP
        headers""""""
        cls = Response
        if b'Content-Type' in headers:
            cls = self.from_content_type(
                content_type=headers[b'Content-type'],
                content_encoding=headers.get(b'Content-Encoding')
            )
        if cls is Response and b'Content-Disposition' in headers:
            cls = self.from_content_disposition(headers[b'Content-Disposition'])
        return cls
    def from_filename(self, filename):
        """"""Return the most appropriate Response class from a file name""""""
        mimetype, encoding = self.mimetypes.guess_type(filename)
        if mimetype and not encoding:
            return self.from_mimetype(mimetype)
        else:
            return Response
    def from_body(self, body):
        """"""Try to guess the appropriate response based on the body content.
        This method is a bit magic and could be improved in the future, but
        it's not meant to be used except for special cases where response types
        cannot be guess using more straightforward methods.""""""
        chunk = body[:5000]
        chunk = to_bytes(chunk)
        if isbinarytext(chunk):
            return self.from_mimetype('application/octet-stream')
        elif b""<html>"" in chunk.lower():
            return self.from_mimetype('text/html')
        elif b""<?xml"" in chunk.lower():
            return self.from_mimetype('text/xml')
        else:
            return self.from_mimetype('text')
    def from_args(self, headers=None, url=None, filename=None, body=None):
        """"""Guess the most appropriate Response class based on
        the given arguments.""""""
        cls = Response
        if headers is not None:
            cls = self.from_headers(headers)
        if cls is Response and url is not None:
            cls = self.from_filename(url)
        if cls is Response and filename is not None:
            cls = self.from_filename(filename)
        if cls is Response and body is not None:
            cls = self.from_body(body)
        return cls
responsetypes = ResponseTypes()",[61]
"def _get_renderer(figure, print_method=None, *, draw_disabled=False):
    """"""
    Get the renderer that would be used to save a `~.Figure`, and cache it on
    the figure.
    If *draw_disabled* is True, additionally replace drawing methods on
    *renderer* by no-ops.  This is used by the tight-bbox-saving renderer,
    which needs to walk through the artist tree to compute the tight-bbox, but
    for which the output file may be closed early.
    """"""
    # This is implemented by triggering a draw, then immediately jumping out of
    # Figure.draw() by raising an exception.
    class Done(Exception):
        pass
    def _draw(renderer): raise Done(renderer)
    with cbook._setattr_cm(figure, draw=_draw):
        if print_method is None:
            fmt = figure.canvas.get_default_filetype()
            print_method = getattr(figure.canvas, f""print_{fmt}"")
        try:
            print_method(io.BytesIO(), dpi=figure.dpi)
        except Done as exc:
            renderer, = figure._cachedRenderer, = exc.args
    if draw_disabled:
        for meth_name in dir(RendererBase):
            if (meth_name.startswith(""draw_"")
                    or meth_name in [""open_group"", ""close_group""]):
                setattr(renderer, meth_name, lambda *args, **kwargs: None)
    return renderer

def _is_non_interactive_terminal_ipython(ip):
    """"""
    Return whether we are in a a terminal IPython, but non interactive.
    When in _terminal_ IPython, ip.parent will have and `interact` attribute,
    if this attribute is False we do not setup eventloop integration as the
    user will _not_ interact with IPython. In all other case (ZMQKernel, or is
    interactive), we do.
    """"""
    return (hasattr(ip, 'parent')
            and (ip.parent is not None)
            and getattr(ip.parent, 'interact', None) is False)

class FigureCanvasBase:
    """"""
    The canvas the figure renders into.
    Attributes
    ----------
    figure : `matplotlib.figure.Figure`
        A high-level figure instance.
    """"""
    # Set to one of {""qt5"", ""qt4"", ""gtk3"", ""wx"", ""tk"", ""macosx""} if an
    # interactive framework is required, or None otherwise.
    required_interactive_framework = None
    events = [
        'resize_event',
        'draw_event',
        'key_press_event',
        'key_release_event',
        'button_press_event',
        'button_release_event',
        'scroll_event',
        'motion_notify_event',
        'pick_event',
        'figure_enter_event',
        'figure_leave_event',
        'axes_enter_event',
        'axes_leave_event',
        'close_event'
    ]
    fixed_dpi = None
    filetypes = _default_filetypes
    @cbook._classproperty
    def supports_blit(cls):
        return (hasattr(cls, ""copy_from_bbox"")
                and hasattr(cls, ""restore_region""))
    def __init__(self, figure):
        self._fix_ipython_backend2gui()
        self._is_idle_drawing = True
        self._is_saving = False
        figure.set_canvas(self)
        self.figure = figure
        self.manager = None
        # a dictionary from event name to a dictionary that maps cid->func
        self.callbacks = cbook.CallbackRegistry()
        self.widgetlock = widgets.LockDraw()
        self._button = None  # the button pressed
        self._key = None  # the key pressed
        self._lastx, self._lasty = None, None
        self.button_pick_id = self.mpl_connect('button_press_event', self.pick)
        self.scroll_pick_id = self.mpl_connect('scroll_event', self.pick)
        self.mouse_grabber = None  # the axes currently grabbing mouse
        self.toolbar = None  # NavigationToolbar2 will set me
        self._is_idle_drawing = False
    @classmethod
    @functools.lru_cache()
    def _fix_ipython_backend2gui(cls):
        # Fix hard-coded module -> toolkit mapping in IPython (used for
        # `ipython --auto`).  This cannot be done at import time due to
        # ordering issues, so we do it when creating a canvas, and should only
        # be done once per class (hence the `lru_cache(1)`).
        if ""IPython"" not in sys.modules:
            return
        import IPython
        ip = IPython.get_ipython()
        if not ip:
            return
        from IPython.core import pylabtools as pt
        if (not hasattr(pt, ""backend2gui"")
                or not hasattr(ip, ""enable_matplotlib"")):
            # In case we ever move the patch to IPython and remove these APIs,","[1, 6, 8, 9, 28, 29, 30, 31, 32, 33]"
"        0  0
        1  1
        2  2
        3  3
        4  4
        """"""
        if isinstance(other, (Series, dict)):
            if isinstance(other, dict):
                other = Series(other)
            if other.name is None and not ignore_index:
                raise TypeError(
                    ""Can only append a Series if ignore_index=True""
                    "" or if the Series has a name""
                )
            if other.name is None:
                index = None
            else:
                # other must have the same index name as self, otherwise
                # index name will be reset
                index = Index([other.name], name=self.index.name)
            idx_diff = other.index.difference(self.columns)
            try:
                combined_columns = self.columns.append(idx_diff)
            except TypeError:
                combined_columns = self.columns.astype(object).append(idx_diff)
            other = other.reindex(combined_columns, copy=False)
            other = DataFrame(
                other.values.reshape((1, len(other))),
                index=index,
                columns=combined_columns,
            )
            other = other._convert(datetime=True, timedelta=True)
            if not self.columns.equals(combined_columns):
                self = self.reindex(columns=combined_columns)
        elif isinstance(other, list) and not isinstance(other[0], DataFrame):
            other = DataFrame(other)
            if (self.columns.get_indexer(other.columns) >= 0).all():
                other = other.reindex(columns=self.columns)
        from pandas.core.reshape.concat import concat
        if isinstance(other, (list, tuple)):
            to_concat = [self] + other
        else:
            to_concat = [self, other]
        return concat(
            to_concat,
            ignore_index=ignore_index,
            verify_integrity=verify_integrity,
            sort=sort,
        )
    def join(self, other, on=None, how=""left"", lsuffix="""", rsuffix="""", sort=False):
        """"""
        Join columns of another DataFrame.
        Join columns with `other` DataFrame either on index or on a key
        column. Efficiently join multiple DataFrame objects by index at once by
        passing a list.
        Parameters
        ----------
        other : DataFrame, Series, or list of DataFrame
            Index should be similar to one of the columns in this one. If a
            Series is passed, its name attribute must be set, and that will be
            used as the column name in the resulting joined DataFrame.
        on : str, list of str, or array-like, optional
            Column or index level name(s) in the caller to join on the index
            in `other`, otherwise joins index-on-index. If multiple
            values given, the `other` DataFrame must have a MultiIndex. Can
            pass an array as the join key if it is not already contained in
            the calling DataFrame. Like an Excel VLOOKUP operation.
        how : {'left', 'right', 'outer', 'inner'}, default 'left'
            How to handle the operation of the two objects.
            * left: use calling frame's index (or column if on is specified)
            * right: use `other`'s index.
            * outer: form union of calling frame's index (or column if on is
              specified) with `other`'s index, and sort it.
              lexicographically.
            * inner: form intersection of calling frame's index (or column if
              on is specified) with `other`'s index, preserving the order
              of the calling's one.
        lsuffix : str, default ''
            Suffix to use from left frame's overlapping columns.
        rsuffix : str, default ''
            Suffix to use from right frame's overlapping columns.
        sort : bool, default False
            Order result DataFrame lexicographically by the join key. If False,
            the order of the join key depends on the join type (how keyword).
        Returns
        -------
        DataFrame
            A dataframe containing columns from both the caller and `other`.
        See Also
        --------
        DataFrame.merge : For column(s)-on-columns(s) operations.
        Notes
        -----
        Parameters `on`, `lsuffix`, and `rsuffix` are not supported when
        passing a list of `DataFrame` objects.
        Support for specifying index levels as the `on` parameter was added
        in version 0.23.0.
        Examples
        --------
        >>> df = pd.DataFrame({'key': ['K0', 'K1', 'K2', 'K3', 'K4', 'K5'],
        ...                    'A': ['A0', 'A1', 'A2', 'A3', 'A4', 'A5']})
        >>> df
          key   A
        0  K0  A0
        1  K1  A1
        2  K2  A2
        3  K3  A3
        4  K4  A4
        5  K5  A5
        >>> other = pd.DataFrame({'key': ['K0', 'K1', 'K2'],
        ...                       'B': ['B0', 'B1', 'B2']})
","[36, 37, 38, 39]"
"        4D tensor with shape:
        `(batch, channels, rows, cols)`
        if `data_format` is `""channels_first""`
        or 4D tensor with shape:
        `(batch, rows, cols, channels)`
        if `data_format` is `""channels_last""`.
    # Output shape
        4D tensor with shape:
        `(batch, filters, new_rows, new_cols)`
        if `data_format` is `""channels_first""`
        or 4D tensor with shape:
        `(batch, new_rows, new_cols, filters)`
        if `data_format` is `""channels_last""`.
        `rows` and `cols` values might have changed due to padding.
        If `output_padding` is specified:
        ```
        new_rows = (rows - 1) * strides[0] + kernel_size[0] - 2 * padding[0] + output_padding[0]
        new_cols = (cols - 1) * strides[1] + kernel_size[1] - 2 * padding[1] + output_padding[1]
        ```
    # References
        - [A guide to convolution arithmetic for deep learning](https://arxiv.org/abs/1603.07285v1)
        - [Deconvolutional Networks](http://www.matthewzeiler.com/pubs/cvpr2010/cvpr2010.pdf)
    """"""
    @interfaces.legacy_deconv2d_support
    def __init__(self, filters,
                 kernel_size,
                 strides=(1, 1),
                 padding='valid',
                 output_padding=None,
                 data_format=None,
                 activation=None,
                 use_bias=True,
                 kernel_initializer='glorot_uniform',
                 bias_initializer='zeros',
                 kernel_regularizer=None,
                 bias_regularizer=None,
                 activity_regularizer=None,
                 kernel_constraint=None,
                 bias_constraint=None,
                 **kwargs):
        super(Conv2DTranspose, self).__init__(
            filters,
            kernel_size,
            strides=strides,
            padding=padding,
            data_format=data_format,
            activation=activation,
            use_bias=use_bias,
            kernel_initializer=kernel_initializer,
            bias_initializer=bias_initializer,
            kernel_regularizer=kernel_regularizer,
            bias_regularizer=bias_regularizer,
            activity_regularizer=activity_regularizer,
            kernel_constraint=kernel_constraint,
            bias_constraint=bias_constraint,
            **kwargs)
        self.output_padding = output_padding
        if self.output_padding is not None:
            self.output_padding = conv_utils.normalize_tuple(
                self.output_padding, 2, 'output_padding')
            for stride, out_pad in zip(self.strides, self.output_padding):
                if out_pad >= stride:
                    raise ValueError('Stride ' + str(self.strides) + ' must be '
                                     'greater than output padding ' +
                                     str(self.output_padding))
    def build(self, input_shape):
        if len(input_shape) != 4:
            raise ValueError('Inputs should have rank ' +
                             str(4) +
                             '; Received input shape:', str(input_shape))
        if self.data_format == 'channels_first':
            channel_axis = 1
        else:
            channel_axis = -1
        if input_shape[channel_axis] is None:
            raise ValueError('The channel dimension of the inputs '
                             'should be defined. Found `None`.')
        input_dim = input_shape[channel_axis]
        kernel_shape = self.kernel_size + (self.filters, input_dim)
        self.kernel = self.add_weight(shape=kernel_shape,
                                      initializer=self.kernel_initializer,
                                      name='kernel',
                                      regularizer=self.kernel_regularizer,
                                      constraint=self.kernel_constraint)
        if self.use_bias:
            self.bias = self.add_weight(shape=(self.filters,),
                                        initializer=self.bias_initializer,
                                        name='bias',
                                        regularizer=self.bias_regularizer,
                                        constraint=self.bias_constraint)
        else:
            self.bias = None
        # Set input spec.
        self.input_spec = InputSpec(ndim=4, axes={channel_axis: input_dim})
        self.built = True
    def call(self, inputs):
        input_shape = K.shape(inputs)
        batch_size = input_shape[0]
        if self.data_format == 'channels_first':
            h_axis, w_axis = 2, 3
        else:
            h_axis, w_axis = 1, 2
        height, width = input_shape[h_axis], input_shape[w_axis]
        kernel_h, kernel_w = self.kernel_size
        stride_h, stride_w = self.strides
        if self.output_padding is None:
            out_pad_h = out_pad_w = None
        else:
            out_pad_h, out_pad_w = self.output_padding
        # Infer the dynamic output shape:
        out_height = conv_utils.deconv_length(height,
                                              stride_h, kernel_h,
                                              self.padding,
                                              out_pad_h)
        out_width = conv_utils.deconv_length(width,
                                             stride_w, kernel_w,
                                             self.padding,","[123, 127]"
"            rb = [blk if inplace else blk.copy()]
            for i, (s, d) in enumerate(zip(src_list, dest_list)):
                # TODO: assert/validate that `d` is always a scalar?
                new_rb = []
                for b in rb:
                    m = masks[i][b.mgr_locs.indexer]
                    convert = i == src_len
                    result = b._replace_coerce(
                        mask=m,
                        to_replace=s,
                        value=d,
                        inplace=inplace,
                        convert=convert,
                        regex=regex,
                    )
                    if m.any():
                        new_rb = _extend_blocks(result, new_rb)
                    else:
                        new_rb.append(b)
                rb = new_rb
            result_blocks.extend(rb)
        bm = self.__class__(result_blocks, self.axes)
        bm._consolidate_inplace()
        return bm
    def is_consolidated(self):
        """"""
        Return True if more than one block with the same dtype
        """"""
        if not self._known_consolidated:
            self._consolidate_check()
        return self._is_consolidated
    def _consolidate_check(self):
        ftypes = [blk.ftype for blk in self.blocks]
        self._is_consolidated = len(ftypes) == len(set(ftypes))
        self._known_consolidated = True
    @property
    def is_mixed_type(self):
        # Warning, consolidation needs to get checked upstairs
        self._consolidate_inplace()
        return len(self.blocks) > 1
    @property
    def is_numeric_mixed_type(self):
        # Warning, consolidation needs to get checked upstairs
        self._consolidate_inplace()
        return all(block.is_numeric for block in self.blocks)
    @property
    def is_datelike_mixed_type(self):
        # Warning, consolidation needs to get checked upstairs
        self._consolidate_inplace()
        return any(block.is_datelike for block in self.blocks)
    @property
    def any_extension_types(self):
        """"""Whether any of the blocks in this manager are extension blocks""""""
        return any(block.is_extension for block in self.blocks)
    @property
    def is_view(self):
        """""" return a boolean if we are a single block and are a view """"""
        if len(self.blocks) == 1:
            return self.blocks[0].is_view
        # It is technically possible to figure out which blocks are views
        # e.g. [ b.values.base is not None for b in self.blocks ]
        # but then we have the case of possibly some blocks being a view
        # and some blocks not. setting in theory is possible on the non-view
        # blocks w/o causing a SettingWithCopy raise/warn. But this is a bit
        # complicated
        return False
    def get_bool_data(self, copy=False):
        """"""
        Parameters
        ----------
        copy : boolean, default False
            Whether to copy the blocks
        """"""
        self._consolidate_inplace()
        return self.combine([b for b in self.blocks if b.is_bool], copy)
    def get_numeric_data(self, copy=False):
        """"""
        Parameters
        ----------
        copy : boolean, default False
            Whether to copy the blocks
        """"""
        self._consolidate_inplace()
        return self.combine([b for b in self.blocks if b.is_numeric], copy)
    def combine(self, blocks, copy=True):
        """""" return a new manager with the blocks """"""
        if len(blocks) == 0:
            return self.make_empty()
        # FIXME: optimization potential
        indexer = np.sort(np.concatenate([b.mgr_locs.as_array for b in blocks]))
        inv_indexer = lib.get_reverse_indexer(indexer, self.shape[0])
        new_blocks = []
        for b in blocks:
            b = b.copy(deep=copy)
            b.mgr_locs = algos.take_1d(
                inv_indexer, b.mgr_locs.as_array, axis=0, allow_fill=False
            )
            new_blocks.append(b)
        axes = list(self.axes)
        axes[0] = self.items.take(indexer)
        return self.__class__(new_blocks, axes, do_integrity_check=False)
    def get_slice(self, slobj, axis=0):
        if axis >= self.ndim:
            raise IndexError(""Requested axis not found in manager"")
        if axis == 0:
            new_blocks = self._slice_take_blocks_ax0(slobj)
        else:
            slicer = [slice(None)] * (axis + 1)",[15]
"""""""
Customisable progressbar decorator for iterators.
Includes a default (x)range iterator printing to stderr.
Usage:
  >>> from tqdm import trange[, tqdm]
  >>> for i in trange(10): #same as: for i in tqdm(xrange(10))
  ...     ...
""""""
# future division is important to divide integers and get as
# a result precise floating numbers (instead of truncated int)
from __future__ import division, absolute_import
# import compatibility functions and utilities
from ._utils import _supports_unicode, _environ_cols, _range, _unich
import sys
from time import time

__author__ = {""github.com/"": [""noamraph"", ""obiwanus"", ""kmike"", ""hadim"",
                              ""casperdcl"", ""lrq3000""]}
__all__ = ['tqdm', 'trange', 'format_interval', 'format_meter']

def format_sizeof(num, suffix=''):
    """"""
    Formats a number (greater than unity) with SI Order of Magnitude prefixes.
    Parameters
    ----------
    num  : float
        Number ( >= 1) to format.
    suffix  : str, optional
        Post-postfix [default: ''].
    Returns
    -------
    out  : str
        Number with Order of Magnitude SI unit postfix.
    """"""
    for unit in ['', 'K', 'M', 'G', 'T', 'P', 'E', 'Z']:
        if abs(num) < 1000.0:
            if abs(num) < 100.0:
                if abs(num) < 10.0:
                    return '{0:1.2f}'.format(num) + unit + suffix
                return '{0:2.1f}'.format(num) + unit + suffix
            return '{0:3.0f}'.format(num) + unit + suffix
        num /= 1000.0
    return '{0:3.1f}Y'.format(num) + suffix

def format_interval(t):
    """"""
    Formats a number of seconds as a clock time, [H:]MM:SS
    Parameters
    ----------
    t  : int
        Number of seconds.
    Returns
    -------
    out  : str
        [H:]MM:SS
    """"""
    mins, s = divmod(int(t), 60)
    h, m = divmod(mins, 60)
    if h:
        return '{0:d}:{1:02d}:{2:02d}'.format(h, m, s)
    else:
        return '{0:02d}:{1:02d}'.format(m, s)

def format_meter(n, total, elapsed, ncols=None, prefix='', ascii=False,
                 unit='it', unit_scale=False):
    """"""
    Return a string-based progress bar given some parameters
    Parameters
    ----------
    n  : int
        Number of finished iterations.
    total  : int
        The expected total number of iterations. If meaningless (), only basic
        progress statistics are displayed (no ETA).
    elapsed  : float
        Number of seconds passed since start.
    ncols  : int, optional
        The width of the entire output message. If specified, dynamically
        resizes the progress meter to stay within this bound
        [default: None]. The fallback meter width is 10 for the progress bar
        + no limit for the iterations counter and statistics. If 0, will not
        print any meter (only stats).
    prefix  : str, optional
        Prefix message (included in total width) [default: ''].
    ascii  : bool, optional
        If not set, use unicode (smooth blocks) to fill the meter
        [default: False]. The fallback is to use ASCII characters (1-9 #).
    unit  : str, optional
        The iteration unit [default: 'it'].
    unit_scale  : bool, optional
        If set, the number of iterations will printed with an appropriate
        SI metric prefix (K = 10^3, M = 10^6, etc.) [default: False].
    Returns
    -------
    out  : Formatted meter and stats, ready to display.
    """"""
    # in case the total is wrong (n is above the total), then
    # we switch to the mode without showing the total prediction
    # (since ETA would be wrong anyway)
    if total and n > total:
        total = None
    elapsed_str = format_interval(elapsed)
    rate_fmt = ((format_sizeof(n / elapsed) if unit_scale else
                 '{0:5.2f}'.format(n / elapsed)) if elapsed else
                '?') \
        + unit + '/s'
    if unit_scale:
        n_fmt = format_sizeof(n)
        total_fmt = format_sizeof(total) if total else None
    else:
        n_fmt = str(n)
        total_fmt = str(total)
","[40, 41, 42]"
"                    break
        finally:
            try:
                if enqueuer is not None:
                    enqueuer.stop()
            finally:
                if val_enqueuer is not None:
                    val_enqueuer.stop()
        callbacks.on_train_end()
        return self.history
    @interfaces.legacy_generator_methods_support
    def evaluate_generator(self, generator, steps=None,
                           max_queue_size=10,
                           workers=1,
                           use_multiprocessing=False,
                           verbose=0):
        """"""Evaluates the model on a data generator.
        The generator should return the same kind of data
        as accepted by `test_on_batch`.
        # Arguments
            generator: Generator yielding tuples (inputs, targets)
                or (inputs, targets, sample_weights)
                or an instance of Sequence (keras.utils.Sequence)
                object in order to avoid duplicate data
                when using multiprocessing.
            steps: Total number of steps (batches of samples)
                to yield from `generator` before stopping.
                Optional for `Sequence`: if unspecified, will use
                the `len(generator)` as a number of steps.
            max_queue_size: maximum size for the generator queue
            workers: Integer. Maximum number of processes to spin up
                when using process based threading.
                If unspecified, `workers` will default to 1. If 0, will
                execute the generator on the main thread.
            use_multiprocessing: if True, use process based threading.
                Note that because
                this implementation relies on multiprocessing,
                you should not pass
                non picklable arguments to the generator
                as they can't be passed
                easily to children processes.
            verbose: verbosity mode, 0 or 1.
        # Returns
            Scalar test loss (if the model has a single output and no metrics)
            or list of scalars (if the model has multiple outputs
            and/or metrics). The attribute `model.metrics_names` will give you
            the display labels for the scalar outputs.
        # Raises
            ValueError: In case the generator yields
                data in an invalid format.
        """"""
        self._make_test_function()
        stateful_metric_indices = []
        if hasattr(self, 'metrics'):
            for i, m in enumerate(self.metrics):
                if isinstance(m, Layer) and m.stateful:
                    m.reset_states()
            stateful_metric_indices = [
                i for i, name in enumerate(self.metrics_names)
                if str(name) in self.stateful_metric_names]
        else:
            stateful_metric_indices = []
        steps_done = 0
        wait_time = 0.01
        outs_per_batch = []
        batch_sizes = []
        is_sequence = isinstance(generator, Sequence)
        if not is_sequence and use_multiprocessing and workers > 1:
            warnings.warn(
                UserWarning('Using a generator with `use_multiprocessing=True`'
                            ' and multiple workers may duplicate your data.'
                            ' Please consider using the`keras.utils.Sequence'
                            ' class.'))
        if steps is None:
            if is_sequence:
                steps = len(generator)
            else:
                raise ValueError('`steps=None` is only valid for a generator'
                                 ' based on the `keras.utils.Sequence` class.'
                                 ' Please specify `steps` or use the'
                                 ' `keras.utils.Sequence` class.')
        enqueuer = None
        try:
            if workers > 0:
                if is_sequence:
                    enqueuer = OrderedEnqueuer(generator,
                                               use_multiprocessing=use_multiprocessing)
                else:
                    enqueuer = GeneratorEnqueuer(generator,
                                                 use_multiprocessing=use_multiprocessing,
                                                 wait_time=wait_time)
                enqueuer.start(workers=workers, max_queue_size=max_queue_size)
                output_generator = enqueuer.get()
            else:
                if is_sequence:
                    output_generator = iter(generator)
                else:
                    output_generator = generator
            if verbose == 1:
                progbar = Progbar(target=steps)
            while steps_done < steps:
                generator_output = next(output_generator)
                if not hasattr(generator_output, '__len__'):
                    raise ValueError('Output of generator should be a tuple '
                                     '(x, y, sample_weight) '
                                     'or (x, y). Found: ' +
                                     str(generator_output))
                if len(generator_output) == 2:
                    x, y = generator_output
                    sample_weight = None
                elif len(generator_output) == 3:
                    x, y, sample_weight = generator_output
                else:
                    raise ValueError('Output of generator should be a tuple '
                                     '(x, y, sample_weight) '","[62, 63, 64]"
"""""""
Base and utility classes for tseries type pandas objects.
""""""
import operator
from typing import List, Optional, Set
import numpy as np
from pandas._libs import NaT, iNaT, join as libjoin, lib
from pandas._libs.algos import unique_deltas
from pandas._libs.tslibs import timezones
from pandas.compat.numpy import function as nv
from pandas.errors import AbstractMethodError
from pandas.util._decorators import Appender, cache_readonly
from pandas.core.dtypes.common import (
    ensure_int64,
    is_bool_dtype,
    is_dtype_equal,
    is_float,
    is_integer,
    is_list_like,
    is_period_dtype,
    is_scalar,
)
from pandas.core.dtypes.concat import concat_compat
from pandas.core.dtypes.generic import ABCIndex, ABCIndexClass, ABCSeries
from pandas.core import algorithms
from pandas.core.accessor import PandasDelegate
from pandas.core.arrays import ExtensionArray, ExtensionOpsMixin
from pandas.core.arrays.datetimelike import (
    DatetimeLikeArrayMixin,
    _ensure_datetimelike_to_i8,
)
import pandas.core.indexes.base as ibase
from pandas.core.indexes.base import Index, _index_shared_docs
from pandas.core.indexes.numeric import Int64Index
from pandas.core.ops import get_op_result_name
from pandas.core.tools.timedeltas import to_timedelta
from pandas.tseries.frequencies import DateOffset, to_offset
from .extension import (
    ExtensionIndex,
    inherit_names,
    make_wrapped_arith_op,
    make_wrapped_comparison_op,
)
_index_doc_kwargs = dict(ibase._index_doc_kwargs)

def _join_i8_wrapper(joinf, with_indexers: bool = True):
    """"""
    Create the join wrapper methods.
    """"""
    @staticmethod  # type: ignore
    def wrapper(left, right):
        if isinstance(left, (np.ndarray, ABCIndex, ABCSeries, DatetimeLikeArrayMixin)):
            left = left.view(""i8"")
        if isinstance(right, (np.ndarray, ABCIndex, ABCSeries, DatetimeLikeArrayMixin)):
            right = right.view(""i8"")
        results = joinf(left, right)
        if with_indexers:
            # dtype should be timedelta64[ns] for TimedeltaIndex
            #  and datetime64[ns] for DatetimeIndex
            dtype = left.dtype.base
            join_index, left_indexer, right_indexer = results
            join_index = join_index.view(dtype)
            return join_index, left_indexer, right_indexer
        return results
    return wrapper

@inherit_names(
    [""inferred_freq"", ""_isnan"", ""_resolution"", ""resolution""],
    DatetimeLikeArrayMixin,
    cache=True,
)
@inherit_names(
    [""__iter__"", ""mean"", ""freq"", ""freqstr"", ""_ndarray_values"", ""asi8"", ""_box_values""],
    DatetimeLikeArrayMixin,
)
class DatetimeIndexOpsMixin(ExtensionIndex, ExtensionOpsMixin):
    """"""
    Common ops mixin to support a unified interface datetimelike Index.
    """"""
    _data: ExtensionArray
    freq: Optional[DateOffset]
    freqstr: Optional[str]
    _resolution: int
    _bool_ops: List[str] = []
    _field_ops: List[str] = []
    hasnans = cache_readonly(DatetimeLikeArrayMixin._hasnans.fget)  # type: ignore
    _hasnans = hasnans  # for index / array -agnostic code
    @property
    def is_all_dates(self) -> bool:
        return True
    def unique(self, level=None):
        if level is not None:
            self._validate_index_level(level)
        result = self._data.unique()
        # Note: if `self` is already unique, then self.unique() should share
        #  a `freq` with self.  If not already unique, then self.freq must be
        #  None, so again sharing freq is correct.
        return self._shallow_copy(result._data)
    @classmethod
    def _create_comparison_method(cls, op):
        """"""
        Create a comparison method that dispatches to ``cls.values``.
        """"""
        return make_wrapped_comparison_op(f""__{op.__name__}__"")
    # ------------------------------------------------------------------------
    # Abstract data attributes",[30]
"import logging
from six.moves.urllib.parse import urljoin
from w3lib.url import safe_url_string
from scrapy.http import HtmlResponse
from scrapy.utils.response import get_meta_refresh
from scrapy.exceptions import IgnoreRequest, NotConfigured
logger = logging.getLogger(__name__)

class BaseRedirectMiddleware(object):
    enabled_setting = 'REDIRECT_ENABLED'
    def __init__(self, settings):
        if not settings.getbool(self.enabled_setting):
            raise NotConfigured
        self.max_redirect_times = settings.getint('REDIRECT_MAX_TIMES')
        self.priority_adjust = settings.getint('REDIRECT_PRIORITY_ADJUST')
    @classmethod
    def from_crawler(cls, crawler):
        return cls(crawler.settings)
    def _redirect(self, redirected, request, spider, reason):
        ttl = request.meta.setdefault('redirect_ttl', self.max_redirect_times)
        redirects = request.meta.get('redirect_times', 0) + 1
        if ttl and redirects <= self.max_redirect_times:
            redirected.meta['redirect_times'] = redirects
            redirected.meta['redirect_ttl'] = ttl - 1
            redirected.meta['redirect_urls'] = request.meta.get('redirect_urls', []) + \
                [request.url]
            redirected.meta['redirect_reasons'] = request.meta.get('redirect_reasons', []) + \
                [reason]
            redirected.dont_filter = request.dont_filter
            redirected.priority = request.priority + self.priority_adjust
            logger.debug(""Redirecting (%(reason)s) to %(redirected)s from %(request)s"",
                         {'reason': reason, 'redirected': redirected, 'request': request},
                         extra={'spider': spider})
            return redirected
        else:
            logger.debug(""Discarding %(request)s: max redirections reached"",
                         {'request': request}, extra={'spider': spider})
            raise IgnoreRequest(""max redirections reached"")
    def _redirect_request_using_get(self, request, redirect_url):
        redirected = request.replace(url=redirect_url, method='GET', body='')
        redirected.headers.pop('Content-Type', None)
        redirected.headers.pop('Content-Length', None)
        return redirected

class RedirectMiddleware(BaseRedirectMiddleware):
    """"""
    Handle redirection of requests based on response status
    and meta-refresh html tag.
    """"""
    def process_response(self, request, response, spider):
        if (request.meta.get('dont_redirect', False) or
                response.status in getattr(spider, 'handle_httpstatus_list', []) or
                response.status in request.meta.get('handle_httpstatus_list', []) or
                request.meta.get('handle_httpstatus_all', False)):
            return response
        allowed_status = (301, 302, 303, 307, 308)
        if 'Location' not in response.headers or response.status not in allowed_status:
            return response
        location = safe_url_string(response.headers['location'])
        redirected_url = urljoin(request.url, location)
        if response.status in (301, 307, 308) or request.method == 'HEAD':
            redirected = request.replace(url=redirected_url)
            return self._redirect(redirected, request, spider, response.status)
        redirected = self._redirect_request_using_get(request, redirected_url)
        return self._redirect(redirected, request, spider, response.status)

class MetaRefreshMiddleware(BaseRedirectMiddleware):
    enabled_setting = 'METAREFRESH_ENABLED'
    def __init__(self, settings):
        super(MetaRefreshMiddleware, self).__init__(settings)
        self._ignore_tags = settings.getlist('METAREFRESH_IGNORE_TAGS')
        self._maxdelay = settings.getint('REDIRECT_MAX_METAREFRESH_DELAY',
                                         settings.getint('METAREFRESH_MAXDELAY'))
    def process_response(self, request, response, spider):
        if request.meta.get('dont_redirect', False) or request.method == 'HEAD' or \
                not isinstance(response, HtmlResponse):
            return response
        interval, url = get_meta_refresh(response,
                                         ignore_tags=self._ignore_tags)
        if url and interval < self._maxdelay:
            redirected = self._redirect_request_using_get(request, url)
            return self._redirect(redirected, request, spider, 'meta refresh')
        return response","[1, 72]"
"        # We keep it for compatibility reasons.
        warnings.warn('`Sequential.model` is deprecated. '
                      '`Sequential` is a subclass of `Model`, you can '
                      'just use your `Sequential` instance directly.')
        return self
    def add(self, layer):
        """"""Adds a layer instance on top of the layer stack.
        # Arguments
            layer: layer instance.
        # Raises
            TypeError: If `layer` is not a layer instance.
            ValueError: In case the `layer` argument does not
                know its input shape.
            ValueError: In case the `layer` argument has
                multiple output tensors, or is already connected
                somewhere else (forbidden in `Sequential` models).
        """"""
        if not isinstance(layer, Layer):
            raise TypeError('The added layer must be '
                            'an instance of class Layer. '
                            'Found: ' + str(layer))
        self.built = False
        if not self._layers:
            set_inputs = False
            # First layer in model: check that it is an input layer.
            if not isinstance(layer, InputLayer):
                # Create an input tensor and call `layer` on the input tensor.
                # First, we need to infer the expected input shape and dtype.
                first_layer = layer
                if isinstance(layer, (Model, Sequential)):
                    # We were passed a model as first layer.
                    # This requires a specific way to figure out the
                    # input shape and dtype.
                    if not layer.layers:
                        raise ValueError('Cannot add an empty model '
                                         'to a `Sequential` model.')
                    # In case of nested models: recover the first layer
                    # of the deepest model to infer input shape and dtype.
                    first_layer = layer.layers[0]
                    while isinstance(first_layer, (Model, Sequential)):
                        first_layer = first_layer.layers[0]
                    batch_shape = first_layer.batch_input_shape
                    dtype = first_layer.dtype
                if hasattr(first_layer, 'batch_input_shape'):
                    batch_shape = first_layer.batch_input_shape
                    dtype = first_layer.dtype
                    # Instantiate the input layer.
                    x = Input(
                        batch_shape=batch_shape,
                        dtype=dtype,
                        name=layer.name + '_input')
                    # This will build the current layer
                    # and create the node connecting the current layer
                    # to the input layer we just created.
                    layer(x)
                    set_inputs = True
                else:
                    # The layer doesn't know about its expected shape.
                    # We will have to
                    # build the model lazily on `fit`/etc.
                    batch_shape = None
            else:
                # Corner case where the user passes an InputLayer via `add`.
                assert len(layer._inbound_nodes[-1].output_tensors) == 1
                set_inputs = True
            if set_inputs:
                if len(layer._inbound_nodes[-1].output_tensors) != 1:
                    raise ValueError('All layers in a Sequential model '
                                     'should have a single output tensor. '
                                     'For multi-output layers, '
                                     'use the functional API.')
                self.outputs = [layer._inbound_nodes[-1].output_tensors[0]]
                self.inputs = network.get_source_inputs(self.outputs[0])
        elif self.outputs:
            output_tensor = layer(self.outputs[0])
            if isinstance(output_tensor, list):
                raise TypeError('All layers in a Sequential model '
                                'should have a single output tensor. '
                                'For multi-output layers, '
                                'use the functional API.')
            self.outputs = [output_tensor]
        if self.inputs:
            self.build()
        else:
            self._layers.append(layer)
    def pop(self):
        """"""Removes the last layer in the model.
        # Raises
            TypeError: if there are no layers in the model.
        """"""
        if not self.layers:
            raise TypeError('There are no layers in the model.')
        self._layers.pop()
        self.built = False
        if not self.layers:
            self.outputs = None
            self.inputs = None
        elif self.outputs:
            self.layers[-1]._outbound_nodes = []
            self.outputs = [self.layers[-1].output]
            self.build()
    def build(self, input_shape=None):
        if input_shape and not self.inputs:
            batch_shape = tuple(input_shape)
            dtype = K.floatx()
            x = Input(batch_shape=batch_shape,
                      dtype=dtype,
                      name=self.name + '_input')
            self.inputs = [x]
            for layer in self._layers:
                x = layer(x)
            self.outputs = [x]
            if self._layers:
                self._layers[0].batch_input_shape = batch_shape
        if self.inputs:
            self._init_graph_network(self.inputs,
                                     self.outputs,","[44, 45]"
"        Returns
        -------
        result : datetime
            Previous opening time.
        """"""
        return self._next_opening_time(other, sign=-1)
    def _get_business_hours_by_sec(self, start, end):
        """"""
        Return business hours in a day by seconds.
        """"""
        # create dummy datetime to calculate businesshours in a day
        dtstart = datetime(2014, 4, 1, start.hour, start.minute)
        day = 1 if start < end else 2
        until = datetime(2014, 4, day, end.hour, end.minute)
        return int((until - dtstart).total_seconds())
    @apply_wraps
    def rollback(self, dt):
        """"""
        Roll provided date backward to next offset only if not on offset.
        """"""
        if not self.is_on_offset(dt):
            if self.n >= 0:
                dt = self._prev_opening_time(dt)
            else:
                dt = self._next_opening_time(dt)
            return self._get_closing_time(dt)
        return dt
    @apply_wraps
    def rollforward(self, dt):
        """"""
        Roll provided date forward to next offset only if not on offset.
        """"""
        if not self.is_on_offset(dt):
            if self.n >= 0:
                return self._next_opening_time(dt)
            else:
                return self._prev_opening_time(dt)
        return dt
    def _get_closing_time(self, dt):
        """"""
        Get the closing time of a business hour interval by its opening time.
        Parameters
        ----------
        dt : datetime
            Opening time of a business hour interval.
        Returns
        -------
        result : datetime
            Corresponding closing time.
        """"""
        for i, st in enumerate(self.start):
            if st.hour == dt.hour and st.minute == dt.minute:
                return dt + timedelta(
                    seconds=self._get_business_hours_by_sec(st, self.end[i])
                )
        assert False
    @apply_wraps
    def apply(self, other):
        if isinstance(other, datetime):
            # used for detecting edge condition
            nanosecond = getattr(other, ""nanosecond"", 0)
            # reset timezone and nanosecond
            # other may be a Timestamp, thus not use replace
            other = datetime(
                other.year,
                other.month,
                other.day,
                other.hour,
                other.minute,
                other.second,
                other.microsecond,
            )
            n = self.n
            # adjust other to reduce number of cases to handle
            if n >= 0:
                if other.time() in self.end or not self._is_on_offset(other):
                    other = self._next_opening_time(other)
            else:
                if other.time() in self.start:
                    # adjustment to move to previous business day
                    other = other - timedelta(seconds=1)
                if not self._is_on_offset(other):
                    other = self._next_opening_time(other)
                    other = self._get_closing_time(other)
            # get total business hours by sec in one business day
            businesshours = sum(
                self._get_business_hours_by_sec(st, en)
                for st, en in zip(self.start, self.end)
            )
            bd, r = divmod(abs(n * 60), businesshours // 60)
            if n < 0:
                bd, r = -bd, -r
            # adjust by business days first
            if bd != 0:
                skip_bd = BusinessDay(n=bd)
                # midnight business hour may not on BusinessDay
                if not self.next_bday.is_on_offset(other):
                    prev_open = self._prev_opening_time(other)
                    remain = other - prev_open
                    other = prev_open + skip_bd + remain
                else:
                    other = other + skip_bd
            # remaining business hours to adjust
            bhour_remain = timedelta(minutes=r)
            if n >= 0:
                while bhour_remain != timedelta(0):
                    # business hour left in this business time interval
                    bhour = (
                        self._get_closing_time(self._prev_opening_time(other)) - other
                    )
                    if bhour_remain < bhour:
                        # finish adjusting if possible
                        other += bhour_remain
                        bhour_remain = timedelta(0)",[105]
"        return res + ""\n""
    def __bool__(self) -> bool:
        """"""Return True if the line has leaves or comments.""""""
        return bool(self.leaves or self.comments)

@dataclass
class EmptyLineTracker:
    """"""Provides a stateful method that returns the number of potential extra
    empty lines needed before and after the currently processed line.
    Note: this tracker works on lines that haven't been split yet.  It assumes
    the prefix of the first leaf consists of optional newlines.  Those newlines
    are consumed by `maybe_empty_lines()` and included in the computation.
    """"""
    is_pyi: bool = False
    previous_line: Optional[Line] = None
    previous_after: int = 0
    previous_defs: List[int] = Factory(list)
    def maybe_empty_lines(self, current_line: Line) -> Tuple[int, int]:
        """"""Return the number of extra empty lines before and after the `current_line`.
        This is for separating `def`, `async def` and `class` with extra empty
        lines (two on module-level).
        """"""
        before, after = self._maybe_empty_lines(current_line)
        before -= self.previous_after
        self.previous_after = after
        self.previous_line = current_line
        return before, after
    def _maybe_empty_lines(self, current_line: Line) -> Tuple[int, int]:
        max_allowed = 1
        if current_line.depth == 0:
            max_allowed = 1 if self.is_pyi else 2
        if current_line.leaves:
            # Consume the first leaf's extra newlines.
            first_leaf = current_line.leaves[0]
            before = first_leaf.prefix.count(""\n"")
            before = min(before, max_allowed)
            first_leaf.prefix = """"
        else:
            before = 0
        depth = current_line.depth
        while self.previous_defs and self.previous_defs[-1] >= depth:
            self.previous_defs.pop()
            if self.is_pyi:
                before = 0 if depth else 1
            else:
                before = 1 if depth else 2
        if current_line.is_decorator or current_line.is_def or current_line.is_class:
            return self._maybe_empty_lines_for_class_or_def(current_line, before)
        if (
            self.previous_line
            and self.previous_line.is_import
            and not current_line.is_import
            and depth == self.previous_line.depth
        ):
            return (before or 1), 0
        if (
            self.previous_line
            and self.previous_line.is_class
            and current_line.is_triple_quoted_string
        ):
            return before, 1
        return before, 0
    def _maybe_empty_lines_for_class_or_def(
        self, current_line: Line, before: int
    ) -> Tuple[int, int]:
        if not current_line.is_decorator:
            self.previous_defs.append(current_line.depth)
        if self.previous_line is None:
            # Don't insert empty lines before the first line in the file.
            return 0, 0
        if self.previous_line.is_decorator:
            return 0, 0
        if self.previous_line.depth < current_line.depth and (
            self.previous_line.is_class or self.previous_line.is_def
        ):
            return 0, 0
        if (
            self.previous_line.is_comment
            and self.previous_line.depth == current_line.depth
            and before == 0
        ):
            return 0, 0
        if self.is_pyi:
            if self.previous_line.depth > current_line.depth:
                newlines = 1
            elif current_line.is_class or self.previous_line.is_class:
                if current_line.is_stub_class and self.previous_line.is_stub_class:
                    # No blank line between classes with an empty body
                    newlines = 0
                else:
                    newlines = 1
            elif current_line.is_def and not self.previous_line.is_def:
                # Blank line between a block of functions and a block of non-functions
                newlines = 1
            else:
                newlines = 0
        else:
            newlines = 2
        if current_line.depth and newlines:
            newlines -= 1
        return newlines, 0

@dataclass
class LineGenerator(Visitor[Line]):
    """"""Generates reformatted Line objects.  Empty lines are not emitted.
    Note: destroys the tree it's visiting by mutating prefixes of its leaves
    in ways that will no longer stringify to valid Python code on the tree.
    """"""
    is_pyi: bool = False",[29]
"            if len(left.columns) != len(right):
                raise ValueError(
                    msg.format(req_len=len(left.columns), given_len=len(right))
                )
            right = left._constructor_sliced(right, index=left.columns)
        return right
    if isinstance(right, np.ndarray):
        if right.ndim == 1:
            right = to_series(right)
        elif right.ndim == 2:
            if right.shape == left.shape:
                right = left._constructor(right, index=left.index, columns=left.columns)
            elif right.shape[0] == left.shape[0] and right.shape[1] == 1:
                # Broadcast across columns
                right = np.broadcast_to(right, left.shape)
                right = left._constructor(right, index=left.index, columns=left.columns)
            elif right.shape[1] == left.shape[1] and right.shape[0] == 1:
                # Broadcast along rows
                right = to_series(right[0, :])
            else:
                raise ValueError(
                    ""Unable to coerce to DataFrame, shape ""
                    f""must be {left.shape}: given {right.shape}""
                )
        elif right.ndim > 2:
            raise ValueError(
                f""Unable to coerce to Series/DataFrame, dim must be <= 2: {right.shape}""
            )
    elif is_list_like(right) and not isinstance(right, (ABCSeries, ABCDataFrame)):
        # GH17901
        right = to_series(right)
    if flex is not None and isinstance(right, ABCDataFrame):
        if not left._indexed_same(right):
            if flex:
                left, right = left.align(right, join=""outer"", level=level, copy=False)
            else:
                raise ValueError(
                    ""Can only compare identically-labeled DataFrame objects""
                )
    elif isinstance(right, ABCSeries):
        # axis=1 is default for DataFrame-with-Series op
        axis = left._get_axis_number(axis) if axis is not None else 1
        left, right = left.align(
            right, join=""outer"", axis=axis, level=level, copy=False
        )
    return left, right

def _should_reindex_frame_op(
    left: ""DataFrame"", right, axis, default_axis: int, fill_value, level
) -> bool:
    """"""
    Check if this is an operation between DataFrames that will need to reindex.
    """"""
    assert isinstance(left, ABCDataFrame)
    if not isinstance(right, ABCDataFrame):
        return False
    if fill_value is None and level is None and axis is default_axis:
        # TODO: any other cases we should handle here?
        cols = left.columns.intersection(right.columns)
        if not (cols.equals(left.columns) and cols.equals(right.columns)):
            return True
    return False

def _frame_arith_method_with_reindex(
    left: ""DataFrame"", right: ""DataFrame"", op
) -> ""DataFrame"":
    """"""
    For DataFrame-with-DataFrame operations that require reindexing,
    operate only on shared columns, then reindex.
    Parameters
    ----------
    left : DataFrame
    right : DataFrame
    op : binary operator
    Returns
    -------
    DataFrame
    """"""
    # GH#31623, only operate on shared columns
    cols = left.columns.intersection(right.columns)
    new_left = left[cols]
    new_right = right[cols]
    result = op(new_left, new_right)
    # Do the join on the columns instead of using _align_method_FRAME
    #  to avoid constructing two potentially large/sparse DataFrames
    join_columns, _, _ = left.columns.join(
        right.columns, how=""outer"", level=None, return_indexers=True
    )
    return result.reindex(join_columns, axis=1)

def _arith_method_FRAME(cls, op, special):
    str_rep = _get_opstr(op)
    op_name = _get_op_name(op, special)
    default_axis = _get_frame_op_default_axis(op_name)
    na_op = define_na_arithmetic_op(op, str_rep)
    is_logical = str_rep in [""&"", ""|"", ""^""]
    if op_name in _op_descriptions:
        # i.e. include ""add"" but not ""__add__""
        doc = _make_flex_doc(op_name, ""dataframe"")
    else:
        doc = _arith_doc_FRAME % op_name
    @Appender(doc)
    def f(self, other, axis=default_axis, level=None, fill_value=None):
","[59, 127]"
"                self.units = units
                self.state_size = units
                super(MinimalRNNCell, self).__init__(**kwargs)
            def build(self, input_shape):
                self.kernel = self.add_weight(shape=(input_shape[-1], self.units),
                                              initializer='uniform',
                                              name='kernel')
                self.recurrent_kernel = self.add_weight(
                    shape=(self.units, self.units),
                    initializer='uniform',
                    name='recurrent_kernel')
                self.built = True
            def call(self, inputs, states):
                prev_output = states[0]
                h = K.dot(inputs, self.kernel)
                output = h + K.dot(prev_output, self.recurrent_kernel)
                return output, [output]
        # Let's use this cell in a RNN layer:
        cell = MinimalRNNCell(32)
        x = keras.Input((None, 5))
        layer = RNN(cell)
        y = layer(x)
        # Here's how to use the cell to build a stacked RNN:
        cells = [MinimalRNNCell(32), MinimalRNNCell(64)]
        x = keras.Input((None, 5))
        layer = RNN(cells)
        y = layer(x)
    ```
    """"""
    def __init__(self, cell,
                 return_sequences=False,
                 return_state=False,
                 go_backwards=False,
                 stateful=False,
                 unroll=False,
                 **kwargs):
        if isinstance(cell, (list, tuple)):
            cell = StackedRNNCells(cell)
        if not hasattr(cell, 'call'):
            raise ValueError('`cell` should have a `call` method. '
                             'The RNN was passed:', cell)
        if not hasattr(cell, 'state_size'):
            raise ValueError('The RNN cell should have '
                             'an attribute `state_size` '
                             '(tuple of integers, '
                             'one integer per RNN state).')
        super(RNN, self).__init__(**kwargs)
        self.cell = cell
        self.return_sequences = return_sequences
        self.return_state = return_state
        self.go_backwards = go_backwards
        self.stateful = stateful
        self.unroll = unroll
        self.supports_masking = True
        self.input_spec = [InputSpec(ndim=3)]
        self.state_spec = None
        self._states = None
        self.constants_spec = None
        self._num_constants = None
    @property
    def states(self):
        if self._states is None:
            if isinstance(self.cell.state_size, int):
                num_states = 1
            else:
                num_states = len(self.cell.state_size)
            return [None for _ in range(num_states)]
        return self._states
    @states.setter
    def states(self, states):
        self._states = states
    def compute_output_shape(self, input_shape):
        if isinstance(input_shape, list):
            input_shape = input_shape[0]
        if hasattr(self.cell.state_size, '__len__'):
            output_dim = self.cell.state_size[0]
        else:
            output_dim = self.cell.state_size
        if self.return_sequences:
            output_shape = (input_shape[0], input_shape[1], output_dim)
        else:
            output_shape = (input_shape[0], output_dim)
        if self.return_state:
            state_shape = [(input_shape[0], output_dim) for _ in self.states]
            return [output_shape] + state_shape
        else:
            return output_shape
    def compute_mask(self, inputs, mask):
        if isinstance(mask, list):
            mask = mask[0]
        output_mask = mask if self.return_sequences else None
        if self.return_state:
            state_mask = [None for _ in self.states]
            return [output_mask] + state_mask
        else:
            return output_mask
    def build(self, input_shape):
        # Note input_shape will be list of shapes of initial states and
        # constants if these are passed in __call__.
        if self._num_constants is not None:
            constants_shape = input_shape[-self._num_constants:]
        else:
            constants_shape = None
        if isinstance(input_shape, list):
            input_shape = input_shape[0]
        batch_size = input_shape[0] if self.stateful else None
        input_dim = input_shape[-1]
        self.input_spec[0] = InputSpec(shape=(batch_size, None, input_dim))
","[87, 89, 97]"
"        elif mode == 'max':
            self.monitor_op = np.greater
            self.best = -np.Inf
        else:
            if 'acc' in self.monitor or self.monitor.startswith('fmeasure'):
                self.monitor_op = np.greater
                self.best = -np.Inf
            else:
                self.monitor_op = np.less
                self.best = np.Inf
    def on_epoch_end(self, epoch, logs=None):
        logs = logs or {}
        self.epochs_since_last_save += 1
        if self.epochs_since_last_save >= self.period:
            self.epochs_since_last_save = 0
            filepath = self.filepath.format(epoch=epoch + 1, **logs)
            if self.save_best_only:
                current = logs.get(self.monitor)
                if current is None:
                    warnings.warn('Can save best model only with %s available, '
                                  'skipping.' % (self.monitor), RuntimeWarning)
                else:
                    if self.monitor_op(current, self.best):
                        if self.verbose > 0:
                            print('\nEpoch %05d: %s improved from %0.5f to %0.5f,'
                                  ' saving model to %s'
                                  % (epoch + 1, self.monitor, self.best,
                                     current, filepath))
                        self.best = current
                        if self.save_weights_only:
                            self.model.save_weights(filepath, overwrite=True)
                        else:
                            self.model.save(filepath, overwrite=True)
                    else:
                        if self.verbose > 0:
                            print('\nEpoch %05d: %s did not improve from %0.5f' %
                                  (epoch + 1, self.monitor, self.best))
            else:
                if self.verbose > 0:
                    print('\nEpoch %05d: saving model to %s' % (epoch + 1, filepath))
                if self.save_weights_only:
                    self.model.save_weights(filepath, overwrite=True)
                else:
                    self.model.save(filepath, overwrite=True)

class EarlyStopping(Callback):
    """"""Stop training when a monitored quantity has stopped improving.
    # Arguments
        monitor: quantity to be monitored.
        min_delta: minimum change in the monitored quantity
            to qualify as an improvement, i.e. an absolute
            change of less than min_delta, will count as no
            improvement.
        patience: number of epochs with no improvement
            after which training will be stopped.
        verbose: verbosity mode.
        mode: one of {auto, min, max}. In `min` mode,
            training will stop when the quantity
            monitored has stopped decreasing; in `max`
            mode it will stop when the quantity
            monitored has stopped increasing; in `auto`
            mode, the direction is automatically inferred
            from the name of the monitored quantity.
        baseline: Baseline value for the monitored quantity to reach.
            Training will stop if the model doesn't show improvement
            over the baseline.
    """"""
    def __init__(self,
                 monitor='val_loss',
                 min_delta=0,
                 patience=0,
                 verbose=0,
                 mode='auto',
                 baseline=None):
        super(EarlyStopping, self).__init__()
        self.monitor = monitor
        self.baseline = baseline
        self.patience = patience
        self.verbose = verbose
        self.min_delta = min_delta
        self.wait = 0
        self.stopped_epoch = 0
        if mode not in ['auto', 'min', 'max']:
            warnings.warn('EarlyStopping mode %s is unknown, '
                          'fallback to auto mode.' % mode,
                          RuntimeWarning)
            mode = 'auto'
        if mode == 'min':
            self.monitor_op = np.less
        elif mode == 'max':
            self.monitor_op = np.greater
        else:
            if 'acc' in self.monitor:
                self.monitor_op = np.greater
            else:
                self.monitor_op = np.less
        if self.monitor_op == np.greater:
            self.min_delta *= 1
        else:
            self.min_delta *= -1
    def on_train_begin(self, logs=None):
        # Allow instances to be re-used
        self.wait = 0
        self.stopped_epoch = 0
        if self.baseline is not None:
            self.best = self.baseline
        else:
            self.best = np.Inf if self.monitor_op == np.less else -np.Inf
    def on_epoch_end(self, epoch, logs=None):
        current = logs.get(self.monitor)
        if current is None:
            warnings.warn(
                'Early stopping conditioned on metric `%s` '
                'which is not available. Available metrics are: %s' %
                (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning
            )
            return",[77]
"
class CsvItemExporter(BaseItemExporter):
    def __init__(self, file, include_headers_line=True, join_multivalued=',', **kwargs):
        self._configure(kwargs, dont_fail=True)
        self.include_headers_line = include_headers_line
        file = file if six.PY2 else io.TextIOWrapper(file, line_buffering=True)
        self.csv_writer = csv.writer(file, **kwargs)
        self._headers_not_written = True
        self._join_multivalued = join_multivalued
    def serialize_field(self, field, name, value):
        serializer = field.get('serializer', self._join_if_needed)
        return serializer(value)
    def _join_if_needed(self, value):
        if isinstance(value, (list, tuple)):
            try:
                return self._join_multivalued.join(value)
            except TypeError:  # list in value may not contain strings
                pass
        return value
    def export_item(self, item):
        if self._headers_not_written:
            self._headers_not_written = False
            self._write_headers_and_set_fields_to_export(item)
        fields = self._get_serialized_fields(item, default_value='',
                                             include_empty=True)
        values = list(self._build_row(x for _, x in fields))
        self.csv_writer.writerow(values)
    def _build_row(self, values):
        for s in values:
            try:
                yield to_native_str(s)
            except TypeError:
                yield to_native_str(repr(s))
    def _write_headers_and_set_fields_to_export(self, item):
        if self.include_headers_line:
            if not self.fields_to_export:
                if isinstance(item, dict):
                    # for dicts try using fields of the first item
                    self.fields_to_export = list(item.keys())
                else:
                    # use fields declared in Item
                    self.fields_to_export = list(item.fields.keys())
            row = list(self._build_row(self.fields_to_export))
            self.csv_writer.writerow(row)

class PickleItemExporter(BaseItemExporter):
    def __init__(self, file, protocol=2, **kwargs):
        self._configure(kwargs)
        self.file = file
        self.protocol = protocol
    def export_item(self, item):
        d = dict(self._get_serialized_fields(item))
        pickle.dump(d, self.file, self.protocol)

class MarshalItemExporter(BaseItemExporter):
    def __init__(self, file, **kwargs):
        self._configure(kwargs)
        self.file = file
    def export_item(self, item):
        marshal.dump(dict(self._get_serialized_fields(item)), self.file)

class PprintItemExporter(BaseItemExporter):
    def __init__(self, file, **kwargs):
        self._configure(kwargs)
        self.file = file
    def export_item(self, item):
        itemdict = dict(self._get_serialized_fields(item))
        self.file.write(to_bytes(pprint.pformat(itemdict) + '\n'))

class PythonItemExporter(BaseItemExporter):
    """"""The idea behind this exporter is to have a mechanism to serialize items
    to built-in python types so any serialization library (like
    json, msgpack, binc, etc) can be used on top of it. Its main goal is to
    seamless support what BaseItemExporter does plus nested items.
    """"""
    def _configure(self, options, dont_fail=False):
        self.binary = options.pop('binary', True)
        super(PythonItemExporter, self)._configure(options, dont_fail)
        if self.binary:
            warnings.warn(
                ""PythonItemExporter will drop support for binary export in the future"",
                ScrapyDeprecationWarning)
    def serialize_field(self, field, name, value):
        serializer = field.get('serializer', self._serialize_value)
        return serializer(value)
    def _serialize_value(self, value):
        if isinstance(value, BaseItem):
            return self.export_item(value)
        if isinstance(value, dict):
            return dict(self._serialize_dict(value))
        if is_listlike(value):
            return [self._serialize_value(v) for v in value]
        if self.binary:
            return to_bytes(value, encoding=self.encoding)
        else:
            return to_unicode(value, encoding=self.encoding)
    def _serialize_dict(self, value):
        for key, val in six.iteritems(value):
            key = to_bytes(key) if self.binary else key
            yield key, self._serialize_value(val)
    def export_item(self, item):
        result = dict(self._get_serialized_fields(item))
        if self.binary:
            result = dict(self._serialize_dict(result))","[112, 113, 114, 115]"
"                    # to the input layer we just created.
                    layer(x)
                    set_inputs = True
            else:
                # Corner case where the user passes an InputLayer via `add`.
                assert len(layer._inbound_nodes[-1].output_tensors) == 1
                set_inputs = True
            if set_inputs:
                if len(layer._inbound_nodes[-1].output_tensors) != 1:
                    raise ValueError('All layers in a Sequential model '
                                     'should have a single output tensor. '
                                     'For multi-output layers, '
                                     'use the functional API.')
                self.outputs = [layer._inbound_nodes[-1].output_tensors[0]]
                self.inputs = network.get_source_inputs(self.outputs[0])
        elif self.outputs:
            output_tensor = layer(self.outputs[0])
            if isinstance(output_tensor, list):
                raise TypeError('All layers in a Sequential model '
                                'should have a single output tensor. '
                                'For multi-output layers, '
                                'use the functional API.')
            self.outputs = [output_tensor]
        if self.inputs:
            self.build()
        else:
            self._layers.append(layer)
    def pop(self):
        """"""Removes the last layer in the model.
        # Raises
            TypeError: if there are no layers in the model.
        """"""
        if not self.layers:
            raise TypeError('There are no layers in the model.')
        self._layers.pop()
        self.built = False
        if not self.layers:
            self.outputs = None
            self.inputs = None
        elif self.outputs:
            self.layers[-1]._outbound_nodes = []
            self.outputs = [self.layers[-1].output]
            self.build()
    def build(self, input_shape=None):
        if input_shape and not self.inputs:
            batch_shape = tuple(input_shape)
            dtype = K.floatx()
            x = Input(batch_shape=batch_shape,
                      dtype=dtype,
                      name=self.name + '_input')
            self.inputs = [x]
            for layer in self._layers:
                x = layer(x)
            self.outputs = [x]
            if self._layers:
                self._layers[0].batch_input_shape = batch_shape
        if self.inputs:
            self._init_graph_network(self.inputs,
                                     self.outputs,
                                     name=self.name)
            self.built = True
    def predict_proba(self, x, batch_size=32, verbose=0):
        """"""Generates class probability predictions for the input samples.
        The input samples are processed batch by batch.
        # Arguments
            x: input data, as a Numpy array or list of Numpy arrays
                (if the model has multiple inputs).
            batch_size: integer.
            verbose: verbosity mode, 0 or 1.
        # Returns
            A Numpy array of probability predictions.
        """"""
        preds = self.predict(x, batch_size, verbose)
        if preds.min() < 0. or preds.max() > 1.:
            warnings.warn('Network returning invalid probability values. '
                          'The last layer might not normalize predictions '
                          'into probabilities '
                          '(like softmax or sigmoid would).')
        return preds
    def predict_classes(self, x, batch_size=32, verbose=0):
        """"""Generate class predictions for the input samples.
        The input samples are processed batch by batch.
        # Arguments
            x: input data, as a Numpy array or list of Numpy arrays
                (if the model has multiple inputs).
            batch_size: integer.
            verbose: verbosity mode, 0 or 1.
        # Returns:
            A numpy array of class predictions.
        """"""
        proba = self.predict(x, batch_size=batch_size, verbose=verbose)
        if proba.shape[-1] > 1:
            return proba.argmax(axis=-1)
        else:
            return (proba > 0.5).astype('int32')
    def get_config(self):
        config = []
        for layer in self.layers:
            config.append({
                'class_name': layer.__class__.__name__,
                'config': layer.get_config()
            })
        return copy.deepcopy(config)
    @classmethod
    def from_config(cls, config, custom_objects=None):
        model = cls()
        for conf in config:
            layer = layer_module.deserialize(conf,
                                             custom_objects=custom_objects)
            model.add(layer)","[59, 60, 111, 113, 117, 121, 122]"
"        1  0.01  0.67
        2  0.66  0.03
        3  0.21  0.18
        By providing an integer each column is rounded to the same number
        of decimal places
        >>> df.round(1)
            dogs  cats
        0   0.2   0.3
        1   0.0   0.7
        2   0.7   0.0
        3   0.2   0.2
        With a dict, the number of places for specific columns can be
        specified with the column names as key and the number of decimal
        places as value
        >>> df.round({'dogs': 1, 'cats': 0})
            dogs  cats
        0   0.2   0.0
        1   0.0   1.0
        2   0.7   0.0
        3   0.2   0.0
        Using a Series, the number of places for specific columns can be
        specified with the column names as index and the number of
        decimal places as value
        >>> decimals = pd.Series([0, 1], index=['cats', 'dogs'])
        >>> df.round(decimals)
            dogs  cats
        0   0.2   0.0
        1   0.0   1.0
        2   0.7   0.0
        3   0.2   0.0
        """"""
        from pandas.core.reshape.concat import concat
        def _dict_round(df, decimals):
            for col, vals in df.items():
                try:
                    yield _series_round(vals, decimals[col])
                except KeyError:
                    yield vals
        def _series_round(s, decimals):
            if is_integer_dtype(s) or is_float_dtype(s):
                return s.round(decimals)
            return s
        nv.validate_round(args, kwargs)
        if isinstance(decimals, (dict, Series)):
            if isinstance(decimals, Series):
                if not decimals.index.is_unique:
                    raise ValueError(""Index of decimals must be unique"")
            new_cols = list(_dict_round(self, decimals))
        elif is_integer(decimals):
            # Dispatch to Series.round
            new_cols = [_series_round(v, decimals) for _, v in self.items()]
        else:
            raise TypeError(""decimals must be an integer, a dict-like or a Series"")
        if len(new_cols) > 0:
            return self._constructor(
                concat(new_cols, axis=1), index=self.index, columns=self.columns
            )
        else:
            return self
    # ----------------------------------------------------------------------
    # Statistical methods, etc.
    def corr(self, method=""pearson"", min_periods=1) -> ""DataFrame"":
        """"""
        Compute pairwise correlation of columns, excluding NA/null values.
        Parameters
        ----------
        method : {'pearson', 'kendall', 'spearman'} or callable
            Method of correlation:
            * pearson : standard correlation coefficient
            * kendall : Kendall Tau correlation coefficient
            * spearman : Spearman rank correlation
            * callable: callable with input two 1d ndarrays
                and returning a float. Note that the returned matrix from corr
                will have 1 along the diagonals and will be symmetric
                regardless of the callable's behavior.
                .. versionadded:: 0.24.0
        min_periods : int, optional
            Minimum number of observations required per pair of columns
            to have a valid result. Currently only available for Pearson
            and Spearman correlation.
        Returns
        -------
        DataFrame
            Correlation matrix.
        See Also
        --------
        DataFrame.corrwith : Compute pairwise correlation with another
            DataFrame or Series.
        Series.corr : Compute the correlation between two Series.
        Examples
        --------
        >>> def histogram_intersection(a, b):
        ...     v = np.minimum(a, b).sum().round(decimals=1)
        ...     return v
        >>> df = pd.DataFrame([(.2, .3), (.0, .6), (.6, .0), (.2, .1)],
        ...                   columns=['dogs', 'cats'])
        >>> df.corr(method=histogram_intersection)
              dogs  cats
        dogs   1.0   0.3
        cats   0.3   1.0
        """"""
        numeric_df = self._get_numeric_data()
        cols = numeric_df.columns
        idx = cols.copy()
        mat = numeric_df.values
        if method == ""pearson"":","[124, 127]"
"        self._add_auth_token(headers, url, required=auth_required)
        try:
            display.vvvv(""Calling Galaxy at %s"" % url)
            resp = open_url(to_native(url), data=args, validate_certs=self.validate_certs, headers=headers,
                            method=method, timeout=20)
        except HTTPError as e:
            raise GalaxyError(e, error_context_msg)
        except Exception as e:
            raise AnsibleError(""Unknown error when attempting to call Galaxy at '%s': %s"" % (url, to_native(e)))
        resp_data = to_text(resp.read(), errors='surrogate_or_strict')
        try:
            data = json.loads(resp_data)
        except ValueError:
            raise AnsibleError(""Failed to parse Galaxy response from '%s' as JSON:\n%s""
                               % (resp.url, to_native(resp_data)))
        return data
    def _add_auth_token(self, headers, url, token_type=None, required=False):
        # Don't add the auth token if one is already present
        if 'Authorization' in headers:
            return
        if not self.token and required:
            raise AnsibleError(""No access token or username set. A token can be set with --api-key, with ""
                               ""'ansible-galaxy login', or set in ansible.cfg."")
        if self.token:
            headers.update(self.token.headers())
    @g_connect(['v1'])
    def authenticate(self, github_token):
        """"""
        Retrieve an authentication token
        """"""
        url = _urljoin(self.api_server, self.available_api_versions['v1'], ""tokens"") + '/'
        args = urlencode({""github_token"": github_token})
        resp = open_url(url, data=args, validate_certs=self.validate_certs, method=""POST"")
        data = json.loads(to_text(resp.read(), errors='surrogate_or_strict'))
        return data
    @g_connect(['v1'])
    def create_import_task(self, github_user, github_repo, reference=None, role_name=None):
        """"""
        Post an import request
        """"""
        url = _urljoin(self.api_server, self.available_api_versions['v1'], ""imports"") + '/'
        args = {
            ""github_user"": github_user,
            ""github_repo"": github_repo,
            ""github_reference"": reference if reference else """"
        }
        if role_name:
            args['alternate_role_name'] = role_name
        elif github_repo.startswith('ansible-role'):
            args['alternate_role_name'] = github_repo[len('ansible-role') + 1:]
        data = self._call_galaxy(url, args=urlencode(args), method=""POST"")
        if data.get('results', None):
            return data['results']
        return data
    @g_connect(['v1'])
    def get_import_task(self, task_id=None, github_user=None, github_repo=None):
        """"""
        Check the status of an import task.
        """"""
        url = _urljoin(self.api_server, self.available_api_versions['v1'], ""imports"")
        if task_id is not None:
            url = ""%s?id=%d"" % (url, task_id)
        elif github_user is not None and github_repo is not None:
            url = ""%s?github_user=%s&github_repo=%s"" % (url, github_user, github_repo)
        else:
            raise AnsibleError(""Expected task_id or github_user and github_repo"")
        data = self._call_galaxy(url)
        return data['results']
    @g_connect(['v1'])
    def lookup_role_by_name(self, role_name, notify=True):
        """"""
        Find a role by name.
        """"""
        role_name = to_text(urlquote(to_bytes(role_name)))
        try:
            parts = role_name.split(""."")
            user_name = ""."".join(parts[0:-1])
            role_name = parts[-1]
            if notify:
                display.display(""- downloading role '%s', owned by %s"" % (role_name, user_name))
        except Exception:
            raise AnsibleError(""Invalid role name (%s). Specify role as format: username.rolename"" % role_name)
        url = _urljoin(self.api_server, self.available_api_versions['v1'], ""roles"",
                       ""?owner__username=%s&name=%s"" % (user_name, role_name))
        data = self._call_galaxy(url)
        if len(data[""results""]) != 0:
            return data[""results""][0]
        return None
    @g_connect(['v1'])
    def fetch_role_related(self, related, role_id):
        """"""
        Fetch the list of related items for the given role.
        The url comes from the 'related' field of the role.
        """"""
        results = []
        try:
            url = _urljoin(self.api_server, self.available_api_versions['v1'], ""roles"", role_id, related,
                           ""?page_size=50"")
            data = self._call_galaxy(url)
            results = data['results']
            done = (data.get('next_link', None) is None)
            while not done:
                url = _urljoin(self.api_server, data['next_link'])
                data = self._call_galaxy(url)
                results += data['results']
                done = (data.get('next_link', None) is None)
        except Exception as e:
            display.vvvv(""Unable to retrive role (id=%s) data (%s), but this is not fatal so we continue: %s""
                         % (role_id, related, to_text(e)))
        return results
    @g_connect(['v1'])","[117, 122, 123]"
"                            gen_log.info(""Timeout reading body from %s"", self.context)
                            self.stream.close()
                            return False
            self._read_finished = True
            if not self._write_finished or self.is_client:
                need_delegate_close = False
                with _ExceptionLoggingContext(app_log):
                    delegate.finish()
            # If we're waiting for the application to produce an asynchronous
            # response, and we're not detached, register a close callback
            # on the stream (we didn't need one while we were reading)
            if (
                not self._finish_future.done()
                and self.stream is not None
                and not self.stream.closed()
            ):
                self.stream.set_close_callback(self._on_connection_close)
                await self._finish_future
            if self.is_client and self._disconnect_on_finish:
                self.close()
            if self.stream is None:
                return False
        except httputil.HTTPInputError as e:
            gen_log.info(""Malformed HTTP message from %s: %s"", self.context, e)
            if not self.is_client:
                await self.stream.write(b""HTTP/1.1 400 Bad Request\r\n\r\n"")
            self.close()
            return False
        finally:
            if need_delegate_close:
                with _ExceptionLoggingContext(app_log):
                    delegate.on_connection_close()
            header_future = None  # type: ignore
            self._clear_callbacks()
        return True
    def _clear_callbacks(self) -> None:
        """"""Clears the callback attributes.
        This allows the request handler to be garbage collected more
        quickly in CPython by breaking up reference cycles.
        """"""
        self._write_callback = None
        self._write_future = None  # type: Optional[Future[None]]
        self._close_callback = None  # type: Optional[Callable[[], None]]
        if self.stream is not None:
            self.stream.set_close_callback(None)
    def set_close_callback(self, callback: Optional[Callable[[], None]]) -> None:
        """"""Sets a callback that will be run when the connection is closed.
        Note that this callback is slightly different from
        `.HTTPMessageDelegate.on_connection_close`: The
        `.HTTPMessageDelegate` method is called when the connection is
        closed while recieving a message. This callback is used when
        there is not an active delegate (for example, on the server
        side this callback is used if the client closes the connection
        after sending its request but before receiving all the
        response.
        """"""
        self._close_callback = callback
    def _on_connection_close(self) -> None:
        # Note that this callback is only registered on the IOStream
        # when we have finished reading the request and are waiting for
        # the application to produce its response.
        if self._close_callback is not None:
            callback = self._close_callback
            self._close_callback = None
            callback()
        if not self._finish_future.done():
            future_set_result_unless_cancelled(self._finish_future, None)
        self._clear_callbacks()
    def close(self) -> None:
        if self.stream is not None:
            self.stream.close()
        self._clear_callbacks()
        if not self._finish_future.done():
            future_set_result_unless_cancelled(self._finish_future, None)
    def detach(self) -> iostream.IOStream:
        """"""Take control of the underlying stream.
        Returns the underlying `.IOStream` object and stops all further
        HTTP processing.  May only be called during
        `.HTTPMessageDelegate.headers_received`.  Intended for implementing
        protocols like websockets that tunnel over an HTTP handshake.
        """"""
        self._clear_callbacks()
        stream = self.stream
        self.stream = None  # type: ignore
        if not self._finish_future.done():
            future_set_result_unless_cancelled(self._finish_future, None)
        return stream
    def set_body_timeout(self, timeout: float) -> None:
        """"""Sets the body timeout for a single request.
        Overrides the value from `.HTTP1ConnectionParameters`.
        """"""
        self._body_timeout = timeout
    def set_max_body_size(self, max_body_size: int) -> None:
        """"""Sets the body size limit for a single request.
        Overrides the value from `.HTTP1ConnectionParameters`.
        """"""
        self._max_body_size = max_body_size
    def write_headers(
        self,
        start_line: Union[httputil.RequestStartLine, httputil.ResponseStartLine],
        headers: httputil.HTTPHeaders,
        chunk: bytes = None,
    ) -> ""Future[None]"":
        """"""Implements `.HTTPConnection.write_headers`.""""""
        lines = []
        if self.is_client:
            assert isinstance(start_line, httputil.RequestStartLine)
            self._request_start_line = start_line
            lines.append(utf8(""%s %s HTTP/1.1"" % (start_line[0], start_line[1])))
            # Client requests with a non-empty body must have either a
            # Content-Length or a Transfer-Encoding.
            self._chunking_output = (
                start_line.method in (""POST"", ""PUT"", ""PATCH"")
                and ""Content-Length"" not in headers",[127]
"            The default 'auto' detects the hash algorithm in use.
        extract: True tries extracting the file as an Archive, like tar or zip.
        archive_format: Archive format to try for extracting the file.
            Options are 'auto', 'tar', 'zip', and None.
            'tar' includes tar, tar.gz, and tar.bz files.
            The default 'auto' is ['tar', 'zip'].
            None or an empty list will return no matches found.
        cache_dir: Location to store cached files, when None it
            defaults to the [Keras Directory](/faq/#where-is-the-keras-configuration-filed-stored).
    # Returns
        Path to the downloaded file
    """"""  # noqa
    if cache_dir is None:
        cache_dir = os.path.join(os.path.expanduser('~'), '.keras')
    if md5_hash is not None and file_hash is None:
        file_hash = md5_hash
        hash_algorithm = 'md5'
    datadir_base = os.path.expanduser(cache_dir)
    if not os.access(datadir_base, os.W_OK):
        datadir_base = os.path.join('/tmp', '.keras')
    datadir = os.path.join(datadir_base, cache_subdir)
    if not os.path.exists(datadir):
        os.makedirs(datadir)
    if untar:
        untar_fpath = os.path.join(datadir, fname)
        fpath = untar_fpath + '.tar.gz'
    else:
        fpath = os.path.join(datadir, fname)
    download = False
    if os.path.exists(fpath):
        # File found; verify integrity if a hash was provided.
        if file_hash is not None:
            if not validate_file(fpath, file_hash, algorithm=hash_algorithm):
                print('A local file was found, but it seems to be '
                      'incomplete or outdated because the ' + hash_algorithm +
                      ' file hash does not match the original value of ' +
                      file_hash + ' so we will re-download the data.')
                download = True
    else:
        download = True
    if download:
        print('Downloading data from', origin)
        class ProgressTracker(object):
            # Maintain progbar for the lifetime of download.
            # This design was chosen for Python 2.7 compatibility.
            progbar = None
        def dl_progress(count, block_size, total_size):
            if ProgressTracker.progbar is None:
                if total_size == -1:
                    total_size = None
                ProgressTracker.progbar = Progbar(total_size)
            else:
                ProgressTracker.progbar.update(count * block_size)
        error_msg = 'URL fetch failure on {} : {} -- {}'
        try:
            try:
                urlretrieve(origin, fpath, dl_progress)
            except HTTPError as e:
                raise Exception(error_msg.format(origin, e.code, e.msg))
            except URLError as e:
                raise Exception(error_msg.format(origin, e.errno, e.reason))
        except (Exception, KeyboardInterrupt):
            if os.path.exists(fpath):
                os.remove(fpath)
            raise
        ProgressTracker.progbar = None
    if untar:
        if not os.path.exists(untar_fpath):
            _extract_archive(fpath, datadir, archive_format='tar')
        return untar_fpath
    if extract:
        _extract_archive(fpath, datadir, archive_format)
    return fpath

def _hash_file(fpath, algorithm='sha256', chunk_size=65535):
    """"""Calculates a file sha256 or md5 hash.
    # Example
    ```python
        >>> from keras.data_utils import _hash_file
        >>> _hash_file('/path/to/file.zip')
        'e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855'
    ```
    # Arguments
        fpath: path to the file being validated
        algorithm: hash algorithm, one of 'auto', 'sha256', or 'md5'.
            The default 'auto' detects the hash algorithm in use.
        chunk_size: Bytes to read at a time, important for large files.
    # Returns
        The file hash
    """"""
    if (algorithm == 'sha256') or (algorithm == 'auto' and len(hash) == 64):
        hasher = hashlib.sha256()
    else:
        hasher = hashlib.md5()
    with open(fpath, 'rb') as fpath_file:
        for chunk in iter(lambda: fpath_file.read(chunk_size), b''):
            hasher.update(chunk)
    return hasher.hexdigest()

def validate_file(fpath, file_hash, algorithm='auto', chunk_size=65535):
    """"""Validates a file against a sha256 or md5 hash.
    # Arguments
        fpath: path to the file being validated
        file_hash:  The expected hash string of the file.
            The sha256 and md5 hash algorithms are both supported.
        algorithm: Hash algorithm, one of 'auto', 'sha256', or 'md5'.
            The default 'auto' detects the hash algorithm in use.
        chunk_size: Bytes to read at a time, important for large files.",[14]
"    ):
        """"""
        Get result for Cythonized functions.
        Parameters
        ----------
        how : str, Cythonized function name to be called
        grouper : Grouper object containing pertinent group info
        aggregate : bool, default False
            Whether the result should be aggregated to match the number of
            groups
        cython_dtype : default None
            Type of the array that will be modified by the Cython call. If
            `None`, the type will be inferred from the values of each slice
        needs_values : bool, default False
            Whether the values should be a part of the Cython call
            signature
        needs_mask : bool, default False
            Whether boolean mask needs to be part of the Cython call
            signature
        needs_ngroups : bool, default False
            Whether number of groups is part of the Cython call signature
        result_is_index : bool, default False
            Whether the result of the Cython operation is an index of
            values to be retrieved, instead of the actual values themselves
        pre_processing : function, default None
            Function to be applied to `values` prior to passing to Cython.
            Function should return a tuple where the first element is the
            values to be passed to Cython and the second element is an optional
            type which the values should be converted to after being returned
            by the Cython operation. Raises if `needs_values` is False.
        post_processing : function, default None
            Function to be applied to result of Cython function. Should accept
            an array of values as the first argument and type inferences as its
            second argument, i.e. the signature should be
            (ndarray, Type).
        **kwargs : dict
            Extra arguments to be passed back to Cython funcs
        Returns
        -------
        `Series` or `DataFrame`  with filled values
        """"""
        if result_is_index and aggregate:
            raise ValueError(""'result_is_index' and 'aggregate' cannot both be True!"")
        if post_processing:
            if not callable(pre_processing):
                raise ValueError(""'post_processing' must be a callable!"")
        if pre_processing:
            if not callable(pre_processing):
                raise ValueError(""'pre_processing' must be a callable!"")
            if not needs_values:
                raise ValueError(
                    ""Cannot use 'pre_processing' without specifying 'needs_values'!""
                )
        labels, _, ngroups = grouper.group_info
        output = collections.OrderedDict()
        base_func = getattr(libgroupby, how)
        for name, obj in self._iterate_slices():
            if aggregate:
                result_sz = ngroups
            else:
                result_sz = len(obj.values)
            if not cython_dtype:
                cython_dtype = obj.values.dtype
            result = np.zeros(result_sz, dtype=cython_dtype)
            func = partial(base_func, result, labels)
            inferences = None
            if needs_values:
                vals = obj.values
                if pre_processing:
                    vals, inferences = pre_processing(vals)
                func = partial(func, vals)
            if needs_mask:
                mask = isna(obj.values).view(np.uint8)
                func = partial(func, mask)
            if needs_ngroups:
                func = partial(func, ngroups)
            func(**kwargs)  # Call func to modify indexer values in place
            if result_is_index:
                result = algorithms.take_nd(obj.values, result)
            if post_processing:
                result = post_processing(result, inferences)
            output[name] = result
        if aggregate:
            return self._wrap_aggregated_output(output)
        else:
            return self._wrap_transformed_output(output)
    @Substitution(name=""groupby"")
    @Appender(_common_see_also)
    def shift(self, periods=1, freq=None, axis=0, fill_value=None):
        """"""
        Shift each group by periods observations.
        Parameters
        ----------
        periods : integer, default 1
            number of periods to shift
        freq : frequency string
        axis : axis to shift, default 0
        fill_value : optional
            .. versionadded:: 0.24.0
        Returns
        -------
        Series or DataFrame
            Object shifted within each group.
        """"""
        if freq is not None or axis != 0 or not isna(fill_value):
            return self.apply(lambda x: x.shift(periods, freq, axis, fill_value))
        return self._get_cythonized_result(","[64, 67, 74, 80, 89]"
"                obj = local_vars[variable]
            else:
                if variable not in self._objects:
                    self._objects[variable] = self.extract_object(variable)
                obj = self._objects[variable]
            if arg_str is None:
                # Member access
                if member == 'length':
                    return len(obj)
                return obj[member]
            assert expr.endswith(')')
            # Function call
            if arg_str == '':
                argvals = tuple()
            else:
                argvals = tuple([
                    self.interpret_expression(v, local_vars, allow_recursion)
                    for v in arg_str.split(',')])
            if member == 'split':
                assert argvals == ('',)
                return list(obj)
            if member == 'join':
                assert len(argvals) == 1
                return argvals[0].join(obj)
            if member == 'reverse':
                assert len(argvals) == 0
                obj.reverse()
                return obj
            if member == 'slice':
                assert len(argvals) == 1
                return obj[argvals[0]:]
            if member == 'splice':
                assert isinstance(obj, list)
                index, howMany = argvals
                res = []
                for i in range(index, min(index + howMany, len(obj))):
                    res.append(obj.pop(index))
                return res
            return obj[member](argvals)
        m = re.match(
            r'(?P<in>%s)\[(?P<idx>.+)\]$' % _NAME_RE, expr)
        if m:
            val = local_vars[m.group('in')]
            idx = self.interpret_expression(
                m.group('idx'), local_vars, allow_recursion - 1)
            return val[idx]
        for op, opfunc in _OPERATORS:
            m = re.match(r'(?P<x>.+?)%s(?P<y>.+)' % re.escape(op), expr)
            if not m:
                continue
            x, abort = self.interpret_statement(
                m.group('x'), local_vars, allow_recursion - 1)
            if abort:
                raise ExtractorError(
                    'Premature left-side return of %s in %r' % (op, expr))
            y, abort = self.interpret_statement(
                m.group('y'), local_vars, allow_recursion - 1)
            if abort:
                raise ExtractorError(
                    'Premature right-side return of %s in %r' % (op, expr))
            return opfunc(x, y)
        m = re.match(
            r'^(?P<func>%s)\((?P<args>[a-zA-Z0-9_$,]+)\)$' % _NAME_RE, expr)
        if m:
            fname = m.group('func')
            argvals = tuple([
                int(v) if v.isdigit() else local_vars[v]
                for v in m.group('args').split(',')])
            if fname not in self._functions:
                self._functions[fname] = self.extract_function(fname)
            return self._functions[fname](argvals)
        raise ExtractorError('Unsupported JS expression %r' % expr)
    def extract_object(self, objname):
        obj = {}
        obj_m = re.search(
            (r'(?:var\s+)?%s\s*=\s*\{' % re.escape(objname)) +
            r'\s*(?P<fields>([a-zA-Z$0-9]+\s*:\s*function\(.*?\)\s*\{.*?\}(?:,\s*)?)*)' +
            r'\}\s*;',
            self.code)
        fields = obj_m.group('fields')
        # Currently, it only supports function definitions
        fields_m = re.finditer(
            r'(?P<key>[a-zA-Z$0-9]+)\s*:\s*function'
            r'\((?P<args>[a-z,]+)\){(?P<code>[^}]+)}',
            fields)
        for f in fields_m:
            argnames = f.group('args').split(',')
            obj[f.group('key')] = self.build_function(argnames, f.group('code'))
        return obj
    def extract_function(self, funcname):
        func_m = re.search(
            r'''(?x)
                (?:function\s+%s|[{;,]\s*%s\s*=\s*function|var\s+%s\s*=\s*function)\s*
                \((?P<args>[^)]*)\)\s*
                \{(?P<code>[^}]+)\}''' % (
                re.escape(funcname), re.escape(funcname), re.escape(funcname)),
            self.code)
        if func_m is None:
            raise ExtractorError('Could not find JS function %r' % funcname)
        argnames = func_m.group('args').split(',')
        return self.build_function(argnames, func_m.group('code'))
    def call_function(self, funcname, *args):
        f = self.extract_function(funcname)
        return f(args)
    def build_function(self, argnames, code):
        def resf(args):
            local_vars = dict(zip(argnames, args))
            for stmt in code.split(';'):
                res, abort = self.interpret_statement(stmt, local_vars)
                if abort:
                    break
            return res","[69, 74]"
"    ""3XX"": ""Redirection"",
    ""4XX"": ""Client Error"",
    ""5XX"": ""Server Error"",
    ""DEFAULT"": ""Default Response"",
}

def get_openapi_params(dependant: Dependant) -> List[ModelField]:
    flat_dependant = get_flat_dependant(dependant, skip_repeats=True)
    return (
        flat_dependant.path_params
        + flat_dependant.query_params
        + flat_dependant.header_params
        + flat_dependant.cookie_params
    )

def get_openapi_security_definitions(flat_dependant: Dependant) -> Tuple[Dict, List]:
    security_definitions = {}
    operation_security = []
    for security_requirement in flat_dependant.security_requirements:
        security_definition = jsonable_encoder(
            security_requirement.security_scheme.model,
            by_alias=True,
            include_none=False,
        )
        security_name = security_requirement.security_scheme.scheme_name
        security_definitions[security_name] = security_definition
        operation_security.append({security_name: security_requirement.scopes})
    return security_definitions, operation_security

def get_openapi_operation_parameters(
    all_route_params: Sequence[ModelField],
) -> List[Dict[str, Any]]:
    parameters = []
    for param in all_route_params:
        field_info = get_field_info(param)
        field_info = cast(Param, field_info)
        parameter = {
            ""name"": param.alias,
            ""in"": field_info.in_.value,
            ""required"": param.required,
            ""schema"": field_schema(param, model_name_map={})[0],
        }
        if field_info.description:
            parameter[""description""] = field_info.description
        if field_info.deprecated:
            parameter[""deprecated""] = field_info.deprecated
        parameters.append(parameter)
    return parameters

def get_openapi_operation_request_body(
    *, body_field: Optional[ModelField], model_name_map: Dict[Type[BaseModel], str]
) -> Optional[Dict]:
    if not body_field:
        return None
    assert isinstance(body_field, ModelField)
    body_schema, _, _ = field_schema(
        body_field, model_name_map=model_name_map, ref_prefix=REF_PREFIX
    )
    field_info = cast(Body, get_field_info(body_field))
    request_media_type = field_info.media_type
    required = body_field.required
    request_body_oai: Dict[str, Any] = {}
    if required:
        request_body_oai[""required""] = required
    request_body_oai[""content""] = {request_media_type: {""schema"": body_schema}}
    return request_body_oai

def generate_operation_id(*, route: routing.APIRoute, method: str) -> str:
    if route.operation_id:
        return route.operation_id
    path: str = route.path_format
    return generate_operation_id_for_path(name=route.name, path=path, method=method)

def generate_operation_summary(*, route: routing.APIRoute, method: str) -> str:
    if route.summary:
        return route.summary
    return route.name.replace(""_"", "" "").title()

def get_openapi_operation_metadata(*, route: routing.APIRoute, method: str) -> Dict:
    operation: Dict[str, Any] = {}
    if route.tags:
        operation[""tags""] = route.tags
    operation[""summary""] = generate_operation_summary(route=route, method=method)
    if route.description:
        operation[""description""] = route.description
    operation[""operationId""] = generate_operation_id(route=route, method=method)
    if route.deprecated:
        operation[""deprecated""] = route.deprecated
    return operation

def get_openapi_path(
    *, route: routing.APIRoute, model_name_map: Dict[Type, str]
) -> Tuple[Dict, Dict, Dict]:
    path = {}
    security_schemes: Dict[str, Any] = {}
    definitions: Dict[str, Any] = {}
    assert route.methods is not None, ""Methods must be a list""
    assert route.response_class, ""A response class is needed to generate OpenAPI""
    route_response_media_type: Optional[str] = route.response_class.media_type
    if route.include_in_schema:
        for method in route.methods:
            operation = get_openapi_operation_metadata(route=route, method=method)
            parameters: List[Dict] = []
            flat_dependant = get_flat_dependant(route.dependant, skip_repeats=True)
            security_definitions, operation_security = get_openapi_security_definitions(
                flat_dependant=flat_dependant
            )
            if operation_security:
                operation.setdefault(""security"", []).extend(operation_security)
            if security_definitions:
                security_schemes.update(security_definitions)
            all_route_params = get_openapi_params(route.dependant)
            operation_parameters = get_openapi_operation_parameters(all_route_params)
            parameters.extend(operation_parameters)
            if parameters:
                operation[""parameters""] = parameters
            if method in METHODS_WITH_BODY:
                request_body_oai = get_openapi_operation_request_body(
                    body_field=route.body_field, model_name_map=model_name_map",[123]
"                                             parent_info.api, b_temp_path, apis, validate_certs, force_deps,
                                             parent=parent)
                    checked_parents.add(parent)
            # No extra dependencies were resolved, exit loop
            if deps_exhausted:
                break
        # Now we have resolved the deps to our best extent, now select the latest version for collections with
        # multiple versions found and go from there
        deps_not_checked = set(dependency_map.keys()).difference(checked_parents)
        for collection in deps_not_checked:
            dependency_map[collection].set_latest_version()
            if no_deps or len(dependency_map[collection].dependencies) == 0:
                checked_parents.add(collection)
    return dependency_map

def _get_collection_info(dep_map, existing_collections, collection, requirement, source, b_temp_path, apis,
                         validate_certs, force, parent=None):
    dep_msg = """"
    if parent:
        dep_msg = "" - as dependency of %s"" % parent
    display.vvv(""Processing requirement collection '%s'%s"" % (to_text(collection), dep_msg))
    b_tar_path = None
    if os.path.isfile(to_bytes(collection, errors='surrogate_or_strict')):
        display.vvvv(""Collection requirement '%s' is a tar artifact"" % to_text(collection))
        b_tar_path = to_bytes(collection, errors='surrogate_or_strict')
    elif urlparse(collection).scheme:
        display.vvvv(""Collection requirement '%s' is a URL to a tar artifact"" % collection)
        b_tar_path = _download_file(collection, b_temp_path, None, validate_certs)
    if b_tar_path:
        req = CollectionRequirement.from_tar(b_tar_path, force, parent=parent)
        collection_name = to_text(req)
        if collection_name in dep_map:
            collection_info = dep_map[collection_name]
            collection_info.add_requirement(None, req.latest_version)
        else:
            collection_info = req
    else:
        validate_collection_name(collection)
        display.vvvv(""Collection requirement '%s' is the name of a collection"" % collection)
        if collection in dep_map:
            collection_info = dep_map[collection]
            collection_info.add_requirement(parent, requirement)
        else:
            apis = [source] if source else apis
            collection_info = CollectionRequirement.from_name(collection, apis, requirement, force, parent=parent)
    existing = [c for c in existing_collections if to_text(c) == to_text(collection_info)]
    if existing and not collection_info.force:
        # Test that the installed collection fits the requirement
        existing[0].add_requirement(to_text(collection_info), requirement)
        collection_info = existing[0]
    dep_map[to_text(collection_info)] = collection_info

def _download_file(url, b_path, expected_hash, validate_certs, headers=None):
    bufsize = 65536
    digest = sha256()
    urlsplit = os.path.splitext(to_text(url.rsplit('/', 1)[1]))
    b_file_name = to_bytes(urlsplit[0], errors='surrogate_or_strict')
    b_file_ext = to_bytes(urlsplit[1], errors='surrogate_or_strict')
    b_file_path = tempfile.NamedTemporaryFile(dir=b_path, prefix=b_file_name, suffix=b_file_ext, delete=False).name
    display.vvv(""Downloading %s to %s"" % (url, to_text(b_path)))
    # Galaxy redirs downloads to S3 which reject the request if an Authorization header is attached so don't redir that
    resp = open_url(to_native(url, errors='surrogate_or_strict'), validate_certs=validate_certs, headers=headers,
                    unredirected_headers=['Authorization'])
    with open(b_file_path, 'wb') as download_file:
        data = resp.read(bufsize)
        while data:
            digest.update(data)
            download_file.write(data)
            data = resp.read(bufsize)
    if expected_hash:
        actual_hash = digest.hexdigest()
        display.vvvv(""Validating downloaded file hash %s with expected hash %s"" % (actual_hash, expected_hash))
        if expected_hash != actual_hash:
            raise AnsibleError(""Mismatch artifact hash with downloaded file"")
    return b_file_path

def _extract_tar_file(tar, filename, b_dest, b_temp_path, expected_hash=None):
    n_filename = to_native(filename, errors='surrogate_or_strict')
    try:
        member = tar.getmember(n_filename)
    except KeyError:
        raise AnsibleError(""Collection tar at '%s' does not contain the expected file '%s'."" % (to_native(tar.name),
                                                                                                n_filename))
    with tempfile.NamedTemporaryFile(dir=b_temp_path, delete=False) as tmpfile_obj:
        bufsize = 65536
        sha256_digest = sha256()
        with _tarfile_extract(tar, member) as tar_obj:
            data = tar_obj.read(bufsize)
            while data:
                tmpfile_obj.write(data)
                tmpfile_obj.flush()
                sha256_digest.update(data)
                data = tar_obj.read(bufsize)
        actual_hash = sha256_digest.hexdigest()
        if expected_hash and actual_hash != expected_hash:
            raise AnsibleError(""Checksum mismatch for '%s' inside collection at '%s'""
                               % (n_filename, to_native(tar.name)))
        b_dest_filepath = os.path.join(b_dest, to_bytes(filename, errors='surrogate_or_strict'))
        b_parent_dir = os.path.split(b_dest_filepath)[0]
        if not os.path.exists(b_parent_dir):
            # Seems like Galaxy does not validate if all file entries have a corresponding dir ftype entry. This check
            # makes sure we create the parent directory even if it wasn't set in the metadata.
            os.makedirs(b_parent_dir)
","[31, 33]"
"                h = x._keras_shape[1] + padding[0][0] + padding[0][1]
            else:
                h = None
            if x._keras_shape[2] is not None:
                w = x._keras_shape[2] + padding[1][0] + padding[1][1]
            else:
                w = None
            if x._keras_shape[3] is not None:
                d = x._keras_shape[3] + padding[2][0] + padding[2][1]
            else:
                d = None
            output_keras_shape = (x._keras_shape[0],
                                  h,
                                  w,
                                  d,
                                  x._keras_shape[4])
        y._keras_shape = output_keras_shape
    return y

def stack(x, axis=0):
    return T.stack(x, axis=axis)

def one_hot(indices, num_classes):
    """"""Input: nD integer tensor of shape (batch_size, dim1, dim2, ... dim(n-1))
    Output: (n + 1)D one hot representation of the input
    with shape (batch_size, dim1, dim2, ... dim(n-1), num_classes)
    """"""
    input_shape = tuple((indices.shape[i] for i in range(indices.ndim)))
    indices = T.flatten(indices)
    oh = T.extra_ops.to_one_hot(indices, num_classes)
    oh = T.reshape(oh, input_shape + (num_classes,))
    return oh

def reverse(x, axes):
    """"""Reverse a tensor along the specified axes
    """"""
    if isinstance(axes, int):
        axes = [axes]
    slices = []
    for i in range(x.ndim):
        if i in axes:
            slices.append(py_slice(None, None, -1))
        else:
            slices.append(py_slice(None, None, None))
    return x[slices]

def slice(x, start, size):
    raise NotImplementedError

def pattern_broadcast(x, broadcastable):
    return T.patternbroadcast(x, broadcastable)
# VALUE MANIPULATION

def get_value(x):
    if not hasattr(x, 'get_value'):
        raise TypeError('`get_value` can only be called on a variable. '
                        'If you have an expression instead, use `eval()`.')
    return x.get_value()

def batch_get_value(xs):
    """"""Returns the value of more than one tensor variable,
    as a list of Numpy arrays.
    """"""
    return [get_value(x) for x in xs]

def set_value(x, value):
    x.set_value(np.asarray(value, dtype=x.dtype))

def batch_set_value(tuples):
    for x, value in tuples:
        x.set_value(np.asarray(value, dtype=x.dtype))

def get_variable_shape(x):
    return x.get_value(borrow=True, return_internal_type=True).shape

def print_tensor(x, message=''):
    """"""Print the message and the tensor when evaluated and return the same
    tensor.
    """"""
    p_op = Print(message)
    return p_op(x)

# GRAPH MANIPULATION
class Function(object):
    def __init__(self, inputs, outputs, updates=[], name=None, **kwargs):
        unique_variables_to_update = {}
        for v, nv in updates:
            if v not in unique_variables_to_update:
                unique_variables_to_update[v] = nv
        updates = unique_variables_to_update.items()
        self.function = theano.function(inputs, outputs, updates=updates,
                                        allow_input_downcast=True,
                                        on_unused_input='ignore',
                                        name=name,
                                        **kwargs)
        self.name = name
    def __call__(self, inputs):
        assert isinstance(inputs, (list, tuple))
        return self.function(*inputs)

def _raise_invalid_arg(key):
    msg = 'Invalid argument ""%s"" passed to K.function with Theano backend' % key
    raise ValueError(msg)

def function(inputs, outputs, updates=[], **kwargs):
    if len(kwargs) > 0:
        for key in kwargs.keys():
            if not has_arg(theano.function, key, True):
                _raise_invalid_arg(key)",[89]
"        elif is_integer(other):
            # integer is passed to .shift via
            # _add_datetimelike_methods basically
            # but ufunc may pass integer to _add_delta
            return other
        # raise when input doesn't have freq
        raise raise_on_incompatible(self, None)
    def _is_comparable_dtype(self, dtype: DtypeObj) -> bool:
        """"""
        Can we compare values of the given dtype to our own?
        """"""
        if not isinstance(dtype, PeriodDtype):
            return False
        return dtype.freq == self.freq
    # ------------------------------------------------------------------------
    # Rendering Methods
    def _mpl_repr(self):
        # how to represent ourselves to matplotlib
        return self.astype(object)._values
    @property
    def _formatter_func(self):
        return self.array._formatter(boxed=False)
    # ------------------------------------------------------------------------
    # Indexing
    @cache_readonly
    def _engine(self):
        # To avoid a reference cycle, pass a weakref of self to _engine_type.
        period = weakref.ref(self)
        return self._engine_type(period, len(self))
    @doc(Index.__contains__)
    def __contains__(self, key: Any) -> bool:
        if isinstance(key, Period):
            if key.freq != self.freq:
                return False
            else:
                return key.ordinal in self._engine
        else:
            hash(key)
            try:
                self.get_loc(key)
                return True
            except KeyError:
                return False
    @cache_readonly
    def _int64index(self) -> Int64Index:
        return Int64Index._simple_new(self.asi8, name=self.name)
    # ------------------------------------------------------------------------
    # Index Methods
    def __array_wrap__(self, result, context=None):
        """"""
        Gets called after a ufunc. Needs additional handling as
        PeriodIndex stores internal data as int dtype
        Replace this to __numpy_ufunc__ in future version
        """"""
        if isinstance(context, tuple) and len(context) > 0:
            func = context[0]
            if func is np.add:
                pass
            elif func is np.subtract:
                name = self.name
                left = context[1][0]
                right = context[1][1]
                if isinstance(left, PeriodIndex) and isinstance(right, PeriodIndex):
                    name = left.name if left.name == right.name else None
                    return Index(result, name=name)
                elif isinstance(left, Period) or isinstance(right, Period):
                    return Index(result, name=name)
            elif isinstance(func, np.ufunc):
                if ""M->M"" not in func.types:
                    msg = f""ufunc '{func.__name__}' not supported for the PeriodIndex""
                    # This should be TypeError, but TypeError cannot be raised
                    # from here because numpy catches.
                    raise ValueError(msg)
        if is_bool_dtype(result):
            return result
        # the result is object dtype array of Period
        # cannot pass _simple_new as it is
        return type(self)(result, freq=self.freq, name=self.name)
    def asof_locs(self, where, mask: np.ndarray) -> np.ndarray:
        """"""
        where : array of timestamps
        mask : array of booleans where data is not NA
        """"""
        where_idx = where
        if isinstance(where_idx, DatetimeIndex):
            where_idx = PeriodIndex(where_idx._values, freq=self.freq)
        elif not isinstance(where_idx, PeriodIndex):
            raise TypeError(""asof_locs `where` must be DatetimeIndex or PeriodIndex"")
        elif where_idx.freq != self.freq:
            raise raise_on_incompatible(self, where_idx)
        locs = self.asi8[mask].searchsorted(where_idx.asi8, side=""right"")
        locs = np.where(locs > 0, locs - 1, 0)
        result = np.arange(len(self))[mask].take(locs)
        first = mask.argmax()
        result[(locs == 0) & (where_idx.asi8 < self.asi8[first])] = -1
        return result
    @doc(Index.astype)
    def astype(self, dtype, copy=True, how=""start""):
        dtype = pandas_dtype(dtype)
        if is_datetime64_any_dtype(dtype):
            # 'how' is index-specific, isn't part of the EA interface.
            tz = getattr(dtype, ""tz"", None)
            return self.to_timestamp(how=how).tz_localize(tz)
        # TODO: should probably raise on `how` here, so we don't ignore it.
        return super().astype(dtype, copy=copy)
",[34]
"            if isinstance(block, list):
                # provide consolidation to the interleaved_dtype
                if len(block) > 1:
                    dtype = _interleaved_dtype(block)
                    block = [b.astype(dtype) for b in block]
                    block = _consolidate(block)
                if len(block) != 1:
                    raise ValueError(
                        ""Cannot create SingleBlockManager with more than 1 block""
                    )
                block = block[0]
        if not isinstance(block, Block):
            block = make_block(block, placement=slice(0, len(axis)), ndim=1)
        self.blocks = tuple([block])
    def _post_setstate(self):
        pass
    @property
    def _block(self):
        return self.blocks[0]
    @property
    def _values(self):
        return self._block.values
    @property
    def _blknos(self):
        """""" compat with BlockManager """"""
        return None
    @property
    def _blklocs(self):
        """""" compat with BlockManager """"""
        return None
    def get_slice(self, slobj, axis=0):
        if axis >= self.ndim:
            raise IndexError(""Requested axis not found in manager"")
        return type(self)(self._block._slice(slobj), self.index[slobj], fastpath=True,)
    @property
    def index(self):
        return self.axes[0]
    @property
    def dtype(self):
        return self._block.dtype
    @property
    def array_dtype(self):
        return self._block.array_dtype
    def get_dtype_counts(self):
        return {self.dtype.name: 1}
    def get_dtypes(self):
        return np.array([self._block.dtype])
    def external_values(self):
        """"""The array that Series.values returns""""""
        return self._block.external_values()
    def internal_values(self):
        """"""The array that Series._values returns""""""
        return self._block.internal_values()
    def get_values(self):
        """""" return a dense type view """"""
        return np.array(self._block.to_dense(), copy=False)
    @property
    def _can_hold_na(self) -> bool:
        return self._block._can_hold_na
    def is_consolidated(self):
        return True
    def _consolidate_check(self):
        pass
    def _consolidate_inplace(self):
        pass
    def delete(self, item):
        """"""
        Delete single item from SingleBlockManager.
        Ensures that self.blocks doesn't become empty.
        """"""
        loc = self.items.get_loc(item)
        self._block.delete(loc)
        self.axes[0] = self.axes[0].delete(loc)
    def fast_xs(self, loc):
        """"""
        fast path for getting a cross-section
        return a view of the data
        """"""
        return self._block.values[loc]
    def concat(self, to_concat, new_axis) -> ""SingleBlockManager"":
        """"""
        Concatenate a list of SingleBlockManagers into a single
        SingleBlockManager.
        Used for pd.concat of Series objects with axis=0.
        Parameters
        ----------
        to_concat : list of SingleBlockManagers
        new_axis : Index of the result
        Returns
        -------
        SingleBlockManager
        """"""
        non_empties = [x for x in to_concat if len(x) > 0]
        # check if all series are of the same block type:
        if len(non_empties) > 0:
            blocks = [obj.blocks[0] for obj in non_empties]",[104]
"""""""Built-in metrics.
""""""
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function
import six
from . import backend as K
from .losses import mean_squared_error
from .losses import mean_absolute_error
from .losses import mean_absolute_percentage_error
from .losses import mean_squared_logarithmic_error
from .losses import hinge
from .losses import logcosh
from .losses import squared_hinge
from .losses import categorical_crossentropy
from .losses import sparse_categorical_crossentropy
from .losses import binary_crossentropy
from .losses import kullback_leibler_divergence
from .losses import poisson
from .losses import cosine_proximity
from .utils.generic_utils import deserialize_keras_object
from .utils.generic_utils import serialize_keras_object

def binary_accuracy(y_true, y_pred):
    return K.mean(K.equal(y_true, K.round(y_pred)), axis=-1)

def categorical_accuracy(y_true, y_pred):
    return K.cast(K.equal(K.argmax(y_true, axis=-1),
                          K.argmax(y_pred, axis=-1)),
                  K.floatx())

def sparse_categorical_accuracy(y_true, y_pred):
    # flatten y_true in case it's in shape (num_samples, 1) instead of (num_samples,)
    return K.cast(K.equal(K.flatten(y_true),
                          K.cast(K.argmax(y_pred, axis=-1), K.floatx())),
                  K.floatx())

def top_k_categorical_accuracy(y_true, y_pred, k=5):
    return K.mean(K.in_top_k(y_pred, K.argmax(y_true, axis=-1), k), axis=-1)

def sparse_top_k_categorical_accuracy(y_true, y_pred, k=5):
    return K.mean(K.in_top_k(y_pred, K.cast(K.max(y_true, axis=-1), 'int32'), k),
                  axis=-1)

# Aliases
mse = MSE = mean_squared_error
mae = MAE = mean_absolute_error
mape = MAPE = mean_absolute_percentage_error
msle = MSLE = mean_squared_logarithmic_error
cosine = cosine_proximity

def serialize(metric):
    return serialize_keras_object(metric)

def deserialize(config, custom_objects=None):
    return deserialize_keras_object(config,
                                    module_objects=globals(),
                                    custom_objects=custom_objects,
                                    printable_module_name='metric function')

def get(identifier):
    if isinstance(identifier, dict):
        config = {'class_name': str(identifier), 'config': {}}
        return deserialize(config)
    elif isinstance(identifier, six.string_types):
        return deserialize(str(identifier))
    elif callable(identifier):
        return identifier
    else:
        raise ValueError('Could not interpret '
                         'metric function identifier:', identifier)",[47]
"# Copyright (c) 2020 Matt Martz <matt@sivel.net>
# GNU General Public License v3.0+ (see COPYING or https://www.gnu.org/licenses/gpl-3.0.txt)
# Make coding more python3-ish
from __future__ import (absolute_import, division, print_function)
__metaclass__ = type
import re
from distutils.version import LooseVersion, Version
from ansible.module_utils.six import text_type

# Regular expression taken from
# https://semver.org/#is-there-a-suggested-regular-expression-regex-to-check-a-semver-string
SEMVER_RE = re.compile(
    r'''
    ^
        (?P<major>0|[1-9]\d*)
        \.
        (?P<minor>0|[1-9]\d*)
        \.
        (?P<patch>0|[1-9]\d*)
        (?:
            -
            (?P<prerelease>
                (?:0|[1-9]\d*|\d*[a-zA-Z-][0-9a-zA-Z-]*)
                (?:\.(?:0|[1-9]\d*|\d*[a-zA-Z-][0-9a-zA-Z-]*))*
            )
        )?
        (?:
            \+
            (?P<buildmetadata>[0-9a-zA-Z-]+(?:\.[0-9a-zA-Z-]+)*)
        )?
    $
    ''',
    flags=re.X
)

class _Alpha:
    """"""Class to easily allow comparing strings
    Largely this exists to make comparing an integer and a string on py3
    so that it works like py2.
    """"""
    def __init__(self, specifier):
        self.specifier = specifier
    def __repr__(self):
        return repr(self.specifier)
    def __eq__(self, other):
        if isinstance(other, _Alpha):
            return self.specifier == other.specifier
        elif isinstance(other, str):
            return self.specifier == other
        return False
    def __ne__(self, other):
        return not self.__eq__(other)
    def __lt__(self, other):
        if isinstance(other, _Alpha):
            return self.specifier < other.specifier
        elif isinstance(other, str):
            return self.specifier < other
        elif isinstance(other, _Numeric):
            return False
        raise ValueError
    def __gt__(self, other):
        return not self.__lt__(other)
    def __le__(self, other):
        return self.__lt__(other) or self.__eq__(other)
    def __ge__(self, other):
        return self.__gt__(other) or self.__eq__(other)

class _Numeric:
    """"""Class to easily allow comparing numbers
    Largely this exists to make comparing an integer and a string on py3
    so that it works like py2.
    """"""
    def __init__(self, specifier):
        self.specifier = int(specifier)
    def __repr__(self):
        return repr(self.specifier)
    def __eq__(self, other):
        if isinstance(other, _Numeric):
            return self.specifier == other.specifier
        elif isinstance(other, int):
            return self.specifier == other
        return False
    def __ne__(self, other):
        return not self.__eq__(other)
    def __lt__(self, other):
        if isinstance(other, _Numeric):
            return self.specifier < other.specifier
        elif isinstance(other, int):
            return self.specifier < other
        elif isinstance(other, _Alpha):
            return True
        raise ValueError
    def __gt__(self, other):
        return not self.__lt__(other)
    def __le__(self, other):
        return self.__lt__(other) or self.__eq__(other)
    def __ge__(self, other):
        return self.__gt__(other) or self.__eq__(other)
","[74, 75, 76, 81, 117, 118, 119, 124]"
"            # as is_integer returns True for these
            if not is_period_dtype(self):
                maybe_integer_op_deprecated(self)
            result = self._time_shift(-other)
        elif isinstance(other, Period):
            result = self._sub_period(other)
        # array-like others
        elif is_timedelta64_dtype(other):
            # TimedeltaIndex, ndarray[timedelta64]
            result = self._add_delta(-other)
        elif is_offsetlike(other):
            # Array/Index of DateOffset objects
            result = self._addsub_offset_array(other, operator.sub)
        elif is_datetime64_dtype(other) or is_datetime64tz_dtype(other):
            # DatetimeIndex, ndarray[datetime64]
            result = self._sub_datetime_arraylike(other)
        elif is_period_dtype(other):
            # PeriodIndex
            result = self._sub_period_array(other)
        elif is_integer_dtype(other):
            if not is_period_dtype(self):
                maybe_integer_op_deprecated(self)
            result = self._addsub_int_array(other, operator.sub)
        elif isinstance(other, ABCIndexClass):
            raise TypeError(
                ""cannot subtract {cls} and {typ}"".format(
                    cls=type(self).__name__, typ=type(other).__name__
                )
            )
        elif is_float_dtype(other):
            # Explicitly catch invalid dtypes
            raise TypeError(
                ""cannot subtract {dtype}-dtype from {cls}"".format(
                    dtype=other.dtype, cls=type(self).__name__
                )
            )
        elif is_extension_array_dtype(other):
            # Categorical op will raise; defer explicitly
            return NotImplemented
        else:  # pragma: no cover
            return NotImplemented
        if is_timedelta64_dtype(result) and isinstance(result, np.ndarray):
            from pandas.core.arrays import TimedeltaArray
            # TODO: infer freq?
            return TimedeltaArray(result)
        return result
    def __rsub__(self, other):
        if is_datetime64_dtype(other) and is_timedelta64_dtype(self):
            # ndarray[datetime64] cannot be subtracted from self, so
            # we need to wrap in DatetimeArray/Index and flip the operation
            if not isinstance(other, DatetimeLikeArrayMixin):
                # Avoid down-casting DatetimeIndex
                from pandas.core.arrays import DatetimeArray
                other = DatetimeArray(other)
            return other - self
        elif (
            is_datetime64_any_dtype(self)
            and hasattr(other, ""dtype"")
            and not is_datetime64_any_dtype(other)
        ):
            # GH#19959 datetime - datetime is well-defined as timedelta,
            # but any other type - datetime is not well-defined.
            raise TypeError(
                ""cannot subtract {cls} from {typ}"".format(
                    cls=type(self).__name__, typ=type(other).__name__
                )
            )
        elif is_period_dtype(self) and is_timedelta64_dtype(other):
            # TODO: Can we simplify/generalize these cases at all?
            raise TypeError(
                ""cannot subtract {cls} from {dtype}"".format(
                    cls=type(self).__name__, dtype=other.dtype
                )
            )
        return -(self - other)
    # FIXME: DTA/TDA/PA inplace methods should actually be inplace, GH#24115
    def __iadd__(self, other):
        # alias for __add__
        return self.__add__(other)
    def __isub__(self, other):
        # alias for __sub__
        return self.__sub__(other)
    # --------------------------------------------------------------
    # Comparison Methods
    def _ensure_localized(
        self, arg, ambiguous=""raise"", nonexistent=""raise"", from_utc=False
    ):
        """"""
        Ensure that we are re-localized.
        This is for compat as we can then call this on all datetimelike
        arrays generally (ignored for Period/Timedelta)
        Parameters
        ----------
        arg : Union[DatetimeLikeArray, DatetimeIndexOpsMixin, ndarray]
        ambiguous : str, bool, or bool-ndarray, default 'raise'
        nonexistent : str, default 'raise'
        from_utc : bool, default False
            If True, localize the i8 ndarray to UTC first before converting to
            the appropriate tz. If False, localize directly to the tz.
        Returns
        -------
        localized array
        """"""
        # reconvert to local tz
        tz = getattr(self, ""tz"", None)
        if tz is not None:
            if not isinstance(arg, type(self)):
                arg = self._simple_new(arg)
            if from_utc:
                arg = arg.tz_localize(""UTC"").tz_convert(self.tz)
            else:
                arg = arg.tz_localize(
                    self.tz, ambiguous=ambiguous, nonexistent=nonexistent",[52]
"""""""Functions for discovering and executing various cookiecutter hooks.""""""
import errno
import logging
import os
import subprocess
import sys
import tempfile
from cookiecutter import utils
from cookiecutter.environment import StrictEnvironment
from cookiecutter.exceptions import FailedHookException
logger = logging.getLogger(__name__)
_HOOKS = [
    'pre_gen_project',
    'post_gen_project',
]
EXIT_SUCCESS = 0

def valid_hook(hook_file, hook_name):
    """"""Determine if a hook file is valid.
    :param hook_file: The hook file to consider for validity
    :param hook_name: The hook to find
    :return: The hook file validity
    """"""
    filename = os.path.basename(hook_file)
    basename = os.path.splitext(filename)[0]
    matching_hook = basename == hook_name
    supported_hook = basename in _HOOKS
    backup_file = filename.endswith('~')
    return matching_hook and supported_hook and not backup_file

def find_hook(hook_name, hooks_dir='hooks'):
    """"""Return a dict of all hook scripts provided.
    Must be called with the project template as the current working directory.
    Dict's key will be the hook/script's name, without extension, while values
    will be the absolute path to the script. Missing scripts will not be
    included in the returned dict.
    :param hook_name: The hook to find
    :param hooks_dir: The hook directory in the template
    :return: The absolute path to the hook script or None
    """"""
    logger.debug('hooks_dir is %s', os.path.abspath(hooks_dir))
    if not os.path.isdir(hooks_dir):
        logger.debug('No hooks/dir in template_dir')
        return None
    for hook_file in os.listdir(hooks_dir):
        if valid_hook(hook_file, hook_name):
            return os.path.abspath(os.path.join(hooks_dir, hook_file))
    return None

def run_script(script_path, cwd='.'):
    """"""Execute a script from a working directory.
    :param script_path: Absolute path to the script to run.
    :param cwd: The directory to run the script from.
    """"""
    run_thru_shell = sys.platform.startswith('win')
    if script_path.endswith('.py'):
        script_command = [sys.executable, script_path]
    else:
        script_command = [script_path]
    utils.make_executable(script_path)
    try:
        proc = subprocess.Popen(script_command, shell=run_thru_shell, cwd=cwd)
        exit_status = proc.wait()
        if exit_status != EXIT_SUCCESS:
            raise FailedHookException(
                'Hook script failed (exit status: {})'.format(exit_status)
            )
    except OSError as os_error:
        if os_error.errno == errno.ENOEXEC:
            raise FailedHookException(
                'Hook script failed, might be an empty file or missing a shebang'
            )
        raise FailedHookException('Hook script failed (error: {})'.format(os_error))

def run_script_with_context(script_path, cwd, context):
    """"""Execute a script after rendering it with Jinja.
    :param script_path: Absolute path to the script to run.
    :param cwd: The directory to run the script from.
    :param context: Cookiecutter project template context.
    """"""
    _, extension = os.path.splitext(script_path)
    with open(script_path, 'r', encoding='utf-8') as file:
        contents = file.read()
    with tempfile.NamedTemporaryFile(delete=False, mode='wb', suffix=extension) as temp:
        env = StrictEnvironment(context=context, keep_trailing_newline=True)
        template = env.from_string(contents)
        output = template.render(**context)
        temp.write(output.encode('utf-8'))
    run_script(temp.name, cwd)

def run_hook(hook_name, project_dir, context):
    """"""
    Try to find and execute a hook from the specified project directory.
    :param hook_name: The hook to execute.
    :param project_dir: The directory to execute the script from.
    :param context: Cookiecutter project context.
    """"""
    script = find_hook(hook_name)
    if script is None:
        logger.debug('No %s hook found', hook_name)
        return
    logger.debug('Running hook %s', hook_name)
    run_script_with_context(script, project_dir, context)","[58, 60, 121, 122]"
"    buf = ctypes.create_string_buffer(len(title_bytes))
    buf.value = title_bytes
    try:
        libc.prctl(15, buf, 0, 0, 0)
    except AttributeError:
        return  # Strange libc, just skip this

def remove_start(s, start):
    return s[len(start):] if s is not None and s.startswith(start) else s

def remove_end(s, end):
    return s[:-len(end)] if s is not None and s.endswith(end) else s

def remove_quotes(s):
    if s is None or len(s) < 2:
        return s
    for quote in ('""', ""'"", ):
        if s[0] == quote and s[-1] == quote:
            return s[1:-1]
    return s

def url_basename(url):
    path = compat_urlparse.urlparse(url).path
    return path.strip('/').split('/')[-1]

def base_url(url):
    return re.match(r'https?://[^?#&]+/', url).group()

def urljoin(base, path):
    if isinstance(path, bytes):
        path = path.decode('utf-8')
    if not isinstance(path, compat_str) or not path:
        return None
    if re.match(r'^(?:[a-zA-Z][a-zA-Z0-9+-.]*:)?//', path):
        return path
    if isinstance(base, bytes):
        base = base.decode('utf-8')
    if not isinstance(base, compat_str) or not re.match(
            r'^(?:https?:)?//', base):
        return None
    return compat_urlparse.urljoin(base, path)

class HEADRequest(compat_urllib_request.Request):
    def get_method(self):
        return 'HEAD'

class PUTRequest(compat_urllib_request.Request):
    def get_method(self):
        return 'PUT'

def int_or_none(v, scale=1, default=None, get_attr=None, invscale=1):
    if get_attr:
        if v is not None:
            v = getattr(v, get_attr, None)
    if v == '':
        v = None
    if v is None:
        return default
    try:
        return int(v) * invscale // scale
    except (ValueError, TypeError):
        return default

def str_or_none(v, default=None):
    return default if v is None else compat_str(v)

def str_to_int(int_str):
    """""" A more relaxed version of int_or_none """"""
    if int_str is None:
        return None
    int_str = re.sub(r'[,\.\+]', '', int_str)
    return int(int_str)

def float_or_none(v, scale=1, invscale=1, default=None):
    if v is None:
        return default
    try:
        return float(v) * invscale / scale
    except (ValueError, TypeError):
        return default

def bool_or_none(v, default=None):
    return v if isinstance(v, bool) else default

def strip_or_none(v, default=None):
    return v.strip() if isinstance(v, compat_str) else default

def url_or_none(url):
    if not url or not isinstance(url, compat_str):
        return None
    url = url.strip()
    return url if re.match(r'^(?:[a-zA-Z][\da-zA-Z.+-]*:)?//', url) else None

def parse_duration(s):
    if not isinstance(s, compat_basestring):
        return None
    s = s.strip()
    days, hours, mins, secs, ms = [None] * 5
    m = re.match(r'(?:(?:(?:(?P<days>[0-9]+):)?(?P<hours>[0-9]+):)?(?P<mins>[0-9]+):)?(?P<secs>[0-9]+)(?P<ms>\.[0-9]+)?Z?$', s)
    if m:
        days, hours, mins, secs, ms = m.groups()
    else:
        m = re.match(
            r'''(?ix)(?:P?
                (?:
                    [0-9]+\s*y(?:ears?)?\s*
                )?
                (?:
                    [0-9]+\s*m(?:onths?)?\s*","[79, 80]"
"from thefuck.utils import replace_argument
from thefuck.specific.git import git_support

@git_support
def match(command):
    return ('push' in command.script
            and 'set-upstream' in command.stderr)

@git_support
def get_new_command(command):
    # If --set-upstream or -u are passed, remove it and its argument. This is
    # because the remaining arguments are concatenated onto the command suggested
    # by git, which includes --set-upstream and its argument
    upstream_option_index = -1
    try:
        upstream_option_index = command.script_parts.index('--set-upstream')
    except ValueError:
        pass
    try:
        upstream_option_index = command.script_parts.index('-u')
    except ValueError:
        pass
    if upstream_option_index is not -1:
        command.script_parts.pop(upstream_option_index)
        command.script_parts.pop(upstream_option_index)
    push_upstream = command.stderr.split('\n')[-3].strip().partition('git ')[2]
    return replace_argument("" "".join(command.script_parts), 'push', push_upstream)",[26]
"from datetime import datetime, time, timedelta, tzinfo
import operator
from typing import Optional
import warnings
import numpy as np
from pandas._libs import (
    NaT,
    Timedelta,
    Timestamp,
    index as libindex,
    lib,
    tslib as libts,
)
from pandas._libs.tslibs import ccalendar, fields, parsing, timezones
from pandas.util._decorators import cache_readonly
from pandas.core.dtypes.common import _NS_DTYPE, is_float, is_integer, is_scalar
from pandas.core.dtypes.dtypes import DatetimeTZDtype
from pandas.core.dtypes.missing import is_valid_nat_for_dtype
from pandas.core.accessor import delegate_names
from pandas.core.arrays.datetimes import (
    DatetimeArray,
    tz_to_dtype,
    validate_tz_from_dtype,
)
import pandas.core.common as com
from pandas.core.indexes.base import Index, maybe_extract_name
from pandas.core.indexes.datetimelike import (
    DatetimelikeDelegateMixin,
    DatetimeTimedeltaMixin,
)
from pandas.core.indexes.extension import inherit_names
from pandas.core.ops import get_op_result_name
import pandas.core.tools.datetimes as tools
from pandas.tseries.frequencies import Resolution, to_offset
from pandas.tseries.offsets import prefix_mapping

def _new_DatetimeIndex(cls, d):
    """"""
    This is called upon unpickling, rather than the default which doesn't
    have arguments and breaks __new__
    """"""
    if ""data"" in d and not isinstance(d[""data""], DatetimeIndex):
        # Avoid need to verify integrity by calling simple_new directly
        data = d.pop(""data"")
        result = cls._simple_new(data, **d)
    else:
        with warnings.catch_warnings():
            # TODO: If we knew what was going in to **d, we might be able to
            #  go through _simple_new instead
            warnings.simplefilter(""ignore"")
            result = cls.__new__(cls, **d)
    return result

class DatetimeDelegateMixin(DatetimelikeDelegateMixin):
    # Most attrs are dispatched via datetimelike_{ops,methods}
    # Some are ""raw"" methods, the result is not not re-boxed in an Index
    # We also have a few ""extra"" attrs, which may or may not be raw,
    # which we we dont' want to expose in the .dt accessor.
    _extra_methods = [""to_period"", ""to_perioddelta"", ""to_julian_date"", ""strftime""]
    _extra_raw_methods = [
        ""to_pydatetime"",
        ""_local_timestamps"",
        ""_has_same_tz"",
        ""_format_native_types"",
        ""__iter__"",
    ]
    _extra_raw_properties = [""_box_func"", ""tz"", ""tzinfo"", ""dtype""]
    _delegated_properties = DatetimeArray._datetimelike_ops + _extra_raw_properties
    _delegated_methods = (
        DatetimeArray._datetimelike_methods + _extra_methods + _extra_raw_methods
    )
    _raw_properties = (
        {""date"", ""time"", ""timetz""}
        | set(DatetimeArray._bool_ops)
        | set(_extra_raw_properties)
    )
    _raw_methods = set(_extra_raw_methods)

@inherit_names([""_timezone"", ""is_normalized"", ""_resolution""], DatetimeArray, cache=True)
@inherit_names(
    [
        ""_bool_ops"",
        ""_object_ops"",
        ""_field_ops"",
        ""_datetimelike_ops"",
        ""_datetimelike_methods"",
    ],
    DatetimeArray,
)
@delegate_names(
    DatetimeArray, DatetimeDelegateMixin._delegated_properties, typ=""property""
)
@delegate_names(
    DatetimeArray,
    DatetimeDelegateMixin._delegated_methods,
    typ=""method"",
    overwrite=True,
)
class DatetimeIndex(DatetimeTimedeltaMixin, DatetimeDelegateMixin):
    """"""
    Immutable ndarray of datetime64 data, represented internally as int64, and
    which can be boxed to Timestamp objects that are subclasses of datetime and
    carry metadata such as frequency information.
    Parameters
    ----------
    data : array-like (1-dimensional), optional
        Optional datetime-like data to construct index with.
    copy : bool
        Make a copy of input ndarray.
    freq : str or pandas offset object, optional
        One of pandas date offset strings or corresponding objects. The string
        'infer' can be passed in order to set the frequency of the index as the
        inferred frequency upon creation.
    tz : pytz.timezone or dateutil.tz.tzfile
    ambiguous : 'infer', bool-ndarray, 'NaT', default 'raise'
        When clocks moved backward due to DST, ambiguous times may arise.
        For example in Central European Time (UTC+01), when going from 03:00",[29]
"                output_generator = iter_sequence_infinite(generator)
            else:
                output_generator = generator
        callback_model.stop_training = False
        # Construct epoch logs.
        epoch_logs = {}
        while epoch < epochs:
            for m in model.stateful_metric_functions:
                m.reset_states()
            callbacks.on_epoch_begin(epoch)
            steps_done = 0
            batch_index = 0
            while steps_done < steps_per_epoch:
                generator_output = next(output_generator)
                if not hasattr(generator_output, '__len__'):
                    raise ValueError('Output of generator should be '
                                     'a tuple `(x, y, sample_weight)` '
                                     'or `(x, y)`. Found: ' +
                                     str(generator_output))
                if len(generator_output) == 2:
                    x, y = generator_output
                    sample_weight = None
                elif len(generator_output) == 3:
                    x, y, sample_weight = generator_output
                else:
                    raise ValueError('Output of generator should be '
                                     'a tuple `(x, y, sample_weight)` '
                                     'or `(x, y)`. Found: ' +
                                     str(generator_output))
                # build batch logs
                batch_logs = {}
                if x is None or len(x) == 0:
                    # Handle data tensors support when no input given
                    # step-size = 1 for data tensors
                    batch_size = 1
                elif isinstance(x, list):
                    batch_size = x[0].shape[0]
                elif isinstance(x, dict):
                    batch_size = list(x.values())[0].shape[0]
                else:
                    batch_size = x.shape[0]
                batch_logs['batch'] = batch_index
                batch_logs['size'] = batch_size
                callbacks.on_batch_begin(batch_index, batch_logs)
                outs = model.train_on_batch(x, y,
                                            sample_weight=sample_weight,
                                            class_weight=class_weight)
                outs = to_list(outs)
                for l, o in zip(out_labels, outs):
                    batch_logs[l] = o
                callbacks.on_batch_end(batch_index, batch_logs)
                batch_index += 1
                steps_done += 1
                # Epoch finished.
                if steps_done >= steps_per_epoch and do_validation:
                    if val_gen:
                        val_outs = model.evaluate_generator(
                            val_enqueuer_gen,
                            validation_steps,
                            workers=0)
                    else:
                        # No need for try/except because
                        # data has already been validated.
                        val_outs = model.evaluate(
                            val_x, val_y,
                            batch_size=batch_size,
                            sample_weight=val_sample_weights,
                            verbose=0)
                    val_outs = to_list(val_outs)
                    # Same labels assumed.
                    for l, o in zip(out_labels, val_outs):
                        epoch_logs['val_' + l] = o
                if callback_model.stop_training:
                    break
            callbacks.on_epoch_end(epoch, epoch_logs)
            epoch += 1
            if callback_model.stop_training:
                break
    finally:
        try:
            if enqueuer is not None:
                enqueuer.stop()
        finally:
            if val_enqueuer is not None:
                val_enqueuer.stop()
    callbacks.on_train_end()
    return model.history

def evaluate_generator(model, generator,
                       steps=None,
                       max_queue_size=10,
                       workers=1,
                       use_multiprocessing=False,
                       verbose=0):
    """"""See docstring for `Model.evaluate_generator`.""""""
    model._make_test_function()
    if hasattr(model, 'metrics'):
        for m in model.stateful_metric_functions:
            m.reset_states()
        stateful_metric_indices = [
            i for i, name in enumerate(model.metrics_names)
            if str(name) in model.stateful_metric_names]
    else:
        stateful_metric_indices = []
    steps_done = 0
    outs_per_batch = []
    batch_sizes = []
    is_sequence = isinstance(generator, Sequence)
    if not is_sequence and use_multiprocessing and workers > 1:
        warnings.warn(
            UserWarning('Using a generator with `use_multiprocessing=True`'
                        ' and multiple workers may duplicate your data.'","[122, 123]"
"                )
            else:
                values[field.name] = deepcopy(field.default)
            continue
        v_, errors_ = field.validate(value, values, loc=(schema.in_.value, field.alias))
        if isinstance(errors_, ErrorWrapper):
            errors.append(errors_)
        elif isinstance(errors_, list):
            errors.extend(errors_)
        else:
            values[field.name] = v_
    return values, errors

async def request_body_to_args(
    required_params: List[Field],
    received_body: Optional[Union[Dict[str, Any], FormData]],
) -> Tuple[Dict[str, Any], List[ErrorWrapper]]:
    values = {}
    errors = []
    if required_params:
        field = required_params[0]
        embed = getattr(field.schema, ""embed"", None)
        if len(required_params) == 1 and not embed:
            received_body = {field.alias: received_body}
        for field in required_params:
            value: Any = None
            if received_body is not None:
                if field.shape in sequence_shapes and isinstance(
                    received_body, FormData
                ):
                    value = received_body.getlist(field.alias)
                else:
                    value = received_body.get(field.alias)
            if (
                value is None
                or (isinstance(field.schema, params.Form) and value == """")
                or (
                    isinstance(field.schema, params.Form)
                    and field.shape in sequence_shapes
                    and len(value) == 0
                )
            ):
                if field.required:
                    errors.append(
                        ErrorWrapper(
                            MissingError(), loc=(""body"", field.alias), config=BaseConfig
                        )
                    )
                else:
                    values[field.name] = deepcopy(field.default)
                continue
            if (
                isinstance(field.schema, params.File)
                and lenient_issubclass(field.type_, bytes)
                and isinstance(value, UploadFile)
            ):
                value = await value.read()
            elif (
                field.shape in sequence_shapes
                and isinstance(field.schema, params.File)
                and lenient_issubclass(field.type_, bytes)
                and isinstance(value, sequence_types)
            ):
                awaitables = [sub_value.read() for sub_value in value]
                contents = await asyncio.gather(*awaitables)
                value = sequence_shape_to_type[field.shape](contents)
            v_, errors_ = field.validate(value, values, loc=(""body"", field.alias))
            if isinstance(errors_, ErrorWrapper):
                errors.append(errors_)
            elif isinstance(errors_, list):
                errors.extend(errors_)
            else:
                values[field.name] = v_
    return values, errors

def get_schema_compatible_field(*, field: Field) -> Field:
    out_field = field
    if lenient_issubclass(field.type_, UploadFile):
        use_type: type = bytes
        if field.shape in sequence_shapes:
            use_type = List[bytes]
        out_field = Field(
            name=field.name,
            type_=use_type,
            class_validators=field.class_validators,
            model_config=field.model_config,
            default=field.default,
            required=field.required,
            alias=field.alias,
            schema=field.schema,
        )
    return out_field

def get_body_field(*, dependant: Dependant, name: str) -> Optional[Field]:
    flat_dependant = get_flat_dependant(dependant)
    if not flat_dependant.body_params:
        return None
    first_param = flat_dependant.body_params[0]
    embed = getattr(first_param.schema, ""embed"", None)
    if len(flat_dependant.body_params) == 1 and not embed:
        return get_schema_compatible_field(field=first_param)
    model_name = ""Body_"" + name
    BodyModel = create_model(model_name)
    for f in flat_dependant.body_params:
        BodyModel.__fields__[f.name] = get_schema_compatible_field(field=f)
    required = any(True for f in flat_dependant.body_params if f.required)
    if any(isinstance(f.schema, params.File) for f in flat_dependant.body_params):
        BodySchema: Type[params.Body] = params.File
    elif any(isinstance(f.schema, params.Form) for f in flat_dependant.body_params):
        BodySchema = params.Form
    else:
        BodySchema = params.Body
    field = Field(
        name=""body"",
        type_=BodyModel,
        default=None,
        required=required,
        model_config=BaseConfig,
        class_validators={},
        alias=""body"",
        schema=BodySchema(None),
    )",[124]
"from __future__ import unicode_literals
import re
from .common import InfoExtractor
from ..utils import (
    fix_xml_all_ampersand,
)

class MetacriticIE(InfoExtractor):
    _VALID_URL = r'https?://www\.metacritic\.com/.+?/trailers/(?P<id>\d+)'
    _TEST = {
        'url': 'http://www.metacritic.com/game/playstation-4/infamous-second-son/trailers/3698222',
        'file': '3698222.mp4',
        'info_dict': {
            'title': 'inFamous: Second Son - inSide Sucker Punch: Smoke & Mirrors',
            'description': 'Take a peak behind-the-scenes to see how Sucker Punch brings smoke into the universe of inFAMOUS Second Son on the PS4.',
            'duration': 221,
        },
    }
    def _real_extract(self, url):
        mobj = re.match(self._VALID_URL, url)
        video_id = mobj.group('id')
        webpage = self._download_webpage(url, video_id)
        # The xml is not well formatted, there are raw '&'
        info = self._download_xml('http://www.metacritic.com/video_data?video=' + video_id,
            video_id, 'Downloading info xml', transform_source=fix_xml_all_ampersand)
        clip = next(c for c in info.findall('playList/clip') if c.find('id').text == video_id)
        formats = []
        for videoFile in clip.findall('httpURI/videoFile'):
            rate_str = videoFile.find('rate').text
            video_url = videoFile.find('filePath').text
            formats.append({
                'url': video_url,
                'ext': 'mp4',
                'format_id': rate_str,
                'tbr': int(rate_str),
            })
        self._sort_formats(formats)
        description = self._html_search_regex(r'<b>Description:</b>(.*?)</p>',
            webpage, 'description', flags=re.DOTALL)
        return {
            'id': video_id,
            'title': clip.find('title').text,
            'formats': formats,
            'description': description,
            'duration': int(clip.find('duration').text),
        }","[6, 29]"
"# -*- coding: utf-8 -*-
#
# Copyright 2012-2015 Spotify AB
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
""""""
The implementations of the hdfs clients. The hadoop cli client and the
snakebite client.
""""""

from luigi.contrib.hdfs import config as hdfs_config
from luigi.contrib.hdfs import snakebite_client as hdfs_snakebite_client
from luigi.contrib.hdfs import webhdfs_client as hdfs_webhdfs_client
from luigi.contrib.hdfs import hadoopcli_clients as hdfs_hadoopcli_clients
import luigi.contrib.target
import logging
logger = logging.getLogger('luigi-interface')

def get_autoconfig_client():
    """"""
    Creates the client as specified in the `luigi.cfg` configuration.
    """"""
    configured_client = hdfs_config.get_configured_hdfs_client()
    if configured_client == ""webhdfs"":
        return hdfs_webhdfs_client.WebHdfsClient()
    if configured_client == ""snakebite"":
        return hdfs_snakebite_client.SnakebiteHdfsClient()
    if configured_client == ""snakebite_with_hadoopcli_fallback"":
        return luigi.contrib.target.CascadingClient([hdfs_snakebite_client.SnakebiteHdfsClient(),
                                                     hdfs_hadoopcli_clients.create_hadoopcli_client()])
    if configured_client == ""hadoopcli"":
        return hdfs_hadoopcli_clients.create_hadoopcli_client()
    raise Exception(""Unknown hdfs client "" + configured_client)

def _with_ac(method_name):
    def result(*args, **kwargs):
        return getattr(get_autoconfig_client(), method_name)(*args, **kwargs)
    return result
exists = _with_ac('exists')
rename = _with_ac('rename')
remove = _with_ac('remove')
mkdir = _with_ac('mkdir')
listdir = _with_ac('listdir')","[21, 28, 33, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]"
"#!/usr/bin/env python
#
# Copyright 2009 Facebook
#
# Licensed under the Apache License, Version 2.0 (the ""License""); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
""""""This module contains implementations of various third-party
authentication schemes.
All the classes in this file are class mixins designed to be used with
the `tornado.web.RequestHandler` class.  They are used in two ways:
* On a login handler, use methods such as ``authenticate_redirect()``,
  ``authorize_redirect()``, and ``get_authenticated_user()`` to
  establish the user's identity and store authentication tokens to your
  database and/or cookies.
* In non-login handlers, use methods such as ``facebook_request()``
  or ``twitter_request()`` to use the authentication tokens to make
  requests to the respective services.
They all take slightly different arguments due to the fact all these
services implement authentication and authorization slightly differently.
See the individual service classes below for complete documentation.
Example usage for Google OAuth:
.. testcode::
    class GoogleOAuth2LoginHandler(tornado.web.RequestHandler,
                                   tornado.auth.GoogleOAuth2Mixin):
        @tornado.gen.coroutine
        def get(self):
            if self.get_argument('code', False):
                user = yield self.get_authenticated_user(
                    redirect_uri='http://your.site.com/auth/google',
                    code=self.get_argument('code'))
                # Save the user with e.g. set_secure_cookie
            else:
                yield self.authorize_redirect(
                    redirect_uri='http://your.site.com/auth/google',
                    client_id=self.settings['google_oauth']['key'],
                    scope=['profile', 'email'],
                    response_type='code',
                    extra_params={'approval_prompt': 'auto'})
.. testoutput::
   :hide:

.. versionchanged:: 4.0
   All of the callback interfaces in this module are now guaranteed
   to run their callback with an argument of ``None`` on error.
   Previously some functions would do this while others would simply
   terminate the request on their own.  This change also ensures that
   errors are more consistently reported through the ``Future`` interfaces.
""""""
from __future__ import absolute_import, division, print_function, with_statement
import base64
import binascii
import functools
import hashlib
import hmac
import time
import uuid
from tornado.concurrent import TracebackFuture, return_future
from tornado import gen
from tornado import httpclient
from tornado import escape
from tornado.httputil import url_concat
from tornado.log import gen_log
from tornado.stack_context import ExceptionStackContext
from tornado.util import u, unicode_type, ArgReplacer
try:
    import urlparse  # py2
except ImportError:
    import urllib.parse as urlparse  # py3
try:
    import urllib.parse as urllib_parse  # py3
except ImportError:
    import urllib as urllib_parse  # py2
try:
    long  # py2
except NameError:
    long = int  # py3

class AuthError(Exception):
    pass

def _auth_future_to_callback(callback, future):
    try:
        result = future.result()
    except AuthError as e:
        gen_log.warning(str(e))
        result = None
    callback(result)

def _auth_return_future(f):
    """"""Similar to tornado.concurrent.return_future, but uses the auth
    module's legacy callback interface.
    Note that when using this decorator the ``callback`` parameter
    inside the function will actually be a future.
    """"""
    replacer = ArgReplacer(f, 'callback')
    @functools.wraps(f)
    def wrapper(*args, **kwargs):
        future = TracebackFuture()",[77]
"    else:
        if isinstance(data.columns, MultiIndex):
            result = data
            for i in range(len(clocs)):
                val = clocs[i]
                result = result.unstack(val, fill_value=fill_value)
                clocs = [v if i > v else v - 1 for v in clocs]
            return result
        dummy = data.copy()
        dummy.index = dummy_index
        unstacked = dummy.unstack(""__placeholder__"", fill_value=fill_value)
        if isinstance(unstacked, Series):
            unstcols = unstacked.index
        else:
            unstcols = unstacked.columns
        assert isinstance(unstcols, MultiIndex)  # for mypy
        new_levels = [unstcols.levels[0]] + clevels
        new_names = [data.columns.name] + cnames
        new_codes = [unstcols.codes[0]]
        for rec in recons_codes:
            new_codes.append(rec.take(unstcols.codes[-1]))
    new_columns = MultiIndex(
        levels=new_levels, codes=new_codes, names=new_names, verify_integrity=False
    )
    if isinstance(unstacked, Series):
        unstacked.index = new_columns
    else:
        unstacked.columns = new_columns
    return unstacked

def unstack(obj, level, fill_value=None):
    if isinstance(level, (tuple, list)):
        if len(level) != 1:
            # _unstack_multiple only handles MultiIndexes,
            # and isn't needed for a single level
            return _unstack_multiple(obj, level, fill_value=fill_value)
        else:
            level = level[0]
    # Prioritize integer interpretation (GH #21677):
    if not is_integer(level) and not level == ""__placeholder__"":
        level = obj.index._get_level_number(level)
    if isinstance(obj, DataFrame):
        if isinstance(obj.index, MultiIndex):
            return _unstack_frame(obj, level, fill_value=fill_value)
        else:
            return obj.T.stack(dropna=False)
    else:
        if is_extension_array_dtype(obj.dtype):
            return _unstack_extension_series(obj, level, fill_value)
        unstacker = _Unstacker(
            obj.values,
            obj.index,
            level=level,
            fill_value=fill_value,
            constructor=obj._constructor_expanddim,
        )
        return unstacker.get_result()

def _unstack_frame(obj, level, fill_value=None):
    if obj._is_mixed_type:
        unstacker = partial(
            _Unstacker, index=obj.index, level=level, fill_value=fill_value
        )
        blocks = obj._data.unstack(unstacker, fill_value=fill_value)
        return obj._constructor(blocks)
    else:
        return _Unstacker(
            obj.values,
            obj.index,
            level=level,
            value_columns=obj.columns,
            fill_value=fill_value,
            constructor=obj._constructor,
        ).get_result()

def _unstack_extension_series(series, level, fill_value):
    """"""
    Unstack an ExtensionArray-backed Series.
    The ExtensionDtype is preserved.
    Parameters
    ----------
    series : Series
        A Series with an ExtensionArray for values
    level : Any
        The level name or number.
    fill_value : Any
        The user-level (not physical storage) fill value to use for
        missing values introduced by the reshape. Passed to
        ``series.values.take``.
    Returns
    -------
    DataFrame
        Each column of the DataFrame will have the same dtype as
        the input Series.
    """"""
    # Implementation note: the basic idea is to
    # 1. Do a regular unstack on a dummy array of integers
    # 2. Followup with a columnwise take.
    # We use the dummy take to discover newly-created missing values
    # introduced by the reshape.
    from pandas.core.reshape.concat import concat
    dummy_arr = np.arange(len(series))
    # fill_value=-1, since we will do a series.values.take later
    result = _Unstacker(
        dummy_arr, series.index, level=level, fill_value=-1
    ).get_result()
    out = []
    values = extract_array(series, extract_numpy=False)
    for col, indices in result.items():",[6]
"    password: str

class HTTPAuthorizationCredentials(BaseModel):
    scheme: str
    credentials: str

class HTTPBase(SecurityBase):
    def __init__(
        self, *, scheme: str, scheme_name: str = None, auto_error: bool = True
    ):
        self.model = HTTPBaseModel(scheme=scheme)
        self.scheme_name = scheme_name or self.__class__.__name__
        self.auto_error = auto_error
    async def __call__(
        self, request: Request
    ) -> Optional[HTTPAuthorizationCredentials]:
        authorization: str = request.headers.get(""Authorization"")
        scheme, credentials = get_authorization_scheme_param(authorization)
        if not (authorization and scheme and credentials):
            if self.auto_error:
                raise HTTPException(
                    status_code=HTTP_403_FORBIDDEN, detail=""Not authenticated""
                )
            else:
                return None
        return HTTPAuthorizationCredentials(scheme=scheme, credentials=credentials)

class HTTPBasic(HTTPBase):
    def __init__(
        self, *, scheme_name: str = None, realm: str = None, auto_error: bool = True
    ):
        self.model = HTTPBaseModel(scheme=""basic"")
        self.scheme_name = scheme_name or self.__class__.__name__
        self.realm = realm
        self.auto_error = auto_error
    async def __call__(self, request: Request) -> Optional[HTTPBasicCredentials]:
        authorization: str = request.headers.get(""Authorization"")
        scheme, param = get_authorization_scheme_param(authorization)
        if self.realm:
            unauthorized_headers = {""WWW-Authenticate"": f'Basic realm=""{self.realm}""'}
        else:
            unauthorized_headers = {""WWW-Authenticate"": ""Basic""}
        invalid_user_credentials_exc = HTTPException(
            status_code=HTTP_401_UNAUTHORIZED,
            detail=""Invalid authentication credentials"",
            headers=unauthorized_headers,
        )
        if not authorization or scheme.lower() != ""basic"":
            if self.auto_error:
                raise HTTPException(
                    status_code=HTTP_401_UNAUTHORIZED,
                    detail=""Not authenticated"",
                    headers=unauthorized_headers,
                )
            else:
                return None
        try:
            data = b64decode(param).decode(""ascii"")
        except (ValueError, UnicodeDecodeError, binascii.Error):
            raise invalid_user_credentials_exc
        username, separator, password = data.partition("":"")
        if not (separator):
            raise invalid_user_credentials_exc
        return HTTPBasicCredentials(username=username, password=password)

class HTTPBearer(HTTPBase):
    def __init__(
        self,
        *,
        bearerFormat: str = None,
        scheme_name: str = None,
        auto_error: bool = True,
    ):
        self.model = HTTPBearerModel(bearerFormat=bearerFormat)
        self.scheme_name = scheme_name or self.__class__.__name__
        self.auto_error = auto_error
    async def __call__(
        self, request: Request
    ) -> Optional[HTTPAuthorizationCredentials]:
        authorization: str = request.headers.get(""Authorization"")
        scheme, credentials = get_authorization_scheme_param(authorization)
        if not (authorization and scheme and credentials):
            if self.auto_error:
                raise HTTPException(
                    status_code=HTTP_403_FORBIDDEN, detail=""Not authenticated""
                )
            else:
                return None
        if scheme.lower() != ""bearer"":
            raise HTTPException(
                status_code=HTTP_403_FORBIDDEN,
                detail=""Invalid authentication credentials"",
            )
        return HTTPAuthorizationCredentials(scheme=scheme, credentials=credentials)

class HTTPDigest(HTTPBase):
    def __init__(self, *, scheme_name: str = None, auto_error: bool = True):
        self.model = HTTPBaseModel(scheme=""digest"")
        self.scheme_name = scheme_name or self.__class__.__name__
        self.auto_error = auto_error
    async def __call__(
        self, request: Request
    ) -> Optional[HTTPAuthorizationCredentials]:
        authorization: str = request.headers.get(""Authorization"")
        scheme, credentials = get_authorization_scheme_param(authorization)
        if not (authorization and scheme and credentials):
            if self.auto_error:
                raise HTTPException(
                    status_code=HTTP_403_FORBIDDEN, detail=""Not authenticated""
                )
            else:
                return None
        if scheme.lower() != ""digest"":
            raise HTTPException(
                status_code=HTTP_403_FORBIDDEN,
                detail=""Invalid authentication credentials"",
            )","[96, 97, 98, 99]"
"        if issubclass(new_data.dtype.type, np.number):
            in_range = (
                isna(new_data._values)
                | (new_data > self.min_stamp)
                | (new_data._values == iNaT)
            )
            if not in_range.all():
                return data, False
        date_units = (self.date_unit,) if self.date_unit else self._STAMP_UNITS
        for date_unit in date_units:
            try:
                new_data = to_datetime(new_data, errors=""raise"", unit=date_unit)
            except (ValueError, OverflowError):
                continue
            return new_data, True
        return data, False
    def _try_convert_dates(self):
        raise AbstractMethodError(self)

class SeriesParser(Parser):
    _default_orient = ""index""
    _split_keys = (""name"", ""index"", ""data"")
    def _parse_no_numpy(self):
        data = loads(self.json, precise_float=self.precise_float)
        if self.orient == ""split"":
            decoded = {str(k): v for k, v in data.items()}
            self.check_keys_split(decoded)
            self.obj = create_series_with_explicit_dtype(**decoded)
        else:
            self.obj = create_series_with_explicit_dtype(data, dtype_if_empty=object)
    def _parse_numpy(self):
        load_kwargs = {
            ""dtype"": None,
            ""numpy"": True,
            ""precise_float"": self.precise_float,
        }
        if self.orient in [""columns"", ""index""]:
            load_kwargs[""labelled""] = True
        loads_ = functools.partial(loads, **load_kwargs)
        data = loads_(self.json)
        if self.orient == ""split"":
            decoded = {str(k): v for k, v in data.items()}
            self.check_keys_split(decoded)
            self.obj = create_series_with_explicit_dtype(**decoded)
        elif self.orient in [""columns"", ""index""]:
            self.obj = create_series_with_explicit_dtype(*data, dtype_if_empty=object)
        else:
            self.obj = create_series_with_explicit_dtype(data, dtype_if_empty=object)
    def _try_convert_types(self):
        if self.obj is None:
            return
        obj, result = self._try_convert_data(
            ""data"", self.obj, convert_dates=self.convert_dates
        )
        if result:
            self.obj = obj

class FrameParser(Parser):
    _default_orient = ""columns""
    _split_keys = (""columns"", ""index"", ""data"")
    def _parse_numpy(self):
        json = self.json
        orient = self.orient
        if orient == ""columns"":
            args = loads(
                json,
                dtype=None,
                numpy=True,
                labelled=True,
                precise_float=self.precise_float,
            )
            if len(args):
                args = (args[0].T, args[2], args[1])
            self.obj = DataFrame(*args)
        elif orient == ""split"":
            decoded = loads(
                json, dtype=None, numpy=True, precise_float=self.precise_float
            )
            decoded = {str(k): v for k, v in decoded.items()}
            self.check_keys_split(decoded)
            self.obj = DataFrame(**decoded)
        elif orient == ""values"":
            self.obj = DataFrame(
                loads(json, dtype=None, numpy=True, precise_float=self.precise_float)
            )
        else:
            self.obj = DataFrame(
                *loads(
                    json,
                    dtype=None,
                    numpy=True,
                    labelled=True,
                    precise_float=self.precise_float,
                )
            )
    def _parse_no_numpy(self):
        json = self.json
        orient = self.orient
        if orient == ""columns"":
            self.obj = DataFrame(
                loads(json, precise_float=self.precise_float), dtype=None
            )
        elif orient == ""split"":
            decoded = {
                str(k): v
                for k, v in loads(json, precise_float=self.precise_float).items()
            }
            self.check_keys_split(decoded)
            self.obj = DataFrame(dtype=None, **decoded)
        elif orient == ""index"":
            self.obj = DataFrame.from_dict(
                loads(json, precise_float=self.precise_float),",[13]
"        Returns
        -------
        bool
            True if all elements are the same in both objects, False
            otherwise.
        See Also
        --------
        Series.eq : Compare two Series objects of the same length
            and return a Series where each element is True if the element
            in each Series is equal, False otherwise.
        DataFrame.eq : Compare two DataFrame objects of the same shape and
            return a DataFrame where each element is True if the respective
            element in each DataFrame is equal, False otherwise.
        testing.assert_series_equal : Raises an AssertionError if left and
            right are not equal. Provides an easy interface to ignore
            inequality in dtypes, indexes and precision among others.
        testing.assert_frame_equal : Like assert_series_equal, but targets
            DataFrames.
        numpy.array_equal : Return True if two arrays have the same shape
            and elements, False otherwise.
        Notes
        -----
        This function requires that the elements have the same dtype as their
        respective elements in the other Series or DataFrame. However, the
        column labels do not need to have the same type, as long as they are
        still considered equal.
        Examples
        --------
        >>> df = pd.DataFrame({1: [10], 2: [20]})
        >>> df
            1   2
        0  10  20
        DataFrames df and exactly_equal have the same types and values for
        their elements and column labels, which will return True.
        >>> exactly_equal = pd.DataFrame({1: [10], 2: [20]})
        >>> exactly_equal
            1   2
        0  10  20
        >>> df.equals(exactly_equal)
        True
        DataFrames df and different_column_type have the same element
        types and values, but have different types for the column labels,
        which will still return True.
        >>> different_column_type = pd.DataFrame({1.0: [10], 2.0: [20]})
        >>> different_column_type
           1.0  2.0
        0   10   20
        >>> df.equals(different_column_type)
        True
        DataFrames df and different_data_type have different types for the
        same values for their elements, and will return False even though
        their column labels are the same values and types.
        >>> different_data_type = pd.DataFrame({1: [10.0], 2: [20.0]})
        >>> different_data_type
              1     2
        0  10.0  20.0
        >>> df.equals(different_data_type)
        False
        """"""
        if not isinstance(other, self._constructor):
            return False
        return self._data.equals(other._data)
    # -------------------------------------------------------------------------
    # Unary Methods
    def __neg__(self):
        values = com.values_from_object(self)
        if is_bool_dtype(values):
            arr = operator.inv(values)
        elif (
            is_numeric_dtype(values)
            or is_timedelta64_dtype(values)
            or is_object_dtype(values)
        ):
            arr = operator.neg(values)
        else:
            raise TypeError(f""Unary negative expects numeric dtype, not {values.dtype}"")
        return self.__array_wrap__(arr)
    def __pos__(self):
        values = com.values_from_object(self)
        if is_bool_dtype(values) or is_period_arraylike(values):
            arr = values
        elif (
            is_numeric_dtype(values)
            or is_timedelta64_dtype(values)
            or is_object_dtype(values)
        ):
            arr = operator.pos(values)
        else:
            raise TypeError(f""Unary plus expects numeric dtype, not {values.dtype}"")
        return self.__array_wrap__(arr)
    def __invert__(self):
        if not self.size:
            # inv fails with 0 len
            return self
        arr = operator.inv(com.values_from_object(self))
        return self.__array_wrap__(arr)
    def __nonzero__(self):
        raise ValueError(
            f""The truth value of a {type(self).__name__} is ambiguous. ""
            ""Use a.empty, a.bool(), a.item(), a.any() or a.all().""
        )
    __bool__ = __nonzero__
    def bool(self):
        """"""
        Return the bool of a single element PandasObject.
        This must be a boolean scalar value, either True or False.  Raise a
        ValueError if the PandasObject does not have exactly 1 element, or that
        element is not boolean
","[108, 109]"
"        if not all(ax1.equals(ax2) for ax1, ax2 in zip(self_axes, other_axes)):
            return False
        self._consolidate_inplace()
        other._consolidate_inplace()
        if len(self.blocks) != len(other.blocks):
            return False
        # canonicalize block order, using a tuple combining the type
        # name and then mgr_locs because there might be unconsolidated
        # blocks (say, Categorical) which can only be distinguished by
        # the iteration order
        def canonicalize(block):
            return (block.dtype.name, block.mgr_locs.as_array.tolist())
        self_blocks = sorted(self.blocks, key=canonicalize)
        other_blocks = sorted(other.blocks, key=canonicalize)
        return all(
            block.equals(oblock) for block, oblock in zip(self_blocks, other_blocks)
        )
    def unstack(self, unstacker_func, fill_value):
        """"""Return a blockmanager with all blocks unstacked.
        Parameters
        ----------
        unstacker_func : callable
            A (partially-applied) ``pd.core.reshape._Unstacker`` class.
        fill_value : Any
            fill_value for newly introduced missing values.
        Returns
        -------
        unstacked : BlockManager
        """"""
        n_rows = self.shape[-1]
        dummy = unstacker_func(np.empty((0, 0)), value_columns=self.items)
        new_columns = dummy.get_new_columns()
        new_index = dummy.get_new_index()
        new_blocks = []
        columns_mask = []
        for blk in self.blocks:
            blocks, mask = blk._unstack(
                partial(unstacker_func, value_columns=self.items[blk.mgr_locs.indexer]),
                new_columns,
                n_rows,
                fill_value,
            )
            new_blocks.extend(blocks)
            columns_mask.extend(mask)
        new_columns = new_columns[columns_mask]
        bm = BlockManager(new_blocks, [new_columns, new_index])
        return bm

class SingleBlockManager(BlockManager):
    """""" manage a single block with """"""
    ndim = 1
    _is_consolidated = True
    _known_consolidated = True
    __slots__ = ()
    def __init__(
        self,
        block: Block,
        axis: Union[Index, List[Index]],
        do_integrity_check: bool = False,
        fastpath: bool = False,
    ):
        if isinstance(axis, list):
            if len(axis) != 1:
                raise ValueError(
                    ""cannot create SingleBlockManager with more than 1 axis""
                )
            axis = axis[0]
        # passed from constructor, single block, single axis
        if fastpath:
            self.axes = [axis]
            if isinstance(block, list):
                # empty block
                if len(block) == 0:
                    block = [np.array([])]
                elif len(block) != 1:
                    raise ValueError(
                        ""Cannot create SingleBlockManager with more than 1 block""
                    )
                block = block[0]
        else:
            self.axes = [ensure_index(axis)]
            # create the block here
            if isinstance(block, list):
                # provide consolidation to the interleaved_dtype
                if len(block) > 1:
                    dtype = _interleaved_dtype(block)
                    block = [b.astype(dtype) for b in block]
                    block = _consolidate(block)
                if len(block) != 1:
                    raise ValueError(
                        ""Cannot create SingleBlockManager with more than 1 block""
                    )
                block = block[0]
        if not isinstance(block, Block):
            block = make_block(block, placement=slice(0, len(axis)), ndim=1)
        self.blocks = tuple([block])
    def _post_setstate(self):
        pass
    @property
    def _block(self):
        return self.blocks[0]
    @property
    def _values(self):
        return self._block.values
",[12]
"            fill_axis=fill_axis,
            broadcast_axis=broadcast_axis,
        )
    def rename(self, index=None, **kwargs):
        """"""
        Alter Series index labels or name.
        Function / dict values must be unique (1-to-1). Labels not contained in
        a dict / Series will be left as-is. Extra labels listed don't throw an
        error.
        Alternatively, change ``Series.name`` with a scalar value.
        See the :ref:`user guide <basics.rename>` for more.
        Parameters
        ----------
        index : scalar, hashable sequence, dict-like or function, optional
            dict-like or functions are transformations to apply to
            the index.
            Scalar or hashable sequence-like will alter the ``Series.name``
            attribute.
        copy : bool, default True
            Whether to copy underlying data.
        inplace : bool, default False
            Whether to return a new Series. If True then value of copy is
            ignored.
        level : int or level name, default None
            In case of a MultiIndex, only rename labels in the specified
            level.
        Returns
        -------
        Series
            Series with index labels or name altered.
        See Also
        --------
        Series.rename_axis : Set the name of the axis.
        Examples
        --------
        >>> s = pd.Series([1, 2, 3])
        >>> s
        0    1
        1    2
        2    3
        dtype: int64
        >>> s.rename(""my_name"")  # scalar, changes Series.name
        0    1
        1    2
        2    3
        Name: my_name, dtype: int64
        >>> s.rename(lambda x: x ** 2)  # function, changes labels
        0    1
        1    2
        4    3
        dtype: int64
        >>> s.rename({1: 3, 2: 5})  # mapping, changes labels
        0    1
        3    2
        5    3
        dtype: int64
        """"""
        kwargs[""inplace""] = validate_bool_kwarg(kwargs.get(""inplace"", False), ""inplace"")
        non_mapping = is_scalar(index) or (
            is_list_like(index) and not is_dict_like(index)
        )
        if non_mapping:
            return self._set_name(index, inplace=kwargs.get(""inplace""))
        return super().rename(index=index, **kwargs)
    @Substitution(**_shared_doc_kwargs)
    @Appender(generic.NDFrame.reindex.__doc__)
    def reindex(self, index=None, **kwargs):
        return super().reindex(index=index, **kwargs)
    def drop(
        self,
        labels=None,
        axis=0,
        index=None,
        columns=None,
        level=None,
        inplace=False,
        errors=""raise"",
    ):
        """"""
        Return Series with specified index labels removed.
        Remove elements of a Series based on specifying the index labels.
        When using a multi-index, labels on different levels can be removed
        by specifying the level.
        Parameters
        ----------
        labels : single label or list-like
            Index labels to drop.
        axis : 0, default 0
            Redundant for application on Series.
        index, columns : None
            Redundant for application on Series, but index can be used instead
            of labels.
            .. versionadded:: 0.21.0
        level : int or level name, optional
            For MultiIndex, level for which the labels will be removed.
        inplace : bool, default False
            If True, do operation inplace and return None.
        errors : {'ignore', 'raise'}, default 'raise'
            If 'ignore', suppress error and only existing labels are dropped.
        Returns
        -------
        Series
            Series with specified index labels removed.
        Raises
        ------
        KeyError
            If none of the labels are found in the index.
        See Also
        --------
        Series.reindex : Return only specified index labels of Series.","[67, 68, 69, 70, 72]"
"                        self, func=f, result_type=""expand"", ignore_failures=True
                    )
                    result = opa.get_result()
                    if result.ndim == self.ndim:
                        result = result.iloc[0]
                    return result
                # TODO: why doesnt axis matter here?
                data = _get_data(axis_matters=False)
                with np.errstate(all=""ignore""):
                    result = f(data.values)
                labels = data._get_agg_axis(axis)
        else:
            if numeric_only:
                data = _get_data(axis_matters=True)
                values = data.values
                labels = data._get_agg_axis(axis)
            else:
                values = self.values
            result = f(values)
        if hasattr(result, ""dtype"") and is_object_dtype(result.dtype):
            try:
                if filter_type is None or filter_type == ""numeric"":
                    result = result.astype(np.float64)
                elif filter_type == ""bool"" and notna(result).all():
                    result = result.astype(np.bool_)
            except (ValueError, TypeError):
                # try to coerce to the original dtypes item by item if we can
                if axis == 0:
                    result = coerce_to_dtypes(result, self.dtypes)
        if constructor is not None:
            result = Series(result, index=labels)
        return result
    def nunique(self, axis=0, dropna=True) -> Series:
        """"""
        Count distinct observations over requested axis.
        Return Series with number of distinct observations. Can ignore NaN
        values.
        Parameters
        ----------
        axis : {0 or 'index', 1 or 'columns'}, default 0
            The axis to use. 0 or 'index' for row-wise, 1 or 'columns' for
            column-wise.
        dropna : bool, default True
            Don't include NaN in the counts.
        Returns
        -------
        Series
        See Also
        --------
        Series.nunique: Method nunique for Series.
        DataFrame.count: Count non-NA cells for each column or row.
        Examples
        --------
        >>> df = pd.DataFrame({'A': [1, 2, 3], 'B': [1, 1, 1]})
        >>> df.nunique()
        A    3
        B    1
        dtype: int64
        >>> df.nunique(axis=1)
        0    1
        1    2
        2    2
        dtype: int64
        """"""
        return self.apply(Series.nunique, axis=axis, dropna=dropna)
    def idxmin(self, axis=0, skipna=True) -> Series:
        """"""
        Return index of first occurrence of minimum over requested axis.
        NA/null values are excluded.
        Parameters
        ----------
        axis : {0 or 'index', 1 or 'columns'}, default 0
            The axis to use. 0 or 'index' for row-wise, 1 or 'columns' for column-wise.
        skipna : bool, default True
            Exclude NA/null values. If an entire row/column is NA, the result
            will be NA.
        Returns
        -------
        Series
            Indexes of minima along the specified axis.
        Raises
        ------
        ValueError
            * If the row/column is empty
        See Also
        --------
        Series.idxmin
        Notes
        -----
        This method is the DataFrame version of ``ndarray.argmin``.
        """"""
        axis = self._get_axis_number(axis)
        indices = nanops.nanargmin(self.values, axis=axis, skipna=skipna)
        index = self._get_axis(axis)
        result = [index[i] if i >= 0 else np.nan for i in indices]
        return Series(result, index=self._get_agg_axis(axis))
    def idxmax(self, axis=0, skipna=True) -> Series:
        """"""
        Return index of first occurrence of maximum over requested axis.
        NA/null values are excluded.
        Parameters
        ----------
        axis : {0 or 'index', 1 or 'columns'}, default 0
            The axis to use. 0 or 'index' for row-wise, 1 or 'columns' for column-wise.
        skipna : bool, default True",[35]
"        corresponding element is missing.
    See Also
    --------
    notna : Boolean inverse of pandas.isna.
    Series.isna : Detect missing values in a Series.
    DataFrame.isna : Detect missing values in a DataFrame.
    Index.isna : Detect missing values in an Index.
    Examples
    --------
    Scalar arguments (including strings) result in a scalar boolean.
    >>> pd.isna('dog')
    False
    >>> pd.isna(pd.NA)
    True
    >>> pd.isna(np.nan)
    True
    ndarrays result in an ndarray of booleans.
    >>> array = np.array([[1, np.nan, 3], [4, 5, np.nan]])
    >>> array
    array([[ 1., nan,  3.],
           [ 4.,  5., nan]])
    >>> pd.isna(array)
    array([[False,  True, False],
           [False, False,  True]])
    For indexes, an ndarray of booleans is returned.
    >>> index = pd.DatetimeIndex([""2017-07-05"", ""2017-07-06"", None,
    ...                           ""2017-07-08""])
    >>> index
    DatetimeIndex(['2017-07-05', '2017-07-06', 'NaT', '2017-07-08'],
                  dtype='datetime64[ns]', freq=None)
    >>> pd.isna(index)
    array([False, False,  True, False])
    For Series and DataFrame, the same type is returned, containing booleans.
    >>> df = pd.DataFrame([['ant', 'bee', 'cat'], ['dog', None, 'fly']])
    >>> df
         0     1    2
    0  ant   bee  cat
    1  dog  None  fly
    >>> pd.isna(df)
           0      1      2
    0  False  False  False
    1  False   True  False
    >>> pd.isna(df[1])
    0    False
    1     True
    Name: 1, dtype: bool
    """"""
    return _isna(obj)

isnull = isna

def _isna_new(obj):
    if is_scalar(obj):
        return libmissing.checknull(obj)
    # hack (for now) because MI registers as ndarray
    elif isinstance(obj, ABCMultiIndex):
        raise NotImplementedError(""isna is not defined for MultiIndex"")
    elif isinstance(obj, type):
        return False
    elif isinstance(obj, (ABCSeries, np.ndarray, ABCIndexClass, ABCExtensionArray)):
        return _isna_ndarraylike(obj)
    elif isinstance(obj, ABCDataFrame):
        return obj.isna()
    elif isinstance(obj, list):
        return _isna_ndarraylike(np.asarray(obj, dtype=object))
    elif hasattr(obj, ""__array__""):
        return _isna_ndarraylike(np.asarray(obj))
    else:
        return False

def _isna_old(obj):
    """"""
    Detect missing values, treating None, NaN, INF, -INF as null.
    Parameters
    ----------
    arr: ndarray or object value
    Returns
    -------
    boolean ndarray or boolean
    """"""
    if is_scalar(obj):
        return libmissing.checknull_old(obj)
    # hack (for now) because MI registers as ndarray
    elif isinstance(obj, ABCMultiIndex):
        raise NotImplementedError(""isna is not defined for MultiIndex"")
    elif isinstance(obj, type):
        return False
    elif isinstance(obj, (ABCSeries, np.ndarray, ABCIndexClass, ABCExtensionArray)):
        return _isna_ndarraylike_old(obj)
    elif isinstance(obj, ABCDataFrame):
        return obj.isna()
    elif isinstance(obj, list):
        return _isna_ndarraylike_old(np.asarray(obj, dtype=object))
    elif hasattr(obj, ""__array__""):
        return _isna_ndarraylike_old(np.asarray(obj))
    else:
        return False

_isna = _isna_new

def _use_inf_as_na(key):
    """"""
    Option change callback for na/inf behaviour.
    Choose which replacement for numpy.isnan / -numpy.isfinite is used.
    Parameters","[75, 79, 81, 106, 110, 112]"
"        Should return ``True`` to accept the request or ``False`` to
        reject it. By default, rejects all requests with an origin on
        a host other than this one.
        This is a security protection against cross site scripting attacks on
        browsers, since WebSockets are allowed to bypass the usual same-origin
        policies and don't use CORS headers.
        .. warning::
           This is an important security measure; don't disable it
           without understanding the security implications. In
           particular, if your authentication is cookie-based, you
           must either restrict the origins allowed by
           ``check_origin()`` or implement your own XSRF-like
           protection for websocket connections. See `these
           <https://www.christian-schneider.net/CrossSiteWebSocketHijacking.html>`_
           `articles
           <https://devcenter.heroku.com/articles/websocket-security>`_
           for more.
        To accept all cross-origin traffic (which was the default prior to
        Tornado 4.0), simply override this method to always return ``True``::
            def check_origin(self, origin):
                return True
        To allow connections from any subdomain of your site, you might
        do something like::
            def check_origin(self, origin):
                parsed_origin = urllib.parse.urlparse(origin)
                return parsed_origin.netloc.endswith("".mydomain.com"")
        .. versionadded:: 4.0
        """"""
        parsed_origin = urlparse(origin)
        origin = parsed_origin.netloc
        origin = origin.lower()
        host = self.request.headers.get(""Host"")
        # Check to see that origin matches host directly, including ports
        return origin == host
    def set_nodelay(self, value: bool) -> None:
        """"""Set the no-delay flag for this stream.
        By default, small messages may be delayed and/or combined to minimize
        the number of packets sent.  This can sometimes cause 200-500ms delays
        due to the interaction between Nagle's algorithm and TCP delayed
        ACKs.  To reduce this delay (at the expense of possibly increasing
        bandwidth usage), call ``self.set_nodelay(True)`` once the websocket
        connection is established.
        See `.BaseIOStream.set_nodelay` for additional details.
        .. versionadded:: 3.1
        """"""
        assert self.stream is not None
        self.stream.set_nodelay(value)
    def on_connection_close(self) -> None:
        if self.ws_connection:
            self.ws_connection.on_connection_close()
            self.ws_connection = None
        if not self._on_close_called:
            self._on_close_called = True
            self.on_close()
            self._break_cycles()
    def on_ws_connection_close(
        self, close_code: int = None, close_reason: str = None
    ) -> None:
        self.close_code = close_code
        self.close_reason = close_reason
        self.on_connection_close()
    def _break_cycles(self) -> None:
        # WebSocketHandlers call finish() early, but we don't want to
        # break up reference cycles (which makes it impossible to call
        # self.render_string) until after we've really closed the
        # connection (if it was established in the first place,
        # indicated by status code 101).
        if self.get_status() != 101 or self._on_close_called:
            super(WebSocketHandler, self)._break_cycles()
    def send_error(self, *args: Any, **kwargs: Any) -> None:
        if self.stream is None:
            super(WebSocketHandler, self).send_error(*args, **kwargs)
        else:
            # If we get an uncaught exception during the handshake,
            # we have no choice but to abruptly close the connection.
            # TODO: for uncaught exceptions after the handshake,
            # we can close the connection more gracefully.
            self.stream.close()
    def get_websocket_protocol(self) -> Optional[""WebSocketProtocol""]:
        websocket_version = self.request.headers.get(""Sec-WebSocket-Version"")
        if websocket_version in (""7"", ""8"", ""13""):
            params = _WebSocketParams(
                ping_interval=self.ping_interval,
                ping_timeout=self.ping_timeout,
                max_message_size=self.max_message_size,
                compression_options=self.get_compression_options(),
            )
            return WebSocketProtocol13(self, False, params)
        return None
    def _detach_stream(self) -> IOStream:
        # disable non-WS methods
        for method in [
            ""write"",
            ""redirect"",
            ""set_header"",
            ""set_cookie"",
            ""set_status"",
            ""flush"",
            ""finish"",
        ]:
            setattr(self, method, _raise_not_supported_for_websockets)
        return self.detach()

def _raise_not_supported_for_websockets(*args: Any, **kwargs: Any) -> None:","[61, 62]"
"import logging
from six.moves.urllib.parse import urljoin
from scrapy.http import HtmlResponse
from scrapy.utils.response import get_meta_refresh
from scrapy.utils.python import to_native_str
from scrapy.exceptions import IgnoreRequest, NotConfigured
logger = logging.getLogger(__name__)

class BaseRedirectMiddleware(object):
    enabled_setting = 'REDIRECT_ENABLED'
    def __init__(self, settings):
        if not settings.getbool(self.enabled_setting):
            raise NotConfigured
        self.max_redirect_times = settings.getint('REDIRECT_MAX_TIMES')
        self.priority_adjust = settings.getint('REDIRECT_PRIORITY_ADJUST')
    @classmethod
    def from_crawler(cls, crawler):
        return cls(crawler.settings)
    def _redirect(self, redirected, request, spider, reason):
        ttl = request.meta.setdefault('redirect_ttl', self.max_redirect_times)
        redirects = request.meta.get('redirect_times', 0) + 1
        if ttl and redirects <= self.max_redirect_times:
            redirected.meta['redirect_times'] = redirects
            redirected.meta['redirect_ttl'] = ttl - 1
            redirected.meta['redirect_urls'] = request.meta.get('redirect_urls', []) + \
                [request.url]
            redirected.dont_filter = request.dont_filter
            redirected.priority = request.priority + self.priority_adjust
            logger.debug(""Redirecting (%(reason)s) to %(redirected)s from %(request)s"",
                         {'reason': reason, 'redirected': redirected, 'request': request},
                         extra={'spider': spider})
            return redirected
        else:
            logger.debug(""Discarding %(request)s: max redirections reached"",
                         {'request': request}, extra={'spider': spider})
            raise IgnoreRequest(""max redirections reached"")
    def _redirect_request_using_get(self, request, redirect_url):
        redirected = request.replace(url=redirect_url, method='GET', body='')
        redirected.headers.pop('Content-Type', None)
        redirected.headers.pop('Content-Length', None)
        return redirected

class RedirectMiddleware(BaseRedirectMiddleware):
    """"""Handle redirection of requests based on response status and meta-refresh html tag""""""
    def process_response(self, request, response, spider):
        if (request.meta.get('dont_redirect', False) or
                response.status in getattr(spider, 'handle_httpstatus_list', []) or
                response.status in request.meta.get('handle_httpstatus_list', []) or
                request.meta.get('handle_httpstatus_all', False)):
            return response
        allowed_status = (301, 302, 303, 307)
        if 'Location' not in response.headers or response.status not in allowed_status:
            return response
        # HTTP header is ascii or latin1, redirected url will be percent-encoded utf-8
        location = to_native_str(response.headers['location'].decode('latin1'))
        redirected_url = urljoin(request.url, location)
        if response.status in (301, 307) or request.method == 'HEAD':
            redirected = request.replace(url=redirected_url)
            return self._redirect(redirected, request, spider, response.status)
        redirected = self._redirect_request_using_get(request, redirected_url)
        return self._redirect(redirected, request, spider, response.status)

class MetaRefreshMiddleware(BaseRedirectMiddleware):
    enabled_setting = 'METAREFRESH_ENABLED'
    def __init__(self, settings):
        super(MetaRefreshMiddleware, self).__init__(settings)
        self._maxdelay = settings.getint('REDIRECT_MAX_METAREFRESH_DELAY',
                                         settings.getint('METAREFRESH_MAXDELAY'))
    def process_response(self, request, response, spider):
        if request.meta.get('dont_redirect', False) or request.method == 'HEAD' or \
                not isinstance(response, HtmlResponse):
            return response
        if isinstance(response, HtmlResponse):
            interval, url = get_meta_refresh(response)
            if url and interval < self._maxdelay:
                redirected = self._redirect_request_using_get(request, url)
                return self._redirect(redirected, request, spider, 'meta refresh')
        return response","[5, 68]"
"            col, loc = col
            if not com.is_null_slice(col) and col != 0:
                raise IndexError(f""{self} only contains one item"")
            elif isinstance(col, slice):
                if col != slice(None):
                    raise NotImplementedError(col)
                return self.values[[loc]]
            return self.values[loc]
        else:
            if col != 0:
                raise IndexError(f""{self} only contains one item"")
            return self.values
    def should_store(self, value):
        return isinstance(value, self._holder)
    def set(self, locs, values, check=False):
        assert locs.tolist() == [0]
        self.values = values
    def putmask(
        self, mask, new, align=True, inplace=False, axis=0, transpose=False,
    ):
        """"""
        putmask the data to the block; we must be a single block and not
        generate other blocks
        return the resulting block
        Parameters
        ----------
        mask  : the condition to respect
        new : a ndarray/object
        align : boolean, perform alignment on other/cond, default is True
        inplace : perform inplace modification, default is False
        Returns
        -------
        a new block, the result of the putmask
        """"""
        inplace = validate_bool_kwarg(inplace, ""inplace"")
        # use block's copy logic.
        # .values may be an Index which does shallow copy by default
        new_values = self.values if inplace else self.copy().values
        if isinstance(new, np.ndarray) and len(new) == len(mask):
            new = new[mask]
        mask = _safe_reshape(mask, new_values.shape)
        new_values[mask] = new
        return [self.make_block(values=new_values)]
    def _get_unstack_items(self, unstacker, new_columns):
        """"""
        Get the placement, values, and mask for a Block unstack.
        This is shared between ObjectBlock and ExtensionBlock. They
        differ in that ObjectBlock passes the values, while ExtensionBlock
        passes the dummy ndarray of positions to be used by a take
        later.
        Parameters
        ----------
        unstacker : pandas.core.reshape.reshape._Unstacker
        new_columns : Index
            All columns of the unstacked BlockManager.
        Returns
        -------
        new_placement : ndarray[int]
            The placement of the new columns in `new_columns`.
        new_values : Union[ndarray, ExtensionArray]
            The first return value from _Unstacker.get_new_values.
        mask : ndarray[bool]
            The second return value from _Unstacker.get_new_values.
        """"""
        # shared with ExtensionBlock
        new_items = unstacker.get_new_columns()
        new_placement = new_columns.get_indexer(new_items)
        new_values, mask = unstacker.get_new_values()
        mask = mask.any(0)
        return new_placement, new_values, mask
    def _maybe_coerce_values(self, values):
        """"""
        Unbox to an extension array.
        This will unbox an ExtensionArray stored in an Index or Series.
        ExtensionArrays pass through. No dtype coercion is done.
        Parameters
        ----------
        values : Index, Series, ExtensionArray
        Returns
        -------
        ExtensionArray
        """"""
        return extract_array(values)
    @property
    def _holder(self):
        # For extension blocks, the holder is values-dependent.
        return type(self.values)
    @property
    def fill_value(self):
        # Used in reindex_indexer
        return self.values.dtype.na_value
    @property
    def _can_hold_na(self):
        # The default ExtensionArray._can_hold_na is True
        return self._holder._can_hold_na
    @property
    def is_view(self) -> bool:
        """"""Extension arrays are never treated as views.""""""
        return False
    @property
    def is_numeric(self):
        return self.values.dtype._is_numeric
","[13, 16, 18]"
"    # we stash extra items and reappend them after shuffling
    last_batch = index_array[batch_count * batch_size:]
    index_array = index_array[:batch_count * batch_size]
    index_array = index_array.reshape((batch_count, batch_size))
    np.random.shuffle(index_array)
    index_array = index_array.flatten()
    return np.append(index_array, last_batch)

def make_batches(size, batch_size):
    """"""Returns a list of batch indices (tuples of indices).
    # Arguments
        size: Integer, total size of the data to slice into batches.
        batch_size: Integer, batch size.
    # Returns
        A list of tuples of array indices.
    """"""
    num_batches = (size + batch_size - 1) // batch_size  # round up
    return [(i * batch_size, min(size, (i + 1) * batch_size))
            for i in range(num_batches)]

def weighted_masked_objective(fn):
    """"""Adds support for masking and sample-weighting to an objective function.
    It transforms an objective function `fn(y_true, y_pred)`
    into a sample-weighted, cost-masked objective function
    `fn(y_true, y_pred, weights, mask)`.
    # Arguments
        fn: The objective function to wrap,
            with signature `fn(y_true, y_pred)`.
    # Returns
        A function with signature `fn(y_true, y_pred, weights, mask)`.
    """"""
    if fn is None:
        return None
    def weighted(y_true, y_pred, weights, mask=None):
        """"""Wrapper function.
        # Arguments
            y_true: `y_true` argument of `fn`.
            y_pred: `y_pred` argument of `fn`.
            weights: Weights tensor.
            mask: Mask tensor.
        # Returns
            Scalar tensor.
        """"""
        # score_array has ndim >= 2
        score_array = fn(y_true, y_pred)
        if mask is not None:
            # Cast the mask to floatX to avoid float64 upcasting in Theano
            mask = K.cast(mask, K.floatx())
            # mask should have the same shape as score_array
            score_array *= mask
            #  the loss per batch should be proportional
            #  to the number of unmasked samples.
            score_array /= K.mean(mask)
        # apply sample weighting
        if weights is not None:
            # reduce score_array to same ndim as weight array
            ndim = K.ndim(score_array)
            weight_ndim = K.ndim(weights)
            score_array = K.mean(score_array,
                                 axis=list(range(weight_ndim, ndim)))
            score_array *= weights
            score_array /= K.mean(K.cast(K.not_equal(weights, 0), K.floatx()))
        return K.mean(score_array)
    return weighted

def standardize_weights(y,
                        sample_weight=None,
                        class_weight=None,
                        sample_weight_mode=None):
    """"""Performs sample weight validation and standardization.
    Everything gets normalized to a single sample-wise (or timestep-wise)
    weight array.
    # Arguments
        y: Numpy array of model targets to be weighted.
        sample_weight: User-provided `sample_weight` argument.
        class_weight: User-provided `class_weight` argument.
        sample_weight_mode: One of `None` or `""temporal""`.
            `""temporal""` indicated that we expect 2D weight data
            that will be applied to the last 2 dimensions of
            the targets (i.e. we are weighting timesteps, not samples).
    # Returns
        A Numpy array of target weights, one entry per sample to weight.
    # Raises
        ValueError: In case of invalid user-provided arguments.
    """"""
    if sample_weight_mode is not None:
        if sample_weight_mode != 'temporal':
            raise ValueError('""sample_weight_mode '
                             'should be None or ""temporal"". '
                             'Found: ' + str(sample_weight_mode))
        if len(y.shape) < 3:
            raise ValueError('Found a sample_weight array for '
                             'an input with shape ' +
                             str(y.shape) + '. '
                             'Timestep-wise sample weighting (use of '
                             'sample_weight_mode=""temporal"") is restricted to '
                             'outputs that are at least 3D, i.e. that have '
                             'a time dimension.')
        if sample_weight is not None and len(sample_weight.shape) != 2:
            raise ValueError('Found a sample_weight array with shape ' +
                             str(sample_weight.shape) + '. '
                             'In order to use timestep-wise sample weighting, '
                             'you should pass a 2D sample_weight array.')
    else:
        if sample_weight is not None and len(sample_weight.shape) != 1:
            raise ValueError('Found a sample_weight array with shape ' +
                             str(sample_weight.shape) + '. '
                             'In order to use timestep-wise sample weights, '
                             'you should specify '
                             'sample_weight_mode=""temporal"" '
                             'in compile(). If you just mean to use '",[62]
"    To replace all interaction with the filesystem (e.g. to serve
    static content from a database), override `get_content`,
    `get_content_size`, `get_modified_time`, `get_absolute_path`, and
    `validate_absolute_path`.
    .. versionchanged:: 3.1
       Many of the methods for subclasses were added in Tornado 3.1.
    """"""
    CACHE_MAX_AGE = 86400 * 365 * 10  # 10 years
    _static_hashes = {}  # type: Dict[str, Optional[str]]
    _lock = threading.Lock()  # protects _static_hashes
    def initialize(self, path: str, default_filename: str = None) -> None:
        self.root = path
        self.default_filename = default_filename
    @classmethod
    def reset(cls) -> None:
        with cls._lock:
            cls._static_hashes = {}
    def head(self, path: str) -> Awaitable[None]:
        return self.get(path, include_body=False)
    async def get(self, path: str, include_body: bool = True) -> None:
        # Set up our path instance variables.
        self.path = self.parse_url_path(path)
        del path  # make sure we don't refer to path instead of self.path again
        absolute_path = self.get_absolute_path(self.root, self.path)
        self.absolute_path = self.validate_absolute_path(self.root, absolute_path)
        if self.absolute_path is None:
            return
        self.modified = self.get_modified_time()
        self.set_headers()
        if self.should_return_304():
            self.set_status(304)
            return
        request_range = None
        range_header = self.request.headers.get(""Range"")
        if range_header:
            # As per RFC 2616 14.16, if an invalid Range header is specified,
            # the request will be treated as if the header didn't exist.
            request_range = httputil._parse_request_range(range_header)
        size = self.get_content_size()
        if request_range:
            start, end = request_range
            if (start is not None and start >= size) or end == 0:
                # As per RFC 2616 14.35.1, a range is not satisfiable only: if
                # the first requested byte is equal to or greater than the
                # content, or when a suffix with length 0 is specified
                self.set_status(416)  # Range Not Satisfiable
                self.set_header(""Content-Type"", ""text/plain"")
                self.set_header(""Content-Range"", ""bytes */%s"" % (size,))
                return
            if start is not None and start < 0:
                start += size
            if end is not None and end > size:
                # Clients sometimes blindly use a large range to limit their
                # download size; cap the endpoint at the actual file size.
                end = size
            # Note: only return HTTP 206 if less than the entire range has been
            # requested. Not only is this semantically correct, but Chrome
            # refuses to play audio if it gets an HTTP 206 in response to
            # ``Range: bytes=0-``.
            if size != (end or size) - (start or 0):
                self.set_status(206)  # Partial Content
                self.set_header(
                    ""Content-Range"", httputil._get_content_range(start, end, size)
                )
        else:
            start = end = None
        if start is not None and end is not None:
            content_length = end - start
        elif end is not None:
            content_length = end
        elif start is not None:
            content_length = size - start
        else:
            content_length = size
        self.set_header(""Content-Length"", content_length)
        if include_body:
            content = self.get_content(self.absolute_path, start, end)
            if isinstance(content, bytes):
                content = [content]
            for chunk in content:
                try:
                    self.write(chunk)
                    await self.flush()
                except iostream.StreamClosedError:
                    return
        else:
            assert self.request.method == ""HEAD""
    def compute_etag(self) -> Optional[str]:
        """"""Sets the ``Etag`` header based on static url version.
        This allows efficient ``If-None-Match`` checks against cached
        versions, and sends the correct ``Etag`` for a partial response
        (i.e. the same ``Etag`` as the full file).
        .. versionadded:: 3.1
        """"""
        assert self.absolute_path is not None
        version_hash = self._get_cached_version(self.absolute_path)
        if not version_hash:
            return None
        return '""%s""' % (version_hash,)
    def set_headers(self) -> None:
        """"""Sets the content and caching headers on the response.
        .. versionadded:: 3.1
        """"""
        self.set_header(""Accept-Ranges"", ""bytes"")
        self.set_etag_header()
        if self.modified is not None:
            self.set_header(""Last-Modified"", self.modified)","[53, 61, 62]"
"    True
    >>> pd.notna(np.nan)
    False
    ndarrays result in an ndarray of booleans.
    >>> array = np.array([[1, np.nan, 3], [4, 5, np.nan]])
    >>> array
    array([[ 1., nan,  3.],
           [ 4.,  5., nan]])
    >>> pd.notna(array)
    array([[ True, False,  True],
           [ True,  True, False]])
    For indexes, an ndarray of booleans is returned.
    >>> index = pd.DatetimeIndex([""2017-07-05"", ""2017-07-06"", None,
    ...                          ""2017-07-08""])
    >>> index
    DatetimeIndex(['2017-07-05', '2017-07-06', 'NaT', '2017-07-08'],
                  dtype='datetime64[ns]', freq=None)
    >>> pd.notna(index)
    array([ True,  True, False,  True])
    For Series and DataFrame, the same type is returned, containing booleans.
    >>> df = pd.DataFrame([['ant', 'bee', 'cat'], ['dog', None, 'fly']])
    >>> df
         0     1    2
    0  ant   bee  cat
    1  dog  None  fly
    >>> pd.notna(df)
          0      1     2
    0  True   True  True
    1  True  False  True
    >>> pd.notna(df[1])
    0     True
    1    False
    Name: 1, dtype: bool
    """"""
    res = isna(obj)
    if is_scalar(res):
        return not res
    return ~res

notnull = notna

def _isna_compat(arr, fill_value=np.nan):
    """"""
    Parameters
    ----------
    arr: a numpy array
    fill_value: fill value, default to np.nan
    Returns
    -------
    True if we can fill using this fill_value
    """"""
    dtype = arr.dtype
    if isna(fill_value):
        return not (is_bool_dtype(dtype) or is_integer_dtype(dtype))
    return True

def array_equivalent(left, right, strict_nan=False):
    """"""
    True if two arrays, left and right, have equal non-NaN elements, and NaNs
    in corresponding locations.  False otherwise. It is assumed that left and
    right are NumPy arrays of the same dtype. The behavior of this function
    (particularly with respect to NaNs) is not defined if the dtypes are
    different.
    Parameters
    ----------
    left, right : ndarrays
    strict_nan : bool, default False
        If True, consider NaN and None to be different.
    Returns
    -------
    b : bool
        Returns True if the arrays are equivalent.
    Examples
    --------
    >>> array_equivalent(
    ...     np.array([1, 2, np.nan]),
    ...     np.array([1, 2, np.nan]))
    True
    >>> array_equivalent(
    ...     np.array([1, np.nan, 2]),
    ...     np.array([1, 2, np.nan]))
    False
    """"""
    left, right = np.asarray(left), np.asarray(right)
    # shape compat
    if left.shape != right.shape:
        return False
    # Object arrays can contain None, NaN and NaT.
    # string dtypes must be come to this path for NumPy 1.7.1 compat
    if is_string_dtype(left) or is_string_dtype(right):
        if not strict_nan:
            # isna considers NaN and None to be equivalent.
            return lib.array_equivalent_object(
                ensure_object(left.ravel()), ensure_object(right.ravel())
            )
        for left_value, right_value in zip(left, right):
            if left_value is NaT and right_value is not NaT:
                return False
            elif isinstance(left_value, float) and np.isnan(left_value):
                if not isinstance(right_value, float) or not np.isnan(right_value):
                    return False
            else:
                if left_value != right_value:
                    return False
        return True
",[123]
"        self.width_shift_range = width_shift_range
        self.height_shift_range = height_shift_range
        self.brightness_range = brightness_range
        self.shear_range = shear_range
        self.zoom_range = zoom_range
        self.channel_shift_range = channel_shift_range
        self.fill_mode = fill_mode
        self.cval = cval
        self.horizontal_flip = horizontal_flip
        self.vertical_flip = vertical_flip
        self.rescale = rescale
        self.preprocessing_function = preprocessing_function
        if data_format not in {'channels_last', 'channels_first'}:
            raise ValueError('`data_format` should be `""channels_last""` (channel after row and '
                             'column) or `""channels_first""` (channel before row and column). '
                             'Received arg: ', data_format)
        self.data_format = data_format
        if data_format == 'channels_first':
            self.channel_axis = 1
            self.row_axis = 2
            self.col_axis = 3
        if data_format == 'channels_last':
            self.channel_axis = 3
            self.row_axis = 1
            self.col_axis = 2
        if validation_split and not 0 < validation_split < 1:
            raise ValueError('`validation_split` must be strictly between 0 and 1. '
                             ' Received arg: ', validation_split)
        self._validation_split = validation_split
        self.mean = None
        self.std = None
        self.principal_components = None
        if np.isscalar(zoom_range):
            self.zoom_range = [1 - zoom_range, 1 + zoom_range]
        elif len(zoom_range) == 2:
            self.zoom_range = [zoom_range[0], zoom_range[1]]
        else:
            raise ValueError('`zoom_range` should be a float or '
                             'a tuple or list of two floats. '
                             'Received arg: ', zoom_range)
        if zca_whitening:
            if not featurewise_center:
                self.featurewise_center = True
                warnings.warn('This ImageDataGenerator specifies '
                              '`zca_whitening`, which overrides '
                              'setting of `featurewise_center`.')
            if featurewise_std_normalization:
                self.featurewise_std_normalization = False
                warnings.warn('This ImageDataGenerator specifies '
                              '`zca_whitening` '
                              'which overrides setting of'
                              '`featurewise_std_normalization`.')
        if featurewise_std_normalization:
            if not featurewise_center:
                self.featurewise_center = True
                warnings.warn('This ImageDataGenerator specifies '
                              '`featurewise_std_normalization`, '
                              'which overrides setting of '
                              '`featurewise_center`.')
        if samplewise_std_normalization:
            if not samplewise_center:
                self.samplewise_center = True
                warnings.warn('This ImageDataGenerator specifies '
                              '`samplewise_std_normalization`, '
                              'which overrides setting of '
                              '`samplewise_center`.')
    def flow(self, x, y=None, batch_size=32, shuffle=True, seed=None,
             save_to_dir=None, save_prefix='', save_format='png', subset=None):
        return NumpyArrayIterator(
            x, y, self,
            batch_size=batch_size,
            shuffle=shuffle,
            seed=seed,
            data_format=self.data_format,
            save_to_dir=save_to_dir,
            save_prefix=save_prefix,
            save_format=save_format,
            subset=subset)
    def flow_from_directory(self, directory,
                            target_size=(256, 256), color_mode='rgb',
                            classes=None, class_mode='categorical',
                            batch_size=32, shuffle=True, seed=None,
                            save_to_dir=None,
                            save_prefix='',
                            save_format='png',
                            follow_links=False,
                            subset=None,
                            interpolation='nearest'):
        return DirectoryIterator(
            directory, self,
            target_size=target_size, color_mode=color_mode,
            classes=classes, class_mode=class_mode,
            data_format=self.data_format,
            batch_size=batch_size, shuffle=shuffle, seed=seed,
            save_to_dir=save_to_dir,
            save_prefix=save_prefix,
            save_format=save_format,
            follow_links=follow_links,
            subset=subset,
            interpolation=interpolation)
    def standardize(self, x):
        """"""Apply the normalization configuration to a batch of inputs.
        # Arguments
            x: batch of inputs to be normalized.
        # Returns
            The inputs, normalized.
        """"""
        if self.preprocessing_function:
            x = self.preprocessing_function(x)
        if self.rescale:
            x *= self.rescale
        if self.samplewise_center:
            x -= np.mean(x, keepdims=True)
        if self.samplewise_std_normalization:
            x /= (np.std(x, keepdims=True) + K.epsilon())
        if self.featurewise_center:
            if self.mean is not None:
                x -= self.mean","[115, 116]"
"            path,
            endpoint=endpoint,
            response_model=response_model,
            status_code=status_code,
            tags=tags or [],
            dependencies=dependencies,
            summary=summary,
            description=description,
            response_description=response_description,
            responses=responses or {},
            deprecated=deprecated,
            methods=methods,
            operation_id=operation_id,
            response_model_include=response_model_include,
            response_model_exclude=response_model_exclude,
            response_model_by_alias=response_model_by_alias,
            response_model_exclude_unset=bool(
                response_model_exclude_unset or response_model_skip_defaults
            ),
            include_in_schema=include_in_schema,
            response_class=response_class or self.default_response_class,
            name=name,
            dependency_overrides_provider=self.dependency_overrides_provider,
            callbacks=callbacks,
        )
        self.routes.append(route)
    def api_route(
        self,
        path: str,
        *,
        response_model: Type[Any] = None,
        status_code: int = 200,
        tags: List[str] = None,
        dependencies: Sequence[params.Depends] = None,
        summary: str = None,
        description: str = None,
        response_description: str = ""Successful Response"",
        responses: Dict[Union[int, str], Dict[str, Any]] = None,
        deprecated: bool = None,
        methods: List[str] = None,
        operation_id: str = None,
        response_model_include: Union[SetIntStr, DictIntStrAny] = None,
        response_model_exclude: Union[SetIntStr, DictIntStrAny] = set(),
        response_model_by_alias: bool = True,
        response_model_skip_defaults: bool = None,
        response_model_exclude_unset: bool = False,
        include_in_schema: bool = True,
        response_class: Type[Response] = None,
        name: str = None,
        callbacks: List[APIRoute] = None,
    ) -> Callable:
        if response_model_skip_defaults is not None:
            warning_response_model_skip_defaults_deprecated()  # pragma: nocover
        def decorator(func: Callable) -> Callable:
            self.add_api_route(
                path,
                func,
                response_model=response_model,
                status_code=status_code,
                tags=tags or [],
                dependencies=dependencies,
                summary=summary,
                description=description,
                response_description=response_description,
                responses=responses or {},
                deprecated=deprecated,
                methods=methods,
                operation_id=operation_id,
                response_model_include=response_model_include,
                response_model_exclude=response_model_exclude,
                response_model_by_alias=response_model_by_alias,
                response_model_exclude_unset=bool(
                    response_model_exclude_unset or response_model_skip_defaults
                ),
                include_in_schema=include_in_schema,
                response_class=response_class or self.default_response_class,
                name=name,
                callbacks=callbacks,
            )
            return func
        return decorator
    def add_api_websocket_route(
        self, path: str, endpoint: Callable, name: str = None
    ) -> None:
        route = APIWebSocketRoute(path, endpoint=endpoint, name=name)
        self.routes.append(route)
    def websocket(self, path: str, name: str = None) -> Callable:
        def decorator(func: Callable) -> Callable:
            self.add_api_websocket_route(path, func, name=name)
            return func
        return decorator
    def include_router(
        self,
        router: ""APIRouter"",
        *,
        prefix: str = """",
        tags: List[str] = None,
        dependencies: Sequence[params.Depends] = None,
        responses: Dict[Union[int, str], Dict[str, Any]] = None,
        default_response_class: Optional[Type[Response]] = None,
    ) -> None:
        if prefix:
            assert prefix.startswith(""/""), ""A path prefix must start with '/'""
            assert not prefix.endswith(
                ""/""
            ), ""A path prefix must not end with '/', as the routes will start with '/'""
        else:
            for r in router.routes:
                path = getattr(r, ""path"")
                name = getattr(r, ""name"", ""unknown"")
                if path is not None and not path:
                    raise Exception(
                        f""Prefix and path cannot be both empty (path operation: {name})""
                    )
        if responses is None:
            responses = {}
        for route in router.routes:
            if isinstance(route, APIRoute):
                combined_responses = {**responses, **route.responses}
                self.add_api_route(",[88]
"            deadline,
            functools.partial(stack_context.wrap(callback), *args, **kwargs),
            self)
        heapq.heappush(self._timeouts, timeout)
        return timeout
    def remove_timeout(self, timeout):
        # Removing from a heap is complicated, so just leave the defunct
        # timeout object in the queue (see discussion in
        # http://docs.python.org/library/heapq.html).
        # If this turns out to be a problem, we could add a garbage
        # collection pass whenever there are too many dead timeouts.
        timeout.callback = None
        self._cancellations += 1
    def add_callback(self, callback, *args, **kwargs):
        if self._closing:
            return
        # Blindly insert into self._callbacks. This is safe even
        # from signal handlers because deque.append is atomic.
        self._callbacks.append(functools.partial(
            stack_context.wrap(callback), *args, **kwargs))
        if thread.get_ident() != self._thread_ident:
            # This will write one byte but Waker.consume() reads many
            # at once, so it's ok to write even when not strictly
            # necessary.
            self._waker.wake()
        else:
            # If we're on the IOLoop's thread, we don't need to wake anyone.
            pass
    def add_callback_from_signal(self, callback, *args, **kwargs):
        with stack_context.NullContext():
            self.add_callback(callback, *args, **kwargs)

class _Timeout(object):
    """"""An IOLoop timeout, a UNIX timestamp and a callback""""""
    # Reduce memory overhead when there are lots of pending callbacks
    __slots__ = ['deadline', 'callback', 'tdeadline']
    def __init__(self, deadline, callback, io_loop):
        if not isinstance(deadline, numbers.Real):
            raise TypeError(""Unsupported deadline %r"" % deadline)
        self.deadline = deadline
        self.callback = callback
        self.tdeadline = (deadline, next(io_loop._timeout_counter))
    # Comparison methods to sort by deadline, with object id as a tiebreaker
    # to guarantee a consistent ordering.  The heapq module uses __le__
    # in python2.5, and __lt__ in 2.6+ (sort() and most other comparisons
    # use __lt__).
    def __lt__(self, other):
        return self.tdeadline < other.tdeadline
    def __le__(self, other):
        return self.tdeadline <= other.tdeadline

class PeriodicCallback(object):
    """"""Schedules the given callback to be called periodically.
    The callback is called every ``callback_time`` milliseconds.
    Note that the timeout is given in milliseconds, while most other
    time-related functions in Tornado use seconds.
    If the callback runs for longer than ``callback_time`` milliseconds,
    subsequent invocations will be skipped to get back on schedule.
    `start` must be called after the `PeriodicCallback` is created.
    .. versionchanged:: 5.0
       The ``io_loop`` argument (deprecated since version 4.1) has been removed.
    """"""
    def __init__(self, callback, callback_time):
        self.callback = callback
        if callback_time <= 0:
            raise ValueError(""Periodic callback must have a positive callback_time"")
        self.callback_time = callback_time
        self._running = False
        self._timeout = None
    def start(self):
        """"""Starts the timer.""""""
        # Looking up the IOLoop here allows to first instantiate the
        # PeriodicCallback in another thread, then start it using
        # IOLoop.add_callback().
        self.io_loop = IOLoop.current()
        self._running = True
        self._next_timeout = self.io_loop.time()
        self._schedule_next()
    def stop(self):
        """"""Stops the timer.""""""
        self._running = False
        if self._timeout is not None:
            self.io_loop.remove_timeout(self._timeout)
            self._timeout = None
    def is_running(self):
        """"""Return True if this `.PeriodicCallback` has been started.
        .. versionadded:: 4.1
        """"""
        return self._running
    def _run(self):
        if not self._running:
            return
        try:
            return self.callback()
        except Exception:
            self.io_loop.handle_callback_exception(self.callback)
        finally:
            self._schedule_next()
    def _schedule_next(self):
        if self._running:
            self._update_next(self.io_loop.time())
            self._timeout = self.io_loop.add_timeout(self._next_timeout, self._run)
    def _update_next(self, current_time):
        if self._next_timeout <= current_time:
            callback_time_sec = self.callback_time / 1000.0
            self._next_timeout += (math.floor((current_time - self._next_timeout) /",[124]
"        """"""
        Get reference dates for the holiday.
        Return reference dates for the holiday also returning the year
        prior to the start_date and year following the end_date.  This ensures
        that any offsets to be applied will yield the holidays within
        the passed in dates.
        """"""
        if self.start_date is not None:
            start_date = self.start_date.tz_localize(start_date.tz)
        if self.end_date is not None:
            end_date = self.end_date.tz_localize(start_date.tz)
        year_offset = DateOffset(years=1)
        reference_start_date = Timestamp(
            datetime(start_date.year - 1, self.month, self.day)
        )
        reference_end_date = Timestamp(
            datetime(end_date.year + 1, self.month, self.day)
        )
        # Don't process unnecessary holidays
        dates = date_range(
            start=reference_start_date,
            end=reference_end_date,
            freq=year_offset,
            tz=start_date.tz,
        )
        return dates
    def _apply_rule(self, dates):
        """"""
        Apply the given offset/observance to a DatetimeIndex of dates.
        Parameters
        ----------
        dates : DatetimeIndex
            Dates to apply the given offset/observance rule
        Returns
        -------
        Dates with rules applied
        """"""
        if self.observance is not None:
            return dates.map(lambda d: self.observance(d))
        if self.offset is not None:
            if not isinstance(self.offset, list):
                offsets = [self.offset]
            else:
                offsets = self.offset
            for offset in offsets:
                # if we are adding a non-vectorized value
                # ignore the PerformanceWarnings:
                with warnings.catch_warnings():
                    warnings.simplefilter(""ignore"", PerformanceWarning)
                    dates += offset
        return dates

holiday_calendars = {}

def register(cls):
    try:
        name = cls.name
    except AttributeError:
        name = cls.__name__
    holiday_calendars[name] = cls

def get_calendar(name):
    """"""
    Return an instance of a calendar based on its name.
    Parameters
    ----------
    name : str
        Calendar name to return an instance of
    """"""
    return holiday_calendars[name]()

class HolidayCalendarMetaClass(type):
    def __new__(cls, clsname, bases, attrs):
        calendar_class = super().__new__(cls, clsname, bases, attrs)
        register(calendar_class)
        return calendar_class

class AbstractHolidayCalendar(metaclass=HolidayCalendarMetaClass):
    """"""
    Abstract interface to create holidays following certain rules.
    """"""
    rules = []  # type: List[Holiday]
    start_date = Timestamp(datetime(1970, 1, 1))
    end_date = Timestamp(datetime(2030, 12, 31))
    _cache = None
    def __init__(self, name=None, rules=None):
        """"""
        Initializes holiday object with a given set a rules.  Normally
        classes just have the rules defined within them.
        Parameters
        ----------
        name : str
            Name of the holiday calendar, defaults to class name
        rules : array of Holiday objects
            A set of rules used to create the holidays.
        """"""
        super().__init__()
        if name is None:
            name = self.__class__.__name__
        self.name = name
        if rules is not None:
            self.rules = rules
    def rule_from_name(self, name):
        for rule in self.rules:
            if rule.name == name:
                return rule",[100]
"""""""
Base and utility classes for tseries type pandas objects.
""""""
from datetime import datetime, timedelta
from typing import Any, List, Optional, Union, cast
import numpy as np
from pandas._libs import NaT, iNaT, join as libjoin, lib
from pandas._libs.tslibs import timezones
from pandas._typing import Label
from pandas.compat.numpy import function as nv
from pandas.errors import AbstractMethodError
from pandas.util._decorators import Appender, cache_readonly, doc
from pandas.core.dtypes.common import (
    ensure_int64,
    is_bool_dtype,
    is_datetime64_any_dtype,
    is_dtype_equal,
    is_integer,
    is_list_like,
    is_object_dtype,
    is_period_dtype,
    is_scalar,
    is_timedelta64_dtype,
)
from pandas.core.dtypes.concat import concat_compat
from pandas.core.dtypes.generic import ABCIndex, ABCIndexClass, ABCSeries
from pandas.core.dtypes.missing import isna
from pandas.core import algorithms
from pandas.core.arrays import DatetimeArray, PeriodArray, TimedeltaArray
from pandas.core.arrays.datetimelike import DatetimeLikeArrayMixin
from pandas.core.base import IndexOpsMixin
import pandas.core.indexes.base as ibase
from pandas.core.indexes.base import Index, _index_shared_docs
from pandas.core.indexes.extension import (
    ExtensionIndex,
    inherit_names,
    make_wrapped_arith_op,
)
from pandas.core.indexes.numeric import Int64Index
from pandas.core.ops import get_op_result_name
from pandas.core.tools.timedeltas import to_timedelta
from pandas.tseries.frequencies import DateOffset, to_offset
from pandas.tseries.offsets import Tick
_index_doc_kwargs = dict(ibase._index_doc_kwargs)

def _join_i8_wrapper(joinf, with_indexers: bool = True):
    """"""
    Create the join wrapper methods.
    """"""
    @staticmethod  # type: ignore
    def wrapper(left, right):
        if isinstance(left, (np.ndarray, ABCIndex, ABCSeries, DatetimeLikeArrayMixin)):
            left = left.view(""i8"")
        if isinstance(right, (np.ndarray, ABCIndex, ABCSeries, DatetimeLikeArrayMixin)):
            right = right.view(""i8"")
        results = joinf(left, right)
        if with_indexers:
            # dtype should be timedelta64[ns] for TimedeltaIndex
            #  and datetime64[ns] for DatetimeIndex
            dtype = left.dtype.base
            join_index, left_indexer, right_indexer = results
            join_index = join_index.view(dtype)
            return join_index, left_indexer, right_indexer
        return results
    return wrapper

def _make_wrapped_arith_op_with_freq(opname: str):
    """"""
    Dispatch the operation to the underlying ExtensionArray, and infer
    the appropriate frequency for the result.
    """"""
    meth = make_wrapped_arith_op(opname)
    def wrapped(self, other):
        result = meth(self, other)
        if result is NotImplemented:
            return NotImplemented
        new_freq = self._get_addsub_freq(other)
        result._freq = new_freq
        return result
    wrapped.__name__ = opname
    return wrapped

@inherit_names(
    [""inferred_freq"", ""_isnan"", ""_resolution"", ""resolution""],
    DatetimeLikeArrayMixin,
    cache=True,
)
@inherit_names(
    [""mean"", ""asi8"", ""_box_func""], DatetimeLikeArrayMixin,
)
class DatetimeIndexOpsMixin(ExtensionIndex):
    """"""
    Common ops mixin to support a unified interface datetimelike Index.
    """"""
    _data: Union[DatetimeArray, TimedeltaArray, PeriodArray]
    freq: Optional[DateOffset]
    freqstr: Optional[str]
    _resolution: int
    _bool_ops: List[str] = []
    _field_ops: List[str] = []
    hasnans = cache_readonly(DatetimeLikeArrayMixin._hasnans.fget)  # type: ignore
    _hasnans = hasnans  # for index / array -agnostic code
    @property
    def is_all_dates(self) -> bool:
        return True
    # ------------------------------------------------------------------------
    # Abstract data attributes",[90]
"from collections import namedtuple
from traceback import format_stack
from .logs import debug
Command = namedtuple('Command', ('script', 'stdout', 'stderr'))
Rule = namedtuple('Rule', ('name', 'match', 'get_new_command',
                           'enabled_by_default', 'side_effect',
                           'priority', 'requires_output'))
class CorrectedCommand(object):
    def __init__(self, script, side_effect, priority):
        self.script = script
        self.side_effect = side_effect
        self.priority = priority
    def __eq__(self, other):
        """"""Ignores `priority` field.""""""
        if isinstance(other, CorrectedCommand):
            return (other.script, other.side_effect) ==\
                   (self.script, self.side_effect)
        else:
            return False
    def __hash__(self):
        return (self.script, self.side_effect).__hash__()
    def __repr__(self):
        return 'CorrectedCommand(script={}, side_effect={}, priority={})'.format(
            self.script, self.side_effect, self.priority)

class RulesNamesList(list):
    """"""Wrapper a top of list for storing rules names.""""""
    def __contains__(self, item):
        return super(RulesNamesList, self).__contains__(item.name)

class Settings(dict):
    def __getattr__(self, item):
        return self.get(item)
    def update(self, **kwargs):
        """"""
        Returns new settings with values from `kwargs` for unset settings.
        """"""
        conf = dict(kwargs)
        conf.update(self)
        return Settings(conf)

class SortedCorrectedCommandsSequence(object):
    """"""List-like collection/wrapper around generator, that:
    - immediately gives access to the first commands through [];
    - realises generator and sorts commands on first access to other
      commands through [], or when len called.
    """"""
    def __init__(self, commands, settings):
        self._settings = settings
        self._commands = commands
        self._cached = self._realise_first()
        self._realised = False
    def _realise_first(self):
        try:
            return [next(self._commands)]
        except StopIteration:
            return []
    def _remove_duplicates(self, corrected_commands):
        """"""Removes low-priority duplicates.""""""
        commands = {command
                    for command in sorted(corrected_commands,
                                          key=lambda command: -command.priority)
                    if command.script != self._cached[0]}
        return commands
    def _realise(self):
        """"""Realises generator, removes duplicates and sorts commands.""""""
        commands = self._remove_duplicates(self._commands)
        self._cached = [self._cached[0]] + sorted(
            commands, key=lambda corrected_command: corrected_command.priority)
        self._realised = True
        debug('SortedCommandsSequence was realised with: {}, after: {}'.format(
            self._cached, '\n'.join(format_stack())), self._settings)
    def __getitem__(self, item):
        if item != 0 and not self._realised:
            self._realise()
        return self._cached[item]
    def __bool__(self):
        return bool(self._cached)
    def __len__(self):
        if not self._realised:
            self._realise()
        return len(self._cached)
    def __iter__(self):
        if not self._realised:
            self._realise()
        return iter(self._cached)","[83, 84, 85]"
"        if not self.index.is_monotonic:
            raise ValueError(""asof requires a sorted index"")
        is_series = isinstance(self, ABCSeries)
        if is_series:
            if subset is not None:
                raise ValueError(""subset is not valid for Series"")
        else:
            if subset is None:
                subset = self.columns
            if not is_list_like(subset):
                subset = [subset]
        is_list = is_list_like(where)
        if not is_list:
            start = self.index[0]
            if isinstance(self.index, PeriodIndex):
                where = Period(where, freq=self.index.freq).ordinal
                start = start.ordinal
            if where < start:
                if not is_series:
                    from pandas import Series
                    return Series(index=self.columns, name=where, dtype=np.float64)
                return np.nan
            # It's always much faster to use a *while* loop here for
            # Series than pre-computing all the NAs. However a
            # *while* loop is extremely expensive for DataFrame
            # so we later pre-compute all the NAs and use the same
            # code path whether *where* is a scalar or list.
            # See PR: https://github.com/pandas-dev/pandas/pull/14476
            if is_series:
                loc = self.index.searchsorted(where, side=""right"")
                if loc > 0:
                    loc -= 1
                values = self._values
                while loc > 0 and isna(values[loc]):
                    loc -= 1
                return values[loc]
        if not isinstance(where, Index):
            where = Index(where) if is_list else Index([where])
        nulls = self.isna() if is_series else self[subset].isna().any(1)
        if nulls.all():
            if is_series:
                return self._constructor(np.nan, index=where, name=self.name)
            elif is_list:
                from pandas import DataFrame
                return DataFrame(np.nan, index=where, columns=self.columns)
            else:
                from pandas import Series
                return Series(np.nan, index=self.columns, name=where[0])
        locs = self.index.asof_locs(where, ~(nulls.values))
        # mask the missing
        missing = locs == -1
        d = self.take(locs)
        data = d.copy()
        data.index = where
        data.loc[missing] = np.nan
        return data if is_list else data.iloc[-1]
    # ----------------------------------------------------------------------
    # Action Methods
    _shared_docs[
        ""isna""
    ] = """"""
        Detect missing values.
        Return a boolean same-sized object indicating if the values are NA.
        NA values, such as None or :attr:`numpy.NaN`, gets mapped to True
        values.
        Everything else gets mapped to False values. Characters such as empty
        strings ``''`` or :attr:`numpy.inf` are not considered NA values
        (unless you set ``pandas.options.mode.use_inf_as_na = True``).
        Returns
        -------
        %(klass)s
            Mask of bool values for each element in %(klass)s that
            indicates whether an element is not an NA value.
        See Also
        --------
        %(klass)s.isnull : Alias of isna.
        %(klass)s.notna : Boolean inverse of isna.
        %(klass)s.dropna : Omit axes labels with missing values.
        isna : Top-level isna.
        Examples
        --------
        Show which entries in a DataFrame are NA.
        >>> df = pd.DataFrame({'age': [5, 6, np.NaN],
        ...                    'born': [pd.NaT, pd.Timestamp('1939-05-27'),
        ...                             pd.Timestamp('1940-04-25')],
        ...                    'name': ['Alfred', 'Batman', ''],
        ...                    'toy': [None, 'Batmobile', 'Joker']})
        >>> df
           age       born    name        toy
        0  5.0        NaT  Alfred       None
        1  6.0 1939-05-27  Batman  Batmobile
        2  NaN 1940-04-25              Joker
        >>> df.isna()
             age   born   name    toy
        0  False   True  False   True
        1  False  False  False  False
        2   True  False  False  False
        Show which entries in a Series are NA.
        >>> ser = pd.Series([5, 6, np.NaN])
        >>> ser
        0    5.0
        1    6.0
        2    NaN
        dtype: float64
","[17, 18]"
"              percentage, rate, rate_fmt, rate_noinv, rate_noinv_fmt,
              rate_inv, rate_inv_fmt, elapsed, remaining, desc, postfix.
            Note that a trailing "": "" is automatically removed after {desc}
            if the latter is empty.
        initial  : int, optional
            The initial counter value. Useful when restarting a progress
            bar [default: 0].
        position  : int, optional
            Specify the line offset to print this bar (starting from 0)
            Automatic if unspecified.
            Useful to manage multiple bars at once (eg, from threads).
        postfix  : dict or *, optional
            Specify additional stats to display at the end of the bar.
            Calls `set_postfix(**postfix)` if possible (dict).
        unit_divisor  : float, optional
            [default: 1000], ignored unless `unit_scale` is True.
        gui  : bool, optional
            WARNING: internal parameter - do not use.
            Use tqdm_gui(...) instead. If set, will attempt to use
            matplotlib animations for a graphical output [default: False].
        Returns
        -------
        out  : decorated iterator.
        """"""
        if file is None:
            file = sys.stderr
        if disable is None and hasattr(file, ""isatty"") and not file.isatty():
            disable = True
        if disable:
            self.iterable = iterable
            self.disable = disable
            self.pos = self._get_free_pos(self)
            self._instances.remove(self)
            self.n = initial
            return
        if kwargs:
            self.disable = True
            self.pos = self._get_free_pos(self)
            self._instances.remove(self)
            raise (TqdmDeprecationWarning(""""""\
`nested` is deprecated and automated. Use position instead for manual control.
"""""", fp_write=getattr(file, 'write', sys.stderr.write)) if ""nested"" in kwargs
                else TqdmKeyError(""Unknown argument(s): "" + str(kwargs)))
        # Preprocess the arguments
        if total is None and iterable is not None:
            try:
                total = len(iterable)
            except (TypeError, AttributeError):
                total = None
        if ((ncols is None) and (file in (sys.stderr, sys.stdout))) or \
                dynamic_ncols:  # pragma: no cover
            if dynamic_ncols:
                dynamic_ncols = _environ_cols_wrapper()
                if dynamic_ncols:
                    ncols = dynamic_ncols(file)
                # elif ncols is not None:
                #     ncols = 79
            else:
                _dynamic_ncols = _environ_cols_wrapper()
                if _dynamic_ncols:
                    ncols = _dynamic_ncols(file)
                # else:
                #     ncols = 79
        if miniters is None:
            miniters = 0
            dynamic_miniters = True
        else:
            dynamic_miniters = False
        if mininterval is None:
            mininterval = 0
        if maxinterval is None:
            maxinterval = 0
        if ascii is None:
            ascii = not _supports_unicode(file)
        if bar_format and not ascii:
            # Convert bar format into unicode since terminal uses unicode
            bar_format = _unicode(bar_format)
        if smoothing is None:
            smoothing = 0
        # Store the arguments
        self.iterable = iterable
        self.desc = desc or ''
        self.total = total
        self.leave = leave
        self.fp = file
        self.ncols = ncols
        self.mininterval = mininterval
        self.maxinterval = maxinterval
        self.miniters = miniters
        self.dynamic_miniters = dynamic_miniters
        self.ascii = ascii
        self.disable = disable
        self.unit = unit
        self.unit_scale = unit_scale
        self.unit_divisor = unit_divisor
        self.gui = gui
        self.dynamic_ncols = dynamic_ncols
        self.smoothing = smoothing
        self.avg_time = None
        self._time = time
        self.bar_format = bar_format
        self.postfix = None
        if postfix:
            try:
                self.set_postfix(refresh=False, **postfix)
            except TypeError:
                self.postfix = postfix
        # Init the iterations counters
        self.last_print_n = initial
        self.n = initial
        # if nested, at initial sp() call we replace '\r' by '\n' to","[50, 51, 52, 53, 54, 55]"
"        Notes
        -----
        * The `.plot` function will be faster for scatterplots where markers
          don't vary in size or color.
        * Any or all of *x*, *y*, *s*, and *c* may be masked arrays, in which
          case all masks will be combined and only unmasked points will be
          plotted.
        * Fundamentally, scatter works with 1-D arrays; *x*, *y*, *s*, and *c*
          may be input as N-D arrays, but within scatter they will be
          flattened. The exception is *c*, which will be flattened only if its
          size matches the size of *x* and *y*.
        """"""
        # Process **kwargs to handle aliases, conflicts with explicit kwargs:
        self._process_unit_info(xdata=x, ydata=y, kwargs=kwargs)
        x = self.convert_xunits(x)
        y = self.convert_yunits(y)
        # np.ma.ravel yields an ndarray, not a masked array,
        # unless its argument is a masked array.
        x = np.ma.ravel(x)
        y = np.ma.ravel(y)
        if x.size != y.size:
            raise ValueError(""x and y must be the same size"")
        if s is None:
            s = (20 if rcParams['_internal.classic_mode'] else
                 rcParams['lines.markersize'] ** 2.0)
        s = np.ma.ravel(s)
        if len(s) not in (1, x.size):
            raise ValueError(""s must be a scalar, or the same size as x and y"")
        c, colors, edgecolors = \
            self._parse_scatter_color_args(
                c, edgecolors, kwargs, x.size,
                get_next_color_func=self._get_patches_for_fill.get_next_color)
        if plotnonfinite and colors is None:
            c = np.ma.masked_invalid(c)
            x, y, s, edgecolors, linewidths = \
                cbook._combine_masks(x, y, s, edgecolors, linewidths)
        else:
            x, y, s, c, colors, edgecolors, linewidths = \
                cbook._combine_masks(
                    x, y, s, c, colors, edgecolors, linewidths)
        scales = s   # Renamed for readability below.
        # load default marker from rcParams
        if marker is None:
            marker = rcParams['scatter.marker']
        if isinstance(marker, mmarkers.MarkerStyle):
            marker_obj = marker
        else:
            marker_obj = mmarkers.MarkerStyle(marker)
        path = marker_obj.get_path().transformed(
            marker_obj.get_transform())
        if not marker_obj.is_filled():
            edgecolors = 'face'
            linewidths = rcParams['lines.linewidth']
        offsets = np.ma.column_stack([x, y])
        collection = mcoll.PathCollection(
                (path,), scales,
                facecolors=colors,
                edgecolors=edgecolors,
                linewidths=linewidths,
                offsets=offsets,
                transOffset=kwargs.pop('transform', self.transData),
                alpha=alpha
                )
        collection.set_transform(mtransforms.IdentityTransform())
        collection.update(kwargs)
        if colors is None:
            collection.set_array(c)
            collection.set_cmap(cmap)
            collection.set_norm(norm)
            collection._scale_norm(norm, vmin, vmax)
        # Classic mode only:
        # ensure there are margins to allow for the
        # finite size of the symbols.  In v2.x, margins
        # are present by default, so we disable this
        # scatter-specific override.
        if rcParams['_internal.classic_mode']:
            if self._xmargin < 0.05 and x.size > 0:
                self.set_xmargin(0.05)
            if self._ymargin < 0.05 and x.size > 0:
                self.set_ymargin(0.05)
        self.add_collection(collection)
        self._request_autoscale_view()
        return collection
    @_preprocess_data(replace_names=[""x"", ""y""], label_namer=""y"")
    @docstring.dedent_interpd
    def hexbin(self, x, y, C=None, gridsize=100, bins=None,
               xscale='linear', yscale='linear', extent=None,
               cmap=None, norm=None, vmin=None, vmax=None,
               alpha=None, linewidths=None, edgecolors='face',
               reduce_C_function=np.mean, mincnt=None, marginals=False,
               **kwargs):
        """"""
        Make a 2D hexagonal binning plot of points *x*, *y*.
        If *C* is *None*, the value of the hexagon is determined by the number
        of points in the hexagon. Otherwise, *C* specifies values at the
        coordinate (x[i], y[i]). For each hexagon, these values are reduced
        using *reduce_C_function*.
        Parameters
        ----------
        x, y : array-like
            The data positions. *x* and *y* must be of the same length.
        C : array-like, optional
            If given, these values are accumulated in the bins. Otherwise,
            every point has a value of 1. Must be of the same length as *x*",[65]
"                    break
        finally:
            try:
                if enqueuer is not None:
                    enqueuer.stop()
            finally:
                if val_enqueuer is not None:
                    val_enqueuer.stop()
        callbacks.on_train_end()
        return self.history
    @interfaces.legacy_generator_methods_support
    def evaluate_generator(self, generator, steps=None,
                           max_queue_size=10,
                           workers=1,
                           use_multiprocessing=False):
        """"""Evaluates the model on a data generator.
        The generator should return the same kind of data
        as accepted by `test_on_batch`.
        # Arguments
            generator: Generator yielding tuples (inputs, targets)
                or (inputs, targets, sample_weights)
                or an instance of Sequence (keras.utils.Sequence)
                object in order to avoid duplicate data
                when using multiprocessing.
            steps: Total number of steps (batches of samples)
                to yield from `generator` before stopping.
                Optional for `Sequence`: if unspecified, will use
                the `len(generator)` as a number of steps.
            max_queue_size: maximum size for the generator queue
            workers: Integer. Maximum number of processes to spin up
                when using process based threading.
                If unspecified, `workers` will default to 1. If 0, will
                execute the generator on the main thread.
            use_multiprocessing: if True, use process based threading.
                Note that because
                this implementation relies on multiprocessing,
                you should not pass
                non picklable arguments to the generator
                as they can't be passed
                easily to children processes.
        # Returns
            Scalar test loss (if the model has a single output and no metrics)
            or list of scalars (if the model has multiple outputs
            and/or metrics). The attribute `model.metrics_names` will give you
            the display labels for the scalar outputs.
        # Raises
            ValueError: In case the generator yields
                data in an invalid format.
        """"""
        self._make_test_function()
        steps_done = 0
        wait_time = 0.01
        all_outs = []
        batch_sizes = []
        is_sequence = isinstance(generator, Sequence)
        if not is_sequence and use_multiprocessing and workers > 1:
            warnings.warn(
                UserWarning('Using a generator with `use_multiprocessing=True`'
                            ' and multiple workers may duplicate your data.'
                            ' Please consider using the`keras.utils.Sequence'
                            ' class.'))
        if steps is None:
            if is_sequence:
                steps = len(generator)
            else:
                raise ValueError('`steps=None` is only valid for a generator'
                                 ' based on the `keras.utils.Sequence` class.'
                                 ' Please specify `steps` or use the'
                                 ' `keras.utils.Sequence` class.')
        enqueuer = None
        try:
            if workers > 0:
                if is_sequence:
                    enqueuer = OrderedEnqueuer(generator,
                                               use_multiprocessing=use_multiprocessing)
                else:
                    enqueuer = GeneratorEnqueuer(generator,
                                                 use_multiprocessing=use_multiprocessing,
                                                 wait_time=wait_time)
                enqueuer.start(workers=workers, max_queue_size=max_queue_size)
                output_generator = enqueuer.get()
            else:
                output_generator = generator
            while steps_done < steps:
                generator_output = next(output_generator)
                if not hasattr(generator_output, '__len__'):
                    raise ValueError('Output of generator should be a tuple '
                                     '(x, y, sample_weight) '
                                     'or (x, y). Found: ' +
                                     str(generator_output))
                if len(generator_output) == 2:
                    x, y = generator_output
                    sample_weight = None
                elif len(generator_output) == 3:
                    x, y, sample_weight = generator_output
                else:
                    raise ValueError('Output of generator should be a tuple '
                                     '(x, y, sample_weight) '
                                     'or (x, y). Found: ' +
                                     str(generator_output))
                outs = self.test_on_batch(x, y, sample_weight=sample_weight)
                if isinstance(x, list):
                    batch_size = x[0].shape[0]
                elif isinstance(x, dict):
                    batch_size = list(x.values())[0].shape[0]
                else:
                    batch_size = x.shape[0]
                if batch_size == 0:
                    raise ValueError('Received an empty batch. '
                                     'Batches should at least contain one item.')
                all_outs.append(outs)
                steps_done += 1
                batch_sizes.append(batch_size)
        finally:",[91]
"        self.fetches = session_kwargs.pop('fetches', [])
        if not isinstance(self.fetches, list):
            self.fetches = [self.fetches]
        # The main use case of `fetches` being passed to a model is the ability
        # to run custom updates
        # (since the outputs of fetches are never returned).
        # This requires us to wrap fetches in `identity` ops.
        self.fetches = [tf.identity(x) for x in self.fetches]
        self.session_kwargs = session_kwargs
        if session_kwargs:
            raise ValueError('Some keys in session_kwargs are not '
                             'supported at this '
                             'time: %s', session_kwargs.keys())
        self._callable_fn = None
        self._feed_arrays = None
        self._feed_symbols = None
        self._symbol_vals = None
        self._session = None
    def _make_callable(self, feed_arrays, feed_symbols, symbol_vals, session):
        """"""Generates a callable that runs the graph.
        # Arguments
            feed_arrays: List of input tensors to be fed
                Numpy arrays at runtime.
            feed_symbols: List of input tensors to be fed
                symbolic tensors at runtime.
            symbol_vals: List of symbolic tensors to be fed to `feed_symbols`.
            session: Session to use to generate the callable.
        # Returns
            Function that runs the graph according to the above options.
        """"""
        # Prepare callable options.
        callable_opts = config_pb2.CallableOptions()
        # Handle external-data feed.
        for x in feed_arrays:
            callable_opts.feed.append(x.name)
        if self.feed_dict:
            for key in sorted(self.feed_dict.keys()):
                callable_opts.feed.append(key.name)
        # Handle symbolic feed.
        for x, y in zip(feed_symbols, symbol_vals):
            connection = callable_opts.tensor_connection.add()
            if x.dtype != y.dtype:
                y = tf.cast(y, dtype=x.dtype)
            from_tensor = tf_ops._as_graph_element(y)
            if from_tensor is None:
                from_tensor = y
            connection.from_tensor = from_tensor.name  # Data tensor
            connection.to_tensor = x.name  # Placeholder
        # Handle fetches.
        for x in self.outputs + self.fetches:
            callable_opts.fetch.append(x.name)
        # Handle updates.
        callable_opts.target.append(self.updates_op.name)
        # Create callable.
        callable_fn = session._make_callable_from_options(callable_opts)
        # Cache parameters corresponding to the generated callable, so that
        # we can detect future mismatches and refresh the callable.
        self._callable_fn = callable_fn
        self._feed_arrays = feed_arrays
        self._feed_symbols = feed_symbols
        self._symbol_vals = symbol_vals
        self._session = session
    def _call(self, inputs):
        if not isinstance(inputs, (list, tuple)):
            raise TypeError('`inputs` should be a list or tuple.')
        session = get_session()
        feed_arrays = []
        array_vals = []
        feed_symbols = []
        symbol_vals = []
        for tensor, value in zip(self.inputs, inputs):
            if value is None:
                continue
            if is_tensor(value):
                # Case: feeding symbolic tensor.
                feed_symbols.append(tensor)
                symbol_vals.append(value)
            else:
                feed_arrays.append(tensor)
                # We need to do array conversion and type casting
                # at this level, since
                # `callable_fn` only supports exact matches.
                array_vals.append(
                    np.asarray(value,
                               dtype=tf.as_dtype(tensor.dtype).as_numpy_dtype))
        if self.feed_dict:
            for key in sorted(self.feed_dict.keys()):
                array_vals.append(
                    np.asarray(self.feed_dict[key],
                               dtype=tf.as_dtype(key.dtype).as_numpy_dtype))
        # Refresh callable if anything has changed.
        if (self._callable_fn is None or
                feed_arrays != self._feed_arrays or
                symbol_vals != self._symbol_vals or
                feed_symbols != self._feed_symbols or
                session != self._session):
            self._make_callable(feed_arrays,
                                feed_symbols,
                                symbol_vals,
                                session)
        fetched = self._callable_fn(*array_vals)
        return fetched[:len(self.outputs)]
    def _legacy_call(self, inputs):
        if not isinstance(inputs, (list, tuple)):
            raise TypeError('`inputs` should be a list or tuple.')
        feed_dict = self.feed_dict.copy()
        for tensor, value in zip(self.inputs, inputs):
            if is_sparse(tensor):
                sparse_coo = value.tocoo()
                indices = np.concatenate(
                    (np.expand_dims(sparse_coo.row, 1),
                     np.expand_dims(sparse_coo.col, 1)), 1)
                value = (indices, sparse_coo.data, sparse_coo.shape)
            feed_dict[tensor] = value
        fetches = self.outputs + [self.updates_op] + self.fetches
        session = get_session()
        updated = session.run(fetches=fetches, feed_dict=feed_dict,
                              **self.session_kwargs)
        return updated[:len(self.outputs)]
","[8, 106]"
"            p_t = p - self.lr * m_t_bar / (K.sqrt(v_t_prime) + self.epsilon)
            new_p = p_t
            # Apply constraints.
            if getattr(p, 'constraint', None) is not None:
                new_p = p.constraint(new_p)
            self.updates.append(K.update(p, new_p))
        return self.updates
    def get_config(self):
        config = {'lr': float(K.get_value(self.lr)),
                  'beta_1': float(K.get_value(self.beta_1)),
                  'beta_2': float(K.get_value(self.beta_2)),
                  'epsilon': self.epsilon,
                  'schedule_decay': self.schedule_decay}
        base_config = super(Nadam, self).get_config()
        return dict(list(base_config.items()) + list(config.items()))

class TFOptimizer(Optimizer):
    """"""Wrapper class for native TensorFlow optimizers.
    """"""
    def __init__(self, optimizer):
        self.optimizer = optimizer
        with K.name_scope(self.__class__.__name__):
            self.iterations = K.variable(0, dtype='int64', name='iterations')
    @interfaces.legacy_get_updates_support
    def get_updates(self, loss, params):
        grads = self.optimizer.compute_gradients(loss, params)
        self.updates = [K.update_add(self.iterations, 1)]
        opt_update = self.optimizer.apply_gradients(
            grads, global_step=self.iterations)
        self.updates.append(opt_update)
        return self.updates
    @property
    def weights(self):
        raise NotImplementedError
    def get_config(self):
        raise NotImplementedError
    def from_config(self, config):
        raise NotImplementedError

# Aliases.
sgd = SGD
rmsprop = RMSprop
adagrad = Adagrad
adadelta = Adadelta
adam = Adam
adamax = Adamax
nadam = Nadam

def serialize(optimizer):
    return serialize_keras_object(optimizer)

def deserialize(config, custom_objects=None):
    """"""Inverse of the `serialize` function.
    # Arguments
        config: Optimizer configuration dictionary.
        custom_objects: Optional dictionary mapping
            names (strings) to custom objects
            (classes and functions)
            to be considered during deserialization.
    # Returns
        A Keras Optimizer instance.
    """"""
    all_classes = {
        'sgd': SGD,
        'rmsprop': RMSprop,
        'adagrad': Adagrad,
        'adadelta': Adadelta,
        'adam': Adam,
        'adamax': Adamax,
        'nadam': Nadam,
        'tfoptimizer': TFOptimizer,
    }
    # Make deserialization case-insensitive for built-in optimizers.
    if config['class_name'].lower() in all_classes:
        config['class_name'] = config['class_name'].lower()
    return deserialize_keras_object(config,
                                    module_objects=all_classes,
                                    custom_objects=custom_objects,
                                    printable_module_name='optimizer')

def get(identifier):
    """"""Retrieves a Keras Optimizer instance.
    # Arguments
        identifier: Optimizer identifier, one of
            - String: name of an optimizer
            - Dictionary: configuration dictionary.
            - Keras Optimizer instance (it will be returned unchanged).
            - TensorFlow Optimizer instance
                (it will be wrapped as a Keras Optimizer).
    # Returns
        A Keras Optimizer instance.
    # Raises
        ValueError: If `identifier` cannot be interpreted.
    """"""
    if K.backend() == 'tensorflow':
        # Wrap TF optimizer instances
        if isinstance(identifier, tf.train.Optimizer):
            return TFOptimizer(identifier)
    if isinstance(identifier, dict):
        return deserialize(identifier)
    elif isinstance(identifier, six.string_types):
        config = {'class_name': str(identifier), 'config': {}}
        return deserialize(config)
    if isinstance(identifier, Optimizer):
        return identifier
    else:
        raise ValueError('Could not interpret optimizer identifier: ' +",[31]
"#!/usr/bin/env python
#
# Copyright 2009 Facebook
#
# Licensed under the Apache License, Version 2.0 (the ""License""); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
""""""An I/O event loop for non-blocking sockets.
Typical applications will use a single `IOLoop` object, in the
`IOLoop.instance` singleton.  The `IOLoop.start` method should usually
be called at the end of the ``main()`` function.  Atypical applications may
use more than one `IOLoop`, such as one `IOLoop` per thread, or per `unittest`
case.
In addition to I/O events, the `IOLoop` can also schedule time-based events.
`IOLoop.add_timeout` is a non-blocking alternative to `time.sleep`.
""""""
from __future__ import absolute_import, division, print_function
import collections
import datetime
import errno
import functools
import heapq
import itertools
import logging
import numbers
import os
import select
import sys
import threading
import time
import traceback
import math
from tornado.concurrent import TracebackFuture, is_future
from tornado.log import app_log, gen_log
from tornado.platform.auto import set_close_exec, Waker
from tornado import stack_context
from tornado.util import PY3, Configurable, errno_from_exception, timedelta_to_seconds, TimeoutError
try:
    import signal
except ImportError:
    signal = None
try:
    from concurrent.futures import ThreadPoolExecutor
except ImportError:
    ThreadPoolExecutor = None
if PY3:
    import _thread as thread
else:
    import thread
try:
    import asyncio
except ImportError:
    asyncio = None

_POLL_TIMEOUT = 3600.0

class IOLoop(Configurable):
    """"""A level-triggered I/O loop.
    We use ``epoll`` (Linux) or ``kqueue`` (BSD and Mac OS X) if they
    are available, or else we fall back on select(). If you are
    implementing a system that needs to handle thousands of
    simultaneous connections, you should use a system that supports
    either ``epoll`` or ``kqueue``.
    Example usage for a simple TCP server:
    .. testcode::
        import errno
        import functools
        import tornado.ioloop
        import socket
        def connection_ready(sock, fd, events):
            while True:
                try:
                    connection, address = sock.accept()
                except socket.error as e:
                    if e.args[0] not in (errno.EWOULDBLOCK, errno.EAGAIN):
                        raise
                    return
                connection.setblocking(0)
                handle_connection(connection, address)
        if __name__ == '__main__':
            sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM, 0)
            sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
            sock.setblocking(0)
            sock.bind(("""", port))
            sock.listen(128)
            io_loop = tornado.ioloop.IOLoop.current()
            callback = functools.partial(connection_ready, sock)
            io_loop.add_handler(sock.fileno(), callback, io_loop.READ)
            io_loop.start()
    .. testoutput::
       :hide:
    By default, a newly-constructed `IOLoop` becomes the thread's current
    `IOLoop`, unless there already is a current `IOLoop`. This behavior
    can be controlled with the ``make_current`` argument to the `IOLoop`
    constructor: if ``make_current=True``, the new `IOLoop` will always
    try to become current and it raises an error if there is already a
    current instance. If ``make_current=False``, the new `IOLoop` will
    not try to become current.",[46]
"        if not isinstance(other, Index):
            return False
        try:
            other = self._is_dtype_compat(other)
            if isinstance(other, type(self)):
                other = other._data
            return self._data.equals(other)
        except (TypeError, ValueError):
            pass
        return False
    # --------------------------------------------------------------------
    # Rendering Methods
    @property
    def _formatter_func(self):
        return self.categories._formatter_func
    def _format_attrs(self):
        """"""
        Return a list of tuples of the (attr,formatted_value)
        """"""
        max_categories = (
            10
            if get_option(""display.max_categories"") == 0
            else get_option(""display.max_categories"")
        )
        attrs = [
            (
                ""categories"",
                ibase.default_pprint(self.categories, max_seq_items=max_categories),
            ),
            (""ordered"", self.ordered),
        ]
        if self.name is not None:
            attrs.append((""name"", ibase.default_pprint(self.name)))
        attrs.append((""dtype"", f""'{self.dtype.name}'""))
        max_seq_items = get_option(""display.max_seq_items"") or len(self)
        if len(self) > max_seq_items:
            attrs.append((""length"", len(self)))
        return attrs
    # --------------------------------------------------------------------
    @property
    def inferred_type(self) -> str:
        return ""categorical""
    @property
    def values(self):
        """""" return the underlying data, which is a Categorical """"""
        return self._data
    @property
    def _has_complex_internals(self) -> bool:
        # used to avoid libreduction code paths, which raise or require conversion
        return True
    @doc(Index.__contains__)
    def __contains__(self, key: Any) -> bool:
        # if key is a NaN, check if any NaN is in self.
        if is_scalar(key) and isna(key):
            return self.hasnans
        hash(key)
        return contains(self, key, container=self._engine)
    @doc(Index.astype)
    def astype(self, dtype, copy=True):
        if is_interval_dtype(dtype):
            from pandas import IntervalIndex
            return IntervalIndex(np.array(self))
        elif is_categorical_dtype(dtype):
            # GH 18630
            dtype = self.dtype.update_dtype(dtype)
            if dtype == self.dtype:
                return self.copy() if copy else self
        return Index.astype(self, dtype=dtype, copy=copy)
    @cache_readonly
    def _isnan(self):
        """""" return if each value is nan""""""
        return self._data.codes == -1
    @doc(Index.fillna)
    def fillna(self, value, downcast=None):
        self._assert_can_do_op(value)
        return CategoricalIndex(self._data.fillna(value), name=self.name)
    @cache_readonly
    def _engine(self):
        # we are going to look things up with the codes themselves.
        # To avoid a reference cycle, bind `codes` to a local variable, so
        # `self` is not passed into the lambda.
        codes = self.codes
        return self._engine_type(lambda: codes, len(self))
    @doc(Index.unique)
    def unique(self, level=None):
        if level is not None:
            self._validate_index_level(level)
        result = self._values.unique()
        # Use _simple_new instead of _shallow_copy to ensure we keep dtype
        #  of result, not self.
        return type(self)._simple_new(result, name=self.name)
    @doc(Index.duplicated)
    def duplicated(self, keep=""first""):
        codes = self.codes.astype(""i8"")
        return duplicated_int64(codes, keep)
    def _to_safe_for_reshape(self):
        """""" convert to object if we are a categorical """"""
        return self.astype(""object"")
    def _maybe_cast_indexer(self, key):
        code = self.categories.get_loc(key)
        code = self.codes.dtype.type(code)
        return code
    @doc(Index.where)
    def where(self, cond, other=None):
        # TODO: Investigate an alternative implementation with","[63, 66]"
"import subprocess
import re
from thefuck.specific.sudo import sudo_support
from thefuck.utils import for_app, replace_command
from thefuck.specific.dnf import dnf_available

regex = re.compile(r'No such command: (.*)\.')

@for_app('dnf')
@sudo_support
def match(command):
    return 'no such command' in command.output.lower()

def _parse_operations(help_text_lines):
    # The regex has to be a bytes-style regex since reading from a file
    # like stdin returns a bytes-style object and a string-style regex
    # wouldn't work.
    operation_regex = re.compile(b'^([a-z-]+) +', re.MULTILINE)
    return operation_regex.findall(help_text_lines)

def _get_operations():
    proc = subprocess.Popen([""dnf"", '--help'],
                            stdout=subprocess.PIPE,
                            stderr=subprocess.PIPE)
    lines = proc.stdout.read()
    return _parse_operations(lines)

@sudo_support
def get_new_command(command):
    misspelled_command = regex.findall(command.output)[0]
    return replace_command(command, misspelled_command, _get_operations())

enabled_by_default = dnf_available","[20, 28]"
"            self._downloader.report_warning(err_msg)
            return {}
    def _mark_watched(self, video_id, video_info, player_response):
        playback_url = url_or_none(try_get(
            player_response,
            lambda x: x['playbackTracking']['videostatsPlaybackUrl']['baseUrl']) or try_get(
            video_info, lambda x: x['videostats_playback_base_url'][0]))
        if not playback_url:
            return
        parsed_playback_url = compat_urlparse.urlparse(playback_url)
        qs = compat_urlparse.parse_qs(parsed_playback_url.query)
        # cpn generation algorithm is reverse engineered from base.js.
        # In fact it works even with dummy cpn.
        CPN_ALPHABET = 'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789-_'
        cpn = ''.join((CPN_ALPHABET[random.randint(0, 256) & 63] for _ in range(0, 16)))
        qs.update({
            'ver': ['2'],
            'cpn': [cpn],
        })
        playback_url = compat_urlparse.urlunparse(
            parsed_playback_url._replace(query=compat_urllib_parse_urlencode(qs, True)))
        self._download_webpage(
            playback_url, video_id, 'Marking watched',
            'Unable to mark watched', fatal=False)
    @staticmethod
    def _extract_urls(webpage):
        # Embedded YouTube player
        entries = [
            unescapeHTML(mobj.group('url'))
            for mobj in re.finditer(r'''(?x)
            (?:
                <iframe[^>]+?src=|
                data-video-url=|
                <embed[^>]+?src=|
                embedSWF\(?:\s*|
                <object[^>]+data=|
                new\s+SWFObject\(
            )
            ([""\'])
                (?P<url>(?:https?:)?//(?:www\.)?youtube(?:-nocookie)?\.com/
                (?:embed|v|p)/[0-9A-Za-z_-]{11}.*?)
            \1''', webpage)]
        # lazyYT YouTube embed
        entries.extend(list(map(
            unescapeHTML,
            re.findall(r'class=""lazyYT"" data-youtube-id=""([^""]+)""', webpage))))
        # Wordpress ""YouTube Video Importer"" plugin
        matches = re.findall(r'''(?x)<div[^>]+
            class=(?P<q1>[\'""])[^\'""]*\byvii_single_video_player\b[^\'""]*(?P=q1)[^>]+
            data-video_id=(?P<q2>[\'""])([^\'""]+)(?P=q2)''', webpage)
        entries.extend(m[-1] for m in matches)
        return entries
    @staticmethod
    def _extract_url(webpage):
        urls = YoutubeIE._extract_urls(webpage)
        return urls[0] if urls else None
    @classmethod
    def extract_id(cls, url):
        mobj = re.match(cls._VALID_URL, url, re.VERBOSE)
        if mobj is None:
            raise ExtractorError('Invalid URL: %s' % url)
        video_id = mobj.group(2)
        return video_id
    @staticmethod
    def _extract_chapters(description, duration):
        if not description:
            return None
        chapter_lines = re.findall(
            r'(?:^|<br\s*/>)([^<]*<a[^>]+onclick=[""\']yt\.www\.watch\.player\.seekTo[^>]+>(\d{1,2}:\d{1,2}(?::\d{1,2})?)</a>[^>]*)(?=$|<br\s*/>)',
            description)
        if not chapter_lines:
            return None
        chapters = []
        for next_num, (chapter_line, time_point) in enumerate(
                chapter_lines, start=1):
            start_time = parse_duration(time_point)
            if start_time is None:
                continue
            if start_time > duration:
                break
            end_time = (duration if next_num == len(chapter_lines)
                        else parse_duration(chapter_lines[next_num][1]))
            if end_time is None:
                continue
            if end_time > duration:
                end_time = duration
            if start_time > end_time:
                break
            chapter_title = re.sub(
                r'<a[^>]+>[^<]+</a>', '', chapter_line).strip(' \t-')
            chapter_title = re.sub(r'\s+', ' ', chapter_title)
            chapters.append({
                'start_time': start_time,
                'end_time': end_time,
                'title': chapter_title,
            })
        return chapters
    def _real_extract(self, url):
        url, smuggled_data = unsmuggle_url(url, {})
        proto = (
            'http' if self._downloader.params.get('prefer_insecure', False)
            else 'https')
        start_time = None
        end_time = None
        parsed_url = compat_urllib_parse_urlparse(url)
        for component in [parsed_url.fragment, parsed_url.query]:
            query = compat_parse_qs(component)
            if start_time is None and 't' in query:
                start_time = parse_duration(query['t'][0])
            if start_time is None and 'start' in query:
                start_time = parse_duration(query['start'][0])
            if end_time is None and 'end' in query:
                end_time = parse_duration(query['end'][0])",[75]
"        """"""
        Set the value inplace, returning a a maybe different typed block.
        Parameters
        ----------
        indexer : tuple, list-like, array-like, slice
            The subset of self.values to set
        value : object
            The value being set
        Returns
        -------
        Block
        Notes
        -----
        `indexer` is a direct slice/positional indexer. `value` must
        be a compatible shape.
        """"""
        transpose = self.ndim == 2
        if isinstance(indexer, np.ndarray) and indexer.ndim > self.ndim:
            raise ValueError(f""Cannot set values with ndim > {self.ndim}"")
        # coerce None values, if appropriate
        if value is None:
            if self.is_numeric:
                value = np.nan
        # coerce if block dtype can store value
        values = self.values
        if self._can_hold_element(value):
            # We only get here for non-Extension Blocks, so _try_coerce_args
            #  is only relevant for DatetimeBlock and TimedeltaBlock
            if lib.is_scalar(value):
                value = convert_scalar_for_putitemlike(value, values.dtype)
        else:
            # current dtype cannot store value, coerce to common dtype
            find_dtype = False
            if hasattr(value, ""dtype""):
                dtype = value.dtype
                find_dtype = True
            elif lib.is_scalar(value) and not isna(value):
                dtype, _ = infer_dtype_from_scalar(value, pandas_dtype=True)
                find_dtype = True
            if find_dtype:
                dtype = find_common_type([values.dtype, dtype])
                if not is_dtype_equal(self.dtype, dtype):
                    b = self.astype(dtype)
                    return b.setitem(indexer, value)
        # value must be storeable at this moment
        if is_extension_array_dtype(getattr(value, ""dtype"", None)):
            # We need to be careful not to allow through strings that
            #  can be parsed to EADtypes
            arr_value = value
        else:
            arr_value = np.array(value)
        # cast the values to a type that can hold nan (if necessary)
        if not self._can_hold_element(value):
            dtype, _ = maybe_promote(arr_value.dtype)
            values = values.astype(dtype)
        if transpose:
            values = values.T
        # length checking
        check_setitem_lengths(indexer, value, values)
        exact_match = (
            len(arr_value.shape)
            and arr_value.shape[0] == values.shape[0]
            and arr_value.size == values.size
        )
        if is_empty_indexer(indexer, arr_value):
            # GH#8669 empty indexers
            pass
        elif is_scalar_indexer(indexer, self.ndim):
            # setting a single element for each dim and with a rhs that could
            #  be e.g. a list; see GH#6043
            values[indexer] = value
        elif (
            exact_match
            and is_categorical_dtype(arr_value.dtype)
            and not is_categorical_dtype(values)
        ):
            # GH25495 - If the current dtype is not categorical,
            # we need to create a new categorical block
            values[indexer] = value
            return self.make_block(Categorical(self.values, dtype=arr_value.dtype))
        # if we are an exact match (ex-broadcasting),
        # then use the resultant dtype
        elif exact_match:
            # We are setting _all_ of the array's values, so can cast to new dtype
            values[indexer] = value
            values = values.astype(arr_value.dtype, copy=False)
        # set
        else:
            values[indexer] = value
        if transpose:
            values = values.T
        block = self.make_block(values)
        return block
    def putmask(
        self,
        mask,
        new,
        align: bool = True,
        inplace: bool = False,
        axis: int = 0,
        transpose: bool = False,
    ):
        """"""
        putmask the data to the block; it is possible that we may create a
        new dtype of block
",[1]
"        `.BboxBase`
            containing the bounding box (in figure inches).
        """"""
        bb = []
        if bbox_extra_artists is None:
            artists = self.get_default_bbox_extra_artists()
        else:
            artists = bbox_extra_artists
        for a in artists:
            bbox = a.get_tightbbox(renderer)
            if bbox is not None and (bbox.width != 0 or bbox.height != 0):
                bb.append(bbox)
        for ax in self.axes:
            if ax.get_visible():
                # some axes don't take the bbox_extra_artists kwarg so we
                # need this conditional....
                try:
                    bbox = ax.get_tightbbox(
                        renderer, bbox_extra_artists=bbox_extra_artists)
                except TypeError:
                    bbox = ax.get_tightbbox(renderer)
                bb.append(bbox)
        bb = [b for b in bb
              if (np.isfinite(b.width) and np.isfinite(b.height)
                  and (b.width != 0 or b.height != 0))]
        if len(bb) == 0:
            return self.bbox_inches
        _bbox = Bbox.union(bb)
        bbox_inches = TransformedBbox(_bbox, Affine2D().scale(1 / self.dpi))
        return bbox_inches
    def init_layoutbox(self):
        """"""Initialize the layoutbox for use in constrained_layout.""""""
        if self._layoutbox is None:
            self._layoutbox = layoutbox.LayoutBox(
                parent=None, name='figlb', artist=self)
            self._layoutbox.constrain_geometry(0., 0., 1., 1.)
    def execute_constrained_layout(self, renderer=None):
        """"""
        Use ``layoutbox`` to determine pos positions within axes.
        See also `.set_constrained_layout_pads`.
        """"""
        from matplotlib._constrained_layout import do_constrained_layout
        _log.debug('Executing constrainedlayout')
        if self._layoutbox is None:
            cbook._warn_external(""Calling figure.constrained_layout, but ""
                                 ""figure not setup to do constrained layout. ""
                                 "" You either called GridSpec without the ""
                                 ""fig keyword, you are using plt.subplot, ""
                                 ""or you need to call figure or subplots ""
                                 ""with the constrained_layout=True kwarg."")
            return
        w_pad, h_pad, wspace, hspace = self.get_constrained_layout_pads()
        # convert to unit-relative lengths
        fig = self
        width, height = fig.get_size_inches()
        w_pad = w_pad / width
        h_pad = h_pad / height
        if renderer is None:
            renderer = layoutbox.get_renderer(fig)
        do_constrained_layout(fig, renderer, h_pad, w_pad, hspace, wspace)
    @cbook._delete_parameter(""3.2"", ""renderer"")
    def tight_layout(self, renderer=None, pad=1.08, h_pad=None, w_pad=None,
                     rect=None):
        """"""
        Adjust the padding between and around subplots.
        To exclude an artist on the axes from the bounding box calculation
        that determines the subplot parameters (i.e. legend, or annotation),
        set ``a.set_in_layout(False)`` for that artist.
        Parameters
        ----------
        renderer : subclass of `~.backend_bases.RendererBase`, optional
            Defaults to the renderer for the figure.  Deprecated.
        pad : float, default: 1.08
            Padding between the figure edge and the edges of subplots,
            as a fraction of the font size.
        h_pad, w_pad : float, default: *pad*
            Padding (height/width) between edges of adjacent subplots,
            as a fraction of the font size.
        rect : tuple (left, bottom, right, top), default: (0, 0, 1, 1)
            A rectangle in normalized figure coordinates into which the whole
            subplots area (including labels) will fit.
        See Also
        --------
        .Figure.set_tight_layout
        .pyplot.tight_layout
        """"""
        from .tight_layout import (
            get_renderer, get_subplotspec_list, get_tight_layout_figure)
        subplotspec_list = get_subplotspec_list(self.axes)
        if None in subplotspec_list:
            cbook._warn_external(""This figure includes Axes that are not ""
                                 ""compatible with tight_layout, so results ""
                                 ""might be incorrect."")
        if renderer is None:
            renderer = get_renderer(self)
        kwargs = get_tight_layout_figure(
            self, self.axes, subplotspec_list, renderer,
            pad=pad, h_pad=h_pad, w_pad=w_pad, rect=rect)
        if kwargs:
            self.subplots_adjust(**kwargs)
    def align_xlabels(self, axs=None):
        """"""
        Align the ylabels of subplots in the same subplot column if label
        alignment is being done automatically (i.e. the label position is
        not manually set).
","[115, 116, 117]"
"                async_def_nl = False
                async_def_indent = 0
        else:                                  # continued statement
            if not line:
                raise TokenError(""EOF in multi-line statement"", (lnum, 0))
            continued = 0
        while pos < max:
            pseudomatch = pseudoprog.match(line, pos)
            if pseudomatch:                                # scan for tokens
                start, end = pseudomatch.span(1)
                spos, epos, pos = (lnum, start), (lnum, end), end
                token, initial = line[start:end], line[start]
                if initial in numchars or \
                   (initial == '.' and token != '.'):      # ordinary number
                    yield (NUMBER, token, spos, epos, line)
                elif initial in '\r\n':
                    newline = NEWLINE
                    if parenlev > 0:
                        newline = NL
                    elif async_def:
                        async_def_nl = True
                    if stashed:
                        yield stashed
                        stashed = None
                    yield (newline, token, spos, epos, line)
                elif initial == '#':
                    assert not token.endswith(""\n"")
                    if stashed:
                        yield stashed
                        stashed = None
                    yield (COMMENT, token, spos, epos, line)
                elif token in triple_quoted:
                    endprog = endprogs[token]
                    endmatch = endprog.match(line, pos)
                    if endmatch:                           # all on one line
                        pos = endmatch.end(0)
                        token = line[start:pos]
                        if stashed:
                            yield stashed
                            stashed = None
                        yield (STRING, token, spos, (lnum, pos), line)
                    else:
                        strstart = (lnum, start)           # multiple lines
                        contstr = line[start:]
                        contline = line
                        break
                elif initial in single_quoted or \
                    token[:2] in single_quoted or \
                    token[:3] in single_quoted:
                    if token[-1] == '\n':                  # continued string
                        strstart = (lnum, start)
                        endprog = (endprogs[initial] or endprogs[token[1]] or
                                   endprogs[token[2]])
                        contstr, needcont = line[start:], 1
                        contline = line
                        break
                    else:                                  # ordinary string
                        if stashed:
                            yield stashed
                            stashed = None
                        yield (STRING, token, spos, epos, line)
                elif initial.isidentifier():               # ordinary name
                    if token in ('async', 'await'):
                        if async_def:
                            yield (ASYNC if token == 'async' else AWAIT,
                                   token, spos, epos, line)
                            continue
                    tok = (NAME, token, spos, epos, line)
                    if token == 'async' and not stashed:
                        stashed = tok
                        continue
                    if token in ('def', 'for'):
                        if (stashed
                                and stashed[0] == NAME
                                and stashed[1] == 'async'):
                            if token == 'def':
                                async_def = True
                                async_def_indent = indents[-1]
                            yield (ASYNC, stashed[1],
                                   stashed[2], stashed[3],
                                   stashed[4])
                            stashed = None
                    if stashed:
                        yield stashed
                        stashed = None
                    yield tok
                elif initial == '\\':                      # continued stmt
                    # This yield is new; needed for better idempotency:
                    if stashed:
                        yield stashed
                        stashed = None
                    yield (NL, token, spos, (lnum, pos), line)
                    continued = 1
                else:
                    if initial in '([{': parenlev = parenlev + 1
                    elif initial in ')]}': parenlev = parenlev - 1
                    if stashed:
                        yield stashed
                        stashed = None
                    yield (OP, token, spos, epos, line)
            else:
                yield (ERRORTOKEN, line[pos],
                           (lnum, pos), (lnum, pos+1), line)
                pos = pos + 1
    if stashed:
        yield stashed
        stashed = None
    for indent in indents[1:]:                 # pop remaining indent levels
        yield (DEDENT, '', (lnum, 0), (lnum, 0), '')
    yield (ENDMARKER, '', (lnum, 0), (lnum, 0), '')
if __name__ == '__main__':                     # testing
    import sys
    if len(sys.argv) > 1: tokenize(open(sys.argv[1]).readline)",[67]
"    """"""Replace all the '&' by '&amp;' in XML""""""
    return re.sub(
        r'&(?!amp;|lt;|gt;|apos;|quot;|#x[0-9a-fA-F]{,4};|#[0-9]{,4};)',
        '&amp;',
        xml_str)

def setproctitle(title):
    assert isinstance(title, compat_str)
    # ctypes in Jython is not complete
    # http://bugs.jython.org/issue2148
    if sys.platform.startswith('java'):
        return
    try:
        libc = ctypes.cdll.LoadLibrary('libc.so.6')
    except OSError:
        return
    except TypeError:
        # LoadLibrary in Windows Python 2.7.13 only expects
        # a bytestring, but since unicode_literals turns
        # every string into a unicode string, it fails.
        return
    title_bytes = title.encode('utf-8')
    buf = ctypes.create_string_buffer(len(title_bytes))
    buf.value = title_bytes
    try:
        libc.prctl(15, buf, 0, 0, 0)
    except AttributeError:
        return  # Strange libc, just skip this

def remove_start(s, start):
    return s[len(start):] if s is not None and s.startswith(start) else s

def remove_end(s, end):
    return s[:-len(end)] if s is not None and s.endswith(end) else s

def remove_quotes(s):
    if s is None or len(s) < 2:
        return s
    for quote in ('""', ""'"", ):
        if s[0] == quote and s[-1] == quote:
            return s[1:-1]
    return s

def url_basename(url):
    path = compat_urlparse.urlparse(url).path
    return path.strip('/').split('/')[-1]

def base_url(url):
    return re.match(r'https?://[^?#&]+/', url).group()

def urljoin(base, path):
    if isinstance(path, bytes):
        path = path.decode('utf-8')
    if not isinstance(path, compat_str) or not path:
        return None
    if re.match(r'^(?:https?:)?//', path):
        return path
    if isinstance(base, bytes):
        base = base.decode('utf-8')
    if not isinstance(base, compat_str) or not re.match(
            r'^(?:https?:)?//', base):
        return None
    return compat_urlparse.urljoin(base, path)

class HEADRequest(compat_urllib_request.Request):
    def get_method(self):
        return 'HEAD'

class PUTRequest(compat_urllib_request.Request):
    def get_method(self):
        return 'PUT'

def int_or_none(v, scale=1, default=None, get_attr=None, invscale=1):
    if get_attr:
        if v is not None:
            v = getattr(v, get_attr, None)
    if v == '':
        v = None
    if v is None:
        return default
    try:
        return int(v) * invscale // scale
    except ValueError:
        return default

def str_or_none(v, default=None):
    return default if v is None else compat_str(v)

def str_to_int(int_str):
    """""" A more relaxed version of int_or_none """"""
    if int_str is None:
        return None
    int_str = re.sub(r'[,\.\+]', '', int_str)
    return int(int_str)

def float_or_none(v, scale=1, invscale=1, default=None):
    if v is None:
        return default
    try:
        return float(v) * invscale / scale
    except ValueError:
        return default

def bool_or_none(v, default=None):
    return v if isinstance(v, bool) else default

def strip_or_none(v):
    return None if v is None else v.strip()
",[64]
"#!/usr/bin/env python
# coding: utf8
""""""A simple example of extracting relations between phrases and entities using
spaCy's named entity recognizer and the dependency parse. Here, we extract
money and currency values (entities labelled as MONEY) and then check the
dependency tree to find the noun phrase they are referring to – for example:
$9.4 million --> Net income.
Compatible with: spaCy v2.0.0+
Last tested with: v2.1.0
""""""
from __future__ import unicode_literals, print_function
import plac
import spacy

TEXTS = [
    ""Net income was $9.4 million compared to the prior year of $2.7 million."",
    ""Revenue exceeded twelve billion dollars, with a loss of $1b."",
]

@plac.annotations(
    model=(""Model to load (needs parser and NER)"", ""positional"", None, str)
)
def main(model=""en_core_web_sm""):
    nlp = spacy.load(model)
    print(""Loaded model '%s'"" % model)
    print(""Processing %d texts"" % len(TEXTS))
    for text in TEXTS:
        doc = nlp(text)
        relations = extract_currency_relations(doc)
        for r1, r2 in relations:
            print(""{:<10}\t{}\t{}"".format(r1.text, r2.ent_type_, r2.text))

def filter_spans(spans):
    # Filter a sequence of spans so they don't contain overlaps
    get_sort_key = lambda span: (span.end - span.start, span.start)
    sorted_spans = sorted(spans, key=get_sort_key, reverse=True)
    result = []
    seen_tokens = set()
    for span in sorted_spans:
        if span.start not in seen_tokens and span.end - 1 not in seen_tokens:
            result.append(span)
            seen_tokens.update(range(span.start, span.end))
    return result

def extract_currency_relations(doc):
    # Merge entities and noun chunks into one token
    spans = list(doc.ents) + list(doc.noun_chunks)
    spans = filter_spans(spans)
    with doc.retokenize() as retokenizer:
        for span in spans:
            retokenizer.merge(span)
    relations = []
    for money in filter(lambda w: w.ent_type_ == ""MONEY"", doc):
        if money.dep_ in (""attr"", ""dobj""):
            subject = [w for w in money.head.lefts if w.dep_ == ""nsubj""]
            if subject:
                subject = subject[0]
                relations.append((subject, money))
        elif money.dep_ == ""pobj"" and money.head.dep_ == ""prep"":
            relations.append((money.head.head, money))
    return relations

if __name__ == ""__main__"":
    plac.call(main)
    # Expected output:
    # Net income      MONEY   $9.4 million
    # the prior year  MONEY   $2.7 million
    # Revenue         MONEY   twelve billion dollars
    # a loss          MONEY   1b","[9, 40, 47]"
"        # for binary ops, use our custom dunder methods
        result = ops.maybe_dispatch_ufunc_to_dunder_op(
            self, ufunc, method, *inputs, **kwargs
        )
        if result is not NotImplemented:
            return result
        mask = np.zeros(len(self), dtype=bool)
        inputs2 = []
        for x in inputs:
            if isinstance(x, IntegerArray):
                mask |= x._mask
                inputs2.append(x._data)
            else:
                inputs2.append(x)
        def reconstruct(x):
            # we don't worry about scalar `x` here, since we
            # raise for reduce up above.
            if is_integer_dtype(x.dtype):
                m = mask.copy()
                return IntegerArray(x, m)
            else:
                x[mask] = np.nan
            return x
        result = getattr(ufunc, method)(*inputs2, **kwargs)
        if isinstance(result, tuple):
            tuple(reconstruct(x) for x in result)
        else:
            return reconstruct(result)
    def __setitem__(self, key, value) -> None:
        _is_scalar = is_scalar(value)
        if _is_scalar:
            value = [value]
        value, mask = coerce_to_array(value, dtype=self.dtype)
        if _is_scalar:
            value = value[0]
            mask = mask[0]
        key = check_array_indexer(self, key)
        self._data[key] = value
        self._mask[key] = mask
    def astype(self, dtype, copy: bool = True) -> ArrayLike:
        """"""
        Cast to a NumPy array or ExtensionArray with 'dtype'.
        Parameters
        ----------
        dtype : str or dtype
            Typecode or data-type to which the array is cast.
        copy : bool, default True
            Whether to copy the data, even if not necessary. If False,
            a copy is made only if the old dtype does not match the
            new dtype.
        Returns
        -------
        ndarray or ExtensionArray
            NumPy ndarray, BooleanArray or IntegerArray with 'dtype' for its dtype.
        Raises
        ------
        TypeError
            if incompatible type with an IntegerDtype, equivalent of same_kind
            casting
        """"""
        from pandas.core.arrays.boolean import BooleanArray, BooleanDtype
        dtype = pandas_dtype(dtype)
        # if we are astyping to an existing IntegerDtype we can fastpath
        if isinstance(dtype, _IntegerDtype):
            result = self._data.astype(dtype.numpy_dtype, copy=False)
            return type(self)(result, mask=self._mask, copy=False)
        elif isinstance(dtype, BooleanDtype):
            result = self._data.astype(""bool"", copy=False)
            return BooleanArray(result, mask=self._mask, copy=False)
        # coerce
        if is_float_dtype(dtype):
            # In astype, we consider dtype=float to also mean na_value=np.nan
            kwargs = dict(na_value=np.nan)
        elif is_datetime64_dtype(dtype):
            kwargs = dict(na_value=np.datetime64(""NaT""))
        else:
            kwargs = {}
        data = self.to_numpy(dtype=dtype, **kwargs)
        return astype_nansafe(data, dtype, copy=False)
    def _values_for_factorize(self) -> Tuple[np.ndarray, float]:
        # TODO: https://github.com/pandas-dev/pandas/issues/30037
        # use masked algorithms, rather than object-dtype / np.nan.
        return self.to_numpy(na_value=np.nan), np.nan
    def _values_for_argsort(self) -> np.ndarray:
        """"""
        Return values for sorting.
        Returns
        -------
        ndarray
            The transformed values should maintain the ordering between values
            within the array.
        See Also
        --------
        ExtensionArray.argsort
        """"""
        data = self._data.copy()
        data[self._mask] = data.min() - 1
        return data
    @classmethod
    def _create_comparison_method(cls, op):
        op_name = op.__name__
        @unpack_zerodim_and_defer(op.__name__)
        def cmp_method(self, other):
            from pandas.arrays import BooleanArray
            mask = None",[115]
"    try:
        subprocess.Popen([exe] + args, stdout=subprocess.PIPE, stderr=subprocess.PIPE).communicate()
    except OSError:
        return False
    return exe

class PagedList(object):
    def __init__(self, pagefunc, pagesize):
        self._pagefunc = pagefunc
        self._pagesize = pagesize
    def __len__(self):
        # This is only useful for tests
        return len(self.getslice())
    def getslice(self, start=0, end=None):
        res = []
        for pagenum in itertools.count(start // self._pagesize):
            firstid = pagenum * self._pagesize
            nextfirstid = pagenum * self._pagesize + self._pagesize
            if start >= nextfirstid:
                continue
            page_results = list(self._pagefunc(pagenum))
            startv = (
                start % self._pagesize
                if firstid <= start < nextfirstid
                else 0)
            endv = (
                ((end - 1) % self._pagesize) + 1
                if (end is not None and firstid <= end <= nextfirstid)
                else None)
            if startv != 0 or endv is not None:
                page_results = page_results[startv:endv]
            res.extend(page_results)
            # A little optimization - if current page is not ""full"", ie. does
            # not contain page_size videos then we can assume that this page
            # is the last one - there are no more ids on further pages -
            # i.e. no need to query again.
            if len(page_results) + startv < self._pagesize:
                break
            # If we got the whole page, but the next page is not interesting,
            # break out early as well
            if end == nextfirstid:
                break
        return res

def uppercase_escape(s):
    return re.sub(
        r'\\U[0-9a-fA-F]{8}',
        lambda m: m.group(0).decode('unicode-escape'), s)
try:
    struct.pack(u'!I', 0)
except TypeError:
    # In Python 2.6 (and some 2.7 versions), struct requires a bytes argument
    def struct_pack(spec, *args):
        if isinstance(spec, compat_str):
            spec = spec.encode('ascii')
        return struct.pack(spec, *args)
    def struct_unpack(spec, *args):
        if isinstance(spec, compat_str):
            spec = spec.encode('ascii')
        return struct.unpack(spec, *args)
else:
    struct_pack = struct.pack
    struct_unpack = struct.unpack

def read_batch_urls(batch_fd):
    def fixup(url):
        if not isinstance(url, compat_str):
            url = url.decode('utf-8', 'replace')
        BOM_UTF8 = u'\xef\xbb\xbf'
        if url.startswith(BOM_UTF8):
            url = url[len(BOM_UTF8):]
        url = url.strip()
        if url.startswith(('#', ';', ']')):
            return False
        return url
    with contextlib.closing(batch_fd) as fd:
        return [url for url in map(fixup, fd) if url]

def urlencode_postdata(*args, **kargs):
    return compat_urllib_parse.urlencode(*args, **kargs).encode('ascii')

def parse_xml(s):
    class TreeBuilder(xml.etree.ElementTree.TreeBuilder):
        def doctype(self, name, pubid, system):
            pass  # Ignore doctypes
    parser = xml.etree.ElementTree.XMLParser(target=TreeBuilder())
    kwargs = {'parser': parser} if sys.version_info >= (2, 7) else {}
    return xml.etree.ElementTree.XML(s.encode('utf-8'), **kwargs)

if sys.version_info < (3, 0) and sys.platform == 'win32':
    def compat_getpass(prompt, *args, **kwargs):
        if isinstance(prompt, compat_str):
            prompt = prompt.encode(preferredencoding())
        return getpass.getpass(prompt, *args, **kwargs)
else:
    compat_getpass = getpass.getpass

US_RATINGS = {
    'G': 0,
    'PG': 10,
    'PG-13': 13,
    'R': 16,
    'NC': 18,
}

def strip_jsonp(code):",[57]
"        matplotlib.axes.Axes.set_aspect
            for a description of aspect handling.
        """"""
        if not (anchor in mtransforms.Bbox.coefs or len(anchor) == 2):
            raise ValueError('argument must be among %s' %
                             ', '.join(mtransforms.Bbox.coefs))
        if share:
            axes = {*self._shared_x_axes.get_siblings(self),
                    *self._shared_y_axes.get_siblings(self)}
        else:
            axes = [self]
        for ax in axes:
            ax._anchor = anchor
        self.stale = True
    def get_data_ratio(self):
        """"""
        Return the aspect ratio of the scaled data.
        Notes
        -----
        This method is intended to be overridden by new projection types.
        """"""
        trf_xmin, trf_xmax = map(
            self.xaxis.get_transform().transform, self.get_xbound())
        trf_ymin, trf_ymax = map(
            self.yaxis.get_transform().transform, self.get_ybound())
        xsize = max(abs(trf_xmax - trf_xmin), 1e-30)
        ysize = max(abs(trf_ymax - trf_ymin), 1e-30)
        return ysize / xsize
    @cbook.deprecated(""3.2"")
    def get_data_ratio_log(self):
        """"""
        Return the aspect ratio of the raw data in log scale.
        Notes
        -----
        Will be used when both axis are in log scale.
        """"""
        xmin, xmax = self.get_xbound()
        ymin, ymax = self.get_ybound()
        xsize = max(abs(math.log10(xmax) - math.log10(xmin)), 1e-30)
        ysize = max(abs(math.log10(ymax) - math.log10(ymin)), 1e-30)
        return ysize / xsize
    def apply_aspect(self, position=None):
        """"""
        Adjust the Axes for a specified data aspect ratio.
        Depending on `.get_adjustable` this will modify either the Axes box
        (position) or the view limits. In the former case, `.get_anchor`
        will affect the position.
        Notes
        -----
        This is called automatically when each Axes is drawn.  You may need
        to call it yourself if you need to update the Axes position and/or
        view limits before the Figure is drawn.
        See Also
        --------
        matplotlib.axes.Axes.set_aspect
            for a description of aspect ratio handling.
        matplotlib.axes.Axes.set_adjustable
            defining the parameter to adjust in order to meet the required
            aspect.
        matplotlib.axes.Axes.set_anchor
            defining the position in case of extra space.
        """"""
        if position is None:
            position = self.get_position(original=True)
        aspect = self.get_aspect()
        if aspect == 'auto':
            self._set_position(position, which='active')
            return
        if aspect == 'equal':
            aspect = 1
        fig_width, fig_height = self.get_figure().get_size_inches()
        fig_aspect = fig_height / fig_width
        if self._adjustable == 'box':
            if self in self._twinned_axes:
                raise RuntimeError(""Adjustable 'box' is not allowed in a ""
                                   ""twinned Axes; use 'datalim' instead"")
            box_aspect = aspect * self.get_data_ratio()
            pb = position.frozen()
            pb1 = pb.shrunk_to_aspect(box_aspect, pb, fig_aspect)
            self._set_position(pb1.anchored(self.get_anchor(), pb), 'active')
            return
        # self._adjustable == 'datalim'
        # reset active to original in case it had been changed by prior use
        # of 'box'
        self._set_position(position, which='active')
        x_trf = self.xaxis.get_transform()
        y_trf = self.yaxis.get_transform()
        xmin, xmax = map(x_trf.transform, self.get_xbound())
        ymin, ymax = map(y_trf.transform, self.get_ybound())
        xsize = max(abs(xmax - xmin), 1e-30)
        ysize = max(abs(ymax - ymin), 1e-30)
        l, b, w, h = position.bounds
        box_aspect = fig_aspect * (h / w)
        data_ratio = box_aspect / aspect
        y_expander = data_ratio * xsize / ysize - 1
        # If y_expander > 0, the dy/dx viewLim ratio needs to increase
        if abs(y_expander) < 0.005:
            return
        dL = self.dataLim
        x0, x1 = map(x_trf.inverted().transform, dL.intervalx)
        y0, y1 = map(y_trf.inverted().transform, dL.intervaly)
        xr = 1.05 * (x1 - x0)
        yr = 1.05 * (y1 - y0)
        xmarg = xsize - xr","[121, 122]"
"""""""
This module contains general purpose URL functions not found in the standard
library.
Some of the functions that used to be imported from this module have been moved
to the w3lib.url module. Always import those from there instead.
""""""
import posixpath
import re
import six
from six.moves.urllib.parse import (ParseResult, urlunparse, urldefrag,
                                    urlparse, parse_qsl, urlencode,
                                    quote, unquote)
if not six.PY2:
    from urllib.parse import unquote_to_bytes
# scrapy.utils.url was moved to w3lib.url and import * ensures this
# move doesn't break old code
from w3lib.url import *
from w3lib.url import _safe_chars
from scrapy.utils.python import to_bytes, to_native_str, to_unicode

def url_is_from_any_domain(url, domains):
    """"""Return True if the url belongs to any of the given domains""""""
    host = parse_url(url).netloc.lower()
    if not host:
        return False
    domains = [d.lower() for d in domains]
    return any((host == d) or (host.endswith('.%s' % d)) for d in domains)

def url_is_from_spider(url, spider):
    """"""Return True if the url belongs to the given spider""""""
    return url_is_from_any_domain(url,
        [spider.name] + list(getattr(spider, 'allowed_domains', [])))

def url_has_any_extension(url, extensions):
    return posixpath.splitext(parse_url(url).path)[1].lower() in extensions

def _safe_ParseResult(parts, encoding='utf8', path_encoding='utf8'):
    return (
        to_native_str(parts.scheme),
        to_native_str(parts.netloc.encode('idna')),
        # default encoding for path component SHOULD be UTF-8
        quote(to_bytes(parts.path, path_encoding), _safe_chars),
        quote(to_bytes(parts.params, path_encoding), _safe_chars),
        # encoding of query and fragment follows page encoding
        # or form-charset (if known and passed)
        quote(to_bytes(parts.query, encoding), _safe_chars),
        quote(to_bytes(parts.fragment, encoding), _safe_chars)
    )

def canonicalize_url(url, keep_blank_values=True, keep_fragments=False,
                     encoding=None):
    """"""Canonicalize the given url by applying the following procedures:
    - sort query arguments, first by key, then by value
    - percent encode paths ; non-ASCII characters are percent-encoded
      using UTF-8 (RFC-3986)
    - percent encode query arguments ; non-ASCII characters are percent-encoded
      using passed `encoding` (UTF-8 by default)
    - normalize all spaces (in query arguments) '+' (plus symbol)
    - normalize percent encodings case (%2f -> %2F)
    - remove query arguments with blank values (unless `keep_blank_values` is True)
    - remove fragments (unless `keep_fragments` is True)
    The url passed can be bytes or unicode, while the url returned is
    always a native str (bytes in Python 2, unicode in Python 3).
    For examples see the tests in tests/test_utils_url.py
    """"""
    # If supplied `encoding` is not compatible with all characters in `url`,
    # fallback to UTF-8 as safety net.
    # UTF-8 can handle all Unicode characters,
    # so we should be covered regarding URL normalization,
    # if not for proper URL expected by remote website.
    try:
        scheme, netloc, path, params, query, fragment = _safe_ParseResult(
            parse_url(url), encoding=encoding)
    except UnicodeEncodeError as e:
        scheme, netloc, path, params, query, fragment = _safe_ParseResult(
            parse_url(url), encoding='utf8')
    # 1. decode query-string as UTF-8 (or keep raw bytes),
    #    sort values,
    #    and percent-encode them back
    if six.PY2:
        keyvals = parse_qsl(query, keep_blank_values)
    else:
        # Python3's urllib.parse.parse_qsl does not work as wanted
        # for percent-encoded characters that do not match passed encoding,
        # they get lost.
        #
        # e.g., 'q=b%a3' becomes [('q', 'b\ufffd')]
        # (ie. with 'REPLACEMENT CHARACTER' (U+FFFD),
        #      instead of \xa3 that you get with Python2's parse_qsl)
        #
        # what we want here is to keep raw bytes, and percent encode them
        # so as to preserve whatever encoding what originally used.
        #
        # See https://tools.ietf.org/html/rfc3987#section-6.4:
        #
        # For example, it is possible to have a URI reference of
        # ""http://www.example.org/r%E9sum%E9.xml#r%C3%A9sum%C3%A9"", where the
        # document name is encoded in iso-8859-1 based on server settings, but
        # where the fragment identifier is encoded in UTF-8 according to
        # [XPointer]. The IRI corresponding to the above URI would be (in XML
        # notation)
        # ""http://www.example.org/r%E9sum%E9.xml#r&#xE9;sum&#xE9;"".
        # Similar considerations apply to query parts.  The functionality of
        # IRIs (namely, to be able to include non-ASCII characters) can only be
        # used if the query part is encoded in UTF-8.
        keyvals = parse_qsl_to_bytes(query, keep_blank_values)
    keyvals.sort()
    query = urlencode(keyvals)
    # 2. decode percent-encoded sequences in path as UTF-8 (or keep raw bytes)
    #    and percent-encode path again (this normalizes to upper-case %XX)
    uqp = _unquotepath(path)
    path = quote(uqp, _safe_chars) or '/'
",[45]
"            # Compatibility since 2015-04-28
            if any(not hasattr(t, 'disable_hard_timeout') for t in six.itervalues(self._tasks)):
                for t in six.itervalues(self._tasks):
                    t.disable_hard_timeout = None
        else:
            logger.info(""No prior state file exists at %s. Starting with clean slate"", self._state_path)
    def get_active_tasks(self, status=None):
        if status:
            for task in six.itervalues(self._status_tasks[status]):
                yield task
        else:
            for task in six.itervalues(self._tasks):
                yield task
    def get_running_tasks(self):
        return six.itervalues(self._status_tasks[RUNNING])
    def get_pending_tasks(self):
        return itertools.chain.from_iterable(six.itervalues(self._status_tasks[status])
                                             for status in [PENDING, RUNNING])
    def num_pending_tasks(self):
        """"""
        Return how many tasks are PENDING + RUNNING. O(1).
        """"""
        return len(self._status_tasks[PENDING]) + len(self._status_tasks[RUNNING])
    def get_task(self, task_id, default=None, setdefault=None):
        if setdefault:
            task = self._tasks.setdefault(task_id, setdefault)
            self._status_tasks[task.status][task.id] = task
            return task
        else:
            return self._tasks.get(task_id, default)
    def has_task(self, task_id):
        return task_id in self._tasks
    def re_enable(self, task, config=None):
        task.scheduler_disable_time = None
        task.failures.clear()
        if config:
            self.set_status(task, FAILED, config)
            task.failures.clear()
    def set_status(self, task, new_status, config=None):
        if new_status == FAILED:
            assert config is not None
        if new_status == DISABLED and task.status == RUNNING:
            return
        if task.status == DISABLED:
            if new_status == DONE:
                self.re_enable(task)
            # don't allow workers to override a scheduler disable
            elif task.scheduler_disable_time is not None:
                return
        if new_status == FAILED and task.can_disable() and task.status != DISABLED:
            task.add_failure()
            if task.has_excessive_failures():
                task.scheduler_disable_time = time.time()
                new_status = DISABLED
                notifications.send_error_email(
                    'Luigi Scheduler: DISABLED {task} due to excessive failures'.format(task=task.id),
                    '{task} failed {failures} times in the last {window} seconds, so it is being '
                    'disabled for {persist} seconds'.format(
                        failures=config.disable_failures,
                        task=task.id,
                        window=config.disable_window,
                        persist=config.disable_persist,
                    ))
        elif new_status == DISABLED:
            task.scheduler_disable_time = None
        self._status_tasks[task.status].pop(task.id)
        self._status_tasks[new_status][task.id] = task
        task.status = new_status
    def fail_dead_worker_task(self, task, config, assistants):
        # If a running worker disconnects, tag all its jobs as FAILED and subject it to the same retry logic
        if task.status == RUNNING and task.worker_running and task.worker_running not in task.stakeholders | assistants:
            logger.info(""Task %r is marked as running by disconnected worker %r -> marking as ""
                        ""FAILED with retry delay of %rs"", task.id, task.worker_running,
                        config.retry_delay)
            task.worker_running = None
            self.set_status(task, FAILED, config)
            task.retry = time.time() + config.retry_delay
    def prune(self, task, config):
        remove = False
        # Mark tasks with no remaining active stakeholders for deletion
        if not task.stakeholders:
            if task.remove is None:
                logger.info(""Task %r has stakeholders %r but none remain connected -> will remove ""
                            ""task in %s seconds"", task.id, task.stakeholders, config.remove_delay)
                task.remove = time.time() + config.remove_delay
        # Re-enable task after the disable time expires
        if task.status == DISABLED and task.scheduler_disable_time is not None:
            if time.time() - fix_time(task.scheduler_disable_time) > config.disable_persist:
                self.re_enable(task, config)
        # Remove tasks that have no stakeholders
        if task.remove and time.time() > task.remove:
            logger.info(""Removing task %r (no connected stakeholders)"", task.id)
            remove = True
        # Reset FAILED tasks to PENDING if max timeout is reached, and retry delay is >= 0
        if task.status == FAILED and config.retry_delay >= 0 and task.retry < time.time():
            self.set_status(task, PENDING, config)
        return remove
    def inactivate_tasks(self, delete_tasks):
        # The terminology is a bit confusing: we used to ""delete"" tasks when they became inactive,
        # but with a pluggable state storage, you might very well want to keep some history of
        # older tasks as well. That's why we call it ""inactivate"" (as in the verb)
        for task in delete_tasks:
            task_obj = self._tasks.pop(task)
            self._status_tasks[task_obj.status].pop(task)
",[59]
"    parser = xml.etree.ElementTree.XMLParser(target=TreeBuilder())
    kwargs = {'parser': parser} if sys.version_info >= (2, 7) else {}
    tree = xml.etree.ElementTree.XML(s.encode('utf-8'), **kwargs)
    # Fix up XML parser in Python 2.x
    if sys.version_info < (3, 0):
        for n in etree_iter(tree):
            if n.text is not None:
                if not isinstance(n.text, compat_str):
                    n.text = n.text.decode('utf-8')
    return tree

US_RATINGS = {
    'G': 0,
    'PG': 10,
    'PG-13': 13,
    'R': 16,
    'NC': 18,
}

def parse_age_limit(s):
    if s is None:
        return None
    m = re.match(r'^(?P<age>\d{1,2})\+?$', s)
    return int(m.group('age')) if m else US_RATINGS.get(s, None)

def strip_jsonp(code):
    return re.sub(
        r'(?s)^[a-zA-Z0-9_]+\s*\(\s*(.*)\);?\s*?(?://[^\n]*)*$', r'\1', code)

def js_to_json(code):
    def fix_kv(m):
        v = m.group(0)
        if v in ('true', 'false', 'null'):
            return v
        if v.startswith('""'):
            return v
        if v.startswith(""'""):
            v = v[1:-1]
            v = re.sub(r""\\\\|\\'|\"""", lambda m: {
                '\\\\': '\\\\',
                ""\\'"": ""'"",
                '""': '\\""',
            }[m.group(0)], v)
        return '""%s""' % v
    res = re.sub(r'''(?x)
        ""(?:[^""\\]*(?:\\\\|\\['""nu]))*[^""\\]*""|
        '(?:[^'\\]*(?:\\\\|\\['""nu]))*[^'\\]*'|
        [a-zA-Z_][.a-zA-Z_0-9]*
        ''', fix_kv, code)
    res = re.sub(r',(\s*[\]}])', lambda m: m.group(1), res)
    return res

def qualities(quality_ids):
    """""" Get a numeric quality value out of a list of possible values """"""
    def q(qid):
        try:
            return quality_ids.index(qid)
        except ValueError:
            return -1
    return q

DEFAULT_OUTTMPL = '%(title)s-%(id)s.%(ext)s'

def limit_length(s, length):
    """""" Add ellipses to overly long strings """"""
    if s is None:
        return None
    ELLIPSES = '...'
    if len(s) > length:
        return s[:length - len(ELLIPSES)] + ELLIPSES
    return s

def version_tuple(v):
    return tuple(int(e) for e in re.split(r'[-.]', v))

def is_outdated_version(version, limit, assume_new=True):
    if not version:
        return not assume_new
    try:
        return version_tuple(version) < version_tuple(limit)
    except ValueError:
        return not assume_new

def ytdl_is_updateable():
    """""" Returns if youtube-dl can be updated with -U """"""
    from zipimport import zipimporter
    return isinstance(globals().get('__loader__'), zipimporter) or hasattr(sys, 'frozen')

def args_to_str(args):
    # Get a short string representation for a subprocess command
    return ' '.join(shlex_quote(a) for a in args)

def mimetype2ext(mt):
    _, _, res = mt.rpartition('/')
    return {
        'x-ms-wmv': 'wmv',
        'x-mp4-fragmented': 'mp4',
        'ttml+xml': 'ttml',
    }.get(res, res)

def urlhandle_detect_ext(url_handle):
    try:
        url_handle.headers
        getheader = lambda h: url_handle.headers[h]
    except AttributeError:  # Python < 3
        getheader = url_handle.info().getheader
    cd = getheader('Content-Disposition')
    if cd:
        m = re.match(r'attachment;\s*filename=""(?P<filename>[^""]+)""', cd)","[40, 41]"
"        `.stack_context`, to avoid picking up the context of the function
        that was interrupted by the signal.
        """"""
        raise NotImplementedError()
    def spawn_callback(self, callback, *args, **kwargs):
        """"""Calls the given callback on the next IOLoop iteration.
        Unlike all other callback-related methods on IOLoop,
        ``spawn_callback`` does not associate the callback with its caller's
        ``stack_context``, so it is suitable for fire-and-forget callbacks
        that should not interfere with the caller.
        .. versionadded:: 4.0
        """"""
        with stack_context.NullContext():
            self.add_callback(callback, *args, **kwargs)
    def add_future(self, future, callback):
        """"""Schedules a callback on the ``IOLoop`` when the given
        `.Future` is finished.
        The callback is invoked with one argument, the
        `.Future`.
        """"""
        assert is_future(future)
        callback = stack_context.wrap(callback)
        future.add_done_callback(
            lambda future: self.add_callback(callback, future))
    def run_in_executor(self, executor, func, *args):
        """"""Runs a function in a ``concurrent.futures.Executor``. If
        ``executor`` is ``None``, the IO loop's default executor will be used.
        Use `functools.partial` to pass keyword arguments to `func`.
        """"""
        if ThreadPoolExecutor is None:
            raise RuntimeError(
                ""concurrent.futures is required to use IOLoop.run_in_executor"")
        if executor is None:
            if not hasattr(self, '_executor'):
                from tornado.process import cpu_count
                self._executor = ThreadPoolExecutor(max_workers=(cpu_count() * 5))
            executor = self._executor
        return executor.submit(func, *args)
    def set_default_executor(self, executor):
        """"""Sets the default executor to use with :meth:`run_in_executor`.""""""
        self._executor = executor
    def _run_callback(self, callback):
        """"""Runs a callback with error handling.
        For use in subclasses.
        """"""
        try:
            ret = callback()
            if ret is not None:
                from tornado import gen
                # Functions that return Futures typically swallow all
                # exceptions and store them in the Future.  If a Future
                # makes it out to the IOLoop, ensure its exception (if any)
                # gets logged too.
                try:
                    ret = gen.convert_yielded(ret)
                except gen.BadYieldError:
                    # It's not unusual for add_callback to be used with
                    # methods returning a non-None and non-yieldable
                    # result, which should just be ignored.
                    pass
                else:
                    self.add_future(ret, self._discard_future_result)
        except Exception:
            self.handle_callback_exception(callback)
    def _discard_future_result(self, future):
        """"""Avoid unhandled-exception warnings from spawned coroutines.""""""
        future.result()
    def handle_callback_exception(self, callback):
        """"""This method is called whenever a callback run by the `IOLoop`
        throws an exception.
        By default simply logs the exception as an error.  Subclasses
        may override this method to customize reporting of exceptions.
        The exception itself is not passed explicitly, but is available
        in `sys.exc_info`.
        """"""
        app_log.error(""Exception in callback %r"", callback, exc_info=True)
    def split_fd(self, fd):
        """"""Returns an (fd, obj) pair from an ``fd`` parameter.
        We accept both raw file descriptors and file-like objects as
        input to `add_handler` and related methods.  When a file-like
        object is passed, we must retain the object itself so we can
        close it correctly when the `IOLoop` shuts down, but the
        poller interfaces favor file descriptors (they will accept
        file-like objects and call ``fileno()`` for you, but they
        always return the descriptor itself).
        This method is provided for use by `IOLoop` subclasses and should
        not generally be used by application code.
        .. versionadded:: 4.0
        """"""
        try:
            return fd.fileno(), fd
        except AttributeError:
            return fd, fd
    def close_fd(self, fd):
        """"""Utility method to close an ``fd``.
        If ``fd`` is a file-like object, we close it directly; otherwise
        we use `os.close`.
        This method is provided for use by `IOLoop` subclasses (in
        implementations of ``IOLoop.close(all_fds=True)`` and should
        not generally be used by application code.
        .. versionadded:: 4.0
        """"""","[46, 47]"
"        try:
            return compat_chr(int(numstr, base))
        except ValueError:
            pass
    # Unknown entity in name, return its literal representation
    return '&%s;' % entity

def unescapeHTML(s):
    if s is None:
        return None
    assert type(s) == compat_str
    return re.sub(
        r'&([^;]+;)', lambda m: _htmlentity_transform(m.group(1)), s)

def get_subprocess_encoding():
    if sys.platform == 'win32' and sys.getwindowsversion()[0] >= 5:
        # For subprocess calls, encode with locale encoding
        # Refer to http://stackoverflow.com/a/9951851/35070
        encoding = preferredencoding()
    else:
        encoding = sys.getfilesystemencoding()
    if encoding is None:
        encoding = 'utf-8'
    return encoding

def encodeFilename(s, for_subprocess=False):
    """"""
    @param s The name of the file
    """"""
    assert type(s) == compat_str
    # Python 3 has a Unicode API
    if sys.version_info >= (3, 0):
        return s
    # Pass '' directly to use Unicode APIs on Windows 2000 and up
    # (Detecting Windows NT 4 is tricky because 'major >= 4' would
    # match Windows 9x series as well. Besides, NT 4 is obsolete.)
    if not for_subprocess and sys.platform == 'win32' and sys.getwindowsversion()[0] >= 5:
        return s
    # Jython assumes filenames are Unicode strings though reported as Python 2.x compatible
    if sys.platform.startswith('java'):
        return s
    return s.encode(get_subprocess_encoding(), 'ignore')

def decodeFilename(b, for_subprocess=False):
    if sys.version_info >= (3, 0):
        return b
    if not isinstance(b, bytes):
        return b
    return b.decode(get_subprocess_encoding(), 'ignore')

def encodeArgument(s):
    if not isinstance(s, compat_str):
        # Legacy code that uses byte strings
        # Uncomment the following line after fixing all post processors
        # assert False, 'Internal error: %r should be of type %r, is %r' % (s, compat_str, type(s))
        s = s.decode('ascii')
    return encodeFilename(s, True)

def decodeArgument(b):
    return decodeFilename(b, True)

def decodeOption(optval):
    if optval is None:
        return optval
    if isinstance(optval, bytes):
        optval = optval.decode(preferredencoding())
    assert isinstance(optval, compat_str)
    return optval

def formatSeconds(secs):
    if secs > 3600:
        return '%d:%02d:%02d' % (secs // 3600, (secs % 3600) // 60, secs % 60)
    elif secs > 60:
        return '%d:%02d' % (secs // 60, secs % 60)
    else:
        return '%d' % secs

def make_HTTPS_handler(params, **kwargs):
    opts_no_check_certificate = params.get('nocheckcertificate', False)
    if hasattr(ssl, 'create_default_context'):  # Python >= 3.4 or 2.7.9
        context = ssl.create_default_context(ssl.Purpose.SERVER_AUTH)
        if opts_no_check_certificate:
            context.check_hostname = False
            context.verify_mode = ssl.CERT_NONE
        try:
            return YoutubeDLHTTPSHandler(params, context=context, **kwargs)
        except TypeError:
            # Python 2.7.8
            # (create_default_context present but HTTPSHandler has no context=)
            pass
    if sys.version_info < (3, 2):
        return YoutubeDLHTTPSHandler(params, **kwargs)
    else:  # Python < 3.4
        context = ssl.SSLContext(ssl.PROTOCOL_TLSv1)
        context.verify_mode = (ssl.CERT_NONE
                               if opts_no_check_certificate
                               else ssl.CERT_REQUIRED)
        context.set_default_verify_paths()
        return YoutubeDLHTTPSHandler(params, context=context, **kwargs)

def bug_reports_message():
    if ytdl_is_updateable():
        update_cmd = 'type  youtube-dl -U  to update'
    else:
        update_cmd = 'see  https://yt-dl.org/update  on how to update'",[15]
"    ext = determine_ext(url)
    if ext == 'm3u8':
        return 'm3u8'
    elif ext == 'f4m':
        return 'f4m'
    return compat_urllib_parse_urlparse(url).scheme

def render_table(header_row, data):
    """""" Render a list of rows, each as a list of values """"""
    table = [header_row] + data
    max_lens = [max(len(compat_str(v)) for v in col) for col in zip(*table)]
    format_str = ' '.join('%-' + compat_str(ml + 1) + 's' for ml in max_lens[:-1]) + '%s'
    return '\n'.join(format_str % tuple(row) for row in table)

def _match_one(filter_part, dct):
    COMPARISON_OPERATORS = {
        '<': operator.lt,
        '<=': operator.le,
        '>': operator.gt,
        '>=': operator.ge,
        '=': operator.eq,
        '!=': operator.ne,
    }
    operator_rex = re.compile(r'''(?x)\s*
        (?P<key>[a-z_]+)
        \s*(?P<op>%s)(?P<none_inclusive>\s*\?)?\s*
        (?:
            (?P<intval>[0-9.]+(?:[kKmMgGtTpPeEzZyY]i?[Bb]?)?)|
            (?P<strval>(?![0-9.])[a-z0-9A-Z]*)
        )
        \s*$
        ''' % '|'.join(map(re.escape, COMPARISON_OPERATORS.keys())))
    m = operator_rex.search(filter_part)
    if m:
        op = COMPARISON_OPERATORS[m.group('op')]
        if m.group('strval') is not None:
            if m.group('op') not in ('=', '!='):
                raise ValueError(
                    'Operator %s does not support string values!' % m.group('op'))
            comparison_value = m.group('strval')
        else:
            try:
                comparison_value = int(m.group('intval'))
            except ValueError:
                comparison_value = parse_filesize(m.group('intval'))
                if comparison_value is None:
                    comparison_value = parse_filesize(m.group('intval') + 'B')
                if comparison_value is None:
                    raise ValueError(
                        'Invalid integer value %r in filter part %r' % (
                            m.group('intval'), filter_part))
        actual_value = dct.get(m.group('key'))
        if actual_value is None:
            return m.group('none_inclusive')
        return op(actual_value, comparison_value)
    UNARY_OPERATORS = {
        '': lambda v: v is not None,
        '!': lambda v: v is None,
    }
    operator_rex = re.compile(r'''(?x)\s*
        (?P<op>%s)\s*(?P<key>[a-z_]+)
        \s*$
        ''' % '|'.join(map(re.escape, UNARY_OPERATORS.keys())))
    m = operator_rex.search(filter_part)
    if m:
        op = UNARY_OPERATORS[m.group('op')]
        actual_value = dct.get(m.group('key'))
        return op(actual_value)
    raise ValueError('Invalid filter part %r' % filter_part)

def match_str(filter_str, dct):
    """""" Filter a dictionary with a simple string syntax. Returns True (=passes filter) or false """"""
    return all(
        _match_one(filter_part, dct) for filter_part in filter_str.split('&'))

def match_filter_func(filter_str):
    def _match_func(info_dict):
        if match_str(filter_str, info_dict):
            return None
        else:
            video_title = info_dict.get('title', info_dict.get('id', 'video'))
            return '%s does not pass filter %s, skipping ..' % (video_title, filter_str)
    return _match_func

def parse_dfxp_time_expr(time_expr):
    if not time_expr:
        return 0.0
    mobj = re.match(r'^(?P<time_offset>\d+(?:\.\d+)?)s?$', time_expr)
    if mobj:
        return float(mobj.group('time_offset'))
    mobj = re.match(r'^(\d+):(\d\d):(\d\d(?:\.\d+)?)$', time_expr)
    if mobj:
        return 3600 * int(mobj.group(1)) + 60 * int(mobj.group(2)) + float(mobj.group(3))

def srt_subtitles_timecode(seconds):
    return '%02d:%02d:%02d,%03d' % (seconds / 3600, (seconds % 3600) / 60, seconds % 60, (seconds % 1) * 1000)

def dfxp2srt(dfxp_data):
    _x = functools.partial(xpath_with_ns, ns_map={
        'ttml': 'http://www.w3.org/ns/ttml',
        'ttaf1': 'http://www.w3.org/2006/10/ttaf1',
    })
    def parse_node(node):
        str_or_empty = functools.partial(str_or_none, default='')
        out = str_or_empty(node.text)
        for child in node:
            if child.tag in (_x('ttml:br'), _x('ttaf1:br'), 'br'):
                out += '\n' + str_or_empty(child.tail)
            elif child.tag in (_x('ttml:span'), _x('ttaf1:span'), 'span'):
                out += str_or_empty(parse_node(child))",[96]
"        ```
        """"""
        if not self.built:
            raise RuntimeError('The model needs to be compiled '
                               'before being used.')
        return self.model.fit_generator(generator,
                                        steps_per_epoch,
                                        epochs,
                                        verbose=verbose,
                                        callbacks=callbacks,
                                        validation_data=validation_data,
                                        validation_steps=validation_steps,
                                        class_weight=class_weight,
                                        max_queue_size=max_queue_size,
                                        workers=workers,
                                        use_multiprocessing=use_multiprocessing,
                                        shuffle=shuffle,
                                        initial_epoch=initial_epoch)
    @interfaces.legacy_generator_methods_support
    def evaluate_generator(self, generator, steps,
                           max_queue_size=10, workers=1,
                           use_multiprocessing=False):
        """"""Evaluates the model on a data generator.
        The generator should return the same kind of data
        as accepted by `test_on_batch`.
        # Arguments
            generator: Generator yielding tuples (inputs, targets)
                or (inputs, targets, sample_weights)
            steps: Total number of steps (batches of samples)
                to yield from `generator` before stopping.
            max_queue_size: maximum size for the generator queue
            workers: maximum number of processes to spin up
            use_multiprocessing: if True, use process based threading.
                Note that because this implementation
                relies on multiprocessing, you should not pass
                non picklable arguments to the generator
                as they can't be passed easily to children processes.
        # Returns
            Scalar test loss (if the model has no metrics)
            or list of scalars (if the model computes other metrics).
            The attribute `model.metrics_names` will give you
            the display labels for the scalar outputs.
        # Raises
            RuntimeError: if the model was never compiled.
        """"""
        if not self.built:
            raise RuntimeError('The model needs to be compiled '
                               'before being used.')
        return self.model.evaluate_generator(generator,
                                             steps,
                                             max_queue_size=max_queue_size,
                                             workers=workers,
                                             use_multiprocessing=use_multiprocessing)
    @interfaces.legacy_generator_methods_support
    def predict_generator(self, generator, steps,
                          max_queue_size=10, workers=1,
                          use_multiprocessing=False, verbose=0):
        """"""Generates predictions for the input samples from a data generator.
        The generator should return the same kind of data as accepted by
        `predict_on_batch`.
        # Arguments
            generator: generator yielding batches of input samples.
            steps: Total number of steps (batches of samples)
                to yield from `generator` before stopping.
            max_queue_size: maximum size for the generator queue
            workers: maximum number of processes to spin up
            use_multiprocessing: if True, use process based threading.
                Note that because this implementation
                relies on multiprocessing, you should not pass
                non picklable arguments to the generator
                as they can't be passed easily to children processes.
            verbose: verbosity mode, 0 or 1.
        # Returns
            A Numpy array of predictions.
        """"""
        if not self.built:
            self.build()
        return self.model.predict_generator(generator, steps,
                                            max_queue_size=max_queue_size,
                                            workers=workers,
                                            use_multiprocessing=use_multiprocessing,
                                            verbose=verbose)
    def get_config(self):
        if isinstance(self.layers[0], legacy_layers.Merge):
            return self.legacy_get_config()
        config = []
        for layer in self.layers:
            config.append({'class_name': layer.__class__.__name__,
                           'config': layer.get_config()})
        return copy.deepcopy(config)
    @classmethod
    def from_config(cls, config, custom_objects=None):
        if 'class_name' not in config[0] or config[0]['class_name'] == 'Merge':
            return cls.legacy_from_config(config)
        model = cls()
        for conf in config:
            layer = layer_module.deserialize(conf, custom_objects=custom_objects)
            model.add(layer)
        return model
    def legacy_get_config(self):
        """"""Retrieves the model configuration as a Python list.
        # Returns
            A list of dicts (each dict is a layer config).
        """"""
        config = []
        if isinstance(self.layers[0], legacy_layers.Merge):
            assert hasattr(self.layers[0], 'layers')
            layers = []
            for layer in self.layers[0].layers:
                layer_config = {'class_name': layer.__class__.__name__,
                                'config': layer.get_config()}
                layers.append(layer_config)","[20, 60]"
"        def _done(result):
            self.crawlers.discard(crawler)
            self._active.discard(d)
            return result
        return d.addBoth(_done)
    def _create_crawler(self, spidercls):
        if isinstance(spidercls, six.string_types):
            spidercls = self.spider_loader.load(spidercls)
        return Crawler(spidercls, self.settings)
    def stop(self):
        """"""
        Stops simultaneously all the crawling jobs taking place.
        Returns a deferred that is fired when they all have ended.
        """"""
        return defer.DeferredList([c.stop() for c in list(self.crawlers)])
    @defer.inlineCallbacks
    def join(self):
        """"""
        join()
        Returns a deferred that is fired when all managed :attr:`crawlers` have
        completed their executions.
        """"""
        while self._active:
            yield defer.DeferredList(self._active)

class CrawlerProcess(CrawlerRunner):
    """"""
    A class to run multiple scrapy crawlers in a process simultaneously.
    This class extends :class:`~scrapy.crawler.CrawlerRunner` by adding support
    for starting a Twisted `reactor`_ and handling shutdown signals, like the
    keyboard interrupt command Ctrl-C. It also configures top-level logging.
    This utility should be a better fit than
    :class:`~scrapy.crawler.CrawlerRunner` if you aren't running another
    Twisted `reactor`_ within your application.
    The CrawlerProcess object must be instantiated with a
    :class:`~scrapy.settings.Settings` object.
    This class shouldn't be needed (since Scrapy is responsible of using it
    accordingly) unless writing scripts that manually handle the crawling
    process. See :ref:`run-from-script` for an example.
    """"""
    def __init__(self, settings):
        super(CrawlerProcess, self).__init__(settings)
        install_shutdown_handlers(self._signal_shutdown)
        configure_logging(settings)
        log_scrapy_info(settings)
    def _signal_shutdown(self, signum, _):
        install_shutdown_handlers(self._signal_kill)
        signame = signal_names[signum]
        logger.info(""Received %(signame)s, shutting down gracefully. Send again to force "",
                    {'signame': signame})
        reactor.callFromThread(self.stop)
    def _signal_kill(self, signum, _):
        install_shutdown_handlers(signal.SIG_IGN)
        signame = signal_names[signum]
        logger.info('Received %(signame)s twice, forcing unclean shutdown',
                    {'signame': signame})
        reactor.callFromThread(self._stop_reactor)
    def start(self, stop_after_crawl=True):
        """"""
        This method starts a Twisted `reactor`_, adjusts its pool size to
        :setting:`REACTOR_THREADPOOL_MAXSIZE`, and installs a DNS cache based
        on :setting:`DNSCACHE_ENABLED` and :setting:`DNSCACHE_SIZE`.
        If `stop_after_crawl` is True, the reactor will be stopped after all
        crawlers have finished, using :meth:`join`.
        :param boolean stop_after_crawl: stop or not the reactor when all
            crawlers have finished
        """"""
        if stop_after_crawl:
            d = self.join()
            # Don't start the reactor if the deferreds are already fired
            if d.called:
                return
            d.addBoth(lambda _: self._stop_reactor())
        cache_size = self.settings.getint('DNSCACHE_SIZE') if self.settings.getbool('DNSCACHE_ENABLED') else 0
        reactor.installResolver(CachingThreadedResolver(reactor, cache_size,
                                                            self.settings.getfloat('DNS_TIMEOUT')))
        tp = reactor.getThreadPool()
        tp.adjustPoolsize(maxthreads=self.settings.getint('REACTOR_THREADPOOL_MAXSIZE'))
        reactor.addSystemEventTrigger('before', 'shutdown', self.stop)
        reactor.run(installSignalHandlers=False)  # blocking call
    def _stop_reactor(self, _=None):
        try:
            reactor.stop()
        except RuntimeError:  # raised if already stopped or in shutdown stage
            pass

def _get_spider_loader(settings):
    """""" Get SpiderLoader instance from settings """"""
    if settings.get('SPIDER_MANAGER_CLASS'):
        warnings.warn(
            'SPIDER_MANAGER_CLASS option is deprecated. '
            'Please use SPIDER_LOADER_CLASS.',
            category=ScrapyDeprecationWarning, stacklevel=2
        )
    cls_path = settings.get('SPIDER_MANAGER_CLASS',
                            settings.get('SPIDER_LOADER_CLASS'))
    loader_cls = load_object(cls_path)
    try:
        verifyClass(ISpiderLoader, loader_cls)
    except DoesNotImplement:
        warnings.warn(
            'SPIDER_LOADER_CLASS (previously named SPIDER_MANAGER_CLASS) does '
            'not fully implement scrapy.interfaces.ISpiderLoader interface. '
            'Please add all missing methods to avoid unexpected runtime errors.',
            category=ScrapyDeprecationWarning, stacklevel=2
        )","[55, 56]"
"# -*- coding: utf-8 -*-
#
# Copyright 2015-2015 Spotify AB
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
""""""
This module provide the function :py:func:`summary` that is used for printing
an `execution summary
<https://github.com/spotify/luigi/blob/master/examples/execution_summary_example.py>`_
at the end of luigi invocations.
""""""
import textwrap
import collections
import functools
import luigi

class execution_summary(luigi.Config):
    summary_length = luigi.IntParameter(default=5)

def _partition_tasks(worker):
    """"""
    Takes a worker and sorts out tasks based on their status.
    Still_pending_not_ext is only used to get upstream_failure, upstream_missing_dependency and run_by_other_worker
    """"""
    task_history = worker._add_task_history
    pending_tasks = {task for(task, status, ext) in task_history if status == 'PENDING'}
    set_tasks = {}
    set_tasks[""completed""] = {task for (task, status, ext) in task_history if status == 'DONE' and task in pending_tasks}
    set_tasks[""already_done""] = {task for (task, status, ext) in task_history
                                 if status == 'DONE' and task not in pending_tasks and task not in set_tasks[""completed""]}
    set_tasks[""failed""] = {task for (task, status, ext) in task_history if status == 'FAILED'}
    set_tasks[""scheduling_error""] = {task for(task, status, ext) in task_history if status == 'UNKNOWN'}
    set_tasks[""still_pending_ext""] = {task for (task, status, ext) in task_history
                                      if status == 'PENDING' and task not in set_tasks[""failed""] and task not in set_tasks[""completed""] and not ext}
    set_tasks[""still_pending_not_ext""] = {task for (task, status, ext) in task_history
                                          if status == 'PENDING' and task not in set_tasks[""failed""] and task not in set_tasks[""completed""] and ext}
    set_tasks[""run_by_other_worker""] = set()
    set_tasks[""upstream_failure""] = set()
    set_tasks[""upstream_missing_dependency""] = set()
    set_tasks[""upstream_run_by_other_worker""] = set()
    set_tasks[""upstream_scheduling_error""] = set()
    set_tasks[""not_run""] = set()
    return set_tasks

def _root_task(worker):
    """"""
    Return the first task scheduled by the worker, corresponding to the root task
    """"""
    return worker._add_task_history[0][0]

def _populate_unknown_statuses(set_tasks):
    """"""
    Add the ""upstream_*"" and ""not_run"" statuses my mutating set_tasks.
    """"""
    visited = set()
    for task in set_tasks[""still_pending_not_ext""]:
        _depth_first_search(set_tasks, task, visited)

def _depth_first_search(set_tasks, current_task, visited):
    """"""
    This dfs checks why tasks are still pending.
    """"""
    visited.add(current_task)
    if current_task in set_tasks[""still_pending_not_ext""]:
        upstream_failure = False
        upstream_missing_dependency = False
        upstream_run_by_other_worker = False
        upstream_scheduling_error = False
        for task in current_task._requires():
            if task not in visited:
                _depth_first_search(set_tasks, task, visited)
            if task in set_tasks[""failed""] or task in set_tasks[""upstream_failure""]:
                set_tasks[""upstream_failure""].add(current_task)
                upstream_failure = True
            if task in set_tasks[""still_pending_ext""] or task in set_tasks[""upstream_missing_dependency""]:
                set_tasks[""upstream_missing_dependency""].add(current_task)
                upstream_missing_dependency = True
            if task in set_tasks[""run_by_other_worker""] or task in set_tasks[""upstream_run_by_other_worker""]:
                set_tasks[""upstream_run_by_other_worker""].add(current_task)
                upstream_run_by_other_worker = True
            if task in set_tasks[""scheduling_error""]:
                set_tasks[""upstream_scheduling_error""].add(current_task)
                upstream_scheduling_error = True
        if not upstream_failure and not upstream_missing_dependency and \
                not upstream_run_by_other_worker and not upstream_scheduling_error and \
                current_task not in set_tasks[""run_by_other_worker""]:
            set_tasks[""not_run""].add(current_task)

def _get_str(task_dict, extra_indent):
    """"""
    This returns a string for each status
    """"""
    summary_length = execution_summary().summary_length
    lines = []
    task_names = sorted(task_dict.keys())
    for task_family in task_names:
        tasks = task_dict[task_family]
        tasks = sorted(tasks, key=lambda x: str(x))
        prefix_size = 8 if extra_indent else 4
        prefix = ' ' * prefix_size
        line = None
        if summary_length > 0 and len(lines) >= summary_length:
            line = prefix + ""...""
            lines.append(line)
            break","[45, 48, 50, 89]"
"        from pandas import PeriodIndex, DatetimeIndex, TimedeltaIndex
        from .numeric import Float64Index, Int64Index, UInt64Index
        from .interval import IntervalIndex
        from .category import CategoricalIndex
        name = maybe_extract_name(name, data, cls)
        if isinstance(data, ABCPandasArray):
            # ensure users don't accidentally put a PandasArray in an index.
            data = data.to_numpy()
        # range
        if isinstance(data, RangeIndex):
            return RangeIndex(start=data, copy=copy, dtype=dtype, name=name)
        elif isinstance(data, range):
            return RangeIndex.from_range(data, dtype=dtype, name=name)
        # categorical
        elif is_categorical_dtype(data) or is_categorical_dtype(dtype):
            return CategoricalIndex(data, dtype=dtype, copy=copy, name=name, **kwargs)
        # interval
        elif (
            is_interval_dtype(data) or is_interval_dtype(dtype)
        ) and not is_object_dtype(dtype):
            closed = kwargs.get(""closed"", None)
            return IntervalIndex(data, dtype=dtype, name=name, copy=copy, closed=closed)
        elif (
            is_datetime64_any_dtype(data)
            or is_datetime64_any_dtype(dtype)
            or ""tz"" in kwargs
        ):
            if is_dtype_equal(_o_dtype, dtype):
                # GH#23524 passing `dtype=object` to DatetimeIndex is invalid,
                #  will raise in the where `data` is already tz-aware.  So
                #  we leave it out of this step and cast to object-dtype after
                #  the DatetimeIndex construction.
                # Note we can pass copy=False because the .astype below
                #  will always make a copy
                return DatetimeIndex(data, copy=False, name=name, **kwargs).astype(
                    object
                )
            else:
                return DatetimeIndex(data, copy=copy, name=name, dtype=dtype, **kwargs)
        elif is_timedelta64_dtype(data) or is_timedelta64_dtype(dtype):
            if is_dtype_equal(_o_dtype, dtype):
                # Note we can pass copy=False because the .astype below
                #  will always make a copy
                return TimedeltaIndex(data, copy=False, name=name, **kwargs).astype(
                    object
                )
            else:
                return TimedeltaIndex(data, copy=copy, name=name, dtype=dtype, **kwargs)
        elif is_period_dtype(data) and not is_object_dtype(dtype):
            return PeriodIndex(data, copy=copy, name=name, **kwargs)
        # extension dtype
        elif is_extension_array_dtype(data) or is_extension_array_dtype(dtype):
            if not (dtype is None or is_object_dtype(dtype)):
                # coerce to the provided dtype
                ea_cls = dtype.construct_array_type()
                data = ea_cls._from_sequence(data, dtype=dtype, copy=False)
            else:
                data = np.asarray(data, dtype=object)
            # coerce to the object dtype
            data = data.astype(object)
            return Index(data, dtype=object, copy=copy, name=name, **kwargs)
        # index-like
        elif isinstance(data, (np.ndarray, Index, ABCSeries)):
            if dtype is not None:
                # we need to avoid having numpy coerce
                # things that look like ints/floats to ints unless
                # they are actually ints, e.g. '0' and 0.0
                # should not be coerced
                # GH 11836
                data = _maybe_cast_with_dtype(data, dtype, copy)
                dtype = data.dtype  # TODO: maybe not for object?
            # maybe coerce to a sub-class
            if is_signed_integer_dtype(data.dtype):
                return Int64Index(data, copy=copy, dtype=dtype, name=name)
            elif is_unsigned_integer_dtype(data.dtype):
                return UInt64Index(data, copy=copy, dtype=dtype, name=name)
            elif is_float_dtype(data.dtype):
                return Float64Index(data, copy=copy, dtype=dtype, name=name)
            elif issubclass(data.dtype.type, np.bool) or is_bool_dtype(data):
                subarr = data.astype(""object"")
            else:
                subarr = com.asarray_tuplesafe(data, dtype=object)
            # asarray_tuplesafe does not always copy underlying data,
            # so need to make sure that this happens
            if copy:
                subarr = subarr.copy()
            if dtype is None:
                new_data, new_dtype = _maybe_cast_data_without_dtype(subarr)
                if new_dtype is not None:
                    return cls(
                        new_data, dtype=new_dtype, copy=False, name=name, **kwargs
                    )
            if kwargs:
                raise TypeError(f""Unexpected keyword arguments {repr(set(kwargs))}"")
            return cls._simple_new(subarr, name, **kwargs)
        elif hasattr(data, ""__array__""):
            return Index(np.asarray(data), dtype=dtype, copy=copy, name=name, **kwargs)
        elif data is None or is_scalar(data):
            raise cls._scalar_data_error(data)
        else:
            if tupleize_cols and is_list_like(data):
                # GH21470: convert iterable to list before determining if empty
                if is_iterator(data):
                    data = list(data)
                if data and all(isinstance(e, tuple) for e in data):
                    # we must be all tuples, otherwise don't construct
                    # 10697
                    from .multi import MultiIndex
                    return MultiIndex.from_tuples(","[22, 23, 24, 25, 26, 56, 57]"
"    @property
    def _step(self):
        """"""
        The value of the `step` parameter (``1`` if this was not supplied).
         .. deprecated:: 0.25.0
            Use ``step`` instead.
        """"""
        # GH 25710
        warnings.warn(
            self._deprecation_message.format(""_step"", ""step""),
            DeprecationWarning,
            stacklevel=2,
        )
        return self.step
    @cache_readonly
    def nbytes(self):
        """"""
        Return the number of bytes in the underlying data.
        """"""
        rng = self._range
        return getsizeof(rng) + sum(
            getsizeof(getattr(rng, attr_name))
            for attr_name in [""start"", ""stop"", ""step""]
        )
    def memory_usage(self, deep=False):
        """"""
        Memory usage of my values
        Parameters
        ----------
        deep : bool
            Introspect the data deeply, interrogate
            `object` dtypes for system-level memory consumption
        Returns
        -------
        bytes used
        Notes
        -----
        Memory usage does not include memory consumed by elements that
        are not components of the array if deep=False
        See Also
        --------
        numpy.ndarray.nbytes
        """"""
        return self.nbytes
    @property
    def dtype(self):
        return np.dtype(np.int64)
    @property
    def is_unique(self):
        """""" return if the index has unique values """"""
        return True
    @cache_readonly
    def is_monotonic_increasing(self):
        return self._range.step > 0 or len(self) <= 1
    @cache_readonly
    def is_monotonic_decreasing(self):
        return self._range.step < 0 or len(self) <= 1
    @property
    def has_duplicates(self):
        return False
    def __contains__(self, key: Union[int, np.integer]) -> bool:
        hash(key)
        try:
            key = ensure_python_int(key)
        except TypeError:
            return False
        return key in self._range
    @Appender(_index_shared_docs[""get_loc""])
    def get_loc(self, key, method=None, tolerance=None):
        if is_integer(key) and method is None and tolerance is None:
            new_key = int(key)
            try:
                return self._range.index(new_key)
            except ValueError:
                raise KeyError(key)
        return super().get_loc(key, method=method, tolerance=tolerance)
    @Appender(_index_shared_docs[""get_indexer""])
    def get_indexer(self, target, method=None, limit=None, tolerance=None):
        if not (method is None and tolerance is None and is_list_like(target)):
            return super().get_indexer(target, method=method, tolerance=tolerance)
        if self.step > 0:
            start, stop, step = self.start, self.stop, self.step
        else:
            # Work on reversed range for simplicity:
            start, stop, step = (self.stop - self.step, self.start + 1, -self.step)
        target_array = np.asarray(target)
        if not (is_integer_dtype(target_array) and target_array.ndim == 1):
            # checks/conversions/roundings are delegated to general method
            return super().get_indexer(target, method=method, tolerance=tolerance)
        locs = target_array - start
        valid = (locs % step == 0) & (locs >= 0) & (target_array < stop)
        locs[~valid] = -1
        locs[valid] = locs[valid] / step
        if step != self.step:
            # We reversed this range: transform to original locs
            locs[valid] = len(self) - 1 - locs[valid]
        return ensure_platform_int(locs)
    def tolist(self):
        return list(self._range)
    @Appender(_index_shared_docs[""_shallow_copy""])
    def _shallow_copy(self, values=None, **kwargs):
        if values is None:
            name = kwargs.get(""name"", self.name)
            return self._simple_new(self._range, name=name)
        else:
            kwargs.setdefault(""name"", self.name)","[93, 94]"
"            xmax = [xmax]
        # Create and combine masked_arrays from input
        y, xmin, xmax = cbook._combine_masks(y, xmin, xmax)
        y = np.ravel(y)
        xmin = np.ravel(xmin)
        xmax = np.ravel(xmax)
        masked_verts = np.ma.empty((len(y), 2, 2))
        masked_verts[:, 0, 0] = xmin
        masked_verts[:, 0, 1] = y
        masked_verts[:, 1, 0] = xmax
        masked_verts[:, 1, 1] = y
        lines = mcoll.LineCollection(masked_verts, colors=colors,
                                     linestyles=linestyles, label=label)
        self.add_collection(lines, autolim=False)
        lines.update(kwargs)
        if len(y) > 0:
            minx = min(xmin.min(), xmax.min())
            maxx = max(xmin.max(), xmax.max())
            miny = y.min()
            maxy = y.max()
            corners = (minx, miny), (maxx, maxy)
            self.update_datalim(corners)
            self._request_autoscale_view()
        return lines
    @_preprocess_data(replace_names=[""x"", ""ymin"", ""ymax"", ""colors""],
                      label_namer=""x"")
    def vlines(self, x, ymin, ymax, colors='k', linestyles='solid',
               label='', **kwargs):
        """"""
        Plot vertical lines.
        Plot vertical lines at each *x* from *ymin* to *ymax*.
        Parameters
        ----------
        x : float or array-like
            x-indexes where to plot the lines.
        ymin, ymax : float or array-like
            Respective beginning and end of each line. If scalars are
            provided, all lines will have same length.
        colors : list of colors, default: 'k'
        linestyles : {'solid', 'dashed', 'dashdot', 'dotted'}, optional
        label : str, default: ''
        Returns
        -------
        `~matplotlib.collections.LineCollection`
        Other Parameters
        ----------------
        **kwargs : `~matplotlib.collections.LineCollection` properties.
        See Also
        --------
        hlines : horizontal lines
        axvline: vertical line across the axes
        """"""
        self._process_unit_info(xdata=x, ydata=[ymin, ymax], kwargs=kwargs)
        # We do the conversion first since not all unitized data is uniform
        x = self.convert_xunits(x)
        ymin = self.convert_yunits(ymin)
        ymax = self.convert_yunits(ymax)
        if not np.iterable(x):
            x = [x]
        if not np.iterable(ymin):
            ymin = [ymin]
        if not np.iterable(ymax):
            ymax = [ymax]
        # Create and combine masked_arrays from input
        x, ymin, ymax = cbook._combine_masks(x, ymin, ymax)
        x = np.ravel(x)
        ymin = np.ravel(ymin)
        ymax = np.ravel(ymax)
        masked_verts = np.ma.empty((len(x), 2, 2))
        masked_verts[:, 0, 0] = x
        masked_verts[:, 0, 1] = ymin
        masked_verts[:, 1, 0] = x
        masked_verts[:, 1, 1] = ymax
        lines = mcoll.LineCollection(masked_verts, colors=colors,
                                     linestyles=linestyles, label=label)
        self.add_collection(lines, autolim=False)
        lines.update(kwargs)
        if len(x) > 0:
            minx = x.min()
            maxx = x.max()
            miny = min(ymin.min(), ymax.min())
            maxy = max(ymin.max(), ymax.max())
            corners = (minx, miny), (maxx, maxy)
            self.update_datalim(corners)
            self._request_autoscale_view()
        return lines
    @_preprocess_data(replace_names=[""positions"", ""lineoffsets"",
                                     ""linelengths"", ""linewidths"",
                                     ""colors"", ""linestyles""])
    @docstring.dedent_interpd
    def eventplot(self, positions, orientation='horizontal', lineoffsets=1,
                  linelengths=1, linewidths=None, colors=None,
                  linestyles='solid', **kwargs):
        """"""
        Plot identical parallel lines at the given positions.
        This type of plot is commonly used in neuroscience for representing
        neural events, where it is usually called a spike raster, dot raster,
        or raster plot.
","[34, 50]"
"        if is_scalar:
            result = np.atleast_1d(result)[0]
        return result

@cbook.deprecation.deprecated('3.2', alternative='TwoSlopeNorm')
class DivergingNorm(TwoSlopeNorm):
    ...

class LogNorm(Normalize):
    """"""Normalize a given value to the 0-1 range on a log scale.""""""
    def _check_vmin_vmax(self):
        if self.vmin > self.vmax:
            raise ValueError(""minvalue must be less than or equal to maxvalue"")
        elif self.vmin <= 0:
            raise ValueError(""minvalue must be positive"")
    def __call__(self, value, clip=None):
        if clip is None:
            clip = self.clip
        result, is_scalar = self.process_value(value)
        result = np.ma.masked_less_equal(result, 0, copy=False)
        self.autoscale_None(result)
        self._check_vmin_vmax()
        vmin, vmax = self.vmin, self.vmax
        if vmin == vmax:
            result.fill(0)
        else:
            if clip:
                mask = np.ma.getmask(result)
                result = np.ma.array(np.clip(result.filled(vmax), vmin, vmax),
                                     mask=mask)
            # in-place equivalent of above can be much faster
            resdat = result.data
            mask = result.mask
            if mask is np.ma.nomask:
                mask = (resdat <= 0)
            else:
                mask |= resdat <= 0
            np.copyto(resdat, 1, where=mask)
            np.log(resdat, resdat)
            resdat -= np.log(vmin)
            resdat /= (np.log(vmax) - np.log(vmin))
            result = np.ma.array(resdat, mask=mask, copy=False)
        if is_scalar:
            result = result[0]
        return result
    def inverse(self, value):
        if not self.scaled():
            raise ValueError(""Not invertible until scaled"")
        self._check_vmin_vmax()
        vmin, vmax = self.vmin, self.vmax
        if np.iterable(value):
            val = np.ma.asarray(value)
            return vmin * np.ma.power((vmax / vmin), val)
        else:
            return vmin * pow((vmax / vmin), value)
    def autoscale(self, A):
        # docstring inherited.
        super().autoscale(np.ma.masked_less_equal(A, 0, copy=False))
    def autoscale_None(self, A):
        # docstring inherited.
        super().autoscale_None(np.ma.masked_less_equal(A, 0, copy=False))

class SymLogNorm(Normalize):
    """"""
    The symmetrical logarithmic scale is logarithmic in both the
    positive and negative directions from the origin.
    Since the values close to zero tend toward infinity, there is a
    need to have a range around zero that is linear.  The parameter
    *linthresh* allows the user to specify the size of this range
    (-*linthresh*, *linthresh*).
    """"""
    def __init__(self, linthresh, linscale=1.0,
                 vmin=None, vmax=None, clip=False):
        """"""
        Parameters
        ----------
        linthresh : float
            The range within which the plot is linear (to avoid having the plot
            go to infinity around zero).
        linscale : float, default: 1
            This allows the linear range (-*linthresh* to *linthresh*) to be
            stretched relative to the logarithmic range. Its value is the
            number of decades to use for each half of the linear range. For
            example, when *linscale* == 1.0 (the default), the space used for
            the positive and negative halves of the linear range will be equal
            to one decade in the logarithmic range.
        """"""
        Normalize.__init__(self, vmin, vmax, clip)
        self.linthresh = float(linthresh)
        self._linscale_adj = (linscale / (1.0 - np.e ** -1))
        if vmin is not None and vmax is not None:
            self._transform_vmin_vmax()
    def __call__(self, value, clip=None):
        if clip is None:
            clip = self.clip
        result, is_scalar = self.process_value(value)
        self.autoscale_None(result)
        vmin, vmax = self.vmin, self.vmax
        if vmin > vmax:
            raise ValueError(""minvalue must be less than or equal to maxvalue"")
        elif vmin == vmax:
            result.fill(0)
        else:
            if clip:
                mask = np.ma.getmask(result)
                result = np.ma.array(np.clip(result.filled(vmax), vmin, vmax),
                                     mask=mask)
            # in-place equivalent of above can be much faster
            resdat = self._transform(result.data)
            resdat -= self._lower
            resdat /= (self._upper - self._lower)","[84, 85, 95, 96, 97, 98, 102]"
"        an ``order`` (int).
        >>> s = pd.Series([0, 2, np.nan, 8])
        >>> s.interpolate(method='polynomial', order=2)
        0    0.000000
        1    2.000000
        2    4.666667
        3    8.000000
        dtype: float64
        Fill the DataFrame forward (that is, going down) along each column
        using linear interpolation.
        Note how the last entry in column 'a' is interpolated differently,
        because there is no entry after it to use for interpolation.
        Note how the first entry in column 'b' remains ``NaN``, because there
        is no entry before it to use for interpolation.
        >>> df = pd.DataFrame([(0.0, np.nan, -1.0, 1.0),
        ...                    (np.nan, 2.0, np.nan, np.nan),
        ...                    (2.0, 3.0, np.nan, 9.0),
        ...                    (np.nan, 4.0, -4.0, 16.0)],
        ...                   columns=list('abcd'))
        >>> df
             a    b    c     d
        0  0.0  NaN -1.0   1.0
        1  NaN  2.0  NaN   NaN
        2  2.0  3.0  NaN   9.0
        3  NaN  4.0 -4.0  16.0
        >>> df.interpolate(method='linear', limit_direction='forward', axis=0)
             a    b    c     d
        0  0.0  NaN -1.0   1.0
        1  1.0  2.0 -2.0   5.0
        2  2.0  3.0 -3.0   9.0
        3  2.0  4.0 -4.0  16.0
        Using polynomial interpolation.
        >>> df['d'].interpolate(method='polynomial', order=2)
        0     1.0
        1     4.0
        2     9.0
        3    16.0
        Name: d, dtype: float64
        """"""
    @Appender(_shared_docs[""interpolate""] % _shared_doc_kwargs)
    def interpolate(
        self,
        method=""linear"",
        axis=0,
        limit=None,
        inplace=False,
        limit_direction=""forward"",
        limit_area=None,
        downcast=None,
        **kwargs
    ):
        """"""
        Interpolate values according to different methods.
        """"""
        inplace = validate_bool_kwarg(inplace, ""inplace"")
        if axis == 0:
            ax = self._info_axis_name
            _maybe_transposed_self = self
        elif axis == 1:
            _maybe_transposed_self = self.T
            ax = 1
        else:
            _maybe_transposed_self = self
        ax = _maybe_transposed_self._get_axis_number(ax)
        if _maybe_transposed_self.ndim == 2:
            alt_ax = 1 - ax
        else:
            alt_ax = ax
        if isinstance(_maybe_transposed_self.index, MultiIndex) and method != ""linear"":
            raise ValueError(
                ""Only `method=linear` interpolation is supported on MultiIndexes.""
            )
        if _maybe_transposed_self._data.get_dtype_counts().get(""object"") == len(
            _maybe_transposed_self.T
        ):
            raise TypeError(
                ""Cannot interpolate with all object-dtype columns ""
                ""in the DataFrame. Try setting at least one ""
                ""column to a numeric dtype.""
            )
        # create/use the index
        if method == ""linear"":
            # prior default
            index = np.arange(len(_maybe_transposed_self._get_axis(alt_ax)))
        else:
            index = _maybe_transposed_self._get_axis(alt_ax)
            methods = {""index"", ""values"", ""nearest"", ""time""}
            is_numeric_or_datetime = (
                is_numeric_dtype(index)
                or is_datetime64_any_dtype(index)
                or is_timedelta64_dtype(index)
            )
            if method not in methods and not is_numeric_or_datetime:
                raise ValueError(
                    ""Index column must be numeric or datetime type when ""
                    ""using {method} method other than linear. ""
                    ""Try setting a numeric or datetime index column before ""
                    ""interpolating."".format(method=method)
                )
        if isna(index).any():
            raise NotImplementedError(
                ""Interpolation with NaNs in the index ""
                ""has not been implemented. Try filling ""
                ""those NaNs before interpolating.""
            )
        data = _maybe_transposed_self._data
        new_data = data.interpolate(
            method=method,
            axis=ax,
            index=index,
            values=_maybe_transposed_self,
            limit=limit,
            limit_direction=limit_direction,
            limit_area=limit_area,","[69, 70]"
"    if sample_weight is not None and class_weight is not None:
        warnings.warn('Found both `sample_weight` and `class_weight`: '
                      '`class_weight` argument will be ignored.')
    if sample_weight is not None:
        if len(sample_weight.shape) > len(y.shape):
            raise ValueError('Found a sample_weight with shape' +
                             str(sample_weight.shape) + '.'
                             'Expected sample_weight with rank '
                             'less than or equal to ' + str(len(y.shape)))
        if y.shape[:sample_weight.ndim] != sample_weight.shape:
            raise ValueError('Found a sample_weight array with shape ' +
                             str(sample_weight.shape) +
                             ' for an input with shape ' +
                             str(y.shape) + '. '
                             'sample_weight cannot be broadcast.')
        return sample_weight
    elif isinstance(class_weight, dict):
        if len(y.shape) > 2:
            raise ValueError('`class_weight` not supported for '
                             '3+ dimensional targets.')
        if y.shape[1] > 1:
            y_classes = np.argmax(y, axis=1)
        elif y.shape[1] == 1:
            y_classes = np.reshape(y, y.shape[0])
        else:
            y_classes = y
        weights = np.asarray([class_weight[cls] for cls in y_classes
                              if cls in class_weight])
        if len(weights) != len(y_classes):
            # subtract the sets to pick all missing classes
            existing_classes = set(y_classes)
            existing_class_weight = set(class_weight.keys())
            raise ValueError('`class_weight` must contain '
                             'all classes in the data.'
                             ' The classes %s exist in the data but not in '
                             '`class_weight`.'
                             % (existing_classes - existing_class_weight))
        return weights
    else:
        if sample_weight_mode is None:
            return np.ones((y.shape[0],), dtype=K.floatx())
        else:
            return np.ones((y.shape[0], y.shape[1]), dtype=K.floatx())

def check_num_samples(ins,
                      batch_size=None,
                      steps=None,
                      steps_name='steps'):
    """"""Checks the number of samples provided for training and evaluation.
    The number of samples is not defined when running with `steps`,
    in which case the number of samples is set to `None`.
    # Arguments
        ins: List of tensors to be fed to the Keras function.
        batch_size: Integer batch size or `None` if not defined.
        steps: Total number of steps (batches of samples)
            before declaring `predict_loop` finished.
            Ignored with the default value of `None`.
        steps_name: The public API's parameter name for `steps`.
    # Raises
        ValueError: when `steps` is `None` and the attribute `ins.shape`
        does not exist. Also raises ValueError when `steps` is not `None`
        and `batch_size` is not `None` because they are mutually
        exclusive.
    # Returns
        When `steps` is `None`, returns the number of samples to be
        processed based on the size of the first dimension of the
        first input Numpy array. When `steps` is not `None` and
        `batch_size` is `None`, returns `None`.
    # Raises
        ValueError: In case of invalid arguments.
    """"""
    if steps is not None and batch_size is not None:
        raise ValueError(
            'If ' + steps_name + ' is set, the `batch_size` must be None.')
    if not ins or any(K.is_tensor(x) for x in ins):
        if steps is None:
            raise ValueError(
                'If your data is in the form of symbolic tensors, '
                'you should specify the `' + steps_name + '` argument '
                '(instead of the `batch_size` argument, '
                'because symbolic tensors are expected to produce '
                'batches of input data).')
        return None
    if hasattr(ins[0], 'shape'):
        return int(ins[0].shape[0])
    return None  # Edge case where ins == [static_learning_phase]

def iter_sequence_infinite(seq):
    """"""Iterate indefinitely over a Sequence.
    # Arguments
        seq: Sequence object
    # Returns
        Generator yielding batches.
    """"""
    while True:
        for item in seq:
            yield item

def is_sequence(seq):
    """"""Determine if an object follows the Sequence API.
    # Arguments
        seq: a possible Sequence object
    # Returns
        boolean, whether the object follows the Sequence API.
    """"""
    # TODO Dref360: Decide which pattern to follow. First needs a new TF Version.
    return (getattr(seq, 'use_sequence_api', False)","[1, 2, 3, 4, 18, 19, 23, 24, 25, 26, 30, 31, 33, 42, 44, 45, 46, 47]"
"    `line_length` and `fast` options are passed to :func:`format_file_contents`.
    """"""
    if src.suffix == "".pyi"":
        mode |= FileMode.PYI
    with tokenize.open(src) as src_buffer:
        src_contents = src_buffer.read()
    try:
        dst_contents = format_file_contents(
            src_contents, line_length=line_length, fast=fast, mode=mode
        )
    except NothingChanged:
        return False
    if write_back == write_back.YES:
        with open(src, ""w"", encoding=src_buffer.encoding) as f:
            f.write(dst_contents)
    elif write_back == write_back.DIFF:
        src_name = f""{src}  (original)""
        dst_name = f""{src}  (formatted)""
        diff_contents = diff(src_contents, dst_contents, src_name, dst_name)
        if lock:
            lock.acquire()
        try:
            sys.stdout.write(diff_contents)
        finally:
            if lock:
                lock.release()
    return True

def format_stdin_to_stdout(
    line_length: int,
    fast: bool,
    write_back: WriteBack = WriteBack.NO,
    mode: FileMode = FileMode.AUTO_DETECT,
) -> bool:
    """"""Format file on stdin. Return True if changed.
    If `write_back` is True, write reformatted code back to stdout.
    `line_length`, `fast`, `is_pyi`, and `force_py36` arguments are passed to
    :func:`format_file_contents`.
    """"""
    src = sys.stdin.read()
    dst = src
    try:
        dst = format_file_contents(src, line_length=line_length, fast=fast, mode=mode)
        return True
    except NothingChanged:
        return False
    finally:
        if write_back == WriteBack.YES:
            sys.stdout.write(dst)
        elif write_back == WriteBack.DIFF:
            src_name = ""<stdin>  (original)""
            dst_name = ""<stdin>  (formatted)""
            sys.stdout.write(diff(src, dst, src_name, dst_name))

def format_file_contents(
    src_contents: str,
    *,
    line_length: int,
    fast: bool,
    mode: FileMode = FileMode.AUTO_DETECT,
) -> FileContent:
    """"""Reformat contents a file and return new contents.
    If `fast` is False, additionally confirm that the reformatted code is
    valid by calling :func:`assert_equivalent` and :func:`assert_stable` on it.
    `line_length` is passed to :func:`format_str`.
    """"""
    if src_contents.strip() == """":
        raise NothingChanged
    dst_contents = format_str(src_contents, line_length=line_length, mode=mode)
    if src_contents == dst_contents:
        raise NothingChanged
    if not fast:
        assert_equivalent(src_contents, dst_contents)
        assert_stable(src_contents, dst_contents, line_length=line_length, mode=mode)
    return dst_contents

def format_str(
    src_contents: str, line_length: int, *, mode: FileMode = FileMode.AUTO_DETECT
) -> FileContent:
    """"""Reformat a string and return new contents.
    `line_length` determines how many characters per line are allowed.
    """"""
    src_node = lib2to3_parse(src_contents)
    dst_contents = """"
    future_imports = get_future_imports(src_node)
    is_pyi = bool(mode & FileMode.PYI)
    py36 = bool(mode & FileMode.PYTHON36) or is_python36(src_node)
    normalize_strings = not bool(mode & FileMode.NO_STRING_NORMALIZATION)
    lines = LineGenerator(
        remove_u_prefix=py36 or ""unicode_literals"" in future_imports,
        is_pyi=is_pyi,
        normalize_strings=normalize_strings,
    )
    elt = EmptyLineTracker(is_pyi=is_pyi)
    empty_line = Line()
    after = 0
    for current_line in lines.visit(src_node):
        for _ in range(after):
            dst_contents += str(empty_line)
        before, after = elt.maybe_empty_lines(current_line)
        for _ in range(before):
            dst_contents += str(empty_line)
        for line in split_line(current_line, line_length=line_length, py36=py36):
            dst_contents += str(line)
    return dst_contents

GRAMMARS = [
    pygram.python_grammar_no_print_statement_no_exec_statement,
    pygram.python_grammar_no_print_statement,
    pygram.python_grammar,
]

def lib2to3_parse(src_txt: str) -> Node:
    """"""Given a string with source, return the lib2to3 Node.""""""","[4, 5, 14, 23, 42, 53, 57]"
"    # Arguments
        tuples: a list of tuples `(tensor, value)`.
            `value` should be a Numpy array.
    """"""
    if tuples:
        assign_ops = []
        feed_dict = {}
        for x, value in tuples:
            value = np.asarray(value, dtype=dtype(x))
            tf_dtype = tf.as_dtype(x.dtype.name.split('_')[0])
            if hasattr(x, '_assign_placeholder'):
                assign_placeholder = x._assign_placeholder
                assign_op = x._assign_op
            else:
                assign_placeholder = tf.placeholder(tf_dtype,
                                                    shape=value.shape)
                assign_op = x.assign(assign_placeholder)
                x._assign_placeholder = assign_placeholder
                x._assign_op = assign_op
            assign_ops.append(assign_op)
            feed_dict[assign_placeholder] = value
        get_session().run(assign_ops, feed_dict=feed_dict)

def get_variable_shape(x):
    """"""Returns the shape of a variable.
    # Arguments
        x: A variable.
    # Returns
        A tuple of integers.
    """"""
    return int_shape(x)

def print_tensor(x, message=''):
    """"""Prints `message` and the tensor value when evaluated.
     Note that `print_tensor` returns a new tensor identical to `x`
     which should be used in the following code. Otherwise the
     print operation is not taken into account during evaluation.
     # Example
     ```python
         >>> x = K.print_tensor(x, message=""x is: "")
     ```
    # Arguments
        x: Tensor to print.
        message: Message to print jointly with the tensor.
    # Returns
        The same tensor `x`, unchanged.
    """"""
    return tf.Print(x, [x], message)

# GRAPH MANIPULATION
class Function(object):
    """"""Runs a computation graph.
    It's possible to pass arguments to `tf.Session.run()` via `session_kwargs`.
    In particular additional operations via `fetches` argument and additional
    tensor substitutions via `feed_dict` arguments. Note that given
    substitutions are merged with substitutions from `inputs`. Even though
    `feed_dict` is passed once in the constructor (called in `model.compile()`)
    we can modify the values in the dictionary. Through this feed_dict we can
    provide additional substitutions besides Keras inputs.
    # Arguments
        inputs: Feed placeholders to the computation graph.
        outputs: Output tensors to fetch.
        updates: Additional update ops to be run at function call.
        name: a name to help users identify what this function does.
        session_kwargs: arguments to `tf.Session.run()`:
            `fetches`, `feed_dict`,
            `options`, `run_metadata`
    """"""
    def __init__(self, inputs, outputs,
                 updates=None,
                 name=None,
                 **session_kwargs):
        updates = updates or []
        if not isinstance(inputs, (list, tuple)):
            raise TypeError('`inputs` to a TensorFlow backend function '
                            'should be a list or tuple.')
        if not isinstance(outputs, (list, tuple)):
            raise TypeError('`outputs` of a TensorFlow backend function '
                            'should be a list or tuple.')
        if not isinstance(updates, (list, tuple)):
            raise TypeError('`updates` in a TensorFlow backend function '
                            'should be a list or tuple.')
        self.inputs = list(inputs)
        self.outputs = list(outputs)
        with tf.control_dependencies(self.outputs):
            updates_ops = []
            for update in updates:
                if isinstance(update, tuple):
                    p, new_p = update
                    updates_ops.append(tf.assign(p, new_p))
                else:
                    # assumed already an op
                    updates_ops.append(update)
            self.updates_op = tf.group(*updates_ops)
        self.name = name
        # additional tensor substitutions
        self.feed_dict = session_kwargs.pop('feed_dict', {})
        # additional operations
        self.fetches = session_kwargs.pop('fetches', [])
        if not isinstance(self.fetches, list):
            self.fetches = [self.fetches]
        # The main use case of `fetches` being passed to a model is the ability
        # to run custom updates
        # (since the outputs of fetches are never returned).
        # This requires us to wrap fetches in `identity` ops.
        self.fetches = [tf.identity(x) for x in self.fetches]
        self.session_kwargs = session_kwargs
        if session_kwargs:
            raise ValueError('Some keys in session_kwargs are not '
                             'supported at this '
                             'time: %s', session_kwargs.keys())
        self._callable_fn = None
        self._feed_arrays = None",[120]
"        finally:
            if enqueuer is not None:
                enqueuer.stop()
        callbacks.on_train_end()
        return self.history
    @interfaces.legacy_generator_methods_support
    def evaluate_generator(self, generator, steps,
                           max_queue_size=10,
                           workers=1,
                           use_multiprocessing=False):
        """"""Evaluates the model on a data generator.
        The generator should return the same kind of data
        as accepted by `test_on_batch`.
        # Arguments
            generator: Generator yielding tuples (inputs, targets)
                or (inputs, targets, sample_weights)
                or an instance of Sequence (keras.utils.Sequence)
                    object in order to avoid duplicate data
                    when using multiprocessing.
            steps: Total number of steps (batches of samples)
                to yield from `generator` before stopping.
                Not used if using Sequence.
            max_queue_size: maximum size for the generator queue
            workers: maximum number of processes to spin up
                when using process based threading
            use_multiprocessing: if True, use process based threading.
                Note that because
                this implementation relies on multiprocessing,
                you should not pass
                non picklable arguments to the generator
                as they can't be passed
                easily to children processes.
        # Returns
            Scalar test loss (if the model has a single output and no metrics)
            or list of scalars (if the model has multiple outputs
            and/or metrics). The attribute `model.metrics_names` will give you
            the display labels for the scalar outputs.
        # Raises
            ValueError: In case the generator yields
                data in an invalid format.
        """"""
        self._make_test_function()
        steps_done = 0
        wait_time = 0.01
        all_outs = []
        batch_sizes = []
        is_sequence = isinstance(generator, Sequence)
        if not is_sequence and use_multiprocessing and workers > 1:
            warnings.warn(
                UserWarning('Using a generator with `use_multiprocessing=True`'
                            ' and multiple workers may duplicate your data.'
                            ' Please consider using the`keras.utils.Sequence'
                            ' class.'))
        if is_sequence:
            steps = len(generator)
        enqueuer = None
        try:
            if is_sequence:
                enqueuer = OrderedEnqueuer(generator,
                                           use_multiprocessing=use_multiprocessing)
            else:
                enqueuer = GeneratorEnqueuer(generator,
                                             use_multiprocessing=use_multiprocessing,
                                             wait_time=wait_time)
            enqueuer.start(workers=workers, max_queue_size=max_queue_size)
            output_generator = enqueuer.get()
            while steps_done < steps:
                generator_output = next(output_generator)
                if not hasattr(generator_output, '__len__'):
                    raise ValueError('Output of generator should be a tuple '
                                     '(x, y, sample_weight) '
                                     'or (x, y). Found: ' +
                                     str(generator_output))
                if len(generator_output) == 2:
                    x, y = generator_output
                    sample_weight = None
                elif len(generator_output) == 3:
                    x, y, sample_weight = generator_output
                else:
                    raise ValueError('Output of generator should be a tuple '
                                     '(x, y, sample_weight) '
                                     'or (x, y). Found: ' +
                                     str(generator_output))
                outs = self.test_on_batch(x, y, sample_weight=sample_weight)
                if isinstance(x, list):
                    batch_size = len(x[0])
                elif isinstance(x, dict):
                    batch_size = len(list(x.values())[0])
                else:
                    batch_size = len(x)
                if batch_size == 0:
                    raise ValueError('Received an empty batch. '
                                     'Batches should at least contain one item.')
                all_outs.append(outs)
                steps_done += 1
                batch_sizes.append(batch_size)
        finally:
            if enqueuer is not None:
                enqueuer.stop()
        if not isinstance(outs, list):
            return np.average(np.asarray(all_outs),
                              weights=batch_sizes)
        else:
            averages = []
            for i in range(len(outs)):
                averages.append(np.average([out[i] for out in all_outs],
                                           weights=batch_sizes))
            return averages
    @interfaces.legacy_generator_methods_support
    def predict_generator(self, generator, steps,
                          max_queue_size=10,
                          workers=1,
                          use_multiprocessing=False,","[8, 25, 60, 61, 123]"
"            output_shape = output_shape[0]
        if self.merge_mode == 'concat':
            output_shape = list(output_shape)
            output_shape[-1] *= 2
            output_shape = tuple(output_shape)
        elif self.merge_mode is None:
            output_shape = [output_shape, copy.copy(output_shape)]
        if self.return_state:
            if self.merge_mode is None:
                return output_shape + state_shape + copy.copy(state_shape)
            return [output_shape] + state_shape + copy.copy(state_shape)
        return output_shape
    def call(self, inputs, training=None, mask=None, initial_state=None):
        kwargs = {}
        if has_arg(self.layer.call, 'training'):
            kwargs['training'] = training
        if has_arg(self.layer.call, 'mask'):
            kwargs['mask'] = mask
        if initial_state is not None and has_arg(self.layer.call, 'initial_state'):
            if not isinstance(initial_state, list):
                raise ValueError(
                    'When passing `initial_state` to a Bidirectional RNN, the state '
                    'should be a list containing the states of the underlying RNNs. '
                    'Found: ' + str(initial_state))
            forward_state = initial_state[:len(initial_state) // 2]
            backward_state = initial_state[len(initial_state) // 2:]
            y = self.forward_layer.call(inputs, initial_state=forward_state, **kwargs)
            y_rev = self.backward_layer.call(inputs, initial_state=backward_state, **kwargs)
        else:
            y = self.forward_layer.call(inputs, **kwargs)
            y_rev = self.backward_layer.call(inputs, **kwargs)
        if self.return_state:
            states = y[1:] + y_rev[1:]
            y = y[0]
            y_rev = y_rev[0]
        if self.return_sequences:
            y_rev = K.reverse(y_rev, 1)
        if self.merge_mode == 'concat':
            output = K.concatenate([y, y_rev])
        elif self.merge_mode == 'sum':
            output = y + y_rev
        elif self.merge_mode == 'ave':
            output = (y + y_rev) / 2
        elif self.merge_mode == 'mul':
            output = y * y_rev
        elif self.merge_mode is None:
            output = [y, y_rev]
        # Properly set learning phase
        if (getattr(y, '_uses_learning_phase', False) or
           getattr(y_rev, '_uses_learning_phase', False)):
            if self.merge_mode is None:
                for out in output:
                    out._uses_learning_phase = True
            else:
                output._uses_learning_phase = True
        if self.return_state:
            if self.merge_mode is None:
                return output + states
            return [output] + states
        return output
    def reset_states(self):
        self.forward_layer.reset_states()
        self.backward_layer.reset_states()
    def build(self, input_shape):
        with K.name_scope(self.forward_layer.name):
            self.forward_layer.build(input_shape)
        with K.name_scope(self.backward_layer.name):
            self.backward_layer.build(input_shape)
        self.built = True
    def compute_mask(self, inputs, mask):
        if self.return_sequences:
            if not self.merge_mode:
                return [mask, mask]
            else:
                return mask
        else:
            return None
    @property
    def trainable_weights(self):
        if hasattr(self.forward_layer, 'trainable_weights'):
            return (self.forward_layer.trainable_weights +
                    self.backward_layer.trainable_weights)
        return []
    @property
    def non_trainable_weights(self):
        if hasattr(self.forward_layer, 'non_trainable_weights'):
            return (self.forward_layer.non_trainable_weights +
                    self.backward_layer.non_trainable_weights)
        return []
    @property
    def updates(self):
        if hasattr(self.forward_layer, 'updates'):
            return self.forward_layer.updates + self.backward_layer.updates
        return []
    @property
    def losses(self):
        if hasattr(self.forward_layer, 'losses'):
            return self.forward_layer.losses + self.backward_layer.losses
        return []
    @property
    def constraints(self):
        constraints = {}
        if hasattr(self.forward_layer, 'constraints'):
            constraints.update(self.forward_layer.constraints)
            constraints.update(self.backward_layer.constraints)
        return constraints
    def get_config(self):
        config = {'merge_mode': self.merge_mode}
        base_config = super(Bidirectional, self).get_config()","[23, 24, 25, 26, 27]"
"            new_values = new_values.astype(""object"")
        else:
            sorted_values = sorted_values.astype(name, copy=False)
        # fill in our values & mask
        libreshape.unstack(
            sorted_values,
            mask.view(""u1""),
            stride,
            length,
            width,
            new_values,
            new_mask.view(""u1""),
        )
        # reconstruct dtype if needed
        if needs_i8_conversion(values):
            new_values = new_values.view(values.dtype)
        return new_values, new_mask
    def get_new_columns(self):
        if self.value_columns is None:
            if self.lift == 0:
                return self.removed_level._shallow_copy(name=self.removed_name)
            lev = self.removed_level.insert(0, item=self.removed_level._na_value)
            return lev.rename(self.removed_name)
        stride = len(self.removed_level) + self.lift
        width = len(self.value_columns)
        propagator = np.repeat(np.arange(width), stride)
        if isinstance(self.value_columns, MultiIndex):
            new_levels = self.value_columns.levels + (self.removed_level_full,)
            new_names = self.value_columns.names + (self.removed_name,)
            new_codes = [lab.take(propagator) for lab in self.value_columns.codes]
        else:
            new_levels = [self.value_columns, self.removed_level_full]
            new_names = [self.value_columns.name, self.removed_name]
            new_codes = [propagator]
        # The two indices differ only if the unstacked level had unused items:
        if len(self.removed_level_full) != len(self.removed_level):
            # In this case, we remap the new codes to the original level:
            repeater = self.removed_level_full.get_indexer(self.removed_level)
            if self.lift:
                repeater = np.insert(repeater, 0, -1)
        else:
            # Otherwise, we just use each level item exactly once:
            repeater = np.arange(stride) - self.lift
        # The entire level is then just a repetition of the single chunk:
        new_codes.append(np.tile(repeater, width))
        return MultiIndex(
            levels=new_levels, codes=new_codes, names=new_names, verify_integrity=False
        )
    def get_new_index(self):
        result_codes = [lab.take(self.compressor) for lab in self.sorted_labels[:-1]]
        # construct the new index
        if len(self.new_index_levels) == 1:
            level, level_codes = self.new_index_levels[0], result_codes[0]
            if (level_codes == -1).any():
                level = level.insert(len(level), level._na_value)
            return level.take(level_codes).rename(self.new_index_names[0])
        return MultiIndex(
            levels=self.new_index_levels,
            codes=result_codes,
            names=self.new_index_names,
            verify_integrity=False,
        )

def _unstack_multiple(data, clocs, fill_value=None):
    if len(clocs) == 0:
        return data
    # NOTE: This doesn't deal with hierarchical columns yet
    index = data.index
    # GH 19966 Make sure if MultiIndexed index has tuple name, they will be
    # recognised as a whole
    if clocs in index.names:
        clocs = [clocs]
    clocs = [index._get_level_number(i) for i in clocs]
    rlocs = [i for i in range(index.nlevels) if i not in clocs]
    clevels = [index.levels[i] for i in clocs]
    ccodes = [index.codes[i] for i in clocs]
    cnames = [index.names[i] for i in clocs]
    rlevels = [index.levels[i] for i in rlocs]
    rcodes = [index.codes[i] for i in rlocs]
    rnames = [index.names[i] for i in rlocs]
    shape = [len(x) for x in clevels]
    group_index = get_group_index(ccodes, shape, sort=False, xnull=False)
    comp_ids, obs_ids = compress_group_index(group_index, sort=False)
    recons_codes = decons_obs_group_ids(comp_ids, obs_ids, shape, ccodes, xnull=False)
    if rlocs == []:
        # Everything is in clocs, so the dummy df has a regular index
        dummy_index = Index(obs_ids, name=""__placeholder__"")
    else:
        dummy_index = MultiIndex(
            levels=rlevels + [obs_ids],
            codes=rcodes + [comp_ids],
            names=rnames + [""__placeholder__""],
            verify_integrity=False,
        )
    if isinstance(data, Series):
        dummy = data.copy()
        dummy.index = dummy_index
        unstacked = dummy.unstack(""__placeholder__"", fill_value=fill_value)
        new_levels = clevels
        new_names = cnames
        new_codes = recons_codes
    else:
        if isinstance(data.columns, MultiIndex):
            result = data",[105]
"            raise TypeError(""RangeIndex(...) must be called with integers"")
        start = ensure_python_int(start) if start is not None else 0
        if stop is None:
            start, stop = 0, start
        else:
            stop = ensure_python_int(stop)
        step = ensure_python_int(step) if step is not None else 1
        if step == 0:
            raise ValueError(""Step must not be zero"")
        rng = range(start, stop, step)
        return cls._simple_new(rng, dtype=dtype, name=name)
    @classmethod
    def from_range(cls, data, name=None, dtype=None):
        """"""
        Create RangeIndex from a range object.
        Returns
        -------
        RangeIndex
        """"""
        if not isinstance(data, range):
            raise TypeError(
                ""{0}(...) must be called with object coercible to a ""
                ""range, {1} was passed"".format(cls.__name__, repr(data))
            )
        cls._validate_dtype(dtype)
        return cls._simple_new(data, dtype=dtype, name=name)
    @classmethod
    def _simple_new(cls, values, name=None, dtype=None):
        result = object.__new__(cls)
        # handle passed None, non-integers
        if values is None:
            # empty
            values = range(0, 0, 1)
        elif not isinstance(values, range):
            return Index(values, dtype=dtype, name=name)
        result._range = values
        result.name = name
        result._reset_identity()
        return result
    # --------------------------------------------------------------------
    @staticmethod
    def _validate_dtype(dtype):
        """""" require dtype to be None or int64 """"""
        if not (dtype is None or is_int64_dtype(dtype)):
            raise TypeError(""Invalid to pass a non-int64 dtype to RangeIndex"")
    @cache_readonly
    def _constructor(self):
        """""" return the class to use for construction """"""
        return Int64Index
    @property
    def _data(self):
        """"""
        An int array that for performance reasons is created only when needed.
        The constructed array is saved in ``_cached_data``. This allows us to
        check if the array has been created without accessing ``_data`` and
        triggering the construction.
        """"""
        if self._cached_data is None:
            self._cached_data = np.arange(
                self.start, self.stop, self.step, dtype=np.int64
            )
        return self._cached_data
    @cache_readonly
    def _int64index(self):
        return Int64Index._simple_new(self._data, name=self.name)
    def _get_data_as_items(self):
        """""" return a list of tuples of start, stop, step """"""
        rng = self._range
        return [(""start"", rng.start), (""stop"", rng.stop), (""step"", rng.step)]
    def __reduce__(self):
        d = self._get_attributes_dict()
        d.update(dict(self._get_data_as_items()))
        return ibase._new_Index, (self.__class__, d), None
    # --------------------------------------------------------------------
    # Rendering Methods
    def _format_attrs(self):
        """"""
        Return a list of tuples of the (attr, formatted_value)
        """"""
        attrs = self._get_data_as_items()
        if self.name is not None:
            attrs.append((""name"", ibase.default_pprint(self.name)))
        return attrs
    def _format_data(self, name=None):
        # we are formatting thru the attributes
        return None
    def _format_with_header(self, header, na_rep=""NaN"", **kwargs):
        return header + list(map(pprint_thing, self._range))
    # --------------------------------------------------------------------
    _deprecation_message = (
        ""RangeIndex.{} is deprecated and will be ""
        ""removed in a future version. Use RangeIndex.{} ""
        ""instead""
    )
    @cache_readonly
    def start(self):
        """"""
        The value of the `start` parameter (``0`` if this was not supplied).
        """"""
        # GH 25710
        return self._range.start
","[53, 54, 56, 57, 58]"
"""""""
SQL-style merge routines
""""""
import copy
import datetime
from functools import partial
import string
from typing import TYPE_CHECKING, Optional, Tuple, Union
import warnings
import numpy as np
from pandas._libs import Timedelta, hashtable as libhashtable, lib
import pandas._libs.join as libjoin
from pandas._typing import FrameOrSeries
from pandas.errors import MergeError
from pandas.util._decorators import Appender, Substitution
from pandas.core.dtypes.common import (
    ensure_float64,
    ensure_int64,
    ensure_object,
    is_array_like,
    is_bool,
    is_bool_dtype,
    is_categorical_dtype,
    is_datetime64tz_dtype,
    is_dtype_equal,
    is_extension_array_dtype,
    is_float_dtype,
    is_integer,
    is_integer_dtype,
    is_list_like,
    is_number,
    is_numeric_dtype,
    is_object_dtype,
    needs_i8_conversion,
)
from pandas.core.dtypes.generic import ABCDataFrame, ABCSeries
from pandas.core.dtypes.missing import isna, na_value_for_dtype
from pandas import Categorical, Index, MultiIndex
from pandas.core import groupby
import pandas.core.algorithms as algos
from pandas.core.arrays.categorical import _recode_for_categories
import pandas.core.common as com
from pandas.core.construction import extract_array
from pandas.core.frame import _merge_doc
from pandas.core.internals import concatenate_block_managers
from pandas.core.sorting import is_int64_overflow_possible
if TYPE_CHECKING:
    from pandas import DataFrame, Series  # noqa:F401

@Substitution(""\nleft : DataFrame"")
@Appender(_merge_doc, indents=0)
def merge(
    left,
    right,
    how: str = ""inner"",
    on=None,
    left_on=None,
    right_on=None,
    left_index: bool = False,
    right_index: bool = False,
    sort: bool = False,
    suffixes=(""_x"", ""_y""),
    copy: bool = True,
    indicator: bool = False,
    validate=None,
) -> ""DataFrame"":
    op = _MergeOperation(
        left,
        right,
        how=how,
        on=on,
        left_on=left_on,
        right_on=right_on,
        left_index=left_index,
        right_index=right_index,
        sort=sort,
        suffixes=suffixes,
        copy=copy,
        indicator=indicator,
        validate=validate,
    )
    return op.get_result()

if __debug__:
    merge.__doc__ = _merge_doc % ""\nleft : DataFrame""

def _groupby_and_merge(by, on, left: ""DataFrame"", right: ""DataFrame"", merge_pieces):
    """"""
    groupby & merge; we are always performing a left-by type operation
    Parameters
    ----------
    by: field to group
    on: duplicates field
    left: DataFrame
    right: DataFrame
    merge_pieces: function for merging
    """"""
    pieces = []
    if not isinstance(by, (list, tuple)):
        by = [by]
    lby = left.groupby(by, sort=False)
    rby: Optional[groupby.DataFrameGroupBy] = None
    # if we can groupby the rhs
    # then we can get vastly better perf
    try:
        rby = right.groupby(by, sort=False)
    except KeyError:
        pass
    for key, lhs in lby:
        if rby is None:
            rhs = right
        else:","[8, 15]"
"from __future__ import unicode_literals
import re
from .subtitles import SubtitlesInfoExtractor
from .common import ExtractorError
from ..utils import parse_iso8601

class DRTVIE(SubtitlesInfoExtractor):
    _VALID_URL = r'http://(?:www\.)?dr\.dk/tv/se/(?:[^/]+/)+(?P<id>[\da-z-]+)(?:[/#?]|$)'
    _TEST = {
        'url': 'http://www.dr.dk/tv/se/partiets-mand/partiets-mand-7-8',
        'md5': '4a7e1dd65cdb2643500a3f753c942f25',
        'info_dict': {
            'id': 'partiets-mand-7-8',
            'ext': 'mp4',
            'title': 'Partiets mand (7:8)',
            'description': 'md5:a684b90a8f9336cd4aab94b7647d7862',
            'timestamp': 1403047940,
            'upload_date': '20140617',
            'duration': 1299.040,
        },
    }
    def _real_extract(self, url):
        mobj = re.match(self._VALID_URL, url)
        video_id = mobj.group('id')
        programcard = self._download_json(
            'http://www.dr.dk/mu/programcard/expanded/%s' % video_id, video_id, 'Downloading video JSON')
        data = programcard['Data'][0]
        title = data['Title']
        description = data['Description']
        timestamp = parse_iso8601(data['CreatedTime'][:-5])
        thumbnail = None
        duration = None
        restricted_to_denmark = False
        formats = []
        subtitles = {}
        for asset in data['Assets']:
            if asset['Kind'] == 'Image':
                thumbnail = asset['Uri']
            elif asset['Kind'] == 'VideoResource':
                duration = asset['DurationInMilliseconds'] / 1000.0
                restricted_to_denmark = asset['RestrictedToDenmark']
                for link in asset['Links']:
                    target = link['Target']
                    uri = link['Uri']
                    formats.append({
                        'url': uri + '?hdcore=3.3.0&plugin=aasp-3.3.0.99.43' if target == 'HDS' else uri,
                        'format_id': target,
                        'ext': link['FileFormat'],
                        'preference': -1 if target == 'HDS' else -2,
                    })
                subtitles_list = asset.get('SubtitlesList')
                if isinstance(subtitles_list, list):
                    LANGS = {
                        'Danish': 'dk',
                    }
                    for subs in subtitles_list:
                        lang = subs['Language']
                        subtitles[LANGS.get(lang, lang)] = subs['Uri']
        if not formats and restricted_to_denmark:
            raise ExtractorError(
                'Unfortunately, DR is not allowed to show this program outside Denmark.', expected=True)
        self._sort_formats(formats)
        if self._downloader.params.get('listsubtitles', False):
            self._list_available_subtitles(video_id, subtitles)
            return
        return {
            'id': video_id,
            'title': title,
            'description': description,
            'thumbnail': thumbnail,
            'timestamp': timestamp,
            'duration': duration,
            'formats': formats,
            'subtitles': self.extract_subtitles(video_id, subtitles),
        }","[2, 3, 27, 28, 37]"
"            `ambiguous` parameter dictates how ambiguous times should be
            handled.
            - 'infer' will attempt to infer fall dst-transition hours based on
              order
            - bool-ndarray where True signifies a DST time, False signifies a
              non-DST time (note that this flag is only applicable for
              ambiguous times)
            - 'NaT' will return NaT where there are ambiguous times
            - 'raise' will raise an AmbiguousTimeError if there are ambiguous
              times.
        nonexistent : 'shift_forward', 'shift_backward, 'NaT', timedelta, \
default 'raise'
            A nonexistent time does not exist in a particular timezone
            where clocks moved forward due to DST.
            - 'shift_forward' will shift the nonexistent time forward to the
              closest existing time
            - 'shift_backward' will shift the nonexistent time backward to the
              closest existing time
            - 'NaT' will return NaT where there are nonexistent times
            - timedelta objects will shift nonexistent times by the timedelta
            - 'raise' will raise an NonExistentTimeError if there are
              nonexistent times.
            .. versionadded:: 0.24.0
        Returns
        -------
        Same type as self
            Array/Index converted to the specified time zone.
        Raises
        ------
        TypeError
            If the Datetime Array/Index is tz-aware and tz is not None.
        See Also
        --------
        DatetimeIndex.tz_convert : Convert tz-aware DatetimeIndex from
            one time zone to another.
        Examples
        --------
        >>> tz_naive = pd.date_range('2018-03-01 09:00', periods=3)
        >>> tz_naive
        DatetimeIndex(['2018-03-01 09:00:00', '2018-03-02 09:00:00',
                       '2018-03-03 09:00:00'],
                      dtype='datetime64[ns]', freq='D')
        Localize DatetimeIndex in US/Eastern time zone:
        >>> tz_aware = tz_naive.tz_localize(tz='US/Eastern')
        >>> tz_aware
        DatetimeIndex(['2018-03-01 09:00:00-05:00',
                       '2018-03-02 09:00:00-05:00',
                       '2018-03-03 09:00:00-05:00'],
                      dtype='datetime64[ns, US/Eastern]', freq='D')
        With the ``tz=None``, we can remove the time zone information
        while keeping the local time (not converted to UTC):
        >>> tz_aware.tz_localize(None)
        DatetimeIndex(['2018-03-01 09:00:00', '2018-03-02 09:00:00',
                       '2018-03-03 09:00:00'],
                      dtype='datetime64[ns]', freq='D')
        Be careful with DST changes. When there is sequential data, pandas can
        infer the DST time:
        >>> s = pd.to_datetime(pd.Series(['2018-10-28 01:30:00',
        ...                               '2018-10-28 02:00:00',
        ...                               '2018-10-28 02:30:00',
        ...                               '2018-10-28 02:00:00',
        ...                               '2018-10-28 02:30:00',
        ...                               '2018-10-28 03:00:00',
        ...                               '2018-10-28 03:30:00']))
        >>> s.dt.tz_localize('CET', ambiguous='infer')
        0   2018-10-28 01:30:00+02:00
        1   2018-10-28 02:00:00+02:00
        2   2018-10-28 02:30:00+02:00
        3   2018-10-28 02:00:00+01:00
        4   2018-10-28 02:30:00+01:00
        5   2018-10-28 03:00:00+01:00
        6   2018-10-28 03:30:00+01:00
        dtype: datetime64[ns, CET]
        In some cases, inferring the DST is impossible. In such cases, you can
        pass an ndarray to the ambiguous parameter to set the DST explicitly
        >>> s = pd.to_datetime(pd.Series(['2018-10-28 01:20:00',
        ...                               '2018-10-28 02:36:00',
        ...                               '2018-10-28 03:46:00']))
        >>> s.dt.tz_localize('CET', ambiguous=np.array([True, True, False]))
        0   2018-10-28 01:20:00+02:00
        1   2018-10-28 02:36:00+02:00
        2   2018-10-28 03:46:00+01:00
        dtype: datetime64[ns, CET]
        If the DST transition causes nonexistent times, you can shift these
        dates forward or backwards with a timedelta object or `'shift_forward'`
        or `'shift_backwards'`.
        >>> s = pd.to_datetime(pd.Series(['2015-03-29 02:30:00',
        ...                               '2015-03-29 03:30:00']))
        >>> s.dt.tz_localize('Europe/Warsaw', nonexistent='shift_forward')
        0   2015-03-29 03:00:00+02:00
        1   2015-03-29 03:30:00+02:00
        dtype: datetime64[ns, Europe/Warsaw]
        >>> s.dt.tz_localize('Europe/Warsaw', nonexistent='shift_backward')
        0   2015-03-29 01:59:59.999999999+01:00
        1   2015-03-29 03:30:00+02:00
        dtype: datetime64[ns, Europe/Warsaw]
        >>> s.dt.tz_localize('Europe/Warsaw', nonexistent=pd.Timedelta('1H'))
        0   2015-03-29 03:30:00+02:00
        1   2015-03-29 03:30:00+02:00
        dtype: datetime64[ns, Europe/Warsaw]
        """"""
        nonexistent_options = (""raise"", ""NaT"", ""shift_forward"", ""shift_backward"")
        if nonexistent not in nonexistent_options and not isinstance(
            nonexistent, timedelta
        ):
            raise ValueError(
                ""The nonexistent argument must be one of 'raise', ""","[58, 66]"
"        # given a key, try to figure out a location for a partial slice
        if not isinstance(key, str):
            return key
        raise NotImplementedError
    @Substitution(klass=""TimedeltaIndex"")
    @Appender(_shared_docs[""searchsorted""])
    def searchsorted(self, value, side=""left"", sorter=None):
        if isinstance(value, (np.ndarray, Index)):
            value = np.array(value, dtype=_TD_DTYPE, copy=False)
        else:
            value = Timedelta(value).asm8.view(_TD_DTYPE)
        return self.values.searchsorted(value, side=side, sorter=sorter)
    def is_type_compatible(self, typ) -> bool:
        return typ == self.inferred_type or typ == ""timedelta""
    @property
    def inferred_type(self) -> str:
        return ""timedelta64""
    def insert(self, loc, item):
        """"""
        Make new Index inserting new item at location
        Parameters
        ----------
        loc : int
        item : object
            If not either a Python datetime or a numpy integer-like, returned
            Index dtype will be object rather than datetime.
        Returns
        -------
        new_index : Index
        """"""
        # try to convert if possible
        if isinstance(item, self._data._recognized_scalars):
            item = Timedelta(item)
        elif is_valid_nat_for_dtype(item, self.dtype):
            # GH 18295
            item = self._na_value
        elif is_scalar(item) and isna(item):
            # i.e. datetime64(""NaT"")
            raise TypeError(
                f""cannot insert {type(self).__name__} with incompatible label""
            )
        freq = None
        if isinstance(item, Timedelta) or (is_scalar(item) and isna(item)):
            # check freq can be preserved on edge cases
            if self.freq is not None:
                if (loc == 0 or loc == -len(self)) and item + self.freq == self[0]:
                    freq = self.freq
                elif (loc == len(self)) and item - self.freq == self[-1]:
                    freq = self.freq
            item = Timedelta(item).asm8.view(_TD_DTYPE)
        try:
            new_tds = np.concatenate(
                (self[:loc].asi8, [item.view(np.int64)], self[loc:].asi8)
            )
            return self._shallow_copy(new_tds, freq=freq)
        except (AttributeError, TypeError):
            # fall back to object index
            if isinstance(item, str):
                return self.astype(object).insert(loc, item)
            raise TypeError(""cannot insert TimedeltaIndex with incompatible label"")

TimedeltaIndex._add_comparison_ops()
TimedeltaIndex._add_logical_methods_disabled()

def timedelta_range(
    start=None, end=None, periods=None, freq=None, name=None, closed=None
) -> TimedeltaIndex:
    """"""
    Return a fixed frequency TimedeltaIndex, with day as the default
    frequency.
    Parameters
    ----------
    start : str or timedelta-like, default None
        Left bound for generating timedeltas.
    end : str or timedelta-like, default None
        Right bound for generating timedeltas.
    periods : int, default None
        Number of periods to generate.
    freq : str or DateOffset, default 'D'
        Frequency strings can have multiples, e.g. '5H'.
    name : str, default None
        Name of the resulting TimedeltaIndex.
    closed : str, default None
        Make the interval closed with respect to the given frequency to
        the 'left', 'right', or both sides (None).
    Returns
    -------
    rng : TimedeltaIndex
    Notes
    -----
    Of the four parameters ``start``, ``end``, ``periods``, and ``freq``,
    exactly three must be specified. If ``freq`` is omitted, the resulting
    ``TimedeltaIndex`` will have ``periods`` linearly spaced elements between
    ``start`` and ``end`` (closed on both sides).
    To learn more about the frequency strings, please see `this link
    <https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases>`__.
    Examples
    --------
    >>> pd.timedelta_range(start='1 day', periods=4)
    TimedeltaIndex(['1 days', '2 days', '3 days', '4 days'],
                   dtype='timedelta64[ns]', freq='D')
    The ``closed`` parameter specifies which endpoint is included.  The default
    behavior is to include both endpoints.
    >>> pd.timedelta_range(start='1 day', periods=4, closed='right')","[10, 11, 12, 14]"
"                self._rotation, self._rotation_mode,
                self.figure.dpi, weakref.ref(renderer),
                self._linespacing
                )
    def get_text(self):
        """"""Return the text string.""""""
        return self._text
    def get_verticalalignment(self):
        """"""
        Return the vertical alignment as a string.  Will be one of
        'top', 'center', 'bottom' or 'baseline'.
        """"""
        return self._verticalalignment
    def get_window_extent(self, renderer=None, dpi=None):
        """"""
        Return the `.Bbox` bounding the text, in display units.
        In addition to being used internally, this is useful for specifying
        clickable regions in a png file on a web page.
        Parameters
        ----------
        renderer : Renderer, optional
            A renderer is needed to compute the bounding box.  If the artist
            has already been drawn, the renderer is cached; thus, it is only
            necessary to pass this argument when calling `get_window_extent`
            before the first `draw`.  In practice, it is usually easier to
            trigger a draw first (e.g. by saving the figure).
        dpi : float, optional
            The dpi value for computing the bbox, defaults to
            ``self.figure.dpi`` (*not* the renderer dpi); should be set e.g. if
            to match regions with a figure saved with a custom dpi value.
        """"""
        #return _unit_box
        if not self.get_visible():
            return Bbox.unit()
        if dpi is not None:
            dpi_orig = self.figure.dpi
            self.figure.dpi = dpi
        if self.get_text() == '':
            tx, ty = self._get_xy_display()
            return Bbox.from_bounds(tx, ty, 0, 0)
        if renderer is not None:
            self._renderer = renderer
        if self._renderer is None:
            self._renderer = self.figure._cachedRenderer
        if self._renderer is None:
            raise RuntimeError('Cannot get window extent w/o renderer')
        bbox, info, descent = self._get_layout(self._renderer)
        x, y = self.get_unitless_position()
        x, y = self.get_transform().transform((x, y))
        bbox = bbox.translated(x, y)
        if dpi is not None:
            self.figure.dpi = dpi_orig
        return bbox
    def set_backgroundcolor(self, color):
        """"""
        Set the background color of the text by updating the bbox.
        Parameters
        ----------
        color : color
        See Also
        --------
        .set_bbox : To change the position of the bounding box
        """"""
        if self._bbox_patch is None:
            self.set_bbox(dict(facecolor=color, edgecolor=color))
        else:
            self._bbox_patch.update(dict(facecolor=color))
        self._update_clip_properties()
        self.stale = True
    def set_color(self, color):
        """"""
        Set the foreground color of the text
        Parameters
        ----------
        color : color
        """"""
        # Make sure it is hashable, or get_prop_tup will fail.
        try:
            hash(color)
        except TypeError:
            color = tuple(color)
        self._color = color
        self.stale = True
    def set_horizontalalignment(self, align):
        """"""
        Set the horizontal alignment to one of
        Parameters
        ----------
        align : {'center', 'right', 'left'}
        """"""
        cbook._check_in_list(['center', 'right', 'left'], align=align)
        self._horizontalalignment = align
        self.stale = True
    def set_multialignment(self, align):
        """"""
        Set the text alignment for multiline texts.
        The layout of the bounding box of all the lines is determined by the
        horizontalalignment and verticalalignment properties. This property
        controls the alignment of the text lines within that box.
        Parameters
        ----------
        align : {'left', 'right', 'center'}
        """"""
        cbook._check_in_list(['center', 'right', 'left'], align=align)
        self._multialignment = align
        self.stale = True
    def set_linespacing(self, spacing):","[40, 41, 42, 44, 45, 54, 55, 56, 57, 58, 59, 60]"
"class HEADRequest(compat_urllib_request.Request):
    def get_method(self):
        return ""HEAD""

def int_or_none(v, scale=1, default=None, get_attr=None, invscale=1):
    if get_attr:
        if v is not None:
            v = getattr(v, get_attr, None)
    if v == '':
        v = None
    return default if v is None else (int(v) * invscale // scale)

def str_or_none(v, default=None):
    return default if v is None else compat_str(v)

def str_to_int(int_str):
    """""" A more relaxed version of int_or_none """"""
    if int_str is None:
        return None
    int_str = re.sub(r'[,\.\+]', '', int_str)
    return int(int_str)

def float_or_none(v, scale=1, invscale=1, default=None):
    return default if v is None else (float(v) * invscale / scale)

def parse_duration(s):
    if s is None:
        return None
    s = s.strip()
    m = re.match(
        r'''(?ix)T?
            (?:
                (?:(?P<hours>[0-9]+)\s*(?:[:h]|hours?)\s*)?
                (?P<mins>[0-9]+)\s*(?:[:m]|mins?|minutes?)\s*
            )?
            (?P<secs>[0-9]+)(?P<ms>\.[0-9]+)?\s*(?:s|secs?|seconds?)?$''', s)
    if not m:
        return None
    res = int(m.group('secs'))
    if m.group('mins'):
        res += int(m.group('mins')) * 60
        if m.group('hours'):
            res += int(m.group('hours')) * 60 * 60
    if m.group('ms'):
        res += float(m.group('ms'))
    return res

def prepend_extension(filename, ext):
    name, real_ext = os.path.splitext(filename)
    return '{0}.{1}{2}'.format(name, ext, real_ext)

def check_executable(exe, args=[]):
    """""" Checks if the given binary is installed somewhere in PATH, and returns its name.
    args can be a list of arguments for a short output (like -version) """"""
    try:
        subprocess.Popen([exe] + args, stdout=subprocess.PIPE, stderr=subprocess.PIPE).communicate()
    except OSError:
        return False
    return exe

def get_exe_version(exe, args=['--version'],
                    version_re=r'version\s+([0-9._-a-zA-Z]+)',
                    unrecognized='present'):
    """""" Returns the version of the specified executable,
    or False if the executable is not present """"""
    try:
        out, err = subprocess.Popen(
            [exe] + args,
            stdout=subprocess.PIPE, stderr=subprocess.STDOUT).communicate()
    except OSError:
        return False
    firstline = out.partition(b'\n')[0].decode('ascii', 'ignore')
    m = re.search(version_re, firstline)
    if m:
        return m.group(1)
    else:
        return unrecognized

class PagedList(object):
    def __len__(self):
        # This is only useful for tests
        return len(self.getslice())

class OnDemandPagedList(PagedList):
    def __init__(self, pagefunc, pagesize):
        self._pagefunc = pagefunc
        self._pagesize = pagesize
    def getslice(self, start=0, end=None):
        res = []
        for pagenum in itertools.count(start // self._pagesize):
            firstid = pagenum * self._pagesize
            nextfirstid = pagenum * self._pagesize + self._pagesize
            if start >= nextfirstid:
                continue
            page_results = list(self._pagefunc(pagenum))
            startv = (
                start % self._pagesize
                if firstid <= start < nextfirstid
                else 0)
            endv = (
                ((end - 1) % self._pagesize) + 1
                if (end is not None and firstid <= end <= nextfirstid)
                else None)
            if startv != 0 or endv is not None:
                page_results = page_results[startv:endv]
            res.extend(page_results)
            # A little optimization - if current page is not ""full"", ie. does
            # not contain page_size videos then we can assume that this page","[43, 46, 49, 50]"
"        else:
            reformatted = ""reformatted""
            unchanged = ""left unchanged""
            failed = ""failed to reformat""
        report = []
        if self.change_count:
            s = 's' if self.change_count > 1 else ''
            report.append(
                click.style(f'{self.change_count} file{s} {reformatted}', bold=True)
            )
        if self.same_count:
            s = 's' if self.same_count > 1 else ''
            report.append(f'{self.same_count} file{s} {unchanged}')
        if self.failure_count:
            s = 's' if self.failure_count > 1 else ''
            report.append(
                click.style(f'{self.failure_count} file{s} {failed}', fg='red')
            )
        return ', '.join(report) + '.'

def assert_equivalent(src: str, dst: str) -> None:
    """"""Raises AssertionError if `src` and `dst` aren't equivalent.
    This is a temporary sanity check until Black becomes stable.
    """"""
    import ast
    import traceback
    def _v(node: ast.AST, depth: int = 0) -> Iterator[str]:
        """"""Simple visitor generating strings to compare ASTs by content.""""""
        yield f""{'  ' * depth}{node.__class__.__name__}(""
        for field in sorted(node._fields):
            try:
                value = getattr(node, field)
            except AttributeError:
                continue
            yield f""{'  ' * (depth+1)}{field}=""
            if isinstance(value, list):
                for item in value:
                    if isinstance(item, ast.AST):
                        yield from _v(item, depth + 2)
            elif isinstance(value, ast.AST):
                yield from _v(value, depth + 2)
            else:
                yield f""{'  ' * (depth+2)}{value!r},  # {value.__class__.__name__}""
        yield f""{'  ' * depth})  # /{node.__class__.__name__}""
    try:
        src_ast = ast.parse(src)
    except Exception as exc:
        raise AssertionError(f""cannot parse source: {exc}"") from None
    try:
        dst_ast = ast.parse(dst)
    except Exception as exc:
        log = dump_to_file(''.join(traceback.format_tb(exc.__traceback__)), dst)
        raise AssertionError(
            f""INTERNAL ERROR: Black produced invalid code: {exc}. ""
            f""Please report a bug on https://github.com/ambv/black/issues.  ""
            f""This invalid output might be helpful: {log}""
        ) from None
    src_ast_str = '\n'.join(_v(src_ast))
    dst_ast_str = '\n'.join(_v(dst_ast))
    if src_ast_str != dst_ast_str:
        log = dump_to_file(diff(src_ast_str, dst_ast_str, 'src', 'dst'))
        raise AssertionError(
            f""INTERNAL ERROR: Black produced code that is not equivalent to ""
            f""the source.  ""
            f""Please report a bug on https://github.com/ambv/black/issues.  ""
            f""This diff might be helpful: {log}""
        ) from None

def assert_stable(src: str, dst: str, line_length: int) -> None:
    """"""Raises AssertionError if `dst` reformats differently the second time.
    This is a temporary sanity check until Black becomes stable.
    """"""
    newdst = format_str(dst, line_length=line_length)
    if dst != newdst:
        log = dump_to_file(
            diff(src, dst, 'source', 'first pass'),
            diff(dst, newdst, 'first pass', 'second pass'),
        )
        raise AssertionError(
            f""INTERNAL ERROR: Black produced different code on the second pass ""
            f""of the formatter.  ""
            f""Please report a bug on https://github.com/ambv/black/issues.  ""
            f""This diff might be helpful: {log}""
        ) from None

def dump_to_file(*output: str) -> str:
    """"""Dumps `output` to a temporary file. Returns path to the file.""""""
    import tempfile
    with tempfile.NamedTemporaryFile(
        mode='w', prefix='blk_', suffix='.log', delete=False
    ) as f:
        for lines in output:
            f.write(lines)
            f.write('\n')
    return f.name

def diff(a: str, b: str, a_name: str, b_name: str) -> str:
    """"""Returns a udiff string between strings `a` and `b`.""""""
    import difflib
    a_lines = [line + '\n' for line in a.split('\n')]
    b_lines = [line + '\n' for line in b.split('\n')]
    return ''.join(
        difflib.unified_diff(a_lines, b_lines, fromfile=a_name, tofile=b_name, n=5)
    )

if __name__ == '__main__':",[58]
"        right_mask = notna(self.right)
        if not (left_mask == right_mask).all():
            msg = (
                ""missing values must be missing in the same ""
                ""location both left and right sides""
            )
            raise ValueError(msg)
        if not (self.left[left_mask] <= self.right[left_mask]).all():
            msg = ""left side of interval must be <= right side""
            raise ValueError(msg)
    # ---------
    # Interface
    # ---------
    def __iter__(self):
        return iter(np.asarray(self))
    def __len__(self) -> int:
        return len(self.left)
    def __getitem__(self, value):
        value = check_array_indexer(self, value)
        left = self.left[value]
        right = self.right[value]
        # scalar
        if not isinstance(left, ABCIndexClass):
            if is_scalar(left) and isna(left):
                return self._fill_value
            if np.ndim(left) > 1:
                # GH#30588 multi-dimensional indexer disallowed
                raise ValueError(""multi-dimensional indexing not allowed"")
            return Interval(left, right, self.closed)
        return self._shallow_copy(left, right)
    def __setitem__(self, key, value):
        # na value: need special casing to set directly on numpy arrays
        needs_float_conversion = False
        if is_scalar(value) and isna(value):
            if is_integer_dtype(self.dtype.subtype):
                # can't set NaN on a numpy integer array
                needs_float_conversion = True
            elif is_datetime64_any_dtype(self.dtype.subtype):
                # need proper NaT to set directly on the numpy array
                value = np.datetime64(""NaT"")
            elif is_timedelta64_dtype(self.dtype.subtype):
                # need proper NaT to set directly on the numpy array
                value = np.timedelta64(""NaT"")
            value_left, value_right = value, value
        # scalar interval
        elif is_interval_dtype(value) or isinstance(value, Interval):
            self._check_closed_matches(value, name=""value"")
            value_left, value_right = value.left, value.right
        else:
            # list-like of intervals
            try:
                array = IntervalArray(value)
                value_left, value_right = array.left, array.right
            except TypeError as err:
                # wrong type: not interval or NA
                msg = f""'value' should be an interval type, got {type(value)} instead.""
                raise TypeError(msg) from err
        key = check_array_indexer(self, key)
        # Need to ensure that left and right are updated atomically, so we're
        # forced to copy, update the copy, and swap in the new values.
        left = self.left.copy(deep=True)
        if needs_float_conversion:
            left = left.astype(""float"")
        left.values[key] = value_left
        self._left = left
        right = self.right.copy(deep=True)
        if needs_float_conversion:
            right = right.astype(""float"")
        right.values[key] = value_right
        self._right = right
    def __eq__(self, other):
        # ensure pandas array for list-like and eliminate non-interval scalars
        if is_list_like(other):
            if len(self) != len(other):
                raise ValueError(""Lengths must match to compare"")
            other = array(other)
        elif not isinstance(other, Interval):
            # non-interval scalar -> no matches
            return np.zeros(len(self), dtype=bool)
        # determine the dtype of the elements we want to compare
        if isinstance(other, Interval):
            other_dtype = ""interval""
        elif not is_categorical_dtype(other):
            other_dtype = other.dtype
        else:
            # for categorical defer to categories for dtype
            other_dtype = other.categories.dtype
            # extract intervals if we have interval categories with matching closed
            if is_interval_dtype(other_dtype):
                if self.closed != other.categories.closed:
                    return np.zeros(len(self), dtype=bool)
                other = other.categories.take(other.codes)
        # interval-like -> need same closed and matching endpoints
        if is_interval_dtype(other_dtype):
            if self.closed != other.closed:
                return np.zeros(len(self), dtype=bool)
            return (self.left == other.left) & (self.right == other.right)
        # non-interval/non-object dtype -> no matches
        if not is_object_dtype(other_dtype):
            return np.zeros(len(self), dtype=bool)
        # object dtype -> iteratively check for intervals
        result = np.zeros(len(self), dtype=bool)
        for i, obj in enumerate(other):
            # need object to be an Interval with same closed and endpoints
            if (
                isinstance(obj, Interval)
                and self.closed == obj.closed
                and self.left[i] == obj.left
                and self.right[i] == obj.right
            ):
                result[i] = True","[70, 71, 72, 76, 77, 78]"
"    # This was originally implemented for ComedyCentral, but it also works here
    @staticmethod
    def _transform_rtmp_url(rtmp_video_url):
        m = re.match(r'^rtmpe?://.*?/(?P<finalid>gsp\..+?/.*)$', rtmp_video_url)
        if not m:
            return rtmp_video_url
        base = 'http://mtvnmobile.vo.llnwd.net/kip0/_pxn=1+_pxI0=Ripod-h264+_pxL0=undefined+_pxM0=+_pxK=18639+_pxE=mp4/44620/mtvnorigin/'
        return base + m.group('finalid')
    def _get_thumbnail_url(self, uri, itemdoc):
        search_path = '%s/%s' % (_media_xml_tag('group'), _media_xml_tag('thumbnail'))
        thumb_node = itemdoc.find(search_path)
        if thumb_node is None:
            return None
        else:
            return thumb_node.attrib['url']
    def _extract_video_formats(self, metadataXml):
        if '/error_country_block.swf' in metadataXml:
            raise ExtractorError(u'This video is not available from your country.', expected=True)
        mdoc = xml.etree.ElementTree.fromstring(metadataXml.encode('utf-8'))
        formats = []
        for rendition in mdoc.findall('.//rendition'):
            try:
                _, _, ext = rendition.attrib['type'].partition('/')
                rtmp_video_url = rendition.find('./src').text
                formats.append({'ext': ext,
                                'url': self._transform_rtmp_url(rtmp_video_url),
                                'format_id': rendition.get('bitrate'),
                                'width': int(rendition.get('width')),
                                'height': int(rendition.get('height')),
                                })
            except (KeyError, TypeError):
                raise ExtractorError('Invalid rendition field.')
        return formats
    def _get_video_info(self, itemdoc):
        uri = itemdoc.find('guid').text
        video_id = self._id_from_uri(uri)
        self.report_extraction(video_id)
        mediagen_url = itemdoc.find('%s/%s' % (_media_xml_tag('group'), _media_xml_tag('content'))).attrib['url']
        # Remove the templates, like &device={device}
        mediagen_url = re.sub(r'&[^=]*?={.*?}(?=(&|$))', u'', mediagen_url)
        if 'acceptMethods' not in mediagen_url:
            mediagen_url += '&acceptMethods=fms'
        mediagen_page = self._download_webpage(mediagen_url, video_id,
                                               u'Downloading video urls')
        description_node = itemdoc.find('description')
        if description_node is not None:
            description = description_node.text.strip()
        else:
            description = None
        return {
            'title': itemdoc.find('title').text,
            'formats': self._extract_video_formats(mediagen_page),
            'id': video_id,
            'thumbnail': self._get_thumbnail_url(uri, itemdoc),
            'description': description,
        }
    def _get_videos_info(self, uri):
        video_id = self._id_from_uri(uri)
        data = compat_urllib_parse.urlencode({'uri': uri})
        def fix_ampersand(s):
            """""" Fix unencoded ampersand in XML """"""
            return s.replace(u'& ', '&amp; ')
        idoc = self._download_xml(
            self._FEED_URL + '?' + data, video_id,
            u'Downloading info', transform_source=fix_ampersand)
        return [self._get_video_info(item) for item in idoc.findall('.//item')]

class MTVIE(MTVServicesInfoExtractor):
    _VALID_URL = r'''(?x)^https?://
        (?:(?:www\.)?mtv\.com/videos/.+?/(?P<videoid>[0-9]+)/[^/]+$|
           m\.mtv\.com/videos/video\.rbml\?.*?id=(?P<mgid>[^&]+))'''
    _FEED_URL = 'http://www.mtv.com/player/embed/AS3/rss/'
    _TESTS = [
        {
            u'url': u'http://www.mtv.com/videos/misc/853555/ours-vh1-storytellers.jhtml',
            u'file': u'853555.mp4',
            u'md5': u'850f3f143316b1e71fa56a4edfd6e0f8',
            u'info_dict': {
                u'title': u'Taylor Swift - ""Ours (VH1 Storytellers)""',
                u'description': u'Album: Taylor Swift performs ""Ours"" for VH1 Storytellers at Harvey Mudd College.',
            },
        },
        {
            u'add_ie': ['Vevo'],
            u'url': u'http://www.mtv.com/videos/taylor-swift/916187/everything-has-changed-ft-ed-sheeran.jhtml',
            u'file': u'USCJY1331283.mp4',
            u'md5': u'73b4e7fcadd88929292fe52c3ced8caf',
            u'info_dict': {
                u'title': u'Everything Has Changed',
                u'upload_date': u'20130606',
                u'uploader': u'Taylor Swift',
            },
            u'skip': u'VEVO is only available in some countries',
        },
    ]
    def _get_thumbnail_url(self, uri, itemdoc):
        return 'http://mtv.mtvnimages.com/uri/' + uri
    def _real_extract(self, url):
        mobj = re.match(self._VALID_URL, url)
        video_id = mobj.group('videoid')
        uri = mobj.groupdict().get('mgid')
        if uri is None:
            webpage = self._download_webpage(url, video_id)
    
            # Some videos come from Vevo.com
            m_vevo = re.search(r'isVevoVideo = true;.*?vevoVideoId = ""(.*?)"";',
                               webpage, re.DOTALL)
            if m_vevo:
                vevo_id = m_vevo.group(1);
                self.to_screen(u'Vevo video detected: %s' % vevo_id)
                return self.url_result('vevo:%s' % vevo_id, ie='Vevo')
    
            uri = self._html_search_regex(r'/uri/(.*?)\?', webpage, u'uri')","[67, 69, 72]"
"        dtype: float64
        Empty strings are not considered NA values. ``None`` is considered an
        NA value.
        >>> ser = pd.Series([np.NaN, 2, pd.NaT, '', None, 'I stay'])
        >>> ser
        0       NaN
        1         2
        2       NaT
        3
        4      None
        5    I stay
        dtype: object
        >>> ser.dropna()
        1         2
        3
        5    I stay
        dtype: object
        """"""
        inplace = validate_bool_kwarg(inplace, ""inplace"")
        # Validate the axis parameter
        self._get_axis_number(axis or 0)
        if self._can_hold_na:
            result = remove_na_arraylike(self)
            if inplace:
                self._update_inplace(result)
            else:
                return result
        else:
            if inplace:
                # do nothing
                pass
            else:
                return self.copy()
    # ----------------------------------------------------------------------
    # Time series-oriented methods
    def to_timestamp(self, freq=None, how=""start"", copy=True) -> ""Series"":
        """"""
        Cast to DatetimeIndex of Timestamps, at *beginning* of period.
        Parameters
        ----------
        freq : str, default frequency of PeriodIndex
            Desired frequency.
        how : {'s', 'e', 'start', 'end'}
            Convention for converting period to timestamp; start of period
            vs. end.
        copy : bool, default True
            Whether or not to return a copy.
        Returns
        -------
        Series with DatetimeIndex
        """"""
        new_values = self._values
        if copy:
            new_values = new_values.copy()
        assert isinstance(self.index, PeriodIndex)
        new_index = self.index.to_timestamp(freq=freq, how=how)  # type: ignore
        return self._constructor(new_values, index=new_index).__finalize__(
            self, method=""to_timestamp""
        )
    def to_period(self, freq=None, copy=True) -> ""Series"":
        """"""
        Convert Series from DatetimeIndex to PeriodIndex with desired
        frequency (inferred from index if not passed).
        Parameters
        ----------
        freq : str, default None
            Frequency associated with the PeriodIndex.
        copy : bool, default True
            Whether or not to return a copy.
        Returns
        -------
        Series
            Series with index converted to PeriodIndex.
        """"""
        new_values = self._values
        if copy:
            new_values = new_values.copy()
        assert isinstance(self.index, DatetimeIndex)
        new_index = self.index.to_period(freq=freq)  # type: ignore
        return self._constructor(new_values, index=new_index).__finalize__(
            self, method=""to_period""
        )
    # ----------------------------------------------------------------------
    # Add index
    _AXIS_ORDERS = [""index""]
    _AXIS_REVERSED = False
    _AXIS_LEN = len(_AXIS_ORDERS)
    _info_axis_number = 0
    _info_axis_name = ""index""
    index: ""Index"" = properties.AxisProperty(
        axis=0, doc=""The index (axis labels) of the Series.""
    )
    # ----------------------------------------------------------------------
    # Accessor Methods
    # ----------------------------------------------------------------------
    str = CachedAccessor(""str"", StringMethods)
    dt = CachedAccessor(""dt"", CombinedDatetimelikeProperties)
    cat = CachedAccessor(""cat"", CategoricalAccessor)
    plot = CachedAccessor(""plot"", pandas.plotting.PlotAccessor)
    sparse = CachedAccessor(""sparse"", SparseAccessor)
    # ----------------------------------------------------------------------
    # Add plotting methods to Series
    hist = pandas.plotting.hist_series

Series._add_numeric_operations()
Series._add_series_or_dataframe_operations()
# Add arithmetic!
ops.add_flex_arithmetic_methods(Series)","[62, 89]"
"import re
from thefuck.shells import shell
from thefuck.specific.git import git_support
from thefuck.utils import eager

@git_support
def match(command):
    return ('branch' in command.script
            and ""fatal: A branch named '"" in command.stderr
            and "" already exists."" in command.stderr)

@git_support
@eager
def get_new_command(command):
    branch_name = re.findall(
        r""fatal: A branch named '([^']*)' already exists."", command.stderr)[0]
    new_command_templates = [['git branch -d {0}', 'git branch {0}'],
                             ['git branch -D {0}', 'git branch {0}'],
                             ['git checkout {0}']]
    for new_command_template in new_command_templates:
        yield shell.and_(*new_command_template).format(branch_name)","[8, 9]"
"                    self.priority, self.requires_output) \
                   == (other.name, other.match, other.get_new_command,
                       other.enabled_by_default, other.side_effect,
                       other.priority, other.requires_output)
        else:
            return False
    def __repr__(self):
        return 'Rule(name={}, match={}, get_new_command={}, ' \
               'enabled_by_default={}, side_effect={}, ' \
               'priority={}, requires_output)'.format(
                self.name, self.match, self.get_new_command,
                self.enabled_by_default, self.side_effect,
                self.priority, self.requires_output)
    @classmethod
    def from_path(cls, path):
        """"""Creates rule instance from path.
        :type path: pathlib.Path
        :rtype: Rule
        """"""
        name = path.name[:-3]
        with logs.debug_time(u'Importing rule: {};'.format(name)):
            rule_module = load_source(name, str(path))
            priority = getattr(rule_module, 'priority', DEFAULT_PRIORITY)
        return cls(name, rule_module.match,
                   rule_module.get_new_command,
                   getattr(rule_module, 'enabled_by_default', True),
                   getattr(rule_module, 'side_effect', None),
                   settings.priority.get(name, priority),
                   getattr(rule_module, 'requires_output', True))
    @property
    def is_enabled(self):
        """"""Returns `True` when rule enabled.
        :rtype: bool
        """"""
        if self.name in settings.exclude_rules:
            return False
        elif self.name in settings.rules:
            return True
        elif self.enabled_by_default and ALL_ENABLED in settings.rules:
            return True
        else:
            return False
    def is_match(self, command):
        """"""Returns `True` if rule matches the command.
        :type command: Command
        :rtype: bool
        """"""
        script_only = command.stdout is None and command.stderr is None
        if script_only and self.requires_output:
            return False
        try:
            with logs.debug_time(u'Trying rule: {};'.format(self.name)):
                if compatibility_call(self.match, command):
                    return True
        except Exception:
            logs.rule_failed(self, sys.exc_info())
    def get_corrected_commands(self, command):
        """"""Returns generator with corrected commands.
        :type command: Command
        :rtype: Iterable[CorrectedCommand]
        """"""
        new_commands = compatibility_call(self.get_new_command, command)
        if not isinstance(new_commands, list):
            new_commands = (new_commands,)
        for n, new_command in enumerate(new_commands):
            yield CorrectedCommand(script=new_command,
                                   side_effect=self.side_effect,
                                   priority=(n + 1) * self.priority)

class CorrectedCommand(object):
    """"""Corrected by rule command.""""""
    def __init__(self, script, side_effect, priority):
        """"""Initializes instance with given fields.
        :type script: basestring
        :type side_effect: (Command, basestring) -> None
        :type priority: int
        """"""
        self.script = script
        self.side_effect = side_effect
        self.priority = priority
    def __eq__(self, other):
        """"""Ignores `priority` field.""""""
        if isinstance(other, CorrectedCommand):
            return (other.script, other.side_effect) == \
                   (self.script, self.side_effect)
        else:
            return False
    def __hash__(self):
        return (self.script, self.side_effect).__hash__()
    def __repr__(self):
        return u'CorrectedCommand(script={}, side_effect={}, priority={})'.format(
                self.script, self.side_effect, self.priority)
    def run(self, old_cmd):
        """"""Runs command from rule for passed command.
        :type old_cmd: Command
        """"""
        if self.side_effect:
            compatibility_call(self.side_effect, old_cmd, self.script)
        # This depends on correct setting of PYTHONIOENCODING by the alias:
        logs.debug(u'PYTHONIOENCODING: {}'.format(
            os.environ.get('PYTHONIOENCODING', '>-not-set-<')))",[125]
"                ""specifying a limit for fillna has not been implemented yet""
            )
        codes = self._codes
        # pad / bfill
        if method is not None:
            values = self.to_dense().reshape(-1, len(self))
            values = interpolate_2d(values, method, 0, None, value).astype(
                self.categories.dtype
            )[0]
            codes = _get_codes_for_values(values, self.categories)
        else:
            # If value is a dict or a Series (a dict value has already
            # been converted to a Series)
            if isinstance(value, ABCSeries):
                if not value[~value.isin(self.categories)].isna().all():
                    raise ValueError(""fill value must be in categories"")
                values_codes = _get_codes_for_values(value, self.categories)
                indexer = np.where(values_codes != -1)
                codes[indexer] = values_codes[values_codes != -1]
            # If value is not a dict or Series it should be a scalar
            elif is_hashable(value):
                if not isna(value) and value not in self.categories:
                    raise ValueError(""fill value must be in categories"")
                mask = codes == -1
                if mask.any():
                    codes = codes.copy()
                    if isna(value):
                        codes[mask] = -1
                    else:
                        codes[mask] = self.categories.get_loc(value)
            else:
                raise TypeError(
                    '""value"" parameter must be a scalar, dict '
                    ""or Series, but you passed a ""
                    '""{0}""'.format(type(value).__name__)
                )
        return self._constructor(codes, dtype=self.dtype, fastpath=True)
    def take_nd(self, indexer, allow_fill=None, fill_value=None):
        """"""
        Take elements from the Categorical.
        Parameters
        ----------
        indexer : sequence of int
            The indices in `self` to take. The meaning of negative values in
            `indexer` depends on the value of `allow_fill`.
        allow_fill : bool, default None
            How to handle negative values in `indexer`.
            * False: negative values in `indices` indicate positional indices
              from the right. This is similar to
              :func:`numpy.take`.
            * True: negative values in `indices` indicate missing values
              (the default). These values are set to `fill_value`. Any other
              other negative values raise a ``ValueError``.
            .. versionchanged:: 0.23.0
               Deprecated the default value of `allow_fill`. The deprecated
               default is ``True``. In the future, this will change to
               ``False``.
        fill_value : object
            The value to use for `indices` that are missing (-1), when
            ``allow_fill=True``. This should be the category, i.e. a value
            in ``self.categories``, not a code.
        Returns
        -------
        Categorical
            This Categorical will have the same categories and ordered as
            `self`.
        See Also
        --------
        Series.take : Similar method for Series.
        numpy.ndarray.take : Similar method for NumPy arrays.
        Examples
        --------
        >>> cat = pd.Categorical(['a', 'a', 'b'])
        >>> cat
        [a, a, b]
        Categories (2, object): [a, b]
        Specify ``allow_fill==False`` to have negative indices mean indexing
        from the right.
        >>> cat.take([0, -1, -2], allow_fill=False)
        [a, b, a]
        Categories (2, object): [a, b]
        With ``allow_fill=True``, indices equal to ``-1`` mean ""missing""
        values that should be filled with the `fill_value`, which is
        ``np.nan`` by default.
        >>> cat.take([0, -1, -1], allow_fill=True)
        [a, NaN, NaN]
        Categories (2, object): [a, b]
        The fill value can be specified.
        >>> cat.take([0, -1, -1], allow_fill=True, fill_value='a')
        [a, a, a]
        Categories (3, object): [a, b]
        Specifying a fill value that's not in ``self.categories``
        will raise a ``TypeError``.
        """"""
        indexer = np.asarray(indexer, dtype=np.intp)
        if allow_fill is None:
            if (indexer < 0).any():
                warn(_take_msg, FutureWarning, stacklevel=2)
                allow_fill = True
","[23, 24]"
"        self._paths = [mpath.Path(_seg) for _seg in _segments]
        self.stale = True
    set_verts = set_segments  # for compatibility with PolyCollection
    set_paths = set_segments
    def get_segments(self):
        """"""
        Returns
        -------
        segments : list
            List of segments in the LineCollection. Each list item contains an
            array of vertices.
        """"""
        segments = []
        for path in self._paths:
            vertices = [vertex for vertex, _ in path.iter_segments()]
            vertices = np.asarray(vertices)
            segments.append(vertices)
        return segments
    def _add_offsets(self, segs):
        offsets = self._uniform_offsets
        Nsegs = len(segs)
        Noffs = offsets.shape[0]
        if Noffs == 1:
            for i in range(Nsegs):
                segs[i] = segs[i] + i * offsets
        else:
            for i in range(Nsegs):
                io = i % Noffs
                segs[i] = segs[i] + offsets[io:io + 1]
        return segs
    def set_color(self, c):
        """"""
        Set the color(s) of the LineCollection.
        Parameters
        ----------
        c : color or list of colors
            Matplotlib color argument (all patches have same color), or a
            sequence or rgba tuples; if it is a sequence the patches will
            cycle through the sequence.
        """"""
        self.set_edgecolor(c)
        self.stale = True
    def get_color(self):
        return self._edgecolors
    get_colors = get_color  # for compatibility with old versions

class EventCollection(LineCollection):
    """"""
    A collection of discrete events.
    The events are given by a 1-dimensional array, usually the position of
    something along an axis, such as time or length.  They do not have an
    amplitude and are displayed as vertical or horizontal parallel bars.
    """"""
    _edge_default = True
    def __init__(self,
                 positions,     # Cannot be None.
                 orientation=None,
                 lineoffset=0,
                 linelength=1,
                 linewidth=None,
                 color=None,
                 linestyle='solid',
                 antialiased=None,
                 **kwargs
                 ):
        """"""
        Parameters
        ----------
        positions : 1D array-like object
            Each value is an event.
        orientation : {None, 'horizontal', 'vertical'}, optional
            The orientation of the **collection** (the event bars are along
            the orthogonal direction). Defaults to 'horizontal' if not
            specified or None.
        lineoffset : scalar, optional, default: 0
            The offset of the center of the markers from the origin, in the
            direction orthogonal to *orientation*.
        linelength : scalar, optional, default: 1
            The total height of the marker (i.e. the marker stretches from
            ``lineoffset - linelength/2`` to ``lineoffset + linelength/2``).
        linewidth : scalar or None, optional, default: None
            If it is None, defaults to its rcParams setting, in sequence form.
        color : color, sequence of colors or None, optional, default: None
            If it is None, defaults to its rcParams setting, in sequence form.
        linestyle : str or tuple, optional, default: 'solid'
            Valid strings are ['solid', 'dashed', 'dashdot', 'dotted',
            '-', '--', '-.', ':']. Dash tuples should be of the form::
                (offset, onoffseq),
            where *onoffseq* is an even length tuple of on and off ink
            in points.
        antialiased : {None, 1, 2}, optional
            If it is None, defaults to its rcParams setting, in sequence form.
        **kwargs : optional
            Other keyword arguments are line collection properties.  See
            :class:`~matplotlib.collections.LineCollection` for a list of
            the valid properties.
        Examples
        --------
        .. plot:: gallery/lines_bars_and_markers/eventcollection_demo.py
        """"""
        segment = (lineoffset + linelength / 2.,",[125]
"# Copyright: (c) 2013, James Cammarata <jcammarata@ansible.com>
# Copyright: (c) 2018, Ansible Project
# GNU General Public License v3.0+ (see COPYING or https://www.gnu.org/licenses/gpl-3.0.txt)
from __future__ import (absolute_import, division, print_function)
__metaclass__ = type
import os.path
import re
import shutil
import textwrap
import time
import yaml
from jinja2 import BaseLoader, Environment, FileSystemLoader
import ansible.constants as C
from ansible import context
from ansible.cli import CLI
from ansible.cli.arguments import option_helpers as opt_help
from ansible.errors import AnsibleError, AnsibleOptionsError
from ansible.galaxy import Galaxy, get_collections_galaxy_meta_info
from ansible.galaxy.api import GalaxyAPI
from ansible.galaxy.collection import build_collection, install_collections, parse_collections_requirements_file, \
    publish_collection
from ansible.galaxy.login import GalaxyLogin
from ansible.galaxy.role import GalaxyRole
from ansible.galaxy.token import GalaxyToken
from ansible.module_utils.ansible_release import __version__ as ansible_version
from ansible.module_utils._text import to_bytes, to_native, to_text
from ansible.playbook.role.requirement import RoleRequirement
from ansible.utils.collection_loader import is_collection_ref
from ansible.utils.display import Display
from ansible.utils.plugin_docs import get_versioned_doclink
display = Display()

class GalaxyCLI(CLI):
    '''command to manage Ansible roles in shared repositories, the default of which is Ansible Galaxy *https://galaxy.ansible.com*.'''
    SKIP_INFO_KEYS = (""name"", ""description"", ""readme_html"", ""related"", ""summary_fields"", ""average_aw_composite"", ""average_aw_score"", ""url"")
    def __init__(self, args):
        # Inject role into sys.argv[1] as a backwards compatibility step
        if len(args) > 1 and args[1] not in ['-h', '--help'] and 'role' not in args and 'collection' not in args:
            # TODO: Should we add a warning here and eventually deprecate the implicit role subcommand choice
            args.insert(1, 'role')
        self.api = None
        self.galaxy = None
        super(GalaxyCLI, self).__init__(args)
    def init_parser(self):
        ''' create an options parser for bin/ansible '''
        super(GalaxyCLI, self).init_parser(
            desc=""Perform various Role related operations."",
        )
        # common
        common = opt_help.argparse.ArgumentParser(add_help=False)
        common.add_argument('-s', '--server', dest='api_server', default=C.GALAXY_SERVER, help='The API server destination')
        common.add_argument('-c', '--ignore-certs', action='store_true', dest='ignore_certs', default=C.GALAXY_IGNORE_CERTS,
                            help='Ignore SSL certificate validation errors.')
        opt_help.add_verbosity_options(common)
        # options that apply to more than one action
        user_repo = opt_help.argparse.ArgumentParser(add_help=False)
        user_repo.add_argument('github_user', help='GitHub username')
        user_repo.add_argument('github_repo', help='GitHub repository')
        offline = opt_help.argparse.ArgumentParser(add_help=False)
        offline.add_argument('--offline', dest='offline', default=False, action='store_true',
                             help=""Don't query the galaxy API when creating roles"")
        default_roles_path = C.config.get_configuration_definition('DEFAULT_ROLES_PATH').get('default', '')
        roles_path = opt_help.argparse.ArgumentParser(add_help=False)
        roles_path.add_argument('-p', '--roles-path', dest='roles_path', type=opt_help.unfrack_path(pathsep=True),
                                default=C.DEFAULT_ROLES_PATH, action=opt_help.PrependListAction,
                                help='The path to the directory containing your roles. The default is the first writable one'
                                     'configured via DEFAULT_ROLES_PATH: %s ' % default_roles_path)
        force = opt_help.argparse.ArgumentParser(add_help=False)
        force.add_argument('-f', '--force', dest='force', action='store_true', default=False,
                           help='Force overwriting an existing role or collection')
        # Add sub parser for the Galaxy role type (role or collection)
        type_parser = self.parser.add_subparsers(metavar='TYPE', dest='type')
        type_parser.required = True
        # Define the actions for the collection object type
        collection = type_parser.add_parser('collection',
                                            parents=[common],
                                            help='Manage an Ansible Galaxy collection.')
        collection_parser = collection.add_subparsers(metavar='ACTION', dest='collection')
        collection_parser.required = True
        build_parser = collection_parser.add_parser(
            'build', help='Build an Ansible collection artifact that can be published to Ansible Galaxy.',
            parents=[common, force])
        build_parser.set_defaults(func=self.execute_build)
        build_parser.add_argument(
            'args', metavar='collection', nargs='*', default=('./',),
            help='Path to the collection(s) directory to build. This should be the directory that contains the '
                 'galaxy.yml file. The default is the current working directory.')
        build_parser.add_argument(
            '--output-path', dest='output_path', default='./',
            help='The path in which the collection is built to. The default is the current working directory.')
        self.add_init_parser(collection_parser, [common, force])
        cinstall_parser = collection_parser.add_parser('install', help='Install collection from Ansible Galaxy',
                                                       parents=[force, common])
        cinstall_parser.set_defaults(func=self.execute_install)
        cinstall_parser.add_argument('args', metavar='collection_name', nargs='*',
                                     help='The collection(s) name or path/url to a tar.gz collection artifact. This '
                                          'is mutually exclusive with --requirements-file.')
        cinstall_parser.add_argument('-p', '--collections-path', dest='collections_path', required=True,
                                     help='The path to the directory containing your collections.')
        cinstall_parser.add_argument('-i', '--ignore-errors', dest='ignore_errors', action='store_true', default=False,
                                     help='Ignore errors during installation and continue with the next specified '
                                          'collection. This will not ignore dependency conflict errors.')
        cinstall_parser.add_argument('-r', '--requirements-file', dest='requirements',
                                     help='A file containing a list of collections to be installed.')",[57]
"from thefuck import shells

def match(command, settings):
    return command.script.startswith('vagrant ') and 'run `vagrant up`' in command.stderr.lower()

def get_new_command(command, settings):
    cmds = command.script.split(' ')
    machine = """"
    if len(cmds) >= 3:
        machine = cmds[2]
    return shells.and_(""vagrant up "" +  machine, command.script)",[9]
"""""""
Offsite Spider Middleware
See documentation in docs/topics/spider-middleware.rst
""""""
import re
import logging
import warnings
from scrapy import signals
from scrapy.http import Request
from scrapy.utils.httpobj import urlparse_cached
logger = logging.getLogger(__name__)

class OffsiteMiddleware(object):
    def __init__(self, stats):
        self.stats = stats
    @classmethod
    def from_crawler(cls, crawler):
        o = cls(crawler.stats)
        crawler.signals.connect(o.spider_opened, signal=signals.spider_opened)
        return o
    def process_spider_output(self, response, result, spider):
        for x in result:
            if isinstance(x, Request):
                if x.dont_filter or self.should_follow(x, spider):
                    yield x
                else:
                    domain = urlparse_cached(x).hostname
                    if domain and domain not in self.domains_seen:
                        self.domains_seen.add(domain)
                        logger.debug(
                            ""Filtered offsite request to %(domain)r: %(request)s"",
                            {'domain': domain, 'request': x}, extra={'spider': spider})
                        self.stats.inc_value('offsite/domains', spider=spider)
                    self.stats.inc_value('offsite/filtered', spider=spider)
            else:
                yield x
    def should_follow(self, request, spider):
        regex = self.host_regex
        # hostname can be None for wrong urls (like javascript links)
        host = urlparse_cached(request).hostname or ''
        return bool(regex.search(host))
    def get_host_regex(self, spider):
        """"""Override this method to implement a different offsite policy""""""
        allowed_domains = getattr(spider, 'allowed_domains', None)
        if not allowed_domains:
            return re.compile('')  # allow all by default
        url_pattern = re.compile(""^https?://.*$"")
        for domain in allowed_domains:
            if url_pattern.match(domain):
                message = (""allowed_domains accepts only domains, not URLs. ""
                           ""Ignoring URL entry %s in allowed_domains."" % domain)
                warnings.warn(message, URLWarning)
        domains = [re.escape(d) for d in allowed_domains if d is not None]
        regex = r'^(.*\.)?(%s)$' % '|'.join(domains)
        return re.compile(regex)
    def spider_opened(self, spider):
        self.host_regex = self.get_host_regex(spider)
        self.domains_seen = set()

class URLWarning(Warning):
    pass","[57, 61]"
"        for task in self._state.get_active_tasks():
            self._state.fail_dead_worker_task(task, self._config, assistant_ids)
            self._state.update_status(task, self._config)
            if self._state.may_prune(task):
                logger.info(""Removing task %r"", task.id)
                remove_tasks.append(task.id)
        self._state.inactivate_tasks(remove_tasks)
    def _prune_emails(self):
        if self._config.batch_emails:
            self._email_batcher.update()
    def _update_worker(self, worker_id, worker_reference=None, get_work=False):
        # Keep track of whenever the worker was last active.
        # For convenience also return the worker object.
        worker = self._state.get_worker(worker_id)
        worker.update(worker_reference, get_work=get_work)
        return worker
    def _update_priority(self, task, prio, worker):
        """"""
        Update priority of the given task.
        Priority can only be increased.
        If the task doesn't exist, a placeholder task is created to preserve priority when the task is later scheduled.
        """"""
        task.priority = prio = max(prio, task.priority)
        for dep in task.deps or []:
            t = self._state.get_task(dep)
            if t is not None and prio > t.priority:
                self._update_priority(t, prio, worker)
    @rpc_method()
    def add_task_batcher(self, worker, task_family, batched_args, max_batch_size=float('inf')):
        self._state.set_batcher(worker, task_family, batched_args, max_batch_size)
    @rpc_method()
    def forgive_failures(self, task_id=None):
        status = PENDING
        task = self._state.get_task(task_id)
        if task is None:
            return {""task_id"": task_id, ""status"": None}
        # we forgive only failures
        if task.status == FAILED:
            # forgive but do not forget
            self._update_task_history(task, status)
            self._state.set_status(task, status, self._config)
        return {""task_id"": task_id, ""status"": task.status}
    @rpc_method()
    def add_task(self, task_id=None, status=PENDING, runnable=True,
                 deps=None, new_deps=None, expl=None, resources=None,
                 priority=0, family='', module=None, params=None,
                 assistant=False, tracking_url=None, worker=None, batchable=None,
                 batch_id=None, retry_policy_dict={}, owners=None, **kwargs):
        """"""
        * add task identified by task_id if it doesn't exist
        * if deps is not None, update dependency list
        * update status of task
        * add additional workers/stakeholders
        * update priority when needed
        """"""
        assert worker is not None
        worker_id = worker
        worker = self._update_worker(worker_id)
        retry_policy = self._generate_retry_policy(retry_policy_dict)
        if worker.enabled:
            _default_task = self._make_task(
                task_id=task_id, status=PENDING, deps=deps, resources=resources,
                priority=priority, family=family, module=module, params=params,
            )
        else:
            _default_task = None
        task = self._state.get_task(task_id, setdefault=_default_task)
        if task is None or (task.status != RUNNING and not worker.enabled):
            return
        # for setting priority, we'll sometimes create tasks with unset family and params
        if not task.family:
            task.family = family
        if not getattr(task, 'module', None):
            task.module = module
        if not task.params:
            task.params = _get_default(params, {})
        if batch_id is not None:
            task.batch_id = batch_id
        if status == RUNNING and not task.worker_running:
            task.worker_running = worker_id
            if batch_id:
                task.resources_running = self._state.get_batch_running_tasks(batch_id)[0].resources_running
            task.time_running = time.time()
        if tracking_url is not None or task.status != RUNNING:
            task.tracking_url = tracking_url
            if task.batch_id is not None:
                for batch_task in self._state.get_batch_running_tasks(task.batch_id):
                    batch_task.tracking_url = tracking_url
        if batchable is not None:
            task.batchable = batchable
        if task.remove is not None:
            task.remove = None  # unmark task for removal so it isn't removed after being added
        if expl is not None:
            task.expl = expl
            if task.batch_id is not None:
                for batch_task in self._state.get_batch_running_tasks(task.batch_id):
                    batch_task.expl = expl
        if not (task.status in (RUNNING, BATCH_RUNNING) and status == PENDING) or new_deps:
            # don't allow re-scheduling of task while it is running, it must either fail or succeed first
            if status == PENDING or status != task.status:
                # Update the DB only if there was a acctual change, to prevent noise.
                # We also check for status == PENDING b/c that's the default value
                # (so checking for status != task.status woule lie)
                self._update_task_history(task, status)
            self._state.set_status(task, PENDING if status == SUSPENDED else status, self._config)
        if status == FAILED and self._config.batch_emails:",[117]
"        if not m:
            raise ValueError('Invalid filter specification %r' % filter_spec)
        def _filter(f):
            actual_value = f.get(m.group('key'))
            if actual_value is None:
                return m.group('none_inclusive')
            return op(actual_value, comparison_value)
        return _filter
    def build_format_selector(self, format_spec):
        def syntax_error(note, start):
            message = (
                'Invalid format specification: '
                '{0}\n\t{1}\n\t{2}^'.format(note, format_spec, ' ' * start[1]))
            return SyntaxError(message)
        PICKFIRST = 'PICKFIRST'
        MERGE = 'MERGE'
        SINGLE = 'SINGLE'
        GROUP = 'GROUP'
        FormatSelector = collections.namedtuple('FormatSelector', ['type', 'selector', 'filters'])
        def _parse_filter(tokens):
            filter_parts = []
            for type, string, start, _, _ in tokens:
                if type == tokenize.OP and string == ']':
                    return ''.join(filter_parts)
                else:
                    filter_parts.append(string)
        def _parse_format_selection(tokens, endwith=[]):
            selectors = []
            current_selector = None
            for type, string, start, _, _ in tokens:
                # ENCODING is only defined in python 3.x
                if type == getattr(tokenize, 'ENCODING', None):
                    continue
                elif type in [tokenize.NAME, tokenize.NUMBER]:
                    current_selector = FormatSelector(SINGLE, string, [])
                elif type == tokenize.OP:
                    if string in endwith:
                        break
                    elif string == ')':
                        # ')' will be handled by the parentheses group
                        tokens.restore_last_token()
                        break
                    if string == ',':
                        selectors.append(current_selector)
                        current_selector = None
                    elif string == '/':
                        first_choice = current_selector
                        second_choice = _parse_format_selection(tokens, [','])
                        current_selector = None
                        selectors.append(FormatSelector(PICKFIRST, (first_choice, second_choice), []))
                    elif string == '[':
                        if not current_selector:
                            current_selector = FormatSelector(SINGLE, 'best', [])
                        format_filter = _parse_filter(tokens)
                        current_selector.filters.append(format_filter)
                    elif string == '(':
                        if current_selector:
                            raise syntax_error('Unexpected ""(""', start)
                        current_selector = FormatSelector(GROUP, _parse_format_selection(tokens, [')']), [])
                    elif string == '+':
                        video_selector = current_selector
                        audio_selector = _parse_format_selection(tokens, [','])
                        current_selector = None
                        selectors.append(FormatSelector(MERGE, (video_selector, audio_selector), []))
                    else:
                        raise syntax_error('Operator not recognized: ""{0}""'.format(string), start)
                elif type == tokenize.ENDMARKER:
                    break
            if current_selector:
                selectors.append(current_selector)
            return selectors
        def _build_selector_function(selector):
            if isinstance(selector, list):
                fs = [_build_selector_function(s) for s in selector]
                def selector_function(formats):
                    for f in fs:
                        for format in f(formats):
                            yield format
                return selector_function
            elif selector.type == GROUP:
                selector_function = _build_selector_function(selector.selector)
            elif selector.type == PICKFIRST:
                fs = [_build_selector_function(s) for s in selector.selector]
                def selector_function(formats):
                    for f in fs:
                        picked_formats = list(f(formats))
                        if picked_formats:
                            return picked_formats
                    return []
            elif selector.type == SINGLE:
                format_spec = selector.selector
                def selector_function(formats):
                    if format_spec == 'all':
                        for f in formats:
                            yield f
                    elif format_spec in ['best', 'worst', None]:
                        format_idx = 0 if format_spec == 'worst' else -1
                        audiovideo_formats = [
                            f for f in formats
                            if f.get('vcodec') != 'none' and f.get('acodec') != 'none']
                        if audiovideo_formats:
                            yield audiovideo_formats[format_idx]
                        # for audio only (soundcloud) or video only (imgur) urls, select the best/worst audio format
                        elif (all(f.get('acodec') != 'none' for f in formats) or
                              all(f.get('vcodec') != 'none' for f in formats)):
                            yield formats[format_idx]
                    elif format_spec == 'bestaudio':
                        audio_formats = [
                            f for f in formats
                            if f.get('vcodec') == 'none']
                        if audio_formats:
                            yield audio_formats[-1]
                    elif format_spec == 'worstaudio':
                        audio_formats = [
                            f for f in formats
                            if f.get('vcodec') == 'none']
                        if audio_formats:","[32, 42, 44, 48, 53, 64, 67, 68, 69]"
"        0  1   4
        1  2   5
        2  3   6
        Now, update the labels inplace.
        >>> df.set_axis(['i', 'ii'], axis='columns', inplace=True)
        >>> df
           i  ii
        0  1   4
        1  2   5
        2  3   6
        """"""
        if inplace:
            setattr(self, self._get_axis_name(axis), labels)
        else:
            obj = self.copy()
            obj.set_axis(labels, axis=axis, inplace=True)
            return obj
    def _set_axis(self, axis, labels):
        self._data.set_axis(axis, labels)
        self._clear_item_cache()
    def transpose(self, *args, **kwargs):
        """"""
        Permute the dimensions of the %(klass)s
        Parameters
        ----------
        args : %(args_transpose)s
        copy : bool, default False
            Make a copy of the underlying data. Mixed-dtype data will
            always result in a copy
        **kwargs
            Additional keyword arguments will be passed to the function.
        Returns
        -------
        y : same as input
        Examples
        --------
        >>> p.transpose(2, 0, 1)
        >>> p.transpose(2, 0, 1, copy=True)
        """"""
        # construct the args
        axes, kwargs = self._construct_axes_from_arguments(
            args, kwargs, require_all=True
        )
        axes_names = tuple(self._get_axis_name(axes[a]) for a in self._AXIS_ORDERS)
        axes_numbers = tuple(self._get_axis_number(axes[a]) for a in self._AXIS_ORDERS)
        # we must have unique axes
        if len(axes) != len(set(axes)):
            raise ValueError(f""Must specify {self._AXIS_LEN} unique axes"")
        new_axes = self._construct_axes_dict_from(
            self, [self._get_axis(x) for x in axes_names]
        )
        new_values = self.values.transpose(axes_numbers)
        if kwargs.pop(""copy"", None) or (len(args) and args[-1]):
            new_values = new_values.copy()
        nv.validate_transpose(tuple(), kwargs)
        return self._constructor(new_values, **new_axes).__finalize__(self)
    def swapaxes(self, axis1, axis2, copy=True):
        """"""
        Interchange axes and swap values axes appropriately.
        Returns
        -------
        y : same as input
        """"""
        i = self._get_axis_number(axis1)
        j = self._get_axis_number(axis2)
        if i == j:
            if copy:
                return self.copy()
            return self
        mapping = {i: j, j: i}
        new_axes = (self._get_axis(mapping.get(k, k)) for k in range(self._AXIS_LEN))
        new_values = self.values.swapaxes(i, j)
        if copy:
            new_values = new_values.copy()
        return self._constructor(new_values, *new_axes).__finalize__(self)
    def droplevel(self, level, axis=0):
        """"""
        Return DataFrame with requested index / column level(s) removed.
        .. versionadded:: 0.24.0
        Parameters
        ----------
        level : int, str, or list-like
            If a string is given, must be the name of a level
            If list-like, elements must be names or positional indexes
            of levels.
        axis : {0 or 'index', 1 or 'columns'}, default 0
        Returns
        -------
        DataFrame
            DataFrame with requested index / column level(s) removed.
        Examples
        --------
        >>> df = pd.DataFrame([
        ...     [1, 2, 3, 4],
        ...     [5, 6, 7, 8],
        ...     [9, 10, 11, 12]
        ... ]).set_index([0, 1]).rename_axis(['a', 'b'])
        >>> df.columns = pd.MultiIndex.from_tuples([
        ...    ('c', 'e'), ('d', 'f')
        ... ], names=['level_1', 'level_2'])
        >>> df
        level_1   c   d","[24, 26, 27, 28, 29, 30, 31, 32, 33, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 46, 48, 49, 50, 51, 52, 53, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67]"
"import re
import logging
import six
from scrapy.spiders import Spider
from scrapy.http import Request, XmlResponse
from scrapy.utils.sitemap import Sitemap, sitemap_urls_from_robots
from scrapy.utils.gz import gunzip, is_gzipped
logger = logging.getLogger(__name__)

class SitemapSpider(Spider):
    sitemap_urls = ()
    sitemap_rules = [('', 'parse')]
    sitemap_follow = ['']
    sitemap_alternate_links = False
    def __init__(self, *a, **kw):
        super(SitemapSpider, self).__init__(*a, **kw)
        self._cbs = []
        for r, c in self.sitemap_rules:
            if isinstance(c, six.string_types):
                c = getattr(self, c)
            self._cbs.append((regex(r), c))
        self._follow = [regex(x) for x in self.sitemap_follow]
    def start_requests(self):
        for url in self.sitemap_urls:
            yield Request(url, self._parse_sitemap)
    def _parse_sitemap(self, response):
        if response.url.endswith('/robots.txt'):
            for url in sitemap_urls_from_robots(response.body):
                yield Request(url, callback=self._parse_sitemap)
        else:
            body = self._get_sitemap_body(response)
            if body is None:
                logger.warning(""Ignoring invalid sitemap: %(response)s"",
                               {'response': response}, extra={'spider': self})
                return
            s = Sitemap(body)
            if s.type == 'sitemapindex':
                for loc in iterloc(s, self.sitemap_alternate_links):
                    if any(x.search(loc) for x in self._follow):
                        yield Request(loc, callback=self._parse_sitemap)
            elif s.type == 'urlset':
                for loc in iterloc(s):
                    for r, c in self._cbs:
                        if r.search(loc):
                            yield Request(loc, callback=c)
                            break
    def _get_sitemap_body(self, response):
        """"""Return the sitemap body contained in the given response,
        or None if the response is not a sitemap.
        """"""
        if isinstance(response, XmlResponse):
            return response.body
        elif is_gzipped(response):
            return gunzip(response.body)
        elif response.url.endswith('.xml'):
            return response.body
        elif response.url.endswith('.xml.gz'):
            return gunzip(response.body)

def regex(x):
    if isinstance(x, six.string_types):
        return re.compile(x)
    return x

def iterloc(it, alt=False):
    for d in it:
        yield d['loc']
        # Also consider alternate URLs (xhtml:link rel=""alternate"")
        if alt and 'alternate' in d:
            for l in d['alternate']:
                yield l",[34]
"from time import time
import os
from ..conf import settings
from ..utils import memoize
from .generic import Generic

class Zsh(Generic):
    def app_alias(self, alias_name):
        alias = ""alias {0}='TF_ALIAS={0}"" \
                "" PYTHONIOENCODING=utf-8"" \
                ' TF_SHELL_ALIASES=$(alias)' \
                "" TF_CMD=$(thefuck $(fc -ln -1 | tail -n 1)) &&"" \
                "" eval $TF_CMD"".format(alias_name)
        if settings.alter_history:
            return alias + "" && print -s $TF_CMD'""
        else:
            return alias + ""'""
    def _parse_alias(self, alias):
        name, value = alias.split('=', 1)
        if value[0] == value[-1] == '""' or value[0] == value[-1] == ""'"":
            value = value[1:-1]
        return name, value
    @memoize
    def get_aliases(self):
        raw_aliases = os.environ['TF_SHELL_ALIASES'].split('\n')
        return dict(self._parse_alias(alias)
                    for alias in raw_aliases if alias and '=' in alias)
    def _get_history_file_name(self):
        return os.environ.get(""HISTFILE"",
                              os.path.expanduser('~/.zsh_history'))
    def _get_history_line(self, command_script):
        return u': {}:0;{}\n'.format(int(time()), command_script)
    def _script_from_history(self, line):
        if ';' in line:
            return line.split(';', 1)[1]
        else:
            return ''
    def how_to_configure(self):
        return 'eval $(thefuck --alias)', '~/.zshrc'",[28]
"import re
from thefuck.utils import replace_argument, for_app
from thefuck.specific.sudo import sudo_support

@sudo_support
@for_app('pip', 'pip2', 'pip3')
def match(command):
    return ('pip' in command.script and
            'unknown command' in command.output and
            'maybe you meant' in command.output)

def get_new_command(command):
    broken_cmd = re.findall(r'ERROR: unknown command \""([a-z]+)\""',
                            command.output)[0]
    new_cmd = re.findall(r'maybe you meant \""([a-z]+)\""', command.output)[0]
    return replace_argument(command.script, broken_cmd, new_cmd)","[14, 16]"
"    with at least two elements, a token number and token value.  If
    only two tokens are passed, the resulting output is poor.
    Round-trip invariant for full input:
        Untokenized source will match input source exactly
    Round-trip invariant for limited intput:
        # Output text will tokenize the back to the input
        t1 = [tok[:2] for tok in generate_tokens(f.readline)]
        newcode = untokenize(t1)
        readline = iter(newcode.splitlines(1)).next
        t2 = [tok[:2] for tokin generate_tokens(readline)]
        assert t1 == t2
    """"""
    ut = Untokenizer()
    return ut.untokenize(iterable)
def generate_tokens(readline):
    """"""
    The generate_tokens() generator requires one argument, readline, which
    must be a callable object which provides the same interface as the
    readline() method of built-in file objects. Each call to the function
    should return one line of input as a string.  Alternately, readline
    can be a callable function terminating with StopIteration:
        readline = open(myfile).next    # Example of alternate readline
    The generator produces 5-tuples with these members: the token type; the
    token string; a 2-tuple (srow, scol) of ints specifying the row and
    column where the token begins in the source; a 2-tuple (erow, ecol) of
    ints specifying the row and column where the token ends in the source;
    and the line on which the token was found. The line passed is the
    logical line; continuation lines are included.
    """"""
    lnum = parenlev = continued = 0
    numchars = '0123456789'
    contstr, needcont = '', 0
    contline = None
    indents = [0]
    # 'stashed' and 'async_*' are used for async/await parsing
    stashed = None
    async_def = False
    async_def_indent = 0
    async_def_nl = False
    while 1:                                   # loop over lines in stream
        try:
            line = readline()
        except StopIteration:
            line = ''
        lnum = lnum + 1
        pos, max = 0, len(line)
        if contstr:                            # continued string
            if not line:
                raise TokenError(""EOF in multi-line string"", strstart)
            endmatch = endprog.match(line)
            if endmatch:
                pos = end = endmatch.end(0)
                yield (STRING, contstr + line[:end],
                       strstart, (lnum, end), contline + line)
                contstr, needcont = '', 0
                contline = None
            elif needcont and line[-2:] != '\\\n' and line[-3:] != '\\\r\n':
                yield (ERRORTOKEN, contstr + line,
                           strstart, (lnum, len(line)), contline)
                contstr = ''
                contline = None
                continue
            else:
                contstr = contstr + line
                contline = contline + line
                continue
        elif parenlev == 0 and not continued:  # new statement
            if not line: break
            column = 0
            while pos < max:                   # measure leading whitespace
                if line[pos] == ' ': column = column + 1
                elif line[pos] == '\t': column = (column//tabsize + 1)*tabsize
                elif line[pos] == '\f': column = 0
                else: break
                pos = pos + 1
            if pos == max: break
            if stashed:
                yield stashed
                stashed = None
            if line[pos] in '\r\n':            # skip blank lines
                yield (NL, line[pos:], (lnum, pos), (lnum, len(line)), line)
                continue
            if line[pos] == '#':               # skip comments
                comment_token = line[pos:].rstrip('\r\n')
                nl_pos = pos + len(comment_token)
                yield (COMMENT, comment_token,
                        (lnum, pos), (lnum, pos + len(comment_token)), line)
                yield (NL, line[nl_pos:],
                        (lnum, nl_pos), (lnum, len(line)), line)
                continue
            if column > indents[-1]:           # count indents
                indents.append(column)
                yield (INDENT, line[:pos], (lnum, 0), (lnum, pos), line)
            while column < indents[-1]:        # count dedents
                if column not in indents:
                    raise IndentationError(
                        ""unindent does not match any outer indentation level"",
                        (""<tokenize>"", lnum, pos, line))
                indents = indents[:-1]
                if async_def and async_def_indent >= indents[-1]:
                    async_def = False
                    async_def_nl = False
                    async_def_indent = 0
                yield (DEDENT, '', (lnum, pos), (lnum, pos), line)
            if async_def and async_def_nl and async_def_indent >= indents[-1]:
                async_def = False
                async_def_nl = False
                async_def_indent = 0
        else:                                  # continued statement
            if not line:",[17]
"# Copyright: (c) 2019, Ansible Project
# GNU General Public License v3.0+ (see COPYING or https://www.gnu.org/licenses/gpl-3.0.txt)
from __future__ import (absolute_import, division, print_function)
__metaclass__ = type
from ansible.module_utils.six import string_types
from ansible.playbook.attribute import FieldAttribute
from ansible.utils.collection_loader import AnsibleCollectionLoader

def _ensure_default_collection(collection_list=None):
    default_collection = AnsibleCollectionLoader().default_collection
    if collection_list is None:
        collection_list = []
    if default_collection:  # FIXME: exclude role tasks?
        if isinstance(collection_list, string_types):
            collection_list = [collection_list]
        if default_collection not in collection_list:
            collection_list.insert(0, default_collection)
    # if there's something in the list, ensure that builtin or legacy is always there too
    if collection_list and 'ansible.builtin' not in collection_list and 'ansible.legacy' not in collection_list:
        collection_list.append('ansible.legacy')
    return collection_list

class CollectionSearch:
    # this needs to be populated before we can resolve tasks/roles/etc
    _collections = FieldAttribute(isa='list', listof=string_types, priority=100, default=_ensure_default_collection)
    def _load_collections(self, attr, ds):
        # this will only be called if someone specified a value; call the shared value
        _ensure_default_collection(collection_list=ds)
        if not ds:  # don't return an empty collection list, just return None
            return None
        return ds",[34]
"# -*- coding: utf-8 -*-
#
# Copyright 2012-2015 Spotify AB
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
""""""
The system for scheduling tasks and executing them in order.
Deals with dependencies, priorities, resources, etc.
The :py:class:`~luigi.worker.Worker` pulls tasks from the scheduler (usually over the REST interface) and executes them.
See :doc:`/central_scheduler` for more info.
""""""
import collections
try:
    import cPickle as pickle
except ImportError:
    import pickle
import functools
import itertools
import logging
import os
import re
import time
from luigi import six
from luigi import configuration
from luigi import notifications
from luigi import parameter
from luigi import task_history as history
from luigi.task_status import DISABLED, DONE, FAILED, PENDING, RUNNING, SUSPENDED, UNKNOWN
from luigi.task import Config
logger = logging.getLogger(""luigi.server"")

class Scheduler(object):
    """"""
    Abstract base class.
    Note that the methods all take string arguments, not Task objects...
    """"""""""
    add_task = NotImplemented
    get_work = NotImplemented
    ping = NotImplemented
UPSTREAM_RUNNING = 'UPSTREAM_RUNNING'
UPSTREAM_MISSING_INPUT = 'UPSTREAM_MISSING_INPUT'
UPSTREAM_FAILED = 'UPSTREAM_FAILED'
UPSTREAM_DISABLED = 'UPSTREAM_DISABLED'
UPSTREAM_SEVERITY_ORDER = (
    '',
    UPSTREAM_RUNNING,
    UPSTREAM_MISSING_INPUT,
    UPSTREAM_FAILED,
    UPSTREAM_DISABLED,
)
UPSTREAM_SEVERITY_KEY = UPSTREAM_SEVERITY_ORDER.index
STATUS_TO_UPSTREAM_MAP = {
    FAILED: UPSTREAM_FAILED,
    RUNNING: UPSTREAM_RUNNING,
    PENDING: UPSTREAM_MISSING_INPUT,
    DISABLED: UPSTREAM_DISABLED,
}
TASK_FAMILY_RE = re.compile(r'([^(_]+)[(_]')

class scheduler(Config):
    # TODO(erikbern): the config_path is needed for backwards compatilibity. We
    # should drop the compatibility at some point
    retry_delay = parameter.FloatParameter(default=900.0)
    remove_delay = parameter.FloatParameter(default=600.0)
    worker_disconnect_delay = parameter.FloatParameter(default=60.0)
    state_path = parameter.Parameter(default='/var/lib/luigi-server/state.pickle')
    # Jobs are disabled if we see more than disable_failures failures in disable_window seconds.
    # These disables last for disable_persist seconds.
    disable_window = parameter.IntParameter(default=3600,
                                            config_path=dict(section='scheduler', name='disable-window-seconds'))
    disable_failures = parameter.IntParameter(default=None,
                                              config_path=dict(section='scheduler', name='disable-num-failures'))
    disable_hard_timeout = parameter.IntParameter(default=None,
                                                  config_path=dict(section='scheduler', name='disable-hard-timeout'))
    disable_persist = parameter.IntParameter(default=86400,
                                             config_path=dict(section='scheduler', name='disable-persist-seconds'))
    max_shown_tasks = parameter.IntParameter(default=100000)
    max_graph_nodes = parameter.IntParameter(default=100000)
    prune_done_tasks = parameter.BoolParameter(default=False)
    record_task_history = parameter.BoolParameter(default=False)
    prune_on_get_work = parameter.BoolParameter(default=False)

class Failures(object):
    """"""
    This class tracks the number of failures in a given time window.
    Failures added are marked with the current timestamp, and this class counts
    the number of failures in a sliding time window ending at the present.
    """"""
    def __init__(self, window):
        """"""
        Initialize with the given window.
        :param window: how long to track failures for, as a float (number of seconds).
        """"""
        self.window = window
        self.failures = collections.deque()
        self.first_failure_time = None
    def add_failure(self):
        """"""","[92, 94]"
"        u = urlparse(uri)
        self.host = u.hostname
        self.port = int(u.port or '21')
        self.username = u.username
        self.password = u.password
        self.path = u.path
    def _store_in_thread(self, file):
        file.seek(0)
        ftp = FTP()
        ftp.connect(self.host, self.port)
        ftp.login(self.username, self.password)
        dirname, filename = posixpath.split(self.path)
        ftp_makedirs_cwd(ftp, dirname)
        ftp.storbinary('STOR %s' % filename, file)
        ftp.quit()

class SpiderSlot(object):
    def __init__(self, file, exporter, storage, uri):
        self.file = file
        self.exporter = exporter
        self.storage = storage
        self.uri = uri
        self.itemcount = 0

class FeedExporter(object):
    def __init__(self, settings):
        self.settings = settings
        self.urifmt = settings['FEED_URI']
        if not self.urifmt:
            raise NotConfigured
        self.format = settings['FEED_FORMAT'].lower()
        self.storages = self._load_components('FEED_STORAGES')
        self.exporters = self._load_components('FEED_EXPORTERS')
        if not self._storage_supported(self.urifmt):
            raise NotConfigured
        if not self._exporter_supported(self.format):
            raise NotConfigured
        self.store_empty = settings.getbool('FEED_STORE_EMPTY')
        self.export_fields = settings.getlist('FEED_EXPORT_FIELDS')
        uripar = settings['FEED_URI_PARAMS']
        self._uripar = load_object(uripar) if uripar else lambda x, y: None
    @classmethod
    def from_crawler(cls, crawler):
        o = cls(crawler.settings)
        crawler.signals.connect(o.open_spider, signals.spider_opened)
        crawler.signals.connect(o.close_spider, signals.spider_closed)
        crawler.signals.connect(o.item_scraped, signals.item_scraped)
        return o
    def open_spider(self, spider):
        uri = self.urifmt % self._get_uri_params(spider)
        storage = self._get_storage(uri)
        file = storage.open(spider)
        exporter = self._get_exporter(file, fields_to_export=self.export_fields)
        exporter.start_exporting()
        self.slot = SpiderSlot(file, exporter, storage, uri)
    def close_spider(self, spider):
        slot = self.slot
        if not slot.itemcount and not self.store_empty:
            return
        slot.exporter.finish_exporting()
        logfmt = ""%%s %(format)s feed (%(itemcount)d items) in: %(uri)s""
        log_args = {'format': self.format,
                    'itemcount': slot.itemcount,
                    'uri': slot.uri}
        d = defer.maybeDeferred(slot.storage.store, slot.file)
        d.addCallback(lambda _: logger.info(logfmt % ""Stored"", log_args,
                                            extra={'spider': spider}))
        d.addErrback(lambda f: logger.error(logfmt % ""Error storing"", log_args,
                                            extra={'spider': spider, 'failure': f}))
        return d
    def item_scraped(self, item, spider):
        slot = self.slot
        slot.exporter.export_item(item)
        slot.itemcount += 1
        return item
    def _load_components(self, setting_prefix):
        conf = dict(self.settings['%s_BASE' % setting_prefix])
        conf.update(self.settings[setting_prefix])
        d = {}
        for k, v in conf.items():
            try:
                d[k] = load_object(v)
            except NotConfigured:
                pass
        return d
    def _exporter_supported(self, format):
        if format in self.exporters:
            return True
        logger.error(""Unknown feed format: %(format)s"", {'format': format})
    def _storage_supported(self, uri):
        scheme = urlparse(uri).scheme
        if scheme in self.storages:
            try:
                self._get_storage(uri)
                return True
            except NotConfigured:
                logger.error(""Disabled feed storage scheme: %(scheme)s"",
                             {'scheme': scheme})
        else:
            logger.error(""Unknown feed storage scheme: %(scheme)s"",
                         {'scheme': scheme})
    def _get_exporter(self, *args, **kwargs):
        return self.exporters[self.format](*args, **kwargs)
    def _get_storage(self, uri):
        return self.storages[urlparse(uri).scheme](uri)
    def _get_uri_params(self, spider):
        params = {}
        for k in dir(spider):
            params[k] = getattr(spider, k)
        ts = datetime.utcnow().replace(microsecond=0).isoformat().replace(':', '-')
        params['time'] = ts
        self._uripar(params, spider)",[75]
"        required_version = '10-0'
        if is_outdated_version(
                self._versions[self.basename], required_version):
            warning = ('Your copy of %s is outdated and unable to properly mux separate video and audio files, '
                       'youtube-dl will download single file media. '
                       'Update %s to version %s or newer to fix this.') % (
                           self.basename, self.basename, required_version)
            if self._downloader:
                self._downloader.report_warning(warning)
            return False
        return True

class FFmpegFixupStretchedPP(FFmpegPostProcessor):
    def run(self, info):
        stretched_ratio = info.get('stretched_ratio')
        if stretched_ratio is None or stretched_ratio == 1:
            return [], info
        filename = info['filepath']
        temp_filename = prepend_extension(filename, 'temp')
        options = ['-c', 'copy', '-aspect', '%f' % stretched_ratio]
        self._downloader.to_screen('[ffmpeg] Fixing aspect ratio in ""%s""' % filename)
        self.run_ffmpeg(filename, temp_filename, options)
        os.remove(encodeFilename(filename))
        os.rename(encodeFilename(temp_filename), encodeFilename(filename))
        return [], info

class FFmpegFixupM4aPP(FFmpegPostProcessor):
    def run(self, info):
        if info.get('container') != 'm4a_dash':
            return [], info
        filename = info['filepath']
        temp_filename = prepend_extension(filename, 'temp')
        options = ['-c', 'copy', '-f', 'mp4']
        self._downloader.to_screen('[ffmpeg] Correcting container in ""%s""' % filename)
        self.run_ffmpeg(filename, temp_filename, options)
        os.remove(encodeFilename(filename))
        os.rename(encodeFilename(temp_filename), encodeFilename(filename))
        return [], info

class FFmpegFixupM3u8PP(FFmpegPostProcessor):
    def run(self, info):
        filename = info['filepath']
        if self.get_audio_codec(filename) == 'aac':
            temp_filename = prepend_extension(filename, 'temp')
            options = ['-c', 'copy', '-f', 'mp4', '-bsf:a', 'aac_adtstoasc']
            self._downloader.to_screen('[ffmpeg] Fixing malformed AAC bitstream in ""%s""' % filename)
            self.run_ffmpeg(filename, temp_filename, options)
            os.remove(encodeFilename(filename))
            os.rename(encodeFilename(temp_filename), encodeFilename(filename))
        return [], info

class FFmpegSubtitlesConvertorPP(FFmpegPostProcessor):
    def __init__(self, downloader=None, format=None):
        super(FFmpegSubtitlesConvertorPP, self).__init__(downloader)
        self.format = format
    def run(self, info):
        subs = info.get('requested_subtitles')
        filename = info['filepath']
        new_ext = self.format
        new_format = new_ext
        if new_format == 'vtt':
            new_format = 'webvtt'
        if subs is None:
            self._downloader.to_screen('[ffmpeg] There aren\'t any subtitles to convert')
            return [], info
        self._downloader.to_screen('[ffmpeg] Converting subtitles')
        sub_filenames = []
        for lang, sub in subs.items():
            ext = sub['ext']
            if ext == new_ext:
                self._downloader.to_screen(
                    '[ffmpeg] Subtitle file for %s is already in the requested format' % new_ext)
                continue
            old_file = subtitles_filename(filename, lang, ext)
            sub_filenames.append(old_file)
            new_file = subtitles_filename(filename, lang, new_ext)
            if ext in ('dfxp', 'ttml', 'tt'):
                self._downloader.report_warning(
                    'You have requested to convert dfxp (TTML) subtitles into another format, '
                    'which results in style information loss')
                dfxp_file = old_file
                srt_file = subtitles_filename(filename, lang, 'srt')
                with io.open(dfxp_file, 'rt', encoding='utf-8') as f:
                    srt_data = dfxp2srt(f.read())
                with io.open(srt_file, 'wt', encoding='utf-8') as f:
                    f.write(srt_data)
                old_file = srt_file
                subs[lang] = {
                    'ext': 'srt',
                    'data': srt_data
                }
                if new_ext == 'srt':
                    continue
                else:
                    sub_filenames.append(srt_file)
            self.run_ffmpeg(old_file, new_file, ['-f', new_format])
            with io.open(new_file, 'rt', encoding='utf-8') as f:
                subs[lang] = {
                    'ext': new_ext,
                    'data': f.read(),
                }
",[101]
"        array([[ 0.,  0.,  0.],
               [ 0.,  0.,  0.]], dtype=float32)
    ```
    {{np_implementation}}
    """"""
    return np.prod(int_shape(x))

@symbolic
def cast(x, dtype):
    """"""Casts a tensor to a different dtype and returns it.
    You can cast a Keras variable but it still returns a Keras tensor.
    # Arguments
        x: Keras tensor (or variable).
        dtype: String, either (`'float16'`, `'float32'`, or `'float64'`).
    # Returns
        Keras tensor with dtype `dtype`.
    # Example
    ```python
        >>> from keras import backend as K
        >>> input = K.placeholder((2, 3), dtype='float32')
        >>> input
        <tf.Tensor 'Placeholder_2:0' shape=(2, 3) dtype=float32>
        # It doesn't work in-place as below.
        >>> K.cast(input, dtype='float16')
        <tf.Tensor 'Cast_1:0' shape=(2, 3) dtype=float16>
        >>> input
        <tf.Tensor 'Placeholder_2:0' shape=(2, 3) dtype=float32>
        # you need to assign it.
        >>> input = K.cast(input, dtype='float16')
        >>> input
        <tf.Tensor 'Cast_2:0' shape=(2, 3) dtype=float16>
    ```
    """"""
    return tf.cast(x, dtype)

# UPDATES OPS

@symbolic
def update(x, new_x):
    """"""Update the value of `x` to `new_x`.
    # Arguments
        x: A `Variable`.
        new_x: A tensor of same shape as `x`.
    # Returns
        The variable `x` updated.
    """"""
    return tf_state_ops.assign(x, new_x)

@symbolic
def update_add(x, increment):
    """"""Update the value of `x` by adding `increment`.
    # Arguments
        x: A `Variable`.
        increment: A tensor of same shape as `x`.
    # Returns
        The variable `x` updated.
    """"""
    return tf_state_ops.assign_add(x, increment)

@symbolic
def update_sub(x, decrement):
    """"""Update the value of `x` by subtracting `decrement`.
    # Arguments
        x: A `Variable`.
        decrement: A tensor of same shape as `x`.
    # Returns
        The variable `x` updated.
    """"""
    return tf_state_ops.assign_sub(x, decrement)

@symbolic
def moving_average_update(x, value, momentum):
    """"""Compute the moving average of a variable.
    # Arguments
        x: A `Variable`.
        value: A tensor with the same shape as `x`.
        momentum: The moving average momentum.
    # Returns
        An operation to update the variable.
    """"""
    with tf_ops.colocate_with(x):
        decay = tf_ops.convert_to_tensor(1.0 - momentum)
        if decay.dtype != x.dtype.base_dtype:
            decay = tf_math_ops.cast(decay, x.dtype.base_dtype)
        update_delta = (x - tf_math_ops.cast(value, x.dtype)) * decay
        return tf_state_ops.assign_sub(x, update_delta)

# LINEAR ALGEBRA
def dot(x, y):
    """"""Multiplies 2 tensors (and/or variables) and returns a *tensor*.
    When attempting to multiply a nD tensor
    with a nD tensor, it reproduces the Theano behavior.
    (e.g. `(2, 3) * (4, 3, 5) -> (2, 4, 5)`)
    # Arguments
        x: Tensor or variable.
        y: Tensor or variable.
    # Returns
        A tensor, dot product of `x` and `y`.
    # Examples
    ```python
        # dot product between tensors
        >>> x = K.placeholder(shape=(2, 3))
        >>> y = K.placeholder(shape=(3, 4))","[55, 69, 83]"
"        self._state.inactivate_workers(remove_workers)
        assistant_ids = set(w.id for w in self._state.get_assistants())
        remove_tasks = []
        if assistant_ids:
            necessary_tasks = self._state.get_necessary_tasks()
        else:
            necessary_tasks = ()
        for task in self._state.get_active_tasks():
            self._state.fail_dead_worker_task(task, self._config, assistant_ids)
            if task.id not in necessary_tasks and self._state.prune(task, self._config):
                remove_tasks.append(task.id)
        self._state.inactivate_tasks(remove_tasks)
        logger.info(""Done pruning task graph"")
    def update(self, worker_id, worker_reference=None, get_work=False):
        """"""
        Keep track of whenever the worker was last active.
        """"""
        worker = self._state.get_worker(worker_id)
        worker.update(worker_reference, get_work=get_work)
    def _update_priority(self, task, prio, worker):
        """"""
        Update priority of the given task.
        Priority can only be increased.
        If the task doesn't exist, a placeholder task is created to preserve priority when the task is later scheduled.
        """"""
        task.priority = prio = max(prio, task.priority)
        for dep in task.deps or []:
            t = self._state.get_task(dep)
            if t is not None and prio > t.priority:
                self._update_priority(t, prio, worker)
    def add_task(self, task_id=None, status=PENDING, runnable=True,
                 deps=None, new_deps=None, expl=None, resources=None,
                 priority=0, family='', module=None, params=None,
                 assistant=False, tracking_url=None, **kwargs):
        """"""
        * add task identified by task_id if it doesn't exist
        * if deps is not None, update dependency list
        * update status of task
        * add additional workers/stakeholders
        * update priority when needed
        """"""
        worker_id = kwargs['worker']
        self.update(worker_id)
        task = self._state.get_task(task_id, setdefault=self._make_task(
            task_id=task_id, status=PENDING, deps=deps, resources=resources,
            priority=priority, family=family, module=module, params=params))
        # for setting priority, we'll sometimes create tasks with unset family and params
        if not task.family:
            task.family = family
        if not getattr(task, 'module', None):
            task.module = module
        if not task.params:
            task.params = _get_default(params, {})
        if tracking_url is not None or task.status != RUNNING:
            task.tracking_url = tracking_url
        if task.remove is not None:
            task.remove = None  # unmark task for removal so it isn't removed after being added
        if expl is not None:
            task.expl = expl
        if not (task.status == RUNNING and status == PENDING) or new_deps:
            # don't allow re-scheduling of task while it is running, it must either fail or succeed first
            if status == PENDING or status != task.status:
                # Update the DB only if there was a acctual change, to prevent noise.
                # We also check for status == PENDING b/c that's the default value
                # (so checking for status != task.status woule lie)
                self._update_task_history(task, status)
            self._state.set_status(task, PENDING if status == SUSPENDED else status, self._config)
            if status == FAILED:
                task.retry = self._retry_time(task, self._config)
        if deps is not None:
            task.deps = set(deps)
        if new_deps is not None:
            task.deps.update(new_deps)
        if resources is not None:
            task.resources = resources
        if not assistant:
            task.stakeholders.add(worker_id)
            # Task dependencies might not exist yet. Let's create dummy tasks for them for now.
            # Otherwise the task dependencies might end up being pruned if scheduling takes a long time
            for dep in task.deps or []:
                t = self._state.get_task(dep, setdefault=self._make_task(task_id=dep, status=UNKNOWN, deps=None, priority=priority))
                t.stakeholders.add(worker_id)
        self._update_priority(task, priority, worker_id)
        if runnable:
            task.workers.add(worker_id)
            self._state.get_worker(worker_id).tasks.add(task)
            task.runnable = runnable
    def add_worker(self, worker, info, **kwargs):
        self._state.get_worker(worker).add_info(info)
    def update_resources(self, **resources):
        if self._resources is None:
            self._resources = {}
        self._resources.update(resources)
    def _has_resources(self, needed_resources, used_resources):
        if needed_resources is None:
            return True
        available_resources = self._resources or {}
        for resource, amount in six.iteritems(needed_resources):
            if amount + used_resources[resource] > available_resources.get(resource, 1):
                return False
        return True",[12]
"import luigi.contrib.hadoop
import luigi.contrib.hdfs
logger = logging.getLogger('luigi-interface')

def fix_paths(job):
    """"""
    Coerce input arguments to use temporary files when used for output.
    Return a list of temporary file pairs (tmpfile, destination path) and
    a list of arguments.
    Converts each HdfsTarget to a string for the path.
    """"""
    tmp_files = []
    args = []
    for x in job.args():
        if isinstance(x, luigi.contrib.hdfs.HdfsTarget):  # input/output
            if x.exists() or not job.atomic_output():  # input
                args.append(x.path)
            else:  # output
                x_path_no_slash = x.path[:-1] if x.path[-1] == '/' else x.path
                y = luigi.contrib.hdfs.HdfsTarget(x_path_no_slash + '-luigi-tmp-%09d' % random.randrange(0, 1e10))
                tmp_files.append((y, x_path_no_slash))
                logger.info('Using temp path: %s for path %s', y.path, x.path)
                args.append(y.path)
        else:
            args.append(str(x))
    return (tmp_files, args)

class HadoopJarJobError(Exception):
    pass

class HadoopJarJobRunner(luigi.contrib.hadoop.JobRunner):
    """"""
    JobRunner for `hadoop jar` commands. Used to run a HadoopJarJobTask.
    """"""
    def __init__(self):
        pass
    def run_job(self, job):
        ssh_config = job.ssh()
        if ssh_config:
            host = ssh_config.get(""host"", None)
            key_file = ssh_config.get(""key_file"", None)
            username = ssh_config.get(""username"", None)
            if not host or not key_file or not username or not job.jar():
                raise HadoopJarJobError(""missing some config for HadoopRemoteJarJobRunner"")
            arglist = ['ssh', '-i', key_file,
                       '-o', 'BatchMode=yes']  # no password prompts etc
            if ssh_config.get(""no_host_key_check"", False):
                arglist += ['-o', 'UserKnownHostsFile=/dev/null',
                            '-o', 'StrictHostKeyChecking=no']
            arglist.append('{}@{}'.format(username, host))
        else:
            arglist = []
            if not job.jar() or not os.path.exists(job.jar()):
                logger.error(""Can't find jar: %s, full path %s"", job.jar(), os.path.abspath(job.jar()))
                raise HadoopJarJobError(""job jar does not exist"")
        # TODO(jcrobak): libjars, files, etc. Can refactor out of
        # hadoop.HadoopJobRunner
        hadoop_arglist = luigi.contrib.hdfs.load_hadoop_cmd() + ['jar', job.jar()]
        if job.main():
            hadoop_arglist.append(job.main())
        jobconfs = job.jobconfs()
        for jc in jobconfs:
            hadoop_arglist += ['-D' + jc]
        (tmp_files, job_args) = fix_paths(job)
        hadoop_arglist += job_args
        arglist.extend(hadoop_arglist)
        luigi.contrib.hadoop.run_and_track_hadoop_job(arglist)
        for a, b in tmp_files:
            a.move(b)

class HadoopJarJobTask(luigi.contrib.hadoop.BaseHadoopJobTask):
    """"""
    A job task for `hadoop jar` commands that define a jar and (optional) main method.
    """"""
    def jar(self):
        """"""
        Path to the jar for this Hadoop Job.
        """"""
        return None
    def main(self):
        """"""
        optional main method for this Hadoop Job.
        """"""
        return None
    def job_runner(self):
        # We recommend that you define a subclass, override this method and set up your own config
        return HadoopJarJobRunner()
    def atomic_output(self):
        """"""
        If True, then rewrite output arguments to be temp locations and
        atomically move them into place after the job finishes.
        """"""
        return True
    def ssh(self):
        """"""
        Set this to run hadoop command remotely via ssh. It needs to be a dict that looks like
        {""host"": ""myhost"", ""key_file"": None, ""username"": None, [""no_host_key_check"": False]}
        """"""
        return None
    def args(self):
        """"""
        Returns an array of args to pass to the job (after hadoop jar <jar> <main>).
        """"""",[61]
"        if self.processed % self.check_expired_frequency == 0:
            # This is still quite inefficient for large number of cookies
            self.jar.clear_expired_cookies()
    @property
    def _cookies(self):
        return self.jar._cookies
    def clear_session_cookies(self, *args, **kwargs):
        return self.jar.clear_session_cookies(*args, **kwargs)
    def clear(self):
        return self.jar.clear()
    def __iter__(self):
        return iter(self.jar)
    def __len__(self):
        return len(self.jar)
    def set_policy(self, pol):
        return self.jar.set_policy(pol)
    def make_cookies(self, response, request):
        wreq = WrappedRequest(request)
        wrsp = WrappedResponse(response)
        return self.jar.make_cookies(wrsp, wreq)
    def set_cookie(self, cookie):
        self.jar.set_cookie(cookie)
    def set_cookie_if_ok(self, cookie, request):
        self.jar.set_cookie_if_ok(cookie, WrappedRequest(request))

def potential_domain_matches(domain):
    """"""Potential domain matches for a cookie
    >>> potential_domain_matches('www.example.com')
    ['www.example.com', 'example.com', '.www.example.com', '.example.com']
    """"""
    matches = [domain]
    try:
        start = domain.index('.') + 1
        end = domain.rindex('.')
        while start < end:
            matches.append(domain[start:])
            start = domain.index('.', start) + 1
    except ValueError:
        pass
    return matches + ['.' + d for d in matches]

class _DummyLock(object):
    def acquire(self):
        pass
    def release(self):
        pass

class WrappedRequest(object):
    """"""Wraps a scrapy Request class with methods defined by urllib2.Request class to interact with CookieJar class
    see http://docs.python.org/library/urllib2.html#urllib2.Request
    """"""
    def __init__(self, request):
        self.request = request
    def get_full_url(self):
        return self.request.url
    def get_host(self):
        return urlparse_cached(self.request).netloc
    def get_type(self):
        return urlparse_cached(self.request).scheme
    def is_unverifiable(self):
        """"""Unverifiable should indicate whether the request is unverifiable, as defined by RFC 2965.
        It defaults to False. An unverifiable request is one whose URL the user did not have the
        option to approve. For example, if the request is for an image in an
        HTML document, and the user had no option to approve the automatic
        fetching of the image, this should be true.
        """"""
        return self.request.meta.get('is_unverifiable', False)
    # python3 uses request.unverifiable
    @property
    def unverifiable(self):
        return self.is_unverifiable()
    def get_origin_req_host(self):
        return urlparse_cached(self.request).hostname
    def has_header(self, name):
        return name in self.request.headers
    def get_header(self, name, default=None):
        return to_native_str(self.request.headers.get(name, default))
    def header_items(self):
        return [
            (to_native_str(k), [to_native_str(x) for x in v])
            for k, v in self.request.headers.items()
        ]
    def add_unredirected_header(self, name, value):
        self.request.headers.appendlist(name, value)

class WrappedResponse(object):
    def __init__(self, response):
        self.response = response
    def info(self):
        return self
    # python3 cookiejars calls get_all
    def get_all(self, name, default=None):
        return [to_native_str(v) for v in self.response.headers.getlist(name)]
    # python2 cookiejars calls getheaders","[102, 106, 124]"
"        if not follow_key:
            return slice(start, stop)
        warnings.warn(
            ""indexing past lexsort depth may impact performance."",
            PerformanceWarning,
            stacklevel=10,
        )
        loc = np.arange(start, stop, dtype=""int64"")
        for i, k in enumerate(follow_key, len(lead_key)):
            mask = self.codes[i][loc] == self._get_loc_single_level_index(
                self.levels[i], k
            )
            if not mask.all():
                loc = loc[mask]
            if not len(loc):
                raise KeyError(key)
        return _maybe_to_slice(loc) if len(loc) != stop - start else slice(start, stop)
    def get_loc_level(self, key, level=0, drop_level: bool = True):
        """"""
        Get both the location for the requested label(s) and the
        resulting sliced index.
        Parameters
        ----------
        key : label or sequence of labels
        level : int/level name or list thereof, optional
        drop_level : bool, default True
            If ``False``, the resulting index will not drop any level.
        Returns
        -------
        loc : A 2-tuple where the elements are:
              Element 0: int, slice object or boolean array
              Element 1: The resulting sliced multiindex/index. If the key
              contains all levels, this will be ``None``.
        See Also
        --------
        MultiIndex.get_loc  : Get location for a label or a tuple of labels.
        MultiIndex.get_locs : Get location for a label/slice/list/mask or a
                              sequence of such.
        Examples
        --------
        >>> mi = pd.MultiIndex.from_arrays([list('abb'), list('def')],
        ...                                names=['A', 'B'])
        >>> mi.get_loc_level('b')
        (slice(1, 3, None), Index(['e', 'f'], dtype='object', name='B'))
        >>> mi.get_loc_level('e', level='B')
        (array([False,  True, False], dtype=bool),
        Index(['b'], dtype='object', name='A'))
        >>> mi.get_loc_level(['b', 'e'])
        (1, None)
        """"""
        # different name to distinguish from maybe_droplevels
        def maybe_mi_droplevels(indexer, levels, drop_level: bool):
            if not drop_level:
                return self[indexer]
            # kludgearound
            orig_index = new_index = self[indexer]
            levels = [self._get_level_number(i) for i in levels]
            for i in sorted(levels, reverse=True):
                try:
                    new_index = new_index.droplevel(i)
                except ValueError:
                    # no dropping here
                    return orig_index
            return new_index
        if isinstance(level, (tuple, list)):
            if len(key) != len(level):
                raise AssertionError(
                    ""Key for location must have same length as number of levels""
                )
            result = None
            for lev, k in zip(level, key):
                loc, new_index = self.get_loc_level(k, level=lev)
                if isinstance(loc, slice):
                    mask = np.zeros(len(self), dtype=bool)
                    mask[loc] = True
                    loc = mask
                result = loc if result is None else result & loc
            return result, maybe_mi_droplevels(result, level, drop_level)
        level = self._get_level_number(level)
        # kludge for #1796
        if isinstance(key, list):
            key = tuple(key)
        if isinstance(key, tuple) and level == 0:
            try:
                if key in self.levels[0]:
                    indexer = self._get_level_indexer(key, level=level)
                    new_index = maybe_mi_droplevels(indexer, [0], drop_level)
                    return indexer, new_index
            except TypeError:
                pass
            if not any(isinstance(k, slice) for k in key):
                # partial selection
                # optionally get indexer to avoid re-calculation
                def partial_selection(key, indexer=None):
                    if indexer is None:
                        indexer = self.get_loc(key)
                    ilevels = [
                        i for i in range(len(key)) if key[i] != slice(None, None)
                    ]
                    return indexer, maybe_mi_droplevels(indexer, ilevels, drop_level)
                if len(key) == self.nlevels and self.is_unique:
                    # Complete key in unique index -> standard get_loc",[110]
"            return self.value_regex.match(value)
        else:
            return True

class _BaseEventEntry(_BaseEntry):
    def __init__(self, source=None, source_regex=None, thread_info=None,
                 thread_info_regex=None, prefix=''):
        _BaseEntry.__init__(self, prefix=prefix)
        if type(self) is _BaseEventEntry:
            raise TypeError
        if source is not None:
            assert source_regex is None
        self.line_pattern = re.compile(
            r""""""^%s(?P<indent>(?: {4})*)[0-9:.]{15} """"""
            r""""""(?P<thread_info>[0-9]+-[0-9A-Za-z_-]+[ ]+)?""""""
            r""""""(?P<event_name>[a-z_]*) +(?P<line_number>[0-9]*) """"""
            r""""""+(?P<source>.*)$"""""" % (re.escape(self.prefix,))
        )
        self.source = source
        self.source_regex = (None if source_regex is None else
                             re.compile(source_regex))
        self.thread_info = thread_info
        self.thread_info_regex = (None if thread_info_regex is None else
                             re.compile(thread_info_regex))
    @caching.CachedProperty
    def event_name(self):
        return re.match('^[A-Z][a-z_]*', type(self).__name__).group(0).lower()
    def _check_source(self, source):
        if self.source is not None:
            return source == self.source
        elif self.source_regex is not None:
            return self.source_regex.match(source)
        else:
            return True
    def _check_thread_info(self, thread_info):
        if self.thread_info is not None:
            return thread_info == self.thread_info
        elif self.thread_info_regex is not None:
            return self.thread_info_regex.match(thread_info)
        else:
            return True
    def check(self, s):
        match = self.line_pattern.match(s)
        if not match:
            return False
        _, thread_info, event_name, _, source = match.groups()
        return (event_name == self.event_name and
                self._check_source(source) and
                self._check_thread_info(thread_info))

class CallEntry(_BaseEventEntry):
    pass

class LineEntry(_BaseEventEntry):
    pass

class ReturnEntry(_BaseEventEntry):
    pass

class ExceptionEntry(_BaseEventEntry):
    pass

class OpcodeEntry(_BaseEventEntry):
    pass

class OutputFailure(Exception):
    pass

def assert_output(output, expected_entries, prefix=None):
    lines = tuple(filter(None, output.split('\n')))
    if prefix is not None:
        for line in lines:
            if not line.startswith(prefix):
                raise OutputFailure(line)
    any_mismatch = False
    result = ''
    template = '\n{line!s:%s}   {expected_entry}  {arrow}' % max(map(len, lines))
    for expected_entry, line in zip_longest(expected_entries, lines, fillvalue=""""):
        mismatch = not (expected_entry and expected_entry.check(line))
        any_mismatch |= mismatch
        arrow = '<===' * mismatch
        result += template.format(**locals())
    if len(lines) != len(expected_entries):
        result += '\nOutput has {} lines, while we expect {} lines.'.format(
                len(lines), len(expected_entries))
    if any_mismatch:
        raise OutputFailure(result)

def assert_sample_output(module):
    with sys_tools.OutputCapturer(stdout=False,
                                  stderr=True) as output_capturer:
        module.main()
        
    time = '21:10:42.298924'
    time_pattern = re.sub(r'\d', r'\\d', time)
    def normalise(out):
        return re.sub(time_pattern, time, out).strip()
    output = output_capturer.string_io.getvalue()
    try:
        assert (
                normalise(output) ==
                normalise(module.expected_output)
        )
    except AssertionError:
        print('\n\nActual Output:\n\n' + output)  # to copy paste into expected_output","[91, 110]"
"        -------
        bool
        """"""
        return is_dtype_equal(value.dtype, self.dtype)
    def to_native_types(self, na_rep=""nan"", quoting=None, **kwargs):
        """""" convert to our native types format """"""
        values = self.values
        mask = isna(values)
        itemsize = writers.word_len(na_rep)
        if not self.is_object and not quoting and itemsize:
            values = values.astype(str)
            if values.dtype.itemsize / np.dtype(""U1"").itemsize < itemsize:
                # enlarge for the na_rep
                values = values.astype(f""<U{itemsize}"")
        else:
            values = np.array(values, dtype=""object"")
        values[mask] = na_rep
        return values
    # block actions #
    def copy(self, deep: bool = True):
        """""" copy constructor """"""
        values = self.values
        if deep:
            values = values.copy()
        return self.make_block_same_class(values, ndim=self.ndim)
    def replace(
        self,
        to_replace,
        value,
        inplace: bool = False,
        regex: bool = False,
        convert: bool = True,
    ):
        """"""
        replace the to_replace value with value, possible to create new
        blocks here this is just a call to putmask. regex is not used here.
        It is used in ObjectBlocks.  It is here for API compatibility.
        """"""
        inplace = validate_bool_kwarg(inplace, ""inplace"")
        original_to_replace = to_replace
        # If we cannot replace with own dtype, convert to ObjectBlock and
        # retry
        if not self._can_hold_element(to_replace):
            if not isinstance(to_replace, list):
                if inplace:
                    return [self]
                return [self.copy()]
            to_replace = [x for x in to_replace if self._can_hold_element(x)]
            if not len(to_replace):
                # GH#28084 avoid costly checks since we can infer
                #  that there is nothing to replace in this block
                if inplace:
                    return [self]
                return [self.copy()]
            if len(to_replace) == 1:
                # _can_hold_element checks have reduced this back to the
                #  scalar case and we can avoid a costly object cast
                return self.replace(
                    to_replace[0], value, inplace=inplace, regex=regex, convert=convert,
                )
            # GH 22083, TypeError or ValueError occurred within error handling
            # causes infinite loop. Cast and retry only if not objectblock.
            if is_object_dtype(self):
                raise AssertionError
            # try again with a compatible block
            block = self.astype(object)
            return block.replace(
                to_replace=to_replace,
                value=value,
                inplace=inplace,
                regex=regex,
                convert=convert,
            )
        values = self.values
        if lib.is_scalar(to_replace) and isinstance(values, np.ndarray):
            # The only non-DatetimeLike class that also has a non-trivial
            #  try_coerce_args is ObjectBlock, but that overrides replace,
            #  so does not get here.
            to_replace = convert_scalar_for_putitemlike(to_replace, values.dtype)
        mask = missing.mask_missing(values, to_replace)
        if not mask.any():
            if inplace:
                return [self]
            return [self.copy()]
        try:
            blocks = self.putmask(mask, value, inplace=inplace)
            # Note: it is _not_ the case that self._can_hold_element(value)
            #  is always true at this point.  In particular, that can fail
            #  for:
            #   ""2u"" with bool-dtype, float-dtype
            #   0.5 with int64-dtype
            #   np.nan with int64-dtype
        except (TypeError, ValueError):
            # GH 22083, TypeError or ValueError occurred within error handling
            # causes infinite loop. Cast and retry only if not objectblock.
            if is_object_dtype(self):
                raise
            assert not self._can_hold_element(value), value
            # try again with a compatible block
            block = self.astype(object)
            return block.replace(
                to_replace=original_to_replace,
                value=value,
                inplace=inplace,
                regex=regex,
                convert=convert,
            )
        if convert:
            blocks = [b.convert(numeric=False, copy=not inplace) for b in blocks]
        return blocks","[94, 95, 96, 97, 98]"
"        shownotches : bool, default: False
          If `False` (default), produces a rectangular box plot.
          If `True`, will produce a notched box plot
        showmeans : bool, default: False
          If `True`, will toggle on the rendering of the means
        showcaps  : bool, default: True
          If `True`, will toggle on the rendering of the caps
        showbox  : bool, default: True
          If `True`, will toggle on the rendering of the box
        showfliers : bool, default: True
          If `True`, will toggle on the rendering of the fliers
        boxprops : dict or None (default)
          If provided, will set the plotting style of the boxes
        whiskerprops : dict or None (default)
          If provided, will set the plotting style of the whiskers
        capprops : dict or None (default)
          If provided, will set the plotting style of the caps
        flierprops : dict or None (default)
          If provided will set the plotting style of the fliers
        medianprops : dict or None (default)
          If provided, will set the plotting style of the medians
        meanprops : dict or None (default)
          If provided, will set the plotting style of the means
        meanline : bool, default: False
          If `True` (and *showmeans* is `True`), will try to render the mean
          as a line spanning the full width of the box according to
          *meanprops*. Not recommended if *shownotches* is also True.
          Otherwise, means will be shown as points.
        manage_ticks : bool, default: True
          If True, the tick locations and labels will be adjusted to match the
          boxplot positions.
        zorder : scalar, default: None
          The zorder of the resulting boxplot.
        Returns
        -------
        result : dict
          A dictionary mapping each component of the boxplot to a list
          of the `.Line2D` instances created. That dictionary has the
          following keys (assuming vertical boxplots):
          - ``boxes``: the main body of the boxplot showing the
            quartiles and the median's confidence intervals if
            enabled.
          - ``medians``: horizontal lines at the median of each box.
          - ``whiskers``: the vertical lines extending to the most
            extreme, non-outlier data points.
          - ``caps``: the horizontal lines at the ends of the
            whiskers.
          - ``fliers``: points representing data that extend beyond
            the whiskers (fliers).
          - ``means``: points or lines representing the means.
        Examples
        --------
        .. plot:: gallery/statistics/bxp.py
        """"""
        # lists of artists to be output
        whiskers = []
        caps = []
        boxes = []
        medians = []
        means = []
        fliers = []
        # empty list of xticklabels
        datalabels = []
        # Use default zorder if none specified
        if zorder is None:
            zorder = mlines.Line2D.zorder
        zdelta = 0.1
        def line_props_with_rcdefaults(subkey, explicit, zdelta=0):
            d = {k.split('.')[-1]: v for k, v in rcParams.items()
                 if k.startswith(f'boxplot.{subkey}')}
            d['zorder'] = zorder + zdelta
            if explicit is not None:
                d.update(
                    cbook.normalize_kwargs(explicit, mlines.Line2D._alias_map))
            return d
        # box properties
        if patch_artist:
            final_boxprops = dict(
                linestyle=rcParams['boxplot.boxprops.linestyle'],
                linewidth=rcParams['boxplot.boxprops.linewidth'],
                edgecolor=rcParams['boxplot.boxprops.color'],
                facecolor=('white' if rcParams['_internal.classic_mode'] else
                           rcParams['patch.facecolor']),
                zorder=zorder,
            )
            if boxprops is not None:
                final_boxprops.update(
                    cbook.normalize_kwargs(
                        boxprops, mpatches.PathPatch._alias_map))
        else:
            final_boxprops = line_props_with_rcdefaults('boxprops', boxprops)
        final_whiskerprops = line_props_with_rcdefaults(
            'whiskerprops', whiskerprops)
        final_capprops = line_props_with_rcdefaults(
            'capprops', capprops)
        final_flierprops = line_props_with_rcdefaults(
            'flierprops', flierprops)
        final_medianprops = line_props_with_rcdefaults(
            'medianprops', medianprops, zdelta)","[94, 118, 120, 122, 126]"
"    if (
        not any_callable
        and not any_arraylike
        and not any_groupers
        and match_axis_length
        and level is None
    ):
        if isinstance(obj, DataFrame):
            all_in_columns_index = all(
                g in obj.columns or g in obj.index.names for g in keys
            )
        else:
            assert isinstance(obj, Series)
            all_in_columns_index = all(g in obj.index.names for g in keys)
        if not all_in_columns_index:
            keys = [com.asarray_tuplesafe(keys)]
    if isinstance(level, (tuple, list)):
        if key is None:
            keys = [None] * len(level)
        levels = level
    else:
        levels = [level] * len(keys)
    groupings: List[Grouping] = []
    exclusions: List[Hashable] = []
    # if the actual grouper should be obj[key]
    def is_in_axis(key) -> bool:
        if not _is_label_like(key):
            # items -> .columns for DataFrame, .index for Series
            items = obj.axes[-1]
            try:
                items.get_loc(key)
            except (KeyError, TypeError, InvalidIndexError):
                # TypeError shows up here if we pass e.g. Int64Index
                return False
        return True
    # if the grouper is obj[name]
    def is_in_obj(gpr) -> bool:
        if not hasattr(gpr, ""name""):
            return False
        try:
            return gpr is obj[gpr.name]
        except (KeyError, IndexError):
            return False
    for i, (gpr, level) in enumerate(zip(keys, levels)):
        if is_in_obj(gpr):  # df.groupby(df['name'])
            in_axis, name = True, gpr.name
            exclusions.append(name)
        elif is_in_axis(gpr):  # df.groupby('name')
            if gpr in obj:
                if validate:
                    obj._check_label_or_level_ambiguity(gpr, axis=axis)
                in_axis, name, gpr = True, gpr, obj[gpr]
                exclusions.append(name)
            elif obj._is_level_reference(gpr, axis=axis):
                in_axis, name, level, gpr = False, None, gpr, None
            else:
                raise KeyError(gpr)
        elif isinstance(gpr, Grouper) and gpr.key is not None:
            # Add key to exclusions
            exclusions.append(gpr.key)
            in_axis, name = False, None
        else:
            in_axis, name = False, None
        if is_categorical_dtype(gpr) and len(gpr) != obj.shape[axis]:
            raise ValueError(
                f""Length of grouper ({len(gpr)}) and axis ({obj.shape[axis]}) ""
                ""must be same length""
            )
        # create the Grouping
        # allow us to passing the actual Grouping as the gpr
        ping = (
            Grouping(
                group_axis,
                gpr,
                obj=obj,
                name=name,
                level=level,
                sort=sort,
                observed=observed,
                in_axis=in_axis,
                dropna=dropna,
            )
            if not isinstance(gpr, Grouping)
            else gpr
        )
        groupings.append(ping)
    if len(groupings) == 0 and len(obj):
        raise ValueError(""No group keys passed!"")
    elif len(groupings) == 0:
        groupings.append(Grouping(Index([], dtype=""int""), np.array([], dtype=np.intp)))
    # create the internals grouper
    grouper = ops.BaseGrouper(group_axis, groupings, sort=sort, mutated=mutated)
    return grouper, exclusions, obj

def _is_label_like(val) -> bool:
    return isinstance(val, (str, tuple)) or (val is not None and is_scalar(val))

def _convert_grouper(axis: Index, grouper):
    if isinstance(grouper, dict):
        return grouper.get
    elif isinstance(grouper, Series):
        if grouper.index.equals(axis):
            return grouper._values
        else:
            return grouper.reindex(axis)._values
    elif isinstance(grouper, (list, Series, Index, np.ndarray)):
        if len(grouper) != len(axis):
            raise ValueError(""Grouper and axis must be same length"")
        return grouper
    else:",[47]
")

class FacebookIE(InfoExtractor):
    """"""Information Extractor for Facebook""""""
    _VALID_URL = r'''(?x)
        (?:https?://)?(?:\w+\.)?facebook\.com/
        (?:[^#?]*\#!/)?
        (?:video/video\.php|photo\.php|video/embed)\?(?:.*?)
        (?:v|video_id)=(?P<id>[0-9]+)
        (?:.*)'''
    _LOGIN_URL = 'https://www.facebook.com/login.php?next=http%3A%2F%2Ffacebook.com%2Fhome.php&login_attempt=1'
    _CHECKPOINT_URL = 'https://www.facebook.com/checkpoint/?next=http%3A%2F%2Ffacebook.com%2Fhome.php&_fb_noscript=1'
    _NETRC_MACHINE = 'facebook'
    IE_NAME = 'facebook'
    _TEST = {
        'url': 'https://www.facebook.com/photo.php?v=120708114770723',
        'md5': '48975a41ccc4b7a581abd68651c1a5a8',
        'info_dict': {
            'id': '120708114770723',
            'ext': 'mp4',
            'duration': 279,
            'title': 'PEOPLE ARE AWESOME 2013'
        }
    }
    def report_login(self):
        """"""Report attempt to log in.""""""
        self.to_screen('Logging in')
    def _login(self):
        (useremail, password) = self._get_login_info()
        if useremail is None:
            return
        login_page_req = compat_urllib_request.Request(self._LOGIN_URL)
        login_page_req.add_header('Cookie', 'locale=en_US')
        self.report_login()
        login_page = self._download_webpage(login_page_req, None, note=False,
            errnote='Unable to download login page')
        lsd = self._search_regex(
            r'<input type=""hidden"" name=""lsd"" value=""([^""]*)""',
            login_page, 'lsd')
        lgnrnd = self._search_regex(r'name=""lgnrnd"" value=""([^""]*?)""', login_page, 'lgnrnd')
        login_form = {
            'email': useremail,
            'pass': password,
            'lsd': lsd,
            'lgnrnd': lgnrnd,
            'next': 'http://facebook.com/home.php',
            'default_persistent': '0',
            'legacy_return': '1',
            'timezone': '-60',
            'trynum': '1',
            }
        request = compat_urllib_request.Request(self._LOGIN_URL, compat_urllib_parse.urlencode(login_form))
        request.add_header('Content-Type', 'application/x-www-form-urlencoded')
        try:
            login_results = compat_urllib_request.urlopen(request).read()
            if re.search(r'<form(.*)name=""login""(.*)</form>', login_results) is not None:
                self._downloader.report_warning('unable to log in: bad username/password, or exceded login rate limit (~3/min). Check credentials or wait.')
                return
            check_form = {
                'fb_dtsg': self._search_regex(r'""fb_dtsg"":""(.*?)""', login_results, 'fb_dtsg'),
                'nh': self._search_regex(r'name=""nh"" value=""(\w*?)""', login_results, 'nh'),
                'name_action_selected': 'dont_save',
                'submit[Continue]': self._search_regex(r'<input value=""(.*?)"" name=""submit\[Continue\]""', login_results, 'continue'),
            }
            check_req = compat_urllib_request.Request(self._CHECKPOINT_URL, compat_urllib_parse.urlencode(check_form))
            check_req.add_header('Content-Type', 'application/x-www-form-urlencoded')
            check_response = compat_urllib_request.urlopen(check_req).read()
            if re.search(r'id=""checkpointSubmitButton""', check_response) is not None:
                self._downloader.report_warning('Unable to confirm login, you have to login in your brower and authorize the login.')
        except (compat_urllib_error.URLError, compat_http_client.HTTPException, socket.error) as err:
            self._downloader.report_warning('unable to log in: %s' % compat_str(err))
            return
    def _real_initialize(self):
        self._login()
    def _real_extract(self, url):
        mobj = re.match(self._VALID_URL, url)
        if mobj is None:
            raise ExtractorError('Invalid URL: %s' % url)
        video_id = mobj.group('id')
        url = 'https://www.facebook.com/video/video.php?v=%s' % video_id
        webpage = self._download_webpage(url, video_id)
        BEFORE = '{swf.addParam(param[0], param[1]);});\n'
        AFTER = '.forEach(function(variable) {swf.addVariable(variable[0], variable[1]);});'
        m = re.search(re.escape(BEFORE) + '(.*?)' + re.escape(AFTER), webpage)
        if not m:
            m_msg = re.search(r'class=""[^""]*uiInterstitialContent[^""]*""><div>(.*?)</div>', webpage)
            if m_msg is not None:
                raise ExtractorError(
                    'The video is not available, Facebook said: ""%s""' % m_msg.group(1),
                    expected=True)
            else:
                raise ExtractorError('Cannot parse data')
        data = dict(json.loads(m.group(1)))
        params_raw = compat_urllib_parse.unquote(data['params'])
        params = json.loads(params_raw)
        video_data = params['video_data'][0]
        video_url = video_data.get('hd_src')
        if not video_url:
            video_url = video_data['sd_src']
        if not video_url:
            raise ExtractorError('Cannot find video URL')
        video_duration = int(video_data['video_duration'])
        thumbnail = video_data['thumbnail_src']
        video_title = self._html_search_regex(
            r'<h2 class=""uiHeaderTitle"">([^<]*)</h2>', webpage, 'title')
        info = {
            'id': video_id,
            'title': video_title,
            'url': video_url,
            'ext': 'mp4',
            'duration': video_duration,
            'thumbnail': thumbnail,
        }","[38, 39, 57, 60, 66, 69, 71, 73]"
"        return type(self)(result, name=self.name)
    # --------------------------------------------------------------------
    # List-like Methods
    def delete(self, loc):
        new_i8s = np.delete(self.asi8, loc)
        freq = None
        if is_period_dtype(self):
            freq = self.freq
        elif is_integer(loc):
            if loc in (0, -len(self), -1, len(self) - 1):
                freq = self.freq
        else:
            if is_list_like(loc):
                loc = lib.maybe_indices_to_slice(ensure_int64(np.array(loc)), len(self))
            if isinstance(loc, slice) and loc.step in (1, None):
                if loc.start in (0, None) or loc.stop in (len(self), None):
                    freq = self.freq
        arr = type(self._data)._simple_new(new_i8s, dtype=self.dtype, freq=freq)
        return type(self)._simple_new(arr, name=self.name)
    # --------------------------------------------------------------------
    # Join/Set Methods
    def _wrap_joined_index(self, joined: np.ndarray, other):
        assert other.dtype == self.dtype, (other.dtype, self.dtype)
        name = get_op_result_name(self, other)
        if is_period_dtype(self.dtype):
            freq = self.freq
        else:
            self = cast(DatetimeTimedeltaMixin, self)
            freq = self.freq if self._can_fast_union(other) else None
        new_data = type(self._data)._simple_new(joined, dtype=self.dtype, freq=freq)
        return type(self)._simple_new(new_data, name=name)

class DatetimeTimedeltaMixin(DatetimeIndexOpsMixin, Int64Index):
    """"""
    Mixin class for methods shared by DatetimeIndex and TimedeltaIndex,
    but not PeriodIndex
    """"""
    # Compat for frequency inference, see GH#23789
    _is_monotonic_increasing = Index.is_monotonic_increasing
    _is_monotonic_decreasing = Index.is_monotonic_decreasing
    _is_unique = Index.is_unique
    _freq = lib.no_default
    @property
    def freq(self):
        """"""
        In limited circumstances, our freq may differ from that of our _data.
        """"""
        if self._freq is not lib.no_default:
            return self._freq
        return self._data.freq
    @property
    def freqstr(self):
        """"""
        Return the frequency object as a string if its set, otherwise None.
        """"""
        if self.freq is None:
            return None
        return self.freq.freqstr
    def _with_freq(self, freq):
        index = self.copy(deep=False)
        if freq is None:
            # Even if we _can_ have a freq, we might want to set it to None
            index._freq = None
        elif len(self) == 0 and isinstance(freq, DateOffset):
            # Always valid.  In the TimedeltaArray case, we assume this
            #  is a Tick offset.
            index._freq = freq
        else:
            assert freq == ""infer"", freq
            freq = to_offset(self.inferred_freq)
            index._freq = freq
        return index
    def _shallow_copy(self, values=None, name: Label = lib.no_default):
        name = self.name if name is lib.no_default else name
        cache = self._cache.copy() if values is None else {}
        if values is None:
            values = self._data
        if isinstance(values, np.ndarray):
            # TODO: We would rather not get here
            values = type(self._data)(values, dtype=self.dtype)
        result = type(self)._simple_new(values, name=name)
        result._cache = cache
        return result
    # --------------------------------------------------------------------
    # Set Operation Methods
    @Appender(Index.difference.__doc__)
    def difference(self, other, sort=None):
        new_idx = super().difference(other, sort=sort)._with_freq(None)
        return new_idx
    def intersection(self, other, sort=False):
        """"""
        Specialized intersection for DatetimeIndex/TimedeltaIndex.
        May be much faster than Index.intersection
        Parameters
        ----------
        other : Same type as self or array-like
        sort : False or None, default False
            Sort the resulting index if possible.
            .. versionadded:: 0.24.0
            .. versionchanged:: 0.24.1
               Changed the default to ``False`` to match the behaviour","[72, 73, 75, 76, 79, 80, 81, 82, 83, 84, 85]"
"class HTTPMessage(object):
    """"""Abstract class for HTTP messages.""""""
    def __init__(self, orig):
        self._orig = orig
    def iter_body(self, chunk_size):
        """"""Return an iterator over the body.""""""
        raise NotImplementedError()
    def iter_lines(self, chunk_size):
        """"""Return an iterator over the body yielding (`line`, `line_feed`).""""""
        raise NotImplementedError()
    @property
    def headers(self):
        """"""Return a `str` with the message's headers.""""""
        raise NotImplementedError()
    @property
    def encoding(self):
        """"""Return a `str` with the message's encoding, if known.""""""
        raise NotImplementedError()
    @property
    def body(self):
        """"""Return a `bytes` with the message's body.""""""
        raise NotImplementedError()
    @property
    def content_type(self):
        """"""Return the message content type.""""""
        ct = self._orig.headers.get('Content-Type', '')
        if not isinstance(ct, str):
            ct = ct.decode('utf8')
        return ct

class HTTPResponse(HTTPMessage):
    """"""A :class:`requests.models.Response` wrapper.""""""
    def iter_body(self, chunk_size=1):
        return self._orig.iter_content(chunk_size=chunk_size)
    def iter_lines(self, chunk_size):
        return ((line, b'\n') for line in self._orig.iter_lines(chunk_size))
    #noinspection PyProtectedMember
    @property
    def headers(self):
        original = self._orig.raw._original_response
        version = {9: '0.9', 10: '1.0', 11: '1.1'}[original.version]
        status_line = 'HTTP/{version} {status} {reason}'.format(
            version=version,
            status=original.status,
            reason=original.reason
        )
        headers = [status_line]
        try:
            # `original.msg` is a `http.client.HTTPMessage` on Python 3
            # `_headers` is a 2-tuple
            headers.extend(
                '%s: %s' % header for header in original.msg._headers)
        except AttributeError:
            # and a `httplib.HTTPMessage` on Python 2.x
            # `headers` is a list of `name: val<CRLF>`.
            headers.extend(h.strip() for h in original.msg.headers)
        return '\r\n'.join(headers)
    @property
    def encoding(self):
        return self._orig.encoding or 'utf8'
    @property
    def body(self):
        # Only now the response body is fetched.
        # Shouldn't be touched unless the body is actually needed.
        return self._orig.content

class HTTPRequest(HTTPMessage):
    """"""A :class:`requests.models.Request` wrapper.""""""
    def iter_body(self, chunk_size):
        yield self.body
    def iter_lines(self, chunk_size):
        yield self.body, b''
    @property
    def headers(self):
        url = urlsplit(self._orig.url)
        request_line = '{method} {path}{query} HTTP/1.1'.format(
            method=self._orig.method,
            path=url.path or '/',
            query='?' + url.query if url.query else ''
        )
        headers = dict(self._orig.headers)
        if 'Host' not in headers:
            headers['Host'] = url.netloc.split('@')[-1]
        headers = ['%s: %s' % (name, value)
                   for name, value in headers.items()]
        headers.insert(0, request_line)
        headers = '\r\n'.join(headers).strip()
        if isinstance(headers, bytes):
            # Python < 3
            headers = headers.decode('utf8')
        return headers
    @property
    def encoding(self):
        return 'utf8'
    @property
    def body(self):
        body = self._orig.body
        if isinstance(body, str):
            # Happens with JSON/form request data parsed from the command line.
            body = body.encode('utf8')","[101, 102]"
"    """"""
    if is_scalar(obj):
        return libmissing.checknull_old(obj)
    # hack (for now) because MI registers as ndarray
    elif isinstance(obj, ABCMultiIndex):
        raise NotImplementedError(""isna is not defined for MultiIndex"")
    elif isinstance(obj, type):
        return False
    elif isinstance(obj, (ABCSeries, np.ndarray, ABCIndexClass, ABCExtensionArray)):
        return _isna_ndarraylike_old(obj)
    elif isinstance(obj, ABCDataFrame):
        return obj.isna()
    elif isinstance(obj, list):
        return _isna_ndarraylike_old(np.asarray(obj, dtype=object))
    elif hasattr(obj, ""__array__""):
        return _isna_ndarraylike_old(np.asarray(obj))
    else:
        return False

_isna = _isna_new

def _use_inf_as_na(key):
    """"""
    Option change callback for na/inf behaviour.
    Choose which replacement for numpy.isnan / -numpy.isfinite is used.
    Parameters
    ----------
    flag: bool
        True means treat None, NaN, INF, -INF as null (old way),
        False means None and NaN are null, but INF, -INF are not null
        (new way).
    Notes
    -----
    This approach to setting global module values is discussed and
    approved here:
    * https://stackoverflow.com/questions/4859217/
      programmatically-creating-variables-in-python/4859312#4859312
    """"""
    flag = get_option(key)
    if flag:
        globals()[""_isna""] = _isna_old
    else:
        globals()[""_isna""] = _isna_new

def _isna_ndarraylike(obj):
    values = getattr(obj, ""_values"", obj)
    dtype = values.dtype
    if is_extension_array_dtype(dtype):
        result = values.isna()
    elif is_string_dtype(dtype):
        result = _isna_string_dtype(values, dtype, old=False)
    elif needs_i8_conversion(dtype):
        # this is the NaT pattern
        result = values.view(""i8"") == iNaT
    else:
        result = np.isnan(values)
    # box
    if isinstance(obj, ABCSeries):
        result = obj._constructor(result, index=obj.index, name=obj.name, copy=False)
    return result

def _isna_ndarraylike_old(obj):
    values = getattr(obj, ""_values"", obj)
    dtype = values.dtype
    if is_string_dtype(dtype):
        result = _isna_string_dtype(values, dtype, old=True)
    elif needs_i8_conversion(dtype):
        # this is the NaT pattern
        result = values.view(""i8"") == iNaT
    else:
        result = ~np.isfinite(values)
    # box
    if isinstance(obj, ABCSeries):
        result = obj._constructor(result, index=obj.index, name=obj.name, copy=False)
    return result

def _isna_string_dtype(values: np.ndarray, dtype: np.dtype, old: bool) -> np.ndarray:
    # Working around NumPy ticket 1542
    shape = values.shape
    if is_string_like_dtype(dtype):
        result = np.zeros(values.shape, dtype=bool)
    else:
        result = np.empty(shape, dtype=bool)
        if old:
            vec = libmissing.isnaobj_old(values.ravel())
        else:
            vec = libmissing.isnaobj(values.ravel())
        result[...] = vec.reshape(shape)
    return result

def notna(obj):
    """"""
    Detect non-missing values for an array-like object.
    This function takes a scalar or array-like object and indicates
    whether values are valid (not missing, which is ``NaN`` in numeric
    arrays, ``None`` or ``NaN`` in object arrays, ``NaT`` in datetimelike).
    Parameters
    ----------
    obj : array-like or object value
        Object to check for *not* null or *non*-missing values.
    Returns
    -------
    bool or array-like of bool","[9, 13, 15, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 62, 63, 64, 65, 67, 68, 69, 70, 73, 77, 78, 79, 84]"
"from collections import namedtuple

Command = namedtuple('Command', ('script', 'stdout', 'stderr'))
CorrectedCommand = namedtuple('CorrectedCommand', ('script', 'side_effect', 'priority'))
Rule = namedtuple('Rule', ('name', 'match', 'get_new_command',
                           'enabled_by_default', 'side_effect',
                           'priority', 'requires_output'))

class RulesNamesList(list):
    """"""Wrapper a top of list for storing rules names.""""""
    def __contains__(self, item):
        return super(RulesNamesList, self).__contains__(item.name)

class Settings(dict):
    def __getattr__(self, item):
        return self.get(item)
    def update(self, **kwargs):
        """"""Returns new settings with new values from `kwargs`.""""""
        conf = dict(self)
        conf.update(kwargs)
        return Settings(conf)","[26, 27]"
"                self.toolbar = self.mpl.rcParams['toolbar']
                self.mpl.rcParams['toolbar'] = 'None'
                self.mininterval = max(mininterval, 0.5)
                self.fig, ax = plt.subplots(figsize=(9, 2.2))
                # self.fig.subplots_adjust(bottom=0.2)
                if total:
                    self.xdata = []
                    self.ydata = []
                    self.zdata = []
                else:
                    self.xdata = deque([])
                    self.ydata = deque([])
                    self.zdata = deque([])
                self.line1, = ax.plot(self.xdata, self.ydata, color='b')
                self.line2, = ax.plot(self.xdata, self.zdata, color='k')
                ax.set_ylim(0, 0.001)
                if total:
                    ax.set_xlim(0, 100)
                    ax.set_xlabel('percent')
                    self.fig.legend((self.line1, self.line2), ('cur', 'est'),
                                    loc='center right')
                    # progressbar
                    self.hspan = plt.axhspan(0, 0.001,
                                             xmin=0, xmax=0, color='g')
                else:
                    # ax.set_xlim(-60, 0)
                    ax.set_xlim(0, 60)
                    ax.invert_xaxis()
                    ax.set_xlabel('seconds')
                    ax.legend(('cur', 'est'), loc='lower left')
                ax.grid()
                # ax.set_xlabel('seconds')
                ax.set_ylabel((unit if unit else 'it') + '/s')
                if unit_scale:
                    plt.ticklabel_format(style='sci', axis='y',
                                         scilimits=(0, 0))
                    ax.yaxis.get_offset_text().set_x(-0.15)
                # Remember if external environment is interactive
                self.wasion = plt.isinteractive()
                plt.ion()
                self.ax = ax
        else:
            # Initialize the screen printer
            self.sp = StatusPrinter(self.file)
            if not disable:
                self.sp(format_meter(
                    0, total, 0, ncols, self.desc, ascii, unit, unit_scale))
        # Init the time/iterations counters
        self.start_t = self.last_print_t = time()
        self.last_print_n = 0
        self.n = 0
    def __len__(self):
        return len(self.iterable)
    def __iter__(self):
        ''' Backward-compatibility to use: for x in tqdm(iterable) '''
        # Inlining instance variables as locals (speed optimisation)
        iterable = self.iterable
        # If the bar is disabled, then just walk the iterable
        # (note: keep this check outside the loop for performance)
        if self.disable:
            for obj in iterable:
                yield obj
        else:
            ncols = self.ncols
            mininterval = self.mininterval
            miniters = self.miniters
            dynamic_miniters = self.dynamic_miniters
            unit = self.unit
            unit_scale = self.unit_scale
            ascii = self.ascii
            start_t = self.start_t
            last_print_t = self.last_print_t
            last_print_n = self.last_print_n
            n = self.n
            gui = self.gui
            if gui: # pragma: no cover
                plt = self.plt
                ax = self.ax
                xdata = self.xdata
                ydata = self.ydata
                zdata = self.zdata
                line1 = self.line1
                line2 = self.line2
            else:
                sp = self.sp
            for obj in iterable:
                yield obj
                # Update and print the progressbar.
                # Note: does not call self.update(1) for speed optimisation.
                n += 1
                delta_it = n - last_print_n
                # check the counter first (avoid calls to time())
                if delta_it >= miniters:
                    cur_t = time()
                    delta_t = cur_t - last_print_t
                    if delta_t >= mininterval:
                        elapsed = cur_t - start_t
                        if gui: # pragma: no cover
                            # Inline due to multiple calls
                            total = self.total
                            # instantaneous rate
                            y = delta_it / delta_t
                            # smoothed rate
                            z = n / elapsed
                            # update line data
                            xdata.append(n * 100.0 / total if total else cur_t)
                            ydata.append(y)
                            zdata.append(z)
                            # Discard old values
                            # xmin, xmax = ax.get_xlim()
                            # if (not total) and elapsed > xmin * 1.1:
                            if (not total) and elapsed > 66:
                                xdata.popleft()
                                ydata.popleft()
                                zdata.popleft()
                            ymin, ymax = ax.get_ylim()
                            if y > ymax or z > ymax:","[56, 82, 105]"
"    def ema(x, mu=None, alpha=0.3):
        """"""
        Exponential moving average: smoothing to give progressively lower
        weights to older values.
        Parameters
        ----------
        x  : float
            New value to include in EMA.
        mu  : float, optional
            Previous EMA value.
        alpha  : float, optional
            Smoothing factor in range [0, 1], [default: 0.3].
            Increase to give more weight to recent values.
            Ranges from 0 (yields mu) to 1 (yields x).
        """"""
        return x if mu is None else (alpha * x) + (1 - alpha) * mu
    @staticmethod
    def status_printer(file):
        """"""
        Manage the printing and in-place updating of a line of characters.
        Note that if the string is longer than a line, then in-place
        updating may not work (it will print a new line at each refresh).
        """"""
        fp = file
        fp_flush = getattr(fp, 'flush', lambda: None)  # pragma: no cover
        def fp_write(s):
            fp.write(_unicode(s))
            fp_flush()
        last_len = [0]
        def print_status(s):
            len_s = len(s)
            fp_write('\r' + s + (' ' * max(last_len[0] - len_s, 0)))
            last_len[0] = len_s
        return print_status
    @staticmethod
    def format_meter(n, total, elapsed, ncols=None, prefix='', ascii=False,
                     unit='it', unit_scale=False, rate=None, bar_format=None,
                     postfix=None, unit_divisor=1000, **extra_kwargs):
        """"""
        Return a string-based progress bar given some parameters
        Parameters
        ----------
        n  : int
            Number of finished iterations.
        total  : int
            The expected total number of iterations. If meaningless (), only
            basic progress statistics are displayed (no ETA).
        elapsed  : float
            Number of seconds passed since start.
        ncols  : int, optional
            The width of the entire output message. If specified,
            dynamically resizes the progress meter to stay within this bound
            [default: None]. The fallback meter width is 10 for the progress
            bar + no limit for the iterations counter and statistics. If 0,
            will not print any meter (only stats).
        prefix  : str, optional
            Prefix message (included in total width) [default: ''].
            Use as {desc} in bar_format string.
        ascii  : bool, optional
            If not set, use unicode (smooth blocks) to fill the meter
            [default: False]. The fallback is to use ASCII characters
            (1-9 #).
        unit  : str, optional
            The iteration unit [default: 'it'].
        unit_scale  : bool or int or float, optional
            If 1 or True, the number of iterations will be printed with an
            appropriate SI metric prefix (k = 10^3, M = 10^6, etc.)
            [default: False]. If any other non-zero number, will scale
            `total` and `n`.
        rate  : float, optional
            Manual override for iteration rate.
            If [default: None], uses n/elapsed.
        bar_format  : str, optional
            Specify a custom bar string formatting. May impact performance.
            [default: '{l_bar}{bar}{r_bar}'], where
            l_bar='{desc}: {percentage:3.0f}%|' and
            r_bar='| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, '
              '{rate_fmt}{postfix}]'
            Possible vars: l_bar, bar, r_bar, n, n_fmt, total, total_fmt,
              percentage, rate, rate_fmt, rate_noinv, rate_noinv_fmt,
              rate_inv, rate_inv_fmt, elapsed, elapsed_s,
              remaining, remaining_s, desc, postfix, unit.
            Note that a trailing "": "" is automatically removed after {desc}
            if the latter is empty.
        postfix  : *, optional
            Similar to `prefix`, but placed at the end
            (e.g. for additional stats).
            Note: postfix is usually a string (not a dict) for this method,
            and will if possible be set to postfix = ', ' + postfix.
            However other types are supported (#382).
        unit_divisor  : float, optional
            [default: 1000], ignored unless `unit_scale` is True.
        Returns
        -------
        out  : Formatted meter and stats, ready to display.
        """"""
        # sanity check: total
        if total and n > total:
            total = None
        # apply custom scale if necessary
        if unit_scale and unit_scale not in (True, 1):
            total *= unit_scale
            n *= unit_scale
            if rate:
                rate *= unit_scale  # by default rate = 1 / self.avg_time
            unit_scale = False
        format_interval = tqdm.format_interval
        elapsed_str = format_interval(elapsed)
        # if unspecified, attempt to use rate = average speed
        # (we allow manual override since predicting time is an arcane art)
        if rate is None and elapsed:
            rate = n / elapsed
        inv_rate = 1 / rate if rate else None
        format_sizeof = tqdm.format_sizeof",[112]
"                continue
            for comment in comments:
                if is_type_comment(comment):
                    return True
        return False
    def contains_multiline_strings(self) -> bool:
        for leaf in self.leaves:
            if is_multiline_string(leaf):
                return True
        return False
    def maybe_remove_trailing_comma(self, closing: Leaf) -> bool:
        """"""Remove trailing comma if there is one and it's safe.""""""
        if not (
            self.leaves
            and self.leaves[-1].type == token.COMMA
            and closing.type in CLOSING_BRACKETS
        ):
            return False
        if closing.type == token.RBRACE:
            self.remove_trailing_comma()
            return True
        if closing.type == token.RSQB:
            comma = self.leaves[-1]
            if comma.parent and comma.parent.type == syms.listmaker:
                self.remove_trailing_comma()
                return True
        # For parens let's check if it's safe to remove the comma.
        # Imports are always safe.
        if self.is_import:
            self.remove_trailing_comma()
            return True
        # Otherwise, if the trailing one is the only one, we might mistakenly
        # change a tuple into a different type by removing the comma.
        depth = closing.bracket_depth + 1
        commas = 0
        opening = closing.opening_bracket
        for _opening_index, leaf in enumerate(self.leaves):
            if leaf is opening:
                break
        else:
            return False
        for leaf in self.leaves[_opening_index + 1 :]:
            if leaf is closing:
                break
            bracket_depth = leaf.bracket_depth
            if bracket_depth == depth and leaf.type == token.COMMA:
                commas += 1
                if leaf.parent and leaf.parent.type == syms.arglist:
                    commas += 1
                    break
        if commas > 1:
            self.remove_trailing_comma()
            return True
        return False
    def append_comment(self, comment: Leaf) -> bool:
        """"""Add an inline or standalone comment to the line.""""""
        if (
            comment.type == STANDALONE_COMMENT
            and self.bracket_tracker.any_open_brackets()
        ):
            comment.prefix = """"
            return False
        if comment.type != token.COMMENT:
            return False
        if not self.leaves:
            comment.type = STANDALONE_COMMENT
            comment.prefix = """"
            return False
        self.comments.setdefault(id(self.leaves[-1]), []).append(comment)
        return True
    def comments_after(self, leaf: Leaf) -> List[Leaf]:
        """"""Generate comments that should appear directly after `leaf`.""""""
        return self.comments.get(id(leaf), [])
    def remove_trailing_comma(self) -> None:
        """"""Remove the trailing comma and moves the comments attached to it.""""""
        trailing_comma = self.leaves.pop()
        trailing_comma_comments = self.comments.pop(id(trailing_comma), [])
        self.comments.setdefault(id(self.leaves[-1]), []).extend(
            trailing_comma_comments
        )
    def is_complex_subscript(self, leaf: Leaf) -> bool:
        """"""Return True iff `leaf` is part of a slice with non-trivial exprs.""""""
        open_lsqb = self.bracket_tracker.get_open_lsqb()
        if open_lsqb is None:
            return False
        subscript_start = open_lsqb.next_sibling
        if isinstance(subscript_start, Node):
            if subscript_start.type == syms.listmaker:
                return False
            if subscript_start.type == syms.subscriptlist:
                subscript_start = child_towards(subscript_start, leaf)
        return subscript_start is not None and any(
            n.type in TEST_DESCENDANTS for n in subscript_start.pre_order()
        )
    def __str__(self) -> str:
        """"""Render the line.""""""
        if not self:
            return ""\n""
        indent = ""    "" * self.depth
        leaves = iter(self.leaves)
        first = next(leaves)",[59]
"from thefuck.utils import replace_argument, for_app

@for_app('php')
def match(command):
    return ""php -s"" in command.script

def get_new_command(command):
    return replace_argument(command.script, ""-s"", ""-S"")

requires_output = False","[5, 10, 11]"
"import re
from typing import List
import numpy as np
from pandas.util._decorators import Appender
from pandas.core.dtypes.common import is_extension_array_dtype, is_list_like
from pandas.core.dtypes.concat import concat_compat
from pandas.core.dtypes.generic import ABCMultiIndex
from pandas.core.dtypes.missing import notna
from pandas.core.arrays import Categorical
from pandas.core.frame import DataFrame, _shared_docs
from pandas.core.indexes.base import Index
from pandas.core.reshape.concat import concat
from pandas.core.tools.numeric import to_numeric

@Appender(
    _shared_docs[""melt""]
    % dict(caller=""pd.melt(df, "", versionadded="""", other=""DataFrame.melt"")
)
def melt(
    frame: DataFrame,
    id_vars=None,
    value_vars=None,
    var_name=None,
    value_name=""value"",
    col_level=None,
) -> DataFrame:
    # TODO: what about the existing index?
    # If multiindex, gather names of columns on all level for checking presence
    # of `id_vars` and `value_vars`
    if isinstance(frame.columns, ABCMultiIndex):
        cols = [x for c in frame.columns for x in c]
    else:
        cols = list(frame.columns)
    if id_vars is not None:
        if not is_list_like(id_vars):
            id_vars = [id_vars]
        elif isinstance(frame.columns, ABCMultiIndex) and not isinstance(id_vars, list):
            raise ValueError(
                ""id_vars must be a list of tuples when columns are a MultiIndex""
            )
        else:
            # Check that `id_vars` are in frame
            id_vars = list(id_vars)
            missing = Index(np.ravel(id_vars)).difference(cols)
            if not missing.empty:
                raise KeyError(
                    ""The following 'id_vars' are not present""
                    "" in the DataFrame: {missing}""
                    """".format(missing=list(missing))
                )
    else:
        id_vars = []
    if value_vars is not None:
        if not is_list_like(value_vars):
            value_vars = [value_vars]
        elif isinstance(frame.columns, ABCMultiIndex) and not isinstance(
            value_vars, list
        ):
            raise ValueError(
                ""value_vars must be a list of tuples when columns are a MultiIndex""
            )
        else:
            value_vars = list(value_vars)
            # Check that `value_vars` are in frame
            missing = Index(np.ravel(value_vars)).difference(cols)
            if not missing.empty:
                raise KeyError(
                    ""The following 'value_vars' are not present in""
                    "" the DataFrame: {missing}""
                    """".format(missing=list(missing))
                )
        frame = frame.loc[:, id_vars + value_vars]
    else:
        frame = frame.copy()
    if col_level is not None:  # allow list or other?
        # frame is a copy
        frame.columns = frame.columns.get_level_values(col_level)
    if var_name is None:
        if isinstance(frame.columns, ABCMultiIndex):
            if len(frame.columns.names) == len(set(frame.columns.names)):
                var_name = frame.columns.names
            else:
                var_name = [
                    ""variable_{i}"".format(i=i) for i in range(len(frame.columns.names))
                ]
        else:
            var_name = [
                frame.columns.name if frame.columns.name is not None else ""variable""
            ]
    if isinstance(var_name, str):
        var_name = [var_name]
    N, K = frame.shape
    K -= len(id_vars)
    mdata = {}
    for col in id_vars:
        id_data = frame.pop(col)
        if is_extension_array_dtype(id_data):
            id_data = concat([id_data] * K, ignore_index=True)
        else:
            id_data = np.tile(id_data.values, K)
        mdata[col] = id_data
    mcolumns = id_vars + var_name + [value_name]
    mdata[value_name] = frame.values.ravel(""F"")
    for i, col in enumerate(var_name):
        # asanyarray will keep the columns as an Index
        mdata[col] = np.asanyarray(frame.columns._get_level_values(i)).repeat(N)
    return frame._constructor(mdata, columns=mcolumns)

def lreshape(data: DataFrame, groups, dropna: bool = True, label=None) -> DataFrame:
    """"""
    Reshape long-format data to wide. Generalized inverse of DataFrame.pivot
","[49, 71]"
"            i8 = i8.copy()
            i8[mask] = 0
        return i8.argmax()
    # --------------------------------------------------------------------
    # Rendering Methods
    def _format_with_header(self, header, na_rep=""NaT"", **kwargs):
        return header + list(self._format_native_types(na_rep, **kwargs))
    @property
    def _formatter_func(self):
        raise AbstractMethodError(self)
    def _format_attrs(self):
        """"""
        Return a list of tuples of the (attr,formatted_value).
        """"""
        attrs = super()._format_attrs()
        for attrib in self._attributes:
            if attrib == ""freq"":
                freq = self.freqstr
                if freq is not None:
                    freq = repr(freq)
                attrs.append((""freq"", freq))
        return attrs
    # --------------------------------------------------------------------
    def _convert_scalar_indexer(self, key, kind=None):
        """"""
        We don't allow integer or float indexing on datetime-like when using
        loc.
        Parameters
        ----------
        key : label of the slice bound
        kind : {'ix', 'loc', 'getitem', 'iloc'} or None
        """"""
        assert kind in [""ix"", ""loc"", ""getitem"", ""iloc"", None]
        # we don't allow integer/float indexing for loc
        # we don't allow float indexing for ix/getitem
        if is_scalar(key):
            is_int = is_integer(key)
            is_flt = is_float(key)
            if kind in [""loc""] and (is_int or is_flt):
                self._invalid_indexer(""index"", key)
            elif kind in [""ix"", ""getitem""] and is_flt:
                self._invalid_indexer(""index"", key)
        return super()._convert_scalar_indexer(key, kind=kind)
    __add__ = make_wrapped_arith_op(""__add__"")
    __radd__ = make_wrapped_arith_op(""__radd__"")
    __sub__ = make_wrapped_arith_op(""__sub__"")
    __rsub__ = make_wrapped_arith_op(""__rsub__"")
    __pow__ = make_wrapped_arith_op(""__pow__"")
    __rpow__ = make_wrapped_arith_op(""__rpow__"")
    __mul__ = make_wrapped_arith_op(""__mul__"")
    __rmul__ = make_wrapped_arith_op(""__rmul__"")
    __floordiv__ = make_wrapped_arith_op(""__floordiv__"")
    __rfloordiv__ = make_wrapped_arith_op(""__rfloordiv__"")
    __mod__ = make_wrapped_arith_op(""__mod__"")
    __rmod__ = make_wrapped_arith_op(""__rmod__"")
    __divmod__ = make_wrapped_arith_op(""__divmod__"")
    __rdivmod__ = make_wrapped_arith_op(""__rdivmod__"")
    __truediv__ = make_wrapped_arith_op(""__truediv__"")
    __rtruediv__ = make_wrapped_arith_op(""__rtruediv__"")
    def isin(self, values, level=None):
        """"""
        Compute boolean array of whether each index value is found in the
        passed set of values.
        Parameters
        ----------
        values : set or sequence of values
        Returns
        -------
        is_contained : ndarray (boolean dtype)
        """"""
        if level is not None:
            self._validate_index_level(level)
        if not isinstance(values, type(self)):
            try:
                values = type(self)(values)
            except ValueError:
                return self.astype(object).isin(values)
        return algorithms.isin(self.asi8, values.asi8)
    @Appender(_index_shared_docs[""repeat""] % _index_doc_kwargs)
    def repeat(self, repeats, axis=None):
        nv.validate_repeat(tuple(), dict(axis=axis))
        freq = self.freq if is_period_dtype(self) else None
        return self._shallow_copy(self.asi8.repeat(repeats), freq=freq)
    @Appender(_index_shared_docs[""where""] % _index_doc_kwargs)
    def where(self, cond, other=None):
        other = _ensure_datetimelike_to_i8(other, to_utc=True)
        values = _ensure_datetimelike_to_i8(self, to_utc=True)
        result = np.where(cond, values, other).astype(""i8"")
        result = self._ensure_localized(result, from_utc=True)
        return self._shallow_copy(result)
    def _summary(self, name=None):
        """"""
        Return a summarized representation.
        Parameters
        ----------
        name : str
            Name to use in the summary representation.
        Returns
        -------
        str
            Summarized representation of the index.
        """"""
        formatter = self._formatter_func
        if len(self) > 0:
            index_summary = f"", {formatter(self[0])} to {formatter(self[-1])}""","[98, 99]"
"    If restricted is set, use a stricter subset of allowed characters.
    Set is_id if this is not an arbitrary string, but an ID that should be kept if possible
    """"""
    def replace_insane(char):
        if char == '?' or ord(char) < 32 or ord(char) == 127:
            return ''
        elif char == '""':
            return '' if restricted else '\''
        elif char == ':':
            return '_-' if restricted else ' -'
        elif char in '\\/|*<>':
            return '_'
        if restricted and (char in '!&\'()[]{}$;`^,#' or char.isspace()):
            return '_'
        if restricted and ord(char) > 127:
            return '_'
        return char
    # Handle timestamps
    s = re.sub(r'[0-9]+(?::[0-9]+)+', lambda m: m.group(0).replace(':', '_'), s)
    result = ''.join(map(replace_insane, s))
    if not is_id:
        while '__' in result:
            result = result.replace('__', '_')
        result = result.strip('_')
        # Common case of ""Foreign band name - English song title""
        if restricted and result.startswith('-_'):
            result = result[2:]
        if result.startswith('-'):
            result = '_' + result[len('-'):]
        result = result.lstrip('.')
        if not result:
            result = '_'
    return result

def sanitize_path(s):
    """"""Sanitizes and normalizes path on Windows""""""
    if sys.platform != 'win32':
        return s
    drive_or_unc, _ = os.path.splitdrive(s)
    if sys.version_info < (2, 7) and not drive_or_unc:
        drive_or_unc, _ = os.path.splitunc(s)
    norm_path = os.path.normpath(remove_start(s, drive_or_unc)).split(os.path.sep)
    if drive_or_unc:
        norm_path.pop(0)
    sanitized_path = [
        path_part if path_part in ['.', '..'] else re.sub('(?:[/<>:""\\|\\\\?\\*]|[\s.]$)', '#', path_part)
        for path_part in norm_path]
    if drive_or_unc:
        sanitized_path.insert(0, drive_or_unc + os.path.sep)
    return os.path.join(*sanitized_path)

def orderedSet(iterable):
    """""" Remove all duplicates from the input iterable """"""
    res = []
    for el in iterable:
        if el not in res:
            res.append(el)
    return res

def _htmlentity_transform(entity):
    """"""Transforms an HTML entity to a character.""""""
    # Known non-numeric HTML entity
    if entity in compat_html_entities.name2codepoint:
        return compat_chr(compat_html_entities.name2codepoint[entity])
    mobj = re.match(r'#(x[0-9a-fA-F]+|[0-9]+)', entity)
    if mobj is not None:
        numstr = mobj.group(1)
        if numstr.startswith('x'):
            base = 16
            numstr = '0%s' % numstr
        else:
            base = 10
        return compat_chr(int(numstr, base))
    # Unknown entity in name, return its literal representation
    return ('&%s;' % entity)

def unescapeHTML(s):
    if s is None:
        return None
    assert type(s) == compat_str
    return re.sub(
        r'&([^;]+);', lambda m: _htmlentity_transform(m.group(1)), s)

def get_subprocess_encoding():
    if sys.platform == 'win32' and sys.getwindowsversion()[0] >= 5:
        # For subprocess calls, encode with locale encoding
        # Refer to http://stackoverflow.com/a/9951851/35070
        encoding = preferredencoding()
    else:
        encoding = sys.getfilesystemencoding()
    if encoding is None:
        encoding = 'utf-8'
    return encoding

def encodeFilename(s, for_subprocess=False):
    """"""
    @param s The name of the file
    """"""
    assert type(s) == compat_str
    # Python 3 has a Unicode API
    if sys.version_info >= (3, 0):
        return s
    # Pass '' directly to use Unicode APIs on Windows 2000 and up
    # (Detecting Windows NT 4 is tricky because 'major >= 4' would
    # match Windows 9x series as well. Besides, NT 4 is obsolete.)
    if not for_subprocess and sys.platform == 'win32' and sys.getwindowsversion()[0] >= 5:
        return s
    return s.encode(get_subprocess_encoding(), 'ignore')

def decodeFilename(b, for_subprocess=False):
    if sys.version_info >= (3, 0):",[77]
"        Examples
        --------
        >>> df = pd.DataFrame([[1, 2], [3, 4]], columns=list('AB'))
        >>> df
           A  B
        0  1  2
        1  3  4
        >>> df2 = pd.DataFrame([[5, 6], [7, 8]], columns=list('AB'))
        >>> df.append(df2)
           A  B
        0  1  2
        1  3  4
        0  5  6
        1  7  8
        With `ignore_index` set to True:
        >>> df.append(df2, ignore_index=True)
           A  B
        0  1  2
        1  3  4
        2  5  6
        3  7  8
        The following, while not recommended methods for generating DataFrames,
        show two ways to generate a DataFrame from multiple data sources.
        Less efficient:
        >>> df = pd.DataFrame(columns=['A'])
        >>> for i in range(5):
        ...     df = df.append({'A': i}, ignore_index=True)
        >>> df
           A
        0  0
        1  1
        2  2
        3  3
        4  4
        More efficient:
        >>> pd.concat([pd.DataFrame([i], columns=['A']) for i in range(5)],
        ...           ignore_index=True)
           A
        0  0
        1  1
        2  2
        3  3
        4  4
        """"""
        if isinstance(other, (Series, dict)):
            if isinstance(other, dict):
                other = Series(other)
            if other.name is None and not ignore_index:
                raise TypeError(
                    ""Can only append a Series if ignore_index=True""
                    "" or if the Series has a name""
                )
            if other.name is None:
                index = None
            else:
                # other must have the same index name as self, otherwise
                # index name will be reset
                index = Index([other.name], name=self.index.name)
            idx_diff = other.index.difference(self.columns)
            try:
                combined_columns = self.columns.append(idx_diff)
            except TypeError:
                combined_columns = self.columns.astype(object).append(idx_diff)
            other = other.reindex(combined_columns, copy=False)
            other = DataFrame(
                other.values.reshape((1, len(other))),
                index=index,
                columns=combined_columns,
            )
            other = other._convert(datetime=True, timedelta=True)
            if not self.columns.equals(combined_columns):
                self = self.reindex(columns=combined_columns)
        elif isinstance(other, list):
            if not other:
                pass
            elif not isinstance(other[0], DataFrame):
                other = DataFrame(other)
                if (self.columns.get_indexer(other.columns) >= 0).all():
                    other = other.reindex(columns=self.columns)
        from pandas.core.reshape.concat import concat
        if isinstance(other, (list, tuple)):
            to_concat = [self] + other
        else:
            to_concat = [self, other]
        return concat(
            to_concat,
            ignore_index=ignore_index,
            verify_integrity=verify_integrity,
            sort=sort,
        )
    def join(self, other, on=None, how=""left"", lsuffix="""", rsuffix="""", sort=False):
        """"""
        Join columns of another DataFrame.
        Join columns with `other` DataFrame either on index or on a key
        column. Efficiently join multiple DataFrame objects by index at once by
        passing a list.
        Parameters
        ----------
        other : DataFrame, Series, or list of DataFrame
            Index should be similar to one of the columns in this one. If a
            Series is passed, its name attribute must be set, and that will be
            used as the column name in the resulting joined DataFrame.
        on : str, list of str, or array-like, optional
            Column or index level name(s) in the caller to join on the index
            in `other`, otherwise joins index-on-index. If multiple
            values given, the `other` DataFrame must have a MultiIndex. Can
            pass an array as the join key if it is not already contained in
            the calling DataFrame. Like an Excel VLOOKUP operation.
        how : {'left', 'right', 'outer', 'inner'}, default 'left'
            How to handle the operation of the two objects.
            * left: use calling frame's index (or column if on is specified)","[61, 62, 63, 66, 67, 73, 74, 75, 76, 77, 79]"
"            if data.dtype != ""float64"":
                # coerce floats to 64
                try:
                    data = data.astype(""float64"")
                    result = True
                except (TypeError, ValueError):
                    pass
        # don't coerce 0-len data
        if len(data) and (data.dtype == ""float"" or data.dtype == ""object""):
            # coerce ints if we can
            try:
                new_data = data.astype(""int64"")
                if (new_data == data).all():
                    data = new_data
                    result = True
            except (TypeError, ValueError):
                pass
        # coerce ints to 64
        if data.dtype == ""int"":
            # coerce floats to 64
            try:
                data = data.astype(""int64"")
                result = True
            except (TypeError, ValueError):
                pass
        return data, result
    def _try_convert_to_date(self, data):
        """"""
        Try to parse a ndarray like into a date column.
        Try to coerce object in epoch/iso formats and integer/float in epoch
        formats. Return a boolean if parsing was successful.
        """"""
        # no conversion on empty
        if not len(data):
            return data, False
        new_data = data
        if new_data.dtype == ""object"":
            try:
                new_data = data.astype(""int64"")
            except (TypeError, ValueError, OverflowError):
                pass
        # ignore numbers that are out of range
        if issubclass(new_data.dtype.type, np.number):
            in_range = (
                isna(new_data.values)
                | (new_data > self.min_stamp)
                | (new_data.values == iNaT)
            )
            if not in_range.all():
                return data, False
        date_units = (self.date_unit,) if self.date_unit else self._STAMP_UNITS
        for date_unit in date_units:
            try:
                new_data = to_datetime(new_data, errors=""raise"", unit=date_unit)
            except (ValueError, OverflowError):
                continue
            return new_data, True
        return data, False
    def _try_convert_dates(self):
        raise AbstractMethodError(self)

class SeriesParser(Parser):
    _default_orient = ""index""
    _split_keys = (""name"", ""index"", ""data"")
    def _parse_no_numpy(self):
        data = loads(self.json, precise_float=self.precise_float)
        if self.orient == ""split"":
            decoded = {str(k): v for k, v in data.items()}
            self.check_keys_split(decoded)
            self.obj = create_series_with_explicit_dtype(**decoded)
        else:
            self.obj = create_series_with_explicit_dtype(data, dtype_if_empty=object)
    def _parse_numpy(self):
        load_kwargs = {
            ""dtype"": None,
            ""numpy"": True,
            ""precise_float"": self.precise_float,
        }
        if self.orient in [""columns"", ""index""]:
            load_kwargs[""labelled""] = True
        loads_ = functools.partial(loads, **load_kwargs)
        data = loads_(self.json)
        if self.orient == ""split"":
            decoded = {str(k): v for k, v in data.items()}
            self.check_keys_split(decoded)
            self.obj = create_series_with_explicit_dtype(**decoded)
        elif self.orient in [""columns"", ""index""]:
            self.obj = create_series_with_explicit_dtype(*data, dtype_if_empty=object)
        else:
            self.obj = create_series_with_explicit_dtype(data, dtype_if_empty=object)
    def _try_convert_types(self):
        if self.obj is None:
            return
        obj, result = self._try_convert_data(
            ""data"", self.obj, convert_dates=self.convert_dates
        )
        if result:
            self.obj = obj

class FrameParser(Parser):
    _default_orient = ""columns""
    _split_keys = (""columns"", ""index"", ""data"")
    def _parse_numpy(self):
        json = self.json
        orient = self.orient",[18]
"    def __init__(self,
                 positions,     # Cannot be None.
                 orientation=None,
                 lineoffset=0,
                 linelength=1,
                 linewidth=None,
                 color=None,
                 linestyle='solid',
                 antialiased=None,
                 **kwargs
                 ):
        """"""
        Parameters
        ----------
        positions : 1D array-like object
            Each value is an event.
        orientation : {None, 'horizontal', 'vertical'}, optional
            The orientation of the **collection** (the event bars are along
            the orthogonal direction). Defaults to 'horizontal' if not
            specified or None.
        lineoffset : scalar, optional, default: 0
            The offset of the center of the markers from the origin, in the
            direction orthogonal to *orientation*.
        linelength : scalar, optional, default: 1
            The total height of the marker (i.e. the marker stretches from
            ``lineoffset - linelength/2`` to ``lineoffset + linelength/2``).
        linewidth : scalar or None, optional, default: None
            If it is None, defaults to its rcParams setting, in sequence form.
        color : color, sequence of colors or None, optional, default: None
            If it is None, defaults to its rcParams setting, in sequence form.
        linestyle : str or tuple, optional, default: 'solid'
            Valid strings are ['solid', 'dashed', 'dashdot', 'dotted',
            '-', '--', '-.', ':']. Dash tuples should be of the form::
                (offset, onoffseq),
            where *onoffseq* is an even length tuple of on and off ink
            in points.
        antialiased : {None, 1, 2}, optional
            If it is None, defaults to its rcParams setting, in sequence form.
        **kwargs : optional
            Other keyword arguments are line collection properties.  See
            :class:`~matplotlib.collections.LineCollection` for a list of
            the valid properties.
        Examples
        --------
        .. plot:: gallery/lines_bars_and_markers/eventcollection_demo.py
        """"""
        segment = (lineoffset + linelength / 2.,
                   lineoffset - linelength / 2.)
        if positions is None or len(positions) == 0:
            segments = []
        elif hasattr(positions, 'ndim') and positions.ndim > 1:
            raise ValueError('positions cannot be an array with more than '
                             'one dimension.')
        elif (orientation is None or orientation.lower() == 'none' or
              orientation.lower() == 'horizontal'):
            positions.sort()
            segments = [[(coord1, coord2) for coord2 in segment] for
                        coord1 in positions]
            self._is_horizontal = True
        elif orientation.lower() == 'vertical':
            positions.sort()
            segments = [[(coord2, coord1) for coord2 in segment] for
                        coord1 in positions]
            self._is_horizontal = False
        else:
            cbook._check_in_list(['horizontal', 'vertical'],
                                 orientation=orientation)
        LineCollection.__init__(self,
                                segments,
                                linewidths=linewidth,
                                colors=color,
                                antialiaseds=antialiased,
                                linestyles=linestyle,
                                **kwargs)
        self._linelength = linelength
        self._lineoffset = lineoffset
    def get_positions(self):
        '''
        return an array containing the floating-point values of the positions
        '''
        segments = self.get_segments()
        pos = 0 if self.is_horizontal() else 1
        return [segment[0, pos] for segment in self.get_segments()]
    def set_positions(self, positions):
        '''
        set the positions of the events to the specified value
        '''
        if positions is None or (hasattr(positions, 'len') and
                                 len(positions) == 0):
            self.set_segments([])
            return
        lineoffset = self.get_lineoffset()
        linelength = self.get_linelength()
        segment = (lineoffset + linelength / 2.,
                   lineoffset - linelength / 2.)
        positions = np.asanyarray(positions)
        positions.sort()
        if self.is_horizontal():
            segments = [[(coord1, coord2) for coord2 in segment] for
                        coord1 in positions]
        else:
            segments = [[(coord2, coord1) for coord2 in segment] for
                        coord1 in positions]
        self.set_segments(segments)
    def add_positions(self, position):
        '''
        add one or more events at the specified positions
        '''","[58, 61, 63]"
"        if self.left_index and isinstance(self.left.index, MultiIndex):
            raise MergeError(""left can only have one index"")
        if self.right_index and isinstance(self.right.index, MultiIndex):
            raise MergeError(""right can only have one index"")
        # set 'by' columns
        if self.by is not None:
            if self.left_by is not None or self.right_by is not None:
                raise MergeError(""Can only pass by OR left_by and right_by"")
            self.left_by = self.right_by = self.by
        if self.left_by is None and self.right_by is not None:
            raise MergeError(""missing left_by"")
        if self.left_by is not None and self.right_by is None:
            raise MergeError(""missing right_by"")
        # add 'by' to our key-list so we can have it in the
        # output as a key
        if self.left_by is not None:
            if not is_list_like(self.left_by):
                self.left_by = [self.left_by]
            if not is_list_like(self.right_by):
                self.right_by = [self.right_by]
            if len(self.left_by) != len(self.right_by):
                raise MergeError(""left_by and right_by must be same length"")
            self.left_on = self.left_by + list(self.left_on)
            self.right_on = self.right_by + list(self.right_on)
        # check 'direction' is valid
        if self.direction not in [""backward"", ""forward"", ""nearest""]:
            raise MergeError(
                ""direction invalid: {direction}"".format(direction=self.direction)
            )
    @property
    def _asof_key(self):
        """""" This is our asof key, the 'on' """"""
        return self.left_on[-1]
    def _get_merge_keys(self):
        # note this function has side effects
        (left_join_keys, right_join_keys, join_names) = super()._get_merge_keys()
        # validate index types are the same
        for i, (lk, rk) in enumerate(zip(left_join_keys, right_join_keys)):
            if not is_dtype_equal(lk.dtype, rk.dtype):
                if is_categorical_dtype(lk.dtype) and is_categorical_dtype(rk.dtype):
                    # The generic error message is confusing for categoricals.
                    #
                    # In this function, the join keys include both the original
                    # ones of the merge_asof() call, and also the keys passed
                    # to its by= argument. Unordered but equal categories
                    # are not supported for the former, but will fail
                    # later with a ValueError, so we don't *need* to check
                    # for them here.
                    msg = (
                        ""incompatible merge keys [{i}] {lkdtype} and ""
                        ""{rkdtype}, both sides category, but not equal ones"".format(
                            i=i, lkdtype=repr(lk.dtype), rkdtype=repr(rk.dtype)
                        )
                    )
                else:
                    msg = (
                        ""incompatible merge keys [{i}] {lkdtype} and ""
                        ""{rkdtype}, must be the same type"".format(
                            i=i, lkdtype=repr(lk.dtype), rkdtype=repr(rk.dtype)
                        )
                    )
                raise MergeError(msg)
        # validate tolerance; datetime.timedelta or Timedelta if we have a DTI
        if self.tolerance is not None:
            if self.left_index:
                lt = self.left.index
            else:
                lt = left_join_keys[-1]
            msg = (
                ""incompatible tolerance {tolerance}, must be compat ""
                ""with type {lkdtype}"".format(
                    tolerance=type(self.tolerance), lkdtype=repr(lt.dtype)
                )
            )
            if is_datetimelike(lt):
                if not isinstance(self.tolerance, datetime.timedelta):
                    raise MergeError(msg)
                if self.tolerance < Timedelta(0):
                    raise MergeError(""tolerance must be positive"")
            elif is_int64_dtype(lt):
                if not is_integer(self.tolerance):
                    raise MergeError(msg)
                if self.tolerance < 0:
                    raise MergeError(""tolerance must be positive"")
            elif is_float_dtype(lt):
                if not is_number(self.tolerance):
                    raise MergeError(msg)
                if self.tolerance < 0:
                    raise MergeError(""tolerance must be positive"")
            else:
                raise MergeError(""key must be integer, timestamp or float"")
        # validate allow_exact_matches
        if not is_bool(self.allow_exact_matches):
            msg = ""allow_exact_matches must be boolean, passed {passed}""
            raise MergeError(msg.format(passed=self.allow_exact_matches))
        return left_join_keys, right_join_keys, join_names
    def _get_join_indexers(self):
        """""" return the join indexers """"""
        def flip(xs):
            """""" unlike np.transpose, this returns an array of tuples """"""
            xs = [
                x if not is_extension_array_dtype(x) else x._ndarray_values for x in xs
            ]
            labels = list(string.ascii_lowercase[: len(xs)])
            dtypes = [x.dtype for x in xs]",[95]
"        # a bytestring, but since unicode_literals turns
        # every string into a unicode string, it fails.
        return
    title_bytes = title.encode('utf-8')
    buf = ctypes.create_string_buffer(len(title_bytes))
    buf.value = title_bytes
    try:
        libc.prctl(15, buf, 0, 0, 0)
    except AttributeError:
        return  # Strange libc, just skip this

def remove_start(s, start):
    return s[len(start):] if s is not None and s.startswith(start) else s

def remove_end(s, end):
    return s[:-len(end)] if s is not None and s.endswith(end) else s

def remove_quotes(s):
    if s is None or len(s) < 2:
        return s
    for quote in ('""', ""'"", ):
        if s[0] == quote and s[-1] == quote:
            return s[1:-1]
    return s

def url_basename(url):
    path = compat_urlparse.urlparse(url).path
    return path.strip('/').split('/')[-1]

def base_url(url):
    return re.match(r'https?://[^?#&]+/', url).group()

def urljoin(base, path):
    if not isinstance(path, compat_str) or not path:
        return None
    if re.match(r'^(?:https?:)?//', path):
        return path
    if not isinstance(base, compat_str) or not re.match(r'^(?:https?:)?//', base):
        return None
    return compat_urlparse.urljoin(base, path)

class HEADRequest(compat_urllib_request.Request):
    def get_method(self):
        return 'HEAD'

class PUTRequest(compat_urllib_request.Request):
    def get_method(self):
        return 'PUT'

def int_or_none(v, scale=1, default=None, get_attr=None, invscale=1):
    if get_attr:
        if v is not None:
            v = getattr(v, get_attr, None)
    if v == '':
        v = None
    if v is None:
        return default
    try:
        return int(v) * invscale // scale
    except ValueError:
        return default

def str_or_none(v, default=None):
    return default if v is None else compat_str(v)

def str_to_int(int_str):
    """""" A more relaxed version of int_or_none """"""
    if int_str is None:
        return None
    int_str = re.sub(r'[,\.\+]', '', int_str)
    return int(int_str)

def float_or_none(v, scale=1, invscale=1, default=None):
    if v is None:
        return default
    try:
        return float(v) * invscale / scale
    except ValueError:
        return default

def strip_or_none(v):
    return None if v is None else v.strip()

def parse_duration(s):
    if not isinstance(s, compat_basestring):
        return None
    s = s.strip()
    days, hours, mins, secs, ms = [None] * 5
    m = re.match(r'(?:(?:(?:(?P<days>[0-9]+):)?(?P<hours>[0-9]+):)?(?P<mins>[0-9]+):)?(?P<secs>[0-9]+)(?P<ms>\.[0-9]+)?Z?$', s)
    if m:
        days, hours, mins, secs, ms = m.groups()
    else:
        m = re.match(
            r'''(?ix)(?:P?T)?
                (?:
                    (?P<days>[0-9]+)\s*d(?:ays?)?\s*
                )?
                (?:
                    (?P<hours>[0-9]+)\s*h(?:ours?)?\s*
                )?
                (?:
                    (?P<mins>[0-9]+)\s*m(?:in(?:ute)?s?)?\s*
                )?
                (?:
                    (?P<secs>[0-9]+)(?P<ms>\.[0-9]+)?\s*s(?:ec(?:ond)?s?)?\s*
                )?Z?$''', s)
        if m:
            days, hours, mins, secs, ms = m.groups()
        else:
            m = re.match(r'(?i)(?:(?P<hours>[0-9.]+)\s*(?:hours?)|(?P<mins>[0-9.]+)\s*(?:mins?\.?|minutes?)\s*)Z?$', s)
            if m:",[43]
"    # reduction ops #
    def _reduce(self, name, axis=0, **kwargs):
        func = getattr(self, name, None)
        if func is None:
            raise TypeError(f""Categorical cannot perform the operation {name}"")
        return func(**kwargs)
    @deprecate_kwarg(old_arg_name=""numeric_only"", new_arg_name=""skipna"")
    def min(self, skipna=True):
        """"""
        The minimum value of the object.
        Only ordered `Categoricals` have a minimum!
        .. versionchanged:: 1.0.0
           Returns an NA value on empty arrays
        Raises
        ------
        TypeError
            If the `Categorical` is not `ordered`.
        Returns
        -------
        min : the minimum of this `Categorical`
        """"""
        self.check_for_ordered(""min"")
        if not len(self._codes):
            return self.dtype.na_value
        good = self._codes != -1
        if not good.all():
            if skipna:
                pointer = self._codes[good].min()
            else:
                return np.nan
        else:
            pointer = self._codes.min()
        return self.categories[pointer]
    @deprecate_kwarg(old_arg_name=""numeric_only"", new_arg_name=""skipna"")
    def max(self, skipna=True):
        """"""
        The maximum value of the object.
        Only ordered `Categoricals` have a maximum!
        .. versionchanged:: 1.0.0
           Returns an NA value on empty arrays
        Raises
        ------
        TypeError
            If the `Categorical` is not `ordered`.
        Returns
        -------
        max : the maximum of this `Categorical`
        """"""
        self.check_for_ordered(""max"")
        if not len(self._codes):
            return self.dtype.na_value
        good = self._codes != -1
        if not good.all():
            if skipna:
                pointer = self._codes[good].max()
            else:
                return np.nan
        else:
            pointer = self._codes.max()
        return self.categories[pointer]
    def mode(self, dropna=True):
        """"""
        Returns the mode(s) of the Categorical.
        Always returns `Categorical` even if only one value.
        Parameters
        ----------
        dropna : bool, default True
            Don't consider counts of NaN/NaT.
            .. versionadded:: 0.24.0
        Returns
        -------
        modes : `Categorical` (sorted)
        """"""
        codes = self._codes
        if dropna:
            good = self._codes != -1
            codes = self._codes[good]
        codes = sorted(htable.mode_int64(ensure_int64(codes), dropna))
        return self._constructor(values=codes, dtype=self.dtype, fastpath=True)
    def unique(self):
        """"""
        Return the ``Categorical`` which ``categories`` and ``codes`` are
        unique. Unused categories are NOT returned.
        - unordered category: values and categories are sorted by appearance
          order.
        - ordered category: values are sorted by appearance order, categories
          keeps existing order.
        Returns
        -------
        unique values : ``Categorical``
        See Also
        --------
        pandas.unique
        CategoricalIndex.unique
        Series.unique
        Examples
        --------
        An unordered Categorical will return categories in the
        order of appearance.
","[35, 70]"
"            def cancel_websocket_tasks(app, loop):
                for task in self.websocket_tasks:
                    task.cancel()
        self.websocket_enabled = enable
    # Decorator
    def exception(self, *exceptions):
        """"""Decorate a function to be registered as a handler for exceptions
        :param exceptions: exceptions
        :return: decorated function
        """"""
        def response(handler):
            for exception in exceptions:
                if isinstance(exception, (tuple, list)):
                    for e in exception:
                        self.error_handler.add(e, handler)
                else:
                    self.error_handler.add(exception, handler)
            return handler
        return response
    def register_middleware(self, middleware, attach_to=""request""):
        """"""
        Register an application level middleware that will be attached
        to all the API URLs registered under this application.
        This method is internally invoked by the :func:`middleware`
        decorator provided at the app level.
        :param middleware: Callback method to be attached to the
            middleware
        :param attach_to: The state at which the middleware needs to be
            invoked in the lifecycle of an *HTTP Request*.
            **request** - Invoke before the request is processed
            **response** - Invoke before the response is returned back
        :return: decorated method
        """"""
        if attach_to == ""request"":
            if middleware not in self.request_middleware:
                self.request_middleware.append(middleware)
        if attach_to == ""response"":
            if middleware not in self.response_middleware:
                self.response_middleware.appendleft(middleware)
        return middleware
    def register_named_middleware(
        self, middleware, route_names, attach_to=""request""
    ):
        if attach_to == ""request"":
            for _rn in route_names:
                if _rn not in self.named_request_middleware:
                    self.named_request_middleware[_rn] = deque()
                if middleware not in self.named_request_middleware[_rn]:
                    self.named_request_middleware[_rn].append(middleware)
        if attach_to == ""response"":
            for _rn in route_names:
                if _rn not in self.named_response_middleware:
                    self.named_response_middleware[_rn] = deque()
                if middleware not in self.named_response_middleware[_rn]:
                    self.named_response_middleware[_rn].append(middleware)
    # Decorator
    def middleware(self, middleware_or_request):
        """"""
        Decorate and register middleware to be called before a request.
        Can either be called as *@app.middleware* or
        *@app.middleware('request')*
        :param: middleware_or_request: Optional parameter to use for
            identifying which type of middleware is being registered.
        """"""
        # Detect which way this was called, @middleware or @middleware('AT')
        if callable(middleware_or_request):
            return self.register_middleware(middleware_or_request)
        else:
            return partial(
                self.register_middleware, attach_to=middleware_or_request
            )
    # Static Files
    def static(
        self,
        uri,
        file_or_directory,
        pattern=r""/?.+"",
        use_modified_since=True,
        use_content_range=False,
        stream_large_files=False,
        name=""static"",
        host=None,
        strict_slashes=None,
        content_type=None,
    ):
        """"""
        Register a root to serve files from. The input can either be a
        file or a directory. This method will enable an easy and simple way
        to setup the :class:`Route` necessary to serve the static files.
        :param uri: URL path to be used for serving static content
        :param file_or_directory: Path for the Static file/directory with
            static files
        :param pattern: Regex Pattern identifying the valid static files
        :param use_modified_since: If true, send file modified time, and return
            not modified if the browser's matches the server's
        :param use_content_range: If true, process header for range requests
            and sends the file part that is requested
        :param stream_large_files: If true, use the
            :func:`StreamingHTTPResponse.file_stream` handler rather
            than the :func:`HTTPResponse.file` handler to send the file.
            If this is an integer, this represents the threshold size to
            switch to :func:`StreamingHTTPResponse.file_stream`
        :param name: user defined name used for url_for
        :param host: Host IP or FQDN for the service to use
        :param strict_slashes: Instruct :class:`Sanic` to check if the request
            URLs need to terminate with a */*
        :param content_type: user defined content type for header
        :return: None
        """"""
        static_register(
            self,
            uri,
            file_or_directory,",[63]
"#
# Copyright 2009 Facebook
#
# Licensed under the Apache License, Version 2.0 (the ""License""); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
""""""An I/O event loop for non-blocking sockets.
On Python 3, `.IOLoop` is a wrapper around the `asyncio` event loop.
Typical applications will use a single `IOLoop` object, accessed via
`IOLoop.current` class method. The `IOLoop.start` method (or
equivalently, `asyncio.AbstractEventLoop.run_forever`) should usually
be called at the end of the ``main()`` function. Atypical applications
may use more than one `IOLoop`, such as one `IOLoop` per thread, or
per `unittest` case.
In addition to I/O events, the `IOLoop` can also schedule time-based
events. `IOLoop.add_timeout` is a non-blocking alternative to
`time.sleep`.
""""""
from __future__ import absolute_import, division, print_function
import collections
import datetime
import errno
import functools
import heapq
import itertools
import logging
import numbers
import os
import select
import sys
import threading
import time
import traceback
import math
import weakref
from tornado.concurrent import Future, is_future, chain_future, future_set_exc_info, future_add_done_callback  # noqa: E501
from tornado.log import app_log, gen_log
from tornado.platform.auto import set_close_exec, Waker
from tornado import stack_context
from tornado.util import (
    PY3, Configurable, errno_from_exception, timedelta_to_seconds,
    TimeoutError, unicode_type, import_object,
)
try:
    import signal
except ImportError:
    signal = None
try:
    from concurrent.futures import ThreadPoolExecutor
except ImportError:
    ThreadPoolExecutor = None
if PY3:
    import _thread as thread
else:
    import thread
try:
    import asyncio
except ImportError:
    asyncio = None

_POLL_TIMEOUT = 3600.0

class IOLoop(Configurable):
    """"""A level-triggered I/O loop.
    On Python 3, `IOLoop` is a wrapper around the `asyncio` event
    loop. On Python 2, it uses ``epoll`` (Linux) or ``kqueue`` (BSD
    and Mac OS X) if they are available, or else we fall back on
    select(). If you are implementing a system that needs to handle
    thousands of simultaneous connections, you should use a system
    that supports either ``epoll`` or ``kqueue``.
    Example usage for a simple TCP server:
    .. testcode::
        import errno
        import functools
        import socket
        import tornado.ioloop
        from tornado import gen
        from tornado.iostream import IOStream
        @gen.coroutine
        def handle_connection(connection, address):
            stream = IOStream(connection)
            message = yield stream.read_until_close()
            print(""message from client:"", message.decode().strip())
        def connection_ready(sock, fd, events):
            while True:
                try:
                    connection, address = sock.accept()
                except socket.error as e:
                    if e.args[0] not in (errno.EWOULDBLOCK, errno.EAGAIN):
                        raise
                    return
                connection.setblocking(0)
                handle_connection(connection, address)
        if __name__ == '__main__':
            sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM, 0)
            sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
            sock.setblocking(0)",[49]
"                for key in sorted(role_info[k].keys()):
                    if key in GalaxyCLI.SKIP_INFO_KEYS:
                        continue
                    text.append(u""\t\t%s: %s"" % (key, role_info[k][key]))
            else:
                text.append(u""\t%s: %s"" % (k, role_info[k]))
        return u'\n'.join(text)
    @staticmethod
    def _resolve_path(path):
        return os.path.abspath(os.path.expanduser(os.path.expandvars(path)))
    @staticmethod
    def _validate_collection_name(name):
        if is_collection_ref('ansible_collections.{0}'.format(name)):
            return name
        raise AnsibleError(""Invalid collection name, must be in the format <namespace>.<collection>"")
    @staticmethod
    def _get_skeleton_galaxy_yml(template_path, inject_data):
        with open(to_bytes(template_path, errors='surrogate_or_strict'), 'rb') as template_obj:
            meta_template = to_text(template_obj.read(), errors='surrogate_or_strict')
        galaxy_meta = get_collections_galaxy_meta_info()
        required_config = []
        optional_config = []
        for meta_entry in galaxy_meta:
            config_list = required_config if meta_entry.get('required', False) else optional_config
            value = inject_data.get(meta_entry['key'], None)
            if not value:
                meta_type = meta_entry.get('type', 'str')
                if meta_type == 'str':
                    value = ''
                elif meta_type == 'list':
                    value = []
                elif meta_type == 'dict':
                    value = {}
            meta_entry['value'] = value
            config_list.append(meta_entry)
        link_pattern = re.compile(r""L\(([^)]+),\s+([^)]+)\)"")
        const_pattern = re.compile(r""C\(([^)]+)\)"")
        def comment_ify(v):
            if isinstance(v, list):
                v = "". "".join([l.rstrip('.') for l in v])
            v = link_pattern.sub(r""\1 <\2>"", v)
            v = const_pattern.sub(r""'\1'"", v)
            return textwrap.fill(v, width=117, initial_indent=""# "", subsequent_indent=""# "", break_on_hyphens=False)
        def to_yaml(v):
            return yaml.safe_dump(v, default_flow_style=False).rstrip()
        env = Environment(loader=BaseLoader)
        env.filters['comment_ify'] = comment_ify
        env.filters['to_yaml'] = to_yaml
        template = env.from_string(meta_template)
        meta_value = template.render({'required_config': required_config, 'optional_config': optional_config})
        return meta_value
############################
# execute actions
############################
    def execute_role(self):
        """"""
        Perform the action on an Ansible Galaxy role. Must be combined with a further action like delete/install/init
        as listed below.
        """"""
        # To satisfy doc build
        pass
    def execute_collection(self):
        """"""
        Perform the action on an Ansible Galaxy collection. Must be combined with a further action like init/install as
        listed below.
        """"""
        # To satisfy doc build
        pass
    def execute_build(self):
        """"""
        Build an Ansible Galaxy collection artifact that can be stored in a central repository like Ansible Galaxy.
        By default, this command builds from the current working directory. You can optionally pass in the
        collection input path (where the ``galaxy.yml`` file is).
        """"""
        force = context.CLIARGS['force']
        output_path = GalaxyCLI._resolve_path(context.CLIARGS['output_path'])
        b_output_path = to_bytes(output_path, errors='surrogate_or_strict')
        if not os.path.exists(b_output_path):
            os.makedirs(b_output_path)
        elif os.path.isfile(b_output_path):
            raise AnsibleError(""- the output collection directory %s is a file - aborting"" % to_native(output_path))
        for collection_path in context.CLIARGS['args']:
            collection_path = GalaxyCLI._resolve_path(collection_path)
            build_collection(collection_path, output_path, force)
    def execute_init(self):
        """"""
        Creates the skeleton framework of a role or collection that complies with the Galaxy metadata format.
        Requires a role or collection name. The collection name must be in the format ``<namespace>.<collection>``.
        """"""
        galaxy_type = context.CLIARGS['type']
        init_path = context.CLIARGS['init_path']
        force = context.CLIARGS['force']
        obj_skeleton = context.CLIARGS['{0}_skeleton'.format(galaxy_type)]
        obj_name = context.CLIARGS['{0}_name'.format(galaxy_type)]
        inject_data = dict(
            description='your description',
            ansible_plugin_list_dir=get_versioned_doclink('plugins/plugins.html'),
        )
        if galaxy_type == 'role':",[123]
"    curr = float(start)
    while True:
        yield clip(curr)
        curr += (stop - start) / steps

def decaying(start, stop, decay):
    """"""Yield an infinite series of linearly decaying values.""""""
    curr = float(start)
    while True:
        yield max(curr, stop)
        curr -= decay

def minibatch_by_words(items, size, tuples=True, count_words=len):
    """"""Create minibatches of a given number of words.""""""
    if isinstance(size, int):
        size_ = itertools.repeat(size)
    else:
        size_ = size
    items = iter(items)
    while True:
        batch_size = next(size_)
        batch = []
        while batch_size >= 0:
            try:
                if tuples:
                    doc, gold = next(items)
                else:
                    doc = next(items)
            except StopIteration:
                if batch:
                    yield batch
                return
            batch_size -= count_words(doc)
            if tuples:
                batch.append((doc, gold))
            else:
                batch.append(doc)
        if batch:
            yield batch

def itershuffle(iterable, bufsize=1000):
    """"""Shuffle an iterator. This works by holding `bufsize` items back
    and yielding them sometime later. Obviously, this is not unbiased –
    but should be good enough for batching. Larger bufsize means less bias.
    From https://gist.github.com/andres-erbsen/1307752
    iterable (iterable): Iterator to shuffle.
    bufsize (int): Items to hold back.
    YIELDS (iterable): The shuffled iterator.
    """"""
    iterable = iter(iterable)
    buf = []
    try:
        while True:
            for i in range(random.randint(1, bufsize - len(buf))):
                buf.append(next(iterable))
            random.shuffle(buf)
            for i in range(random.randint(1, bufsize)):
                if buf:
                    yield buf.pop()
                else:
                    break
    except StopIteration:
        random.shuffle(buf)
        while buf:
            yield buf.pop()
        raise StopIteration

def filter_spans(spans):
    """"""Filter a sequence of spans and remove duplicates or overlaps. Useful for
    creating named entities (where one token can only be part of one entity) or
    when merging spans with `Retokenizer.merge`. When spans overlap, the (first)
    longest span is preferred over shorter spans.
    spans (iterable): The spans to filter.
    RETURNS (list): The filtered spans.
    """"""
    get_sort_key = lambda span: (span.end - span.start, span.start)
    sorted_spans = sorted(spans, key=get_sort_key, reverse=True)
    result = []
    seen_tokens = set()
    for span in sorted_spans:
        # Check for end - 1 here because boundaries are inclusive
        if span.start not in seen_tokens and span.end - 1 not in seen_tokens:
            result.append(span)
        seen_tokens.update(range(span.start, span.end))
    result = sorted(result, key=lambda span: span.start)
    return result

def to_bytes(getters, exclude):
    serialized = OrderedDict()
    for key, getter in getters.items():
        # Split to support file names like meta.json
        if key.split(""."")[0] not in exclude:
            serialized[key] = getter()
    return srsly.msgpack_dumps(serialized)

def from_bytes(bytes_data, setters, exclude):
    msg = srsly.msgpack_loads(bytes_data)
    for key, setter in setters.items():
        # Split to support file names like meta.json
        if key.split(""."")[0] not in exclude and key in msg:
            setter(msg[key])
    return msg

def to_disk(path, writers, exclude):
    path = ensure_path(path)
    if not path.exists():
        path.mkdir()
    for key, writer in writers.items():
        # Split to support file names like meta.json
        if key.split(""."")[0] not in exclude:
            writer(path / key)
    return path

def from_disk(path, readers, exclude):
    path = ensure_path(path)",[83]
"from datetime import datetime
import operator
from textwrap import dedent
from typing import TYPE_CHECKING, Any, Dict, FrozenSet, Hashable, Union
import warnings
import numpy as np
from pandas._libs import algos as libalgos, index as libindex, lib
import pandas._libs.join as libjoin
from pandas._libs.lib import is_datetime_array, no_default
from pandas._libs.tslibs import OutOfBoundsDatetime, Timestamp
from pandas._libs.tslibs.period import IncompatibleFrequency
from pandas._libs.tslibs.timezones import tz_compare
from pandas._typing import Label
from pandas.compat import set_function_name
from pandas.compat.numpy import function as nv
from pandas.util._decorators import Appender, Substitution, cache_readonly
from pandas.core.dtypes import concat as _concat
from pandas.core.dtypes.cast import (
    maybe_cast_to_integer_array,
    validate_numeric_casting,
)
from pandas.core.dtypes.common import (
    ensure_categorical,
    ensure_int64,
    ensure_object,
    ensure_platform_int,
    is_bool,
    is_bool_dtype,
    is_categorical,
    is_categorical_dtype,
    is_datetime64_any_dtype,
    is_dtype_equal,
    is_extension_array_dtype,
    is_float,
    is_float_dtype,
    is_hashable,
    is_integer,
    is_integer_dtype,
    is_interval_dtype,
    is_iterator,
    is_list_like,
    is_object_dtype,
    is_period_dtype,
    is_scalar,
    is_signed_integer_dtype,
    is_timedelta64_dtype,
    is_unsigned_integer_dtype,
)
from pandas.core.dtypes.concat import concat_compat
from pandas.core.dtypes.generic import (
    ABCCategorical,
    ABCDataFrame,
    ABCDatetimeIndex,
    ABCIntervalIndex,
    ABCMultiIndex,
    ABCPandasArray,
    ABCPeriodIndex,
    ABCRangeIndex,
    ABCSeries,
    ABCTimedeltaIndex,
)
from pandas.core.dtypes.missing import array_equivalent, isna
from pandas.core import ops
from pandas.core.accessor import CachedAccessor
import pandas.core.algorithms as algos
from pandas.core.arrays import ExtensionArray
from pandas.core.base import IndexOpsMixin, PandasObject
import pandas.core.common as com
from pandas.core.indexers import deprecate_ndim_indexing
from pandas.core.indexes.frozen import FrozenList
import pandas.core.missing as missing
from pandas.core.ops import get_op_result_name
from pandas.core.ops.invalid import make_invalid_op
from pandas.core.strings import StringMethods
from pandas.io.formats.printing import (
    PrettyDict,
    default_pprint,
    format_object_attrs,
    format_object_summary,
    pprint_thing,
)
if TYPE_CHECKING:
    from pandas import Series

__all__ = [""Index""]
_unsortable_types = frozenset((""mixed"", ""mixed-integer""))
_index_doc_kwargs = dict(
    klass=""Index"",
    inplace="""",
    target_klass=""Index"",
    raises_section="""",
    unique=""Index"",
    duplicated=""np.ndarray"",
)
_index_shared_docs = dict()
str_t = str

def _make_comparison_op(op, cls):
    def cmp_method(self, other):
        if isinstance(other, (np.ndarray, Index, ABCSeries, ExtensionArray)):
            if other.ndim > 0 and len(self) != len(other):
                raise ValueError(""Lengths must match to compare"")
        if is_object_dtype(self) and isinstance(other, ABCCategorical):
            left = type(other)(self._values, dtype=other.dtype)
            return op(left, other)
        elif is_object_dtype(self) and isinstance(other, ExtensionArray):
            # e.g. PeriodArray
            with np.errstate(all=""ignore""):
                result = op(self.values, other)
        elif is_object_dtype(self) and not isinstance(self, ABCMultiIndex):
            # don't pass MultiIndex
            with np.errstate(all=""ignore""):
                result = ops.comp_method_OBJECT_ARRAY(op, self.values, other)
        else:",[31]
"#!/usr/bin/env python2
from __future__ import unicode_literals
u'hello'
U""hello""
Ur""hello""
# output

#!/usr/bin/env python2
from __future__ import unicode_literals
""hello""
""hello""
r""hello""","[1, 11]"
"from subprocess import Popen, PIPE
from time import time
import os
import sys
import six
from .. import logs
from ..conf import settings
from ..utils import DEVNULL, cache
from .generic import Generic

@cache('~/.config/fish/config.fish', '~/.config/fish/functions')
def _get_functions(overridden):
    proc = Popen(['fish', '-ic', 'functions'], stdout=PIPE, stderr=DEVNULL)
    functions = proc.stdout.read().decode('utf-8').strip().split('\n')
    return {func: func for func in functions if func not in overridden}

@cache('~/.config/fish/config.fish')
def _get_aliases(overridden):
    aliases = {}
    proc = Popen(['fish', '-ic', 'alias'], stdout=PIPE, stderr=DEVNULL)
    alias_out = proc.stdout.read().decode('utf-8').strip()
    if not alias_out:
        return aliases
    for alias in alias_out.split('\n'):
        for separator in (' ', '='):
            split_alias = alias.replace('alias ', '', 1).split(separator, 1)
            if len(split_alias) == 2:
                name, value = split_alias
                break
        else:
            continue
        if name not in overridden:
            aliases[name] = value
    return aliases

class Fish(Generic):
    def _get_overridden_aliases(self):
        overridden = os.environ.get('THEFUCK_OVERRIDDEN_ALIASES',
                                    os.environ.get('TF_OVERRIDDEN_ALIASES', ''))
        default = {'cd', 'grep', 'ls', 'man', 'open'}
        for alias in overridden.split(','):
            default.add(alias.strip())
        return default
    def app_alias(self, alias_name):
        if settings.alter_history:
            alter_history = ('    builtin history delete --exact'
                             ' --case-sensitive -- $fucked_up_command\n'
                             '    builtin history merge ^ /dev/null\n')
        else:
            alter_history = ''
        # It is VERY important to have the variables declared WITHIN the alias
        return ('function {0} -d ""Correct your previous console command""\n'
                '  set -l fucked_up_command $history[1]\n'
                '  env TF_SHELL=fish TF_ALIAS={0} PYTHONIOENCODING=utf-8'
                ' thefuck $fucked_up_command | read -l unfucked_command\n'
                '  if [ ""$unfucked_command"" != """" ]\n'
                '    eval $unfucked_command\n{1}'
                '  end\n'
                'end').format(alias_name, alter_history)
    def get_aliases(self):
        overridden = self._get_overridden_aliases()
        functions = _get_functions(overridden)
        raw_aliases = _get_aliases(overridden)
        functions.update(raw_aliases)
        return functions
    def _expand_aliases(self, command_script):
        aliases = self.get_aliases()
        binary = command_script.split(' ')[0]
        if binary in aliases and aliases[binary] != binary:
            return command_script.replace(binary, aliases[binary], 1)
        elif binary in aliases:
            return u'fish -ic ""{}""'.format(command_script.replace('""', r'\""'))
        else:
            return command_script
    def _get_history_file_name(self):
        return os.path.expanduser('~/.config/fish/fish_history')
    def _get_history_line(self, command_script):
        return u'- cmd: {}\n   when: {}\n'.format(command_script, int(time()))
    def _script_from_history(self, line):
        if '- cmd: ' in line:
            return line.split('- cmd: ', 1)[1]
        else:
            return ''
    def and_(self, *commands):
        return u'; and '.join(commands)
    def or_(self, *commands):
        return u'; or '.join(commands)
    def how_to_configure(self):
        return self._create_shell_configuration(
            content=u""thefuck --alias | source"",
            path='~/.config/fish/config.fish',
            reload='fish')
    def info(self):
        """"""Returns the name and version of the current shell""""""
        proc = Popen(['fish', '-c', 'echo $FISH_VERSION'],
                     stdout=PIPE, stderr=DEVNULL)
        version = proc.stdout.read().decode('utf-8').strip()
        return u'Fish Shell {}'.format(version)
    def put_to_history(self, command):
        try:
            return self._put_to_history(command)
        except IOError:
            logs.exception(""Can't update history"", sys.exc_info())
    def _put_to_history(self, command_script):
        """"""Puts command script to shell history.""""""
        history_file_name = self._get_history_file_name()
        if os.path.isfile(history_file_name):
            with open(history_file_name, 'a') as history:
                entry = self._get_history_line(command_script)
                if six.PY2:
                    history.write(entry.encode('utf-8'))
                else:
                    history.write(entry)","[107, 109]"
"def match(command, settings):
    return 'ls' in command.script and not ('ls -' in command.script)

def get_new_command(command, settings):
    command = command.script.split(' ')
    command[0] = 'ls -lah'
    return ' '.join(command)",[1]
"        self.start = time.time()
        self.last_update = 0
        self.interval = interval
        self.total_width = 0
        self.seen_so_far = 0
        self.verbose = verbose
        self._dynamic_display = ((hasattr(sys.stdout, 'isatty') and
                                  sys.stdout.isatty()) or
                                 'ipykernel' in sys.modules)
    def update(self, current, values=None, force=False):
        """"""Updates the progress bar.
        # Arguments
            current: Index of current step.
            values: List of tuples (name, value_for_last_step).
                The progress bar will display averages for these values.
            force: Whether to force visual progress update.
        """"""
        values = values or []
        for k, v in values:
            if k not in self.sum_values:
                self.sum_values[k] = [v * (current - self.seen_so_far),
                                      current - self.seen_so_far]
                self.unique_values.append(k)
            else:
                self.sum_values[k][0] += v * (current - self.seen_so_far)
                self.sum_values[k][1] += (current - self.seen_so_far)
        self.seen_so_far = current
        now = time.time()
        info = ' - %.0fs' % (now - self.start)
        if self.verbose == 1:
            if (not force and (now - self.last_update) < self.interval and
                    current < self.target):
                return
            prev_total_width = self.total_width
            if self._dynamic_display:
                sys.stdout.write('\b' * prev_total_width)
                sys.stdout.write('\r')
            else:
                sys.stdout.write('\n')
            if self.target is not None:
                numdigits = int(np.floor(np.log10(self.target))) + 1
                barstr = '%%%dd/%d [' % (numdigits, self.target)
                bar = barstr % current
                prog = float(current) / self.target
                prog_width = int(self.width * prog)
                if prog_width > 0:
                    bar += ('=' * (prog_width - 1))
                    if current < self.target:
                        bar += '>'
                    else:
                        bar += '='
                bar += ('.' * (self.width - prog_width))
                bar += ']'
            else:
                bar = '%7d/Unknown' % current
            self.total_width = len(bar)
            sys.stdout.write(bar)
            if current:
                time_per_unit = (now - self.start) / current
            else:
                time_per_unit = 0
            if self.target is not None and current < self.target:
                eta = time_per_unit * (self.target - current)
                if eta > 3600:
                    eta_format = '%d:%02d:%02d' % (eta // 3600, (eta % 3600) // 60, eta % 60)
                elif eta > 60:
                    eta_format = '%d:%02d' % (eta // 60, eta % 60)
                else:
                    eta_format = '%ds' % eta
                info = ' - ETA: %s' % eta_format
            else:
                if time_per_unit >= 1:
                    info += ' %.0fs/step' % time_per_unit
                elif time_per_unit >= 1e-3:
                    info += ' %.0fms/step' % (time_per_unit * 1e3)
                else:
                    info += ' %.0fus/step' % (time_per_unit * 1e6)
            for k in self.unique_values:
                info += ' - %s:' % k
                if isinstance(self.sum_values[k], list):
                    avg = np.mean(
                        self.sum_values[k][0] / max(1, self.sum_values[k][1]))
                    if abs(avg) > 1e-3:
                        info += ' %.4f' % avg
                    else:
                        info += ' %.4e' % avg
                else:
                    info += ' %s' % self.sum_values[k]
            self.total_width += len(info)
            if prev_total_width > self.total_width:
                info += (' ' * (prev_total_width - self.total_width))
            if self.target is not None and current >= self.target:
                info += '\n'
            sys.stdout.write(info)
            sys.stdout.flush()
        elif self.verbose == 2:
            if self.target is None or current >= self.target:
                for k in self.unique_values:
                    info += ' - %s:' % k
                    avg = np.mean(
                        self.sum_values[k][0] / max(1, self.sum_values[k][1]))
                    if avg > 1e-3:
                        info += ' %.4f' % avg
                    else:
                        info += ' %.4e' % avg
                info += '\n'
                sys.stdout.write(info)
                sys.stdout.flush()
        self.last_update = now
    def add(self, n, values=None):",[34]
"        if is_integer_dtype(result) and not is_datetimelike:
            mask = result == iNaT
            if mask.any():
                result = result.astype(""float64"")
                result[mask] = np.nan
        if kind == ""aggregate"" and self._filter_empty_groups and not counts.all():
            assert result.ndim != 2
            result = result[counts > 0]
        if vdim == 1 and arity == 1:
            result = result[:, 0]
        if how in self._name_functions:
            # TODO
            names = self._name_functions[how]()
        else:
            names = None
        if swapped:
            result = result.swapaxes(0, axis)
        if is_datetime64tz_dtype(orig_values.dtype):
            result = type(orig_values)(result.astype(np.int64), dtype=orig_values.dtype)
        elif is_datetimelike and kind == ""aggregate"":
            result = result.astype(orig_values.dtype)
        return result, names
    def aggregate(self, values, how, axis=0, min_count=-1):
        return self._cython_operation(
            ""aggregate"", values, how, axis, min_count=min_count
        )
    def transform(self, values, how, axis=0, **kwargs):
        return self._cython_operation(""transform"", values, how, axis, **kwargs)
    def _aggregate(
        self,
        result,
        counts,
        values,
        comp_ids,
        agg_func,
        is_numeric,
        is_datetimelike,
        min_count=-1,
    ):
        if values.ndim > 2:
            # punting for now
            raise NotImplementedError(""number of dimensions is currently limited to 2"")
        else:
            agg_func(result, counts, values, comp_ids, min_count)
        return result
    def _transform(
        self,
        result,
        values,
        comp_ids,
        transform_func,
        is_numeric,
        is_datetimelike,
        **kwargs
    ):
        comp_ids, _, ngroups = self.group_info
        if values.ndim > 2:
            # punting for now
            raise NotImplementedError(""number of dimensions is currently limited to 2"")
        else:
            transform_func(result, values, comp_ids, ngroups, is_datetimelike, **kwargs)
        return result
    def agg_series(self, obj, func):
        try:
            return self._aggregate_series_fast(obj, func)
        except AssertionError:
            raise
        except ValueError as err:
            if ""No result."" in str(err):
                # raised in libreduction
                pass
            elif ""Function does not reduce"" in str(err):
                # raised in libreduction
                pass
            else:
                raise
            return self._aggregate_series_pure_python(obj, func)
    def _aggregate_series_fast(self, obj, func):
        func = self._is_builtin_func(func)
        if obj.index._has_complex_internals:
            raise TypeError(""Incompatible index for Cython grouper"")
        group_index, _, ngroups = self.group_info
        # avoids object / Series creation overhead
        dummy = obj._get_values(slice(None, 0))
        indexer = get_group_index_sorter(group_index, ngroups)
        obj = obj.take(indexer)
        group_index = algorithms.take_nd(group_index, indexer, allow_fill=False)
        grouper = libreduction.SeriesGrouper(obj, func, group_index, ngroups, dummy)
        result, counts = grouper.get_result()
        return result, counts
    def _aggregate_series_pure_python(self, obj, func):
        group_index, _, ngroups = self.group_info
        counts = np.zeros(ngroups, dtype=int)
        result = None
        splitter = get_splitter(obj, group_index, ngroups, axis=self.axis)
        for label, group in splitter:
            res = func(group)
            if result is None:
                if isinstance(res, (Series, Index, np.ndarray)):
                    raise ValueError(""Function does not reduce"")
                result = np.empty(ngroups, dtype=""O"")
            counts[label] = group.shape[0]",[91]
""""""" pickle compat """"""
import pickle
import warnings
from pandas.compat import pickle_compat as pc
from pandas.io.common import get_handle, stringify_path

def to_pickle(obj, path, compression=""infer"", protocol=pickle.HIGHEST_PROTOCOL):
    """"""
    Pickle (serialize) object to file.
    Parameters
    ----------
    obj : any object
        Any python object.
    path : str
        File path where the pickled object will be stored.
    compression : {'infer', 'gzip', 'bz2', 'zip', 'xz', None}, default 'infer'
        A string representing the compression to use in the output file. By
        default, infers from the file extension in specified path.
    protocol : int
        Int which indicates which protocol should be used by the pickler,
        default HIGHEST_PROTOCOL (see [1], paragraph 12.1.2). The possible
        values for this parameter depend on the version of Python. For Python
        2.x, possible values are 0, 1, 2. For Python>=3.0, 3 is a valid value.
        For Python >= 3.4, 4 is a valid value. A negative value for the
        protocol parameter is equivalent to setting its value to
        HIGHEST_PROTOCOL.
        .. [1] https://docs.python.org/3/library/pickle.html
        .. versionadded:: 0.21.0
    See Also
    --------
    read_pickle : Load pickled pandas object (or any object) from file.
    DataFrame.to_hdf : Write DataFrame to an HDF5 file.
    DataFrame.to_sql : Write DataFrame to a SQL database.
    DataFrame.to_parquet : Write a DataFrame to the binary parquet format.
    Examples
    --------
    >>> original_df = pd.DataFrame({""foo"": range(5), ""bar"": range(5, 10)})
    >>> original_df
       foo  bar
    0    0    5
    1    1    6
    2    2    7
    3    3    8
    4    4    9
    >>> pd.to_pickle(original_df, ""./dummy.pkl"")
    >>> unpickled_df = pd.read_pickle(""./dummy.pkl"")
    >>> unpickled_df
       foo  bar
    0    0    5
    1    1    6
    2    2    7
    3    3    8
    4    4    9
    >>> import os
    >>> os.remove(""./dummy.pkl"")
    """"""
    path = stringify_path(path)
    f, fh = get_handle(path, ""wb"", compression=compression, is_text=False)
    if protocol < 0:
        protocol = pickle.HIGHEST_PROTOCOL
    try:
        f.write(pickle.dumps(obj, protocol=protocol))
    finally:
        f.close()
        for _f in fh:
            _f.close()

def read_pickle(path, compression=""infer""):
    """"""
    Load pickled pandas object (or any object) from file.
    .. warning::
       Loading pickled data received from untrusted sources can be
       unsafe. See `here <https://docs.python.org/3/library/pickle.html>`__.
    Parameters
    ----------
    path : str
        File path where the pickled object will be loaded.
    compression : {'infer', 'gzip', 'bz2', 'zip', 'xz', None}, default 'infer'
        For on-the-fly decompression of on-disk data. If 'infer', then use
        gzip, bz2, xz or zip if path ends in '.gz', '.bz2', '.xz',
        or '.zip' respectively, and no decompression otherwise.
        Set to None for no decompression.
    Returns
    -------
    unpickled : same type as object stored in file
    See Also
    --------
    DataFrame.to_pickle : Pickle (serialize) DataFrame object to file.
    Series.to_pickle : Pickle (serialize) Series object to file.
    read_hdf : Read HDF5 file into a DataFrame.
    read_sql : Read SQL query or database table into a DataFrame.
    read_parquet : Load a parquet object, returning a DataFrame.
    Notes
    -----
    read_pickle is only guaranteed to be backwards compatible to pandas 0.20.3.
    Examples
    --------
    >>> original_df = pd.DataFrame({""foo"": range(5), ""bar"": range(5, 10)})
    >>> original_df
       foo  bar
    0    0    5
    1    1    6
    2    2    7
    3    3    8
    4    4    9
    >>> pd.to_pickle(original_df, ""./dummy.pkl"")
    >>> unpickled_df = pd.read_pickle(""./dummy.pkl"")
    >>> unpickled_df
       foo  bar","[6, 9, 17, 18, 20, 21, 65, 66, 77, 88, 89, 91, 92, 93, 94]"
"def _tuplify(ndim: int, loc: Hashable) -> Tuple[Union[Hashable, slice], ...]:
    """"""
    Given an indexer for the first dimension, create an equivalent tuple
    for indexing over all dimensions.
    Parameters
    ----------
    ndim : int
    loc : object
    Returns
    -------
    tuple
    """"""
    _tup: List[Union[Hashable, slice]]
    _tup = [slice(None, None) for _ in range(ndim)]
    _tup[0] = loc
    return tuple(_tup)

def convert_to_index_sliceable(obj, key):
    """"""
    If we are index sliceable, then return my slicer, otherwise return None.
    """"""
    idx = obj.index
    if isinstance(key, slice):
        return idx._convert_slice_indexer(key, kind=""getitem"")
    elif isinstance(key, str):
        # we are an actual column
        if key in obj._data.items:
            return None
        # We might have a datetimelike string that we can translate to a
        # slice here via partial string indexing
        if idx._supports_partial_string_indexing:
            try:
                return idx._get_string_slice(key)
            except (KeyError, ValueError, NotImplementedError):
                return None
    return None

def check_bool_indexer(index: Index, key) -> np.ndarray:
    """"""
    Check if key is a valid boolean indexer for an object with such index and
    perform reindexing or conversion if needed.
    This function assumes that is_bool_indexer(key) == True.
    Parameters
    ----------
    index : Index
        Index of the object on which the indexing is done.
    key : list-like
        Boolean indexer to check.
    Returns
    -------
    np.array
        Resulting key.
    Raises
    ------
    IndexError
        If the key does not have the same length as index.
    IndexingError
        If the index of the key is unalignable to index.
    """"""
    result = key
    if isinstance(key, ABCSeries) and not key.index.equals(index):
        result = result.reindex(index)
        mask = isna(result._values)
        if mask.any():
            raise IndexingError(
                ""Unalignable boolean Series provided as ""
                ""indexer (index of the boolean Series and of ""
                ""the indexed object do not match).""
            )
        result = result.astype(bool)._values
    else:
        if is_sparse(result):
            result = result.to_dense()
        result = check_bool_array_indexer(index, result)
    return result

def convert_missing_indexer(indexer):
    """"""
    Reverse convert a missing indexer, which is a dict
    return the scalar indexer and a boolean indicating if we converted
    """"""
    if isinstance(indexer, dict):
        # a missing key (but not a tuple indexer)
        indexer = indexer[""key""]
        if isinstance(indexer, bool):
            raise KeyError(""cannot use a single bool to index into setitem"")
        return indexer, True
    return indexer, False

def convert_from_missing_indexer_tuple(indexer, axes):
    """"""
    Create a filtered indexer that doesn't have any missing indexers.
    """"""
    def get_indexer(_i, _idx):
        return axes[_i].get_loc(_idx[""key""]) if isinstance(_idx, dict) else _idx
    return tuple(get_indexer(_i, _idx) for _i, _idx in enumerate(indexer))

def maybe_convert_ix(*args):
    """"""
    We likely want to take the cross-product.
    """"""
    ixify = True
    for arg in args:
        if not isinstance(arg, (np.ndarray, list, ABCSeries, Index)):
            ixify = False",[85]
"""""""
Functions to generate methods and pin them to the appropriate classes.
""""""
import operator
from pandas.core.dtypes.generic import ABCDataFrame, ABCSeries, ABCSparseArray
from pandas.core.ops.roperator import (
    radd,
    rand_,
    rdivmod,
    rfloordiv,
    rmod,
    rmul,
    ror_,
    rpow,
    rsub,
    rtruediv,
    rxor,
)

def _get_method_wrappers(cls):
    """"""
    Find the appropriate operation-wrappers to use when defining flex/special
    arithmetic, boolean, and comparison operations with the given class.
    Parameters
    ----------
    cls : class
    Returns
    -------
    arith_flex : function or None
    comp_flex : function or None
    arith_special : function
    comp_special : function
    bool_special : function
    Notes
    -----
    None is only returned for SparseArray
    """"""
    # TODO: make these non-runtime imports once the relevant functions
    #  are no longer in __init__
    from pandas.core.ops import (
        _arith_method_FRAME,
        _arith_method_SERIES,
        _bool_method_SERIES,
        _comp_method_FRAME,
        _comp_method_SERIES,
        _flex_comp_method_FRAME,
        _flex_method_SERIES,
    )
    if issubclass(cls, ABCSeries):
        # Just Series
        arith_flex = _flex_method_SERIES
        comp_flex = _flex_method_SERIES
        arith_special = _arith_method_SERIES
        comp_special = _comp_method_SERIES
        bool_special = _bool_method_SERIES
    elif issubclass(cls, ABCDataFrame):
        arith_flex = _arith_method_FRAME
        comp_flex = _flex_comp_method_FRAME
        arith_special = _arith_method_FRAME
        comp_special = _comp_method_FRAME
        bool_special = _arith_method_FRAME
    return arith_flex, comp_flex, arith_special, comp_special, bool_special

def add_special_arithmetic_methods(cls):
    """"""
    Adds the full suite of special arithmetic methods (``__add__``,
    ``__sub__``, etc.) to the class.
    Parameters
    ----------
    cls : class
        special methods will be defined and pinned to this class
    """"""
    _, _, arith_method, comp_method, bool_method = _get_method_wrappers(cls)
    new_methods = _create_methods(
        cls, arith_method, comp_method, bool_method, special=True
    )
    # inplace operators (I feel like these should get passed an `inplace=True`
    # or just be removed
    def _wrap_inplace_method(method):
        """"""
        return an inplace wrapper for this method
        """"""
        def f(self, other):
            result = method(self, other)
            # this makes sure that we are aligned like the input
            # we are updating inplace so we want to ignore is_copy
            self._update_inplace(
                result.reindex_like(self, copy=False)._data, verify_is_copy=False
            )
            return self
        name = method.__name__.strip(""__"")
        f.__name__ = f""__i{name}__""
        return f
    new_methods.update(
        dict(
            __iadd__=_wrap_inplace_method(new_methods[""__add__""]),
            __isub__=_wrap_inplace_method(new_methods[""__sub__""]),
            __imul__=_wrap_inplace_method(new_methods[""__mul__""]),
            __itruediv__=_wrap_inplace_method(new_methods[""__truediv__""]),
            __ifloordiv__=_wrap_inplace_method(new_methods[""__floordiv__""]),
            __imod__=_wrap_inplace_method(new_methods[""__mod__""]),
            __ipow__=_wrap_inplace_method(new_methods[""__pow__""]),
        )
    )
    new_methods.update(
        dict(
            __iand__=_wrap_inplace_method(new_methods[""__and__""]),
            __ior__=_wrap_inplace_method(new_methods[""__or__""]),
            __ixor__=_wrap_inplace_method(new_methods[""__xor__""]),
        )
    )",[95]
"ipython_filename_pattern = re.compile('^<ipython-input-([0-9]+)-.*>$')

def get_local_reprs(frame, watch=()):
    code = frame.f_code
    vars_order = code.co_varnames + code.co_cellvars + code.co_freevars + tuple(frame.f_locals.keys())
    result_items = [(key, utils.get_shortish_repr(value)) for key, value in frame.f_locals.items()]
    result_items.sort(key=lambda key_value: vars_order.index(key_value[0]))
    result = collections.OrderedDict(result_items)
    for variable in watch:
        result.update(sorted(variable.items(frame)))
    return result

class UnavailableSource(object):
    def __getitem__(self, i):
        return u'SOURCE IS UNAVAILABLE'

source_cache = {}

def get_source_from_frame(frame):
    globs = frame.f_globals or {}
    module_name = globs.get('__name__')
    file_name = frame.f_code.co_filename
    cache_key = (module_name, file_name)
    try:
        return source_cache[cache_key]
    except KeyError:
        pass
    loader = globs.get('__loader__')
    source = None
    if hasattr(loader, 'get_source'):
        try:
            source = loader.get_source(module_name)
        except ImportError:
            pass
        if source is not None:
            source = source.splitlines()
    if source is None:
        ipython_filename_match = ipython_filename_pattern.match(file_name)
        if ipython_filename_match:
            entry_number = int(ipython_filename_match.group(1))
            try:
                import IPython
                ipython_shell = IPython.get_ipython()
                ((_, _, source_chunk),) = ipython_shell.history_manager. \
                                  get_range(0, entry_number, entry_number + 1)
                source = source_chunk.splitlines()
            except Exception:
                pass
        else:
            try:
                with open(file_name, 'rb') as fp:
                    source = fp.read().splitlines()
            except utils.file_reading_errors:
                pass
    if source is None:
        source = UnavailableSource()
    # If we just read the source from a file, or if the loader did not
    # apply tokenize.detect_encoding to decode the source into a
    # string, then we should do that ourselves.
    if isinstance(source[0], bytes):
        encoding = 'ascii'
        for line in source[:2]:
            # File coding may be specified. Match pattern from PEP-263
            # (https://www.python.org/dev/peps/pep-0263/)
            match = re.search(br'coding[:=]\s*([-\w.]+)', line)
            if match:
                encoding = match.group(1).decode('ascii')
                break
        source = [pycompat.text_type(sline, encoding, 'replace') for sline in
                  source]
    source_cache[cache_key] = source
    return source

def get_write_function(output, overwrite):
    is_path = isinstance(output, (pycompat.PathLike, str))
    if overwrite and not is_path:
        raise Exception('`overwrite=True` can only be used when writing '
                        'content to file.')
    if output is None:
        def write(s):
            stderr = sys.stderr
            try:
                stderr.write(s)
            except UnicodeEncodeError:
                # God damn Python 2
                stderr.write(utils.shitcode(s))
    elif is_path:
        return FileWriter(output, overwrite).write
    elif callable(output):
        write = output
    else:
        assert isinstance(output, utils.WritableStream)
        def write(s):
            output.write(s)
    return write

class FileWriter(object):
    def __init__(self, path, overwrite):
        self.path = pycompat.text_type(path)
        self.overwrite = overwrite
    def write(self, s):
        with open(self.path, 'w' if self.overwrite else 'a') as output_file:
            output_file.write(s)
        self.overwrite = False

thread_global = threading.local()

class Tracer:
    '''
    Snoop on the function, writing everything it's doing to stderr.
","[69, 115]"
"from datetime import timedelta
import operator
from sys import getsizeof
from typing import Union
import warnings
import numpy as np
from pandas._libs import index as libindex
import pandas.compat as compat
from pandas.compat.numpy import function as nv
from pandas.util._decorators import Appender, cache_readonly
from pandas.core.dtypes.common import (
    ensure_platform_int,
    ensure_python_int,
    is_int64_dtype,
    is_integer,
    is_integer_dtype,
    is_list_like,
    is_scalar,
    is_timedelta64_dtype,
)
from pandas.core.dtypes.generic import ABCTimedeltaIndex
from pandas.core import ops
import pandas.core.common as com
from pandas.core.construction import extract_array
import pandas.core.indexes.base as ibase
from pandas.core.indexes.base import Index, _index_shared_docs
from pandas.core.indexes.numeric import Int64Index
from pandas.core.ops.common import unpack_zerodim_and_defer
from pandas.io.formats.printing import pprint_thing

class RangeIndex(Int64Index):
    """"""
    Immutable Index implementing a monotonic integer range.
    RangeIndex is a memory-saving special case of Int64Index limited to
    representing monotonic ranges. Using RangeIndex may in some instances
    improve computing speed.
    This is the default index type used
    by DataFrame and Series when no explicit index is provided by the user.
    Parameters
    ----------
    start : int (default: 0), or other RangeIndex instance
        If int and ""stop"" is not given, interpreted as ""stop"" instead.
    stop : int (default: 0)
    step : int (default: 1)
    name : object, optional
        Name to be stored in the index.
    copy : bool, default False
        Unused, accepted for homogeneity with other index types.
    Attributes
    ----------
    start
    stop
    step
    Methods
    -------
    from_range
    See Also
    --------
    Index : The base pandas Index type.
    Int64Index : Index of int64 data.
    """"""
    _typ = ""rangeindex""
    _engine_type = libindex.Int64Engine
    _range = None  # type: range
    # check whether self._data has been called
    _cached_data = None  # type: np.ndarray
    # --------------------------------------------------------------------
    # Constructors
    def __new__(
        cls,
        start=None,
        stop=None,
        step=None,
        dtype=None,
        copy=False,
        name=None,
        fastpath=None,
    ):
        if fastpath is not None:
            warnings.warn(
                ""The 'fastpath' keyword is deprecated, and will be ""
                ""removed in a future version."",
                FutureWarning,
                stacklevel=2,
            )
            if fastpath:
                return cls._simple_new(range(start, stop, step), name=name)
        cls._validate_dtype(dtype)
        # RangeIndex
        if isinstance(start, RangeIndex):
            name = start.name if name is None else name
            start = start._range
            return cls._simple_new(start, dtype=dtype, name=name)
        # validate the arguments
        if com.all_none(start, stop, step):
            raise TypeError(""RangeIndex(...) must be called with integers"")
        start = ensure_python_int(start) if start is not None else 0
        if stop is None:
            start, stop = 0, start
        else:
            stop = ensure_python_int(stop)
        step = ensure_python_int(step) if step is not None else 1
        if step == 0:
            raise ValueError(""Step must not be zero"")
",[16]
"                ""direction invalid: {direction}"".format(direction=self.direction)
            )
    @property
    def _asof_key(self):
        """""" This is our asof key, the 'on' """"""
        return self.left_on[-1]
    def _get_merge_keys(self):
        # note this function has side effects
        (left_join_keys, right_join_keys, join_names) = super()._get_merge_keys()
        # validate index types are the same
        for i, (lk, rk) in enumerate(zip(left_join_keys, right_join_keys)):
            if not is_dtype_equal(lk.dtype, rk.dtype):
                if is_categorical_dtype(lk.dtype) and is_categorical_dtype(rk.dtype):
                    # The generic error message is confusing for categoricals.
                    #
                    # In this function, the join keys include both the original
                    # ones of the merge_asof() call, and also the keys passed
                    # to its by= argument. Unordered but equal categories
                    # are not supported for the former, but will fail
                    # later with a ValueError, so we don't *need* to check
                    # for them here.
                    msg = (
                        ""incompatible merge keys [{i}] {lkdtype} and ""
                        ""{rkdtype}, both sides category, but not equal ones"".format(
                            i=i, lkdtype=repr(lk.dtype), rkdtype=repr(rk.dtype)
                        )
                    )
                else:
                    msg = (
                        ""incompatible merge keys [{i}] {lkdtype} and ""
                        ""{rkdtype}, must be the same type"".format(
                            i=i, lkdtype=repr(lk.dtype), rkdtype=repr(rk.dtype)
                        )
                    )
                raise MergeError(msg)
        # validate tolerance; must be a Timedelta if we have a DTI
        if self.tolerance is not None:
            if self.left_index:
                lt = self.left.index
            else:
                lt = left_join_keys[-1]
            msg = (
                ""incompatible tolerance {tolerance}, must be compat ""
                ""with type {lkdtype}"".format(
                    tolerance=type(self.tolerance), lkdtype=repr(lt.dtype)
                )
            )
            if is_datetime64_dtype(lt) or is_datetime64tz_dtype(lt):
                if not isinstance(self.tolerance, Timedelta):
                    raise MergeError(msg)
                if self.tolerance < Timedelta(0):
                    raise MergeError(""tolerance must be positive"")
            elif is_int64_dtype(lt):
                if not is_integer(self.tolerance):
                    raise MergeError(msg)
                if self.tolerance < 0:
                    raise MergeError(""tolerance must be positive"")
            elif is_float_dtype(lt):
                if not is_number(self.tolerance):
                    raise MergeError(msg)
                if self.tolerance < 0:
                    raise MergeError(""tolerance must be positive"")
            else:
                raise MergeError(""key must be integer, timestamp or float"")
        # validate allow_exact_matches
        if not is_bool(self.allow_exact_matches):
            msg = ""allow_exact_matches must be boolean, passed {passed}""
            raise MergeError(msg.format(passed=self.allow_exact_matches))
        return left_join_keys, right_join_keys, join_names
    def _get_join_indexers(self):
        """""" return the join indexers """"""
        def flip(xs):
            """""" unlike np.transpose, this returns an array of tuples """"""
            xs = [
                x if not is_extension_array_dtype(x) else x._ndarray_values for x in xs
            ]
            labels = list(string.ascii_lowercase[: len(xs)])
            dtypes = [x.dtype for x in xs]
            labeled_dtypes = list(zip(labels, dtypes))
            return np.array(list(zip(*xs)), labeled_dtypes)
        # values to compare
        left_values = (
            self.left.index.values if self.left_index else self.left_join_keys[-1]
        )
        right_values = (
            self.right.index.values if self.right_index else self.right_join_keys[-1]
        )
        tolerance = self.tolerance
        # we require sortedness and non-null values in the join keys
        msg_sorted = ""{side} keys must be sorted""
        msg_missings = ""Merge keys contain null values on {side} side""
        if not Index(left_values).is_monotonic:
            if isnull(left_values).any():
                raise ValueError(msg_missings.format(side=""left""))
            else:
                raise ValueError(msg_sorted.format(side=""left""))
        if not Index(right_values).is_monotonic:
            if isnull(right_values).any():
                raise ValueError(msg_missings.format(side=""right""))
            else:
                raise ValueError(msg_sorted.format(side=""right""))
        # initial type conversion as needed
        if needs_i8_conversion(left_values):
            left_values = left_values.view(""i8"")
            right_values = right_values.view(""i8"")
            if tolerance is not None:
                tolerance = tolerance.value",[55]
"import re
from thefuck.shells import shell
from thefuck.specific.git import git_support

@git_support
def match(command):
    return ('did not match any file(s) known to git.' in command.stderr
            and ""Did you forget to 'git add'?"" in command.stderr)

@git_support
def get_new_command(command):
    missing_file = re.findall(
            r""error: pathspec '([^']*)' ""
            r""did not match any file\(s\) known to git."", command.stderr)[0]
    formatme = shell.and_('git add -- {}', '{}')
    return formatme.format(missing_file, command.script)","[7, 8, 14, 15]"
"def read_batch_urls(batch_fd):
    def fixup(url):
        if not isinstance(url, compat_str):
            url = url.decode('utf-8', 'replace')
        BOM_UTF8 = u'\xef\xbb\xbf'
        if url.startswith(BOM_UTF8):
            url = url[len(BOM_UTF8):]
        url = url.strip()
        if url.startswith(('#', ';', ']')):
            return False
        return url
    with contextlib.closing(batch_fd) as fd:
        return [url for url in map(fixup, fd) if url]

def urlencode_postdata(*args, **kargs):
    return compat_urllib_parse.urlencode(*args, **kargs).encode('ascii')

try:
    etree_iter = xml.etree.ElementTree.Element.iter
except AttributeError:  # Python <=2.6
    etree_iter = lambda n: n.findall('.//*')

def parse_xml(s):
    class TreeBuilder(xml.etree.ElementTree.TreeBuilder):
        def doctype(self, name, pubid, system):
            pass  # Ignore doctypes
    parser = xml.etree.ElementTree.XMLParser(target=TreeBuilder())
    kwargs = {'parser': parser} if sys.version_info >= (2, 7) else {}
    tree = xml.etree.ElementTree.XML(s.encode('utf-8'), **kwargs)
    # Fix up XML parser in Python 2.x
    if sys.version_info < (3, 0):
        for n in etree_iter(tree):
            if n.text is not None:
                if not isinstance(n.text, compat_str):
                    n.text = n.text.decode('utf-8')
    return tree

if sys.version_info < (3, 0) and sys.platform == 'win32':
    def compat_getpass(prompt, *args, **kwargs):
        if isinstance(prompt, compat_str):
            prompt = prompt.encode(preferredencoding())
        return getpass.getpass(prompt, *args, **kwargs)
else:
    compat_getpass = getpass.getpass

US_RATINGS = {
    'G': 0,
    'PG': 10,
    'PG-13': 13,
    'R': 16,
    'NC': 18,
}

def strip_jsonp(code):
    return re.sub(r'(?s)^[a-zA-Z0-9_]+\s*\(\s*(.*)\);?\s*?\s*$', r'\1', code)

def js_to_json(code):
    def fix_kv(m):
        key = m.group(2)
        if key.startswith(""'""):
            assert key.endswith(""'"")
            assert '""' not in key
            key = '""%s""' % key[1:-1]
        elif not key.startswith('""'):
            key = '""%s""' % key
        value = m.group(4)
        if value.startswith(""'""):
            assert value.endswith(""'"")
            assert '""' not in value
            value = '""%s""' % value[1:-1]
        return m.group(1) + key + m.group(3) + value
    res = re.sub(r'''(?x)
            ([{,]\s*)
            (""[^""]*""|\'[^\']*\'|[a-z0-9A-Z]+)
            (:\s*)
            ([0-9.]+|true|false|""[^""]*""|\'[^\']*\'|\[|\{)
        ''', fix_kv, code)
    res = re.sub(r',(\s*\])', lambda m: m.group(1), res)
    return res

def qualities(quality_ids):
    """""" Get a numeric quality value out of a list of possible values """"""
    def q(qid):
        try:
            return quality_ids.index(qid)
        except ValueError:
            return -1
    return q

DEFAULT_OUTTMPL = '%(title)s-%(id)s.%(ext)s'
try:
    subprocess_check_output = subprocess.check_output
except AttributeError:
    def subprocess_check_output(*args, **kwargs):
        assert 'input' not in kwargs
        p = subprocess.Popen(*args, stdout=subprocess.PIPE, **kwargs)
        output, _ = p.communicate()
        ret = p.poll()
        if ret:
            raise subprocess.CalledProcessError(ret, p.args, output=output)
        return output

def limit_length(s, length):
    """""" Add ellipses to overly long strings """"""
    if s is None:
        return None
    ELLIPSES = '...'
    if len(s) > length:
        return s[:length - len(ELLIPSES)] + ELLIPSES",[88]
"        #             set_cookie_escaped = compat_urllib_parse.quote(set_cookie, b""%/;:@&=+$,!~*'()?#[] "")
        #             if set_cookie != set_cookie_escaped:
        #                 del response.headers[set_cookie_header]
        #                 response.headers[set_cookie_header] = set_cookie_escaped
        return compat_urllib_request.HTTPCookieProcessor.http_response(self, request, response)
    https_request = compat_urllib_request.HTTPCookieProcessor.http_request
    https_response = http_response

def parse_iso8601(date_str, delimiter='T', timezone=None):
    """""" Return a UNIX timestamp from the given date """"""
    if date_str is None:
        return None
    date_str = re.sub(r'\.[0-9]+', '', date_str)
    if timezone is None:
        m = re.search(
            r'(?:Z$| ?(?P<sign>\+|-)(?P<hours>[0-9]{2}):?(?P<minutes>[0-9]{2})$)',
            date_str)
        if not m:
            timezone = datetime.timedelta()
        else:
            date_str = date_str[:-len(m.group(0))]
            if not m.group('sign'):
                timezone = datetime.timedelta()
            else:
                sign = 1 if m.group('sign') == '+' else -1
                timezone = datetime.timedelta(
                    hours=sign * int(m.group('hours')),
                    minutes=sign * int(m.group('minutes')))
    try:
        date_format = '%Y-%m-%d{0}%H:%M:%S'.format(delimiter)
        dt = datetime.datetime.strptime(date_str, date_format) - timezone
        return calendar.timegm(dt.timetuple())
    except ValueError:
        pass

def unified_strdate(date_str, day_first=True):
    """"""Return a string with the date in the format YYYYMMDD""""""
    if date_str is None:
        return None
    upload_date = None
    # Replace commas
    date_str = date_str.replace(',', ' ')
    # %z (UTC offset) is only supported in python>=3.2
    if not re.match(r'^[0-9]{1,2}-[0-9]{1,2}-[0-9]{4}$', date_str):
        date_str = re.sub(r' ?(\+|-)[0-9]{2}:?[0-9]{2}$', '', date_str)
    # Remove AM/PM + timezone
    date_str = re.sub(r'(?i)\s*(?:AM|PM)(?:\s+[A-Z]+)?', '', date_str)
    format_expressions = [
        '%d %B %Y',
        '%d %b %Y',
        '%B %d %Y',
        '%b %d %Y',
        '%b %dst %Y %I:%M%p',
        '%b %dnd %Y %I:%M%p',
        '%b %dth %Y %I:%M%p',
        '%Y %m %d',
        '%Y-%m-%d',
        '%Y/%m/%d',
        '%Y/%m/%d %H:%M:%S',
        '%Y-%m-%d %H:%M:%S',
        '%Y-%m-%d %H:%M:%S.%f',
        '%d.%m.%Y %H:%M',
        '%d.%m.%Y %H.%M',
        '%Y-%m-%dT%H:%M:%SZ',
        '%Y-%m-%dT%H:%M:%S.%fZ',
        '%Y-%m-%dT%H:%M:%S.%f0Z',
        '%Y-%m-%dT%H:%M:%S',
        '%Y-%m-%dT%H:%M:%S.%f',
        '%Y-%m-%dT%H:%M',
    ]
    if day_first:
        format_expressions.extend([
            '%d-%m-%Y',
            '%d.%m.%Y',
            '%d/%m/%Y',
            '%d/%m/%y',
            '%d/%m/%Y %H:%M:%S',
        ])
    else:
        format_expressions.extend([
            '%m-%d-%Y',
            '%m.%d.%Y',
            '%m/%d/%Y',
            '%m/%d/%y',
            '%m/%d/%Y %H:%M:%S',
        ])
    for expression in format_expressions:
        try:
            upload_date = datetime.datetime.strptime(date_str, expression).strftime('%Y%m%d')
        except ValueError:
            pass
    if upload_date is None:
        timetuple = email.utils.parsedate_tz(date_str)
        if timetuple:
            upload_date = datetime.datetime(*timetuple[:6]).strftime('%Y%m%d')
    return compat_str(upload_date)

def determine_ext(url, default_ext='unknown_video'):
    if url is None:
        return default_ext
    guess = url.partition('?')[0].rpartition('.')[2]
    if re.match(r'^[A-Za-z0-9]+$', guess):
        return guess
    else:
        return default_ext

def subtitles_filename(filename, sub_lang, sub_format):
    return filename.rsplit('.', 1)[0] + '.' + sub_lang + '.' + sub_format

def date_from_str(date_str):
    """"""
    Return a datetime object from a string in the format YYYYMMDD or
    (now|today)[+-][0-9](day|week|month|year)(s)?""""""
    today = datetime.date.today()
    if date_str in ('now', 'today'):
        return today",[103]
"import operator
from shutil import get_terminal_size
from typing import Dict, Hashable, List, Type, Union, cast
from warnings import warn
import numpy as np
from pandas._config import get_option
from pandas._libs import algos as libalgos, hashtable as htable
from pandas._typing import ArrayLike, Dtype, Ordered, Scalar
from pandas.util._decorators import cache_readonly, deprecate_kwarg, doc
from pandas.util._validators import validate_bool_kwarg, validate_fillna_kwargs
from pandas.core.dtypes.cast import (
    coerce_indexer_dtype,
    maybe_cast_to_extension_array,
    maybe_infer_to_datetimelike,
)
from pandas.core.dtypes.common import (
    ensure_int64,
    ensure_object,
    is_categorical_dtype,
    is_datetime64_dtype,
    is_dict_like,
    is_dtype_equal,
    is_extension_array_dtype,
    is_integer_dtype,
    is_iterator,
    is_list_like,
    is_object_dtype,
    is_scalar,
    is_sequence,
    is_timedelta64_dtype,
    needs_i8_conversion,
)
from pandas.core.dtypes.dtypes import CategoricalDtype
from pandas.core.dtypes.generic import ABCIndexClass, ABCSeries
from pandas.core.dtypes.inference import is_hashable
from pandas.core.dtypes.missing import isna, notna
from pandas.core import ops
from pandas.core.accessor import PandasDelegate, delegate_names
import pandas.core.algorithms as algorithms
from pandas.core.algorithms import _get_data_algo, factorize, take_1d, unique1d
from pandas.core.array_algos.transforms import shift
from pandas.core.arrays._mixins import _T, NDArrayBackedExtensionArray
from pandas.core.base import NoNewAttributesMixin, PandasObject, _shared_docs
import pandas.core.common as com
from pandas.core.construction import array, extract_array, sanitize_array
from pandas.core.indexers import check_array_indexer, deprecate_ndim_indexing
from pandas.core.missing import interpolate_2d
from pandas.core.ops.common import unpack_zerodim_and_defer
from pandas.core.sorting import nargsort
from pandas.io.formats import console

def _cat_compare_op(op):
    opname = f""__{op.__name__}__""
    @unpack_zerodim_and_defer(opname)
    def func(self, other):
        if is_list_like(other) and len(other) != len(self):
            # TODO: Could this fail if the categories are listlike objects?
            raise ValueError(""Lengths must match."")
        if not self.ordered:
            if opname in [""__lt__"", ""__gt__"", ""__le__"", ""__ge__""]:
                raise TypeError(
                    ""Unordered Categoricals can only compare equality or not""
                )
        if isinstance(other, Categorical):
            # Two Categoricals can only be be compared if the categories are
            # the same (maybe up to ordering, depending on ordered)
            msg = ""Categoricals can only be compared if 'categories' are the same.""
            if len(self.categories) != len(other.categories):
                raise TypeError(msg + "" Categories are different lengths"")
            elif self.ordered and not (self.categories == other.categories).all():
                raise TypeError(msg)
            elif not set(self.categories) == set(other.categories):
                raise TypeError(msg)
            if not (self.ordered == other.ordered):
                raise TypeError(
                    ""Categoricals can only be compared if 'ordered' is the same""
                )
            if not self.ordered and not self.categories.equals(other.categories):
                # both unordered and different order
                other_codes = _get_codes_for_values(other, self.categories)
            else:
                other_codes = other._codes
            f = getattr(self._codes, opname)
            ret = f(other_codes)
            mask = (self._codes == -1) | (other_codes == -1)
            if mask.any():
                # In other series, the leads to False, so do that here too
                if opname == ""__ne__"":
                    ret[(self._codes == -1) & (other_codes == -1)] = True
                else:
                    ret[mask] = False
            return ret
        if is_scalar(other):
            if other in self.categories:
                i = self.categories.get_loc(other)
                ret = getattr(self._codes, opname)(i)
                if opname not in {""__eq__"", ""__ge__"", ""__gt__""}:
                    # check for NaN needed if we are not equal or larger
                    mask = self._codes == -1
                    ret[mask] = False
                return ret
            else:
                if opname == ""__eq__"":
                    return np.zeros(len(self), dtype=bool)
                elif opname == ""__ne__"":
                    return np.ones(len(self), dtype=bool)
                else:
                    raise TypeError(
                        f""Cannot compare a Categorical for op {opname} with a ""
                        ""scalar, which is not a category.""
                    )
        else:
",[39]
"from subprocess import Popen, PIPE
from time import time
import os
from ..utils import DEVNULL, memoize, cache
from .generic import Generic

class Fish(Generic):
    def _get_overridden_aliases(self):
        overridden_aliases = os.environ.get('TF_OVERRIDDEN_ALIASES', '').strip()
        if overridden_aliases:
            return [alias.strip() for alias in overridden_aliases.split(',')]
        else:
            return ['cd', 'grep', 'ls', 'man', 'open']
    def app_alias(self, fuck):
        # It is VERY important to have the variables declared WITHIN the alias
        return ('function {0} -d ""Correct your previous console command""\n'
                '  set -l fucked_up_command $history[1]\n'
                '  env TF_ALIAS={0} PYTHONIOENCODING=utf-8'
                ' thefuck $fucked_up_command | read -l unfucked_command\n'
                '  if [ ""$unfucked_command"" != """" ]\n'
                '    eval $unfucked_command\n'
                '    history --delete $fucked_up_command\n'
                '    history --merge ^ /dev/null\n'
                '  end\n'
                'end').format(fuck)
    @memoize
    @cache('.config/fish/config.fish', '.config/fish/functions')
    def get_aliases(self):
        overridden = self._get_overridden_aliases()
        proc = Popen(['fish', '-ic', 'functions'], stdout=PIPE, stderr=DEVNULL)
        functions = proc.stdout.read().decode('utf-8').strip().split('\n')
        return {func: func for func in functions if func not in overridden}
    def _expand_aliases(self, command_script):
        aliases = self.get_aliases()
        binary = command_script.split(' ')[0]
        if binary in aliases:
            return u'fish -ic ""{}""'.format(command_script.replace('""', r'\""'))
        else:
            return command_script
    def from_shell(self, command_script):
        """"""Prepares command before running in app.""""""
        return self._expand_aliases(command_script)
    def _get_history_file_name(self):
        return os.path.expanduser('~/.config/fish/fish_history')
    def _get_history_line(self, command_script):
        return u'- cmd: {}\n   when: {}\n'.format(command_script, int(time()))
    def _script_from_history(self, line):
        if '- cmd: ' in line:
            return line.split('- cmd: ', 1)[1]
        else:
            return ''
    def and_(self, *commands):
        return u'; and '.join(commands)
    def how_to_configure(self):
        return (r""eval (thefuck --alias | tr '\n' ';')"",
                '~/.config/fish/config.fish')","[9, 10, 11, 12, 13]"
"        Parameters
        ----------
        i : int
        Returns
        -------
        scalar (int) or Series (slice, sequence)
        """"""
        return self._values[i]
    def _slice(self, slobj: slice, axis: int = 0, kind: str = ""getitem"") -> ""Series"":
        assert kind in [""getitem"", ""iloc""]
        slobj = self.index._convert_slice_indexer(slobj, kind=kind)
        return self._get_values(slobj)
    def __getitem__(self, key):
        key = com.apply_if_callable(key, self)
        if key is Ellipsis:
            return self
        key_is_scalar = is_scalar(key)
        if key_is_scalar:
            key = self.index._convert_scalar_indexer(key, kind=""getitem"")
        if key_is_scalar or isinstance(self.index, MultiIndex):
            # Otherwise index.get_value will raise InvalidIndexError
            try:
                result = self.index.get_value(self, key)
                return result
            except InvalidIndexError:
                pass
            except (KeyError, ValueError):
                if isinstance(key, tuple) and isinstance(self.index, MultiIndex):
                    # kludge
                    pass
                else:
                    raise
        if not key_is_scalar:
            # avoid expensive checks if we know we have a scalar
            if is_iterator(key):
                key = list(key)
            if com.is_bool_indexer(key):
                key = check_bool_indexer(self.index, key)
                return self._get_values(key)
        return self._get_with(key)
    def _get_with(self, key):
        # other: fancy integer or otherwise
        if isinstance(key, slice):
            return self._slice(key)
        elif isinstance(key, ABCDataFrame):
            raise TypeError(
                ""Indexing a Series with DataFrame is not ""
                ""supported, use the appropriate DataFrame column""
            )
        elif isinstance(key, tuple):
            try:
                return self._get_values_tuple(key)
            except ValueError:
                # if we don't have a MultiIndex, we may still be able to handle
                #  a 1-tuple.  see test_1tuple_without_multiindex
                if len(key) == 1:
                    key = key[0]
                    if isinstance(key, slice):
                        return self._get_values(key)
                raise
        if not isinstance(key, (list, np.ndarray, Series, Index)):
            key = list(key)
        if isinstance(key, Index):
            key_type = key.inferred_type
        else:
            key_type = lib.infer_dtype(key, skipna=False)
        # Note: The key_type == ""boolean"" case should be caught by the
        #  com.is_bool_indexer check in __getitem__
        if key_type == ""integer"":
            if self.index.is_integer() or self.index.is_floating():
                return self.loc[key]
            elif isinstance(self.index, IntervalIndex):
                indexer = self.index.get_indexer_for(key)
                return self.iloc[indexer]
            else:
                return self._get_values(key)
        if isinstance(key, (list, tuple)):
            # TODO: de-dup with tuple case handled above?
            # handle the dup indexing case GH#4246
            if len(key) == 1 and isinstance(key[0], slice):
                # [slice(0, 5, None)] will break if you convert to ndarray,
                # e.g. as requested by np.median
                # FIXME: hack
                return self._get_values(key)
            return self.loc[key]
        return self.reindex(key)
    def _get_values_tuple(self, key):
        # mpl hackaround
        if com.any_none(*key):
            # suppress warning from slicing the index with a 2d indexer.
            # eventually we'll want Series itself to warn.
            with warnings.catch_warnings():
                warnings.filterwarnings(
                    ""ignore"", ""Support for multi-dim"", DeprecationWarning
                )
                return self._get_values(key)
        if not isinstance(self.index, MultiIndex):
            raise ValueError(""Can only tuple-index with a MultiIndex"")
        # If key is contained, would have returned by now
        indexer, new_index = self.index.get_loc_level(key)
        return self._constructor(self._values[indexer], index=new_index).__finalize__(
            self
        )
    def _get_values(self, indexer):
        try:",[90]
"        if np.iterable(ticks):
            self.locator = ticker.FixedLocator(ticks, nbins=len(ticks))
        else:
            self.locator = ticks
        if update_ticks:
            self.update_ticks()
        self.stale = True
    def get_ticks(self, minor=False):
        """"""Return the x ticks as a list of locations.""""""
        if self._manual_tick_data_values is None:
            ax = self.ax
            long_axis = (
                ax.yaxis if self.orientation == 'vertical' else ax.xaxis)
            return long_axis.get_majorticklocs()
        else:
            # We made the axes manually, the old way, and the ylim is 0-1,
            # so the majorticklocs are in those units, not data units.
            return self._manual_tick_data_values
    def set_ticklabels(self, ticklabels, update_ticks=True):
        """"""
        Set tick labels.
        Tick labels are updated immediately unless *update_ticks* is *False*,
        in which case one should call `.update_ticks` explicitly.
        """"""
        if isinstance(self.locator, ticker.FixedLocator):
            self.formatter = ticker.FixedFormatter(ticklabels)
            if update_ticks:
                self.update_ticks()
        else:
            cbook._warn_external(""set_ticks() must have been called."")
        self.stale = True
    def minorticks_on(self):
        """"""
        Turns on the minor ticks on the colorbar without extruding
        into the ""extend regions"".
        """"""
        ax = self.ax
        long_axis = ax.yaxis if self.orientation == 'vertical' else ax.xaxis
        if long_axis.get_scale() == 'log':
            long_axis.set_minor_locator(_ColorbarLogLocator(self, base=10.,
                                                            subs='auto'))
            long_axis.set_minor_formatter(ticker.LogFormatterSciNotation())
        else:
            long_axis.set_minor_locator(_ColorbarAutoMinorLocator(self))
    def minorticks_off(self):
        """"""
        Turns off the minor ticks on the colorbar.
        """"""
        ax = self.ax
        long_axis = ax.yaxis if self.orientation == 'vertical' else ax.xaxis
        long_axis.set_minor_locator(ticker.NullLocator())
    def _config_axes(self, X, Y):
        '''
        Make an axes patch and outline.
        '''
        ax = self.ax
        ax.set_frame_on(False)
        ax.set_navigate(False)
        xy = self._outline(X, Y)
        ax.ignore_existing_data_limits = True
        ax.update_datalim(xy)
        ax.set_xlim(*ax.dataLim.intervalx)
        ax.set_ylim(*ax.dataLim.intervaly)
        if self.outline is not None:
            self.outline.remove()
        self.outline = mpatches.Polygon(
            xy, edgecolor=mpl.rcParams['axes.edgecolor'],
            facecolor='none',
            linewidth=mpl.rcParams['axes.linewidth'],
            closed=True,
            zorder=2)
        ax.add_artist(self.outline)
        self.outline.set_clip_box(None)
        self.outline.set_clip_path(None)
        c = mpl.rcParams['axes.facecolor']
        if self.patch is not None:
            self.patch.remove()
        self.patch = mpatches.Polygon(xy, edgecolor=c,
                                      facecolor=c,
                                      linewidth=0.01,
                                      zorder=-1)
        ax.add_artist(self.patch)
        self.update_ticks()
    def _set_label(self):
        if self.orientation == 'vertical':
            self.ax.set_ylabel(self._label, **self._labelkw)
        else:
            self.ax.set_xlabel(self._label, **self._labelkw)
        self.stale = True
    def set_label(self, label, **kw):
        """"""Label the long axis of the colorbar.""""""
        self._label = str(label)
        self._labelkw = kw
        self._set_label()
    def _outline(self, X, Y):
        '''
        Return *x*, *y* arrays of colorbar bounding polygon,
        taking orientation into account.
        '''
        N = X.shape[0]
        ii = [0, 1, N - 2, N - 1, 2 * N - 1, 2 * N - 2, N + 1, N, 0]
        x = X.T.reshape(-1)[ii]
        y = Y.T.reshape(-1)[ii]
        return (np.column_stack([y, x])
                if self.orientation == 'horizontal' else
                np.column_stack([x, y]))
    def _edges(self, X, Y):
        '''
        Return the separator line segments; helper for _add_solids.
        '''
        N = X.shape[0]
        # Using the non-array form of these line segments is much
        # simpler than making them into arrays.",[103]
"        ss0 = ax.get_subplotspec()
        hassubplotspec[ss0.num1:(ss0.num2 + 1)] = True
    for nn, hss in enumerate(hassubplotspec):
        if not hss:
            # this gridspec slot doesn't have an axis so we
            # make a ""ghost"".
            ax = fig.add_subplot(gs[nn])
            ax.set_frame_on(False)
            ax.set_xticks([])
            ax.set_yticks([])
            ax.set_facecolor((1, 0, 0, 0))

def _make_layout_margins(ax, renderer, h_pad, w_pad):
    """"""
    For each axes, make a margin between the *pos* layoutbox and the
    *axes* layoutbox be a minimum size that can accommodate the
    decorations on the axis.
    """"""
    fig = ax.figure
    invTransFig = fig.transFigure.inverted().transform_bbox
    pos = ax.get_position(original=True)
    tightbbox = ax.get_tightbbox(renderer=renderer)
    if tightbbox is None:
        bbox = pos
    else:
        bbox = invTransFig(tightbbox)
    # this can go wrong:
    if not (np.isfinite(bbox.width) and np.isfinite(bbox.height)):
        # just abort, this is likely a bad set of co-ordinates that
        # is transitory...
        return
    # use stored h_pad if it exists
    h_padt = ax._poslayoutbox.h_pad
    if h_padt is None:
        h_padt = h_pad
    w_padt = ax._poslayoutbox.w_pad
    if w_padt is None:
        w_padt = w_pad
    ax._poslayoutbox.edit_left_margin_min(-bbox.x0 +
            pos.x0 + w_padt)
    ax._poslayoutbox.edit_right_margin_min(bbox.x1 -
            pos.x1 + w_padt)
    ax._poslayoutbox.edit_bottom_margin_min(
            -bbox.y0 + pos.y0 + h_padt)
    ax._poslayoutbox.edit_top_margin_min(bbox.y1-pos.y1+h_padt)
    _log.debug('left %f', (-bbox.x0 + pos.x0 + w_pad))
    _log.debug('right %f', (bbox.x1 - pos.x1 + w_pad))
    _log.debug('bottom %f', (-bbox.y0 + pos.y0 + h_padt))
    _log.debug('bbox.y0 %f', bbox.y0)
    _log.debug('pos.y0 %f', pos.y0)
    # Sometimes its possible for the solver to collapse
    # rather than expand axes, so they all have zero height
    # or width.  This stops that...  It *should* have been
    # taken into account w/ pref_width...
    if fig._layoutbox.constrained_layout_called < 1:
        ax._poslayoutbox.constrain_height_min(20, strength='weak')
        ax._poslayoutbox.constrain_width_min(20, strength='weak')
        ax._layoutbox.constrain_height_min(20, strength='weak')
        ax._layoutbox.constrain_width_min(20, strength='weak')
        ax._poslayoutbox.constrain_top_margin(0, strength='weak')
        ax._poslayoutbox.constrain_bottom_margin(0,
                strength='weak')
        ax._poslayoutbox.constrain_right_margin(0, strength='weak')
        ax._poslayoutbox.constrain_left_margin(0, strength='weak')

def _align_spines(fig, gs):
    """"""
    - Align right/left and bottom/top spines of appropriate subplots.
    - Compare size of subplotspec including height and width ratios
       and make sure that the axes spines are at least as large
       as they should be.
    """"""
    # for each gridspec...
    nrows, ncols = gs.get_geometry()
    width_ratios = gs.get_width_ratios()
    height_ratios = gs.get_height_ratios()
    if width_ratios is None:
        width_ratios = np.ones(ncols)
    if height_ratios is None:
        height_ratios = np.ones(nrows)
    # get axes in this gridspec....
    axs = []
    for ax in fig.axes:
        if (hasattr(ax, 'get_subplotspec')
                and ax._layoutbox is not None):
            if ax.get_subplotspec().get_gridspec() == gs:
                axs += [ax]
    rownummin = np.zeros(len(axs), dtype=np.int8)
    rownummax = np.zeros(len(axs), dtype=np.int8)
    colnummin = np.zeros(len(axs), dtype=np.int8)
    colnummax = np.zeros(len(axs), dtype=np.int8)
    width = np.zeros(len(axs))
    height = np.zeros(len(axs))
    for n, ax in enumerate(axs):
        ss0 = ax.get_subplotspec()
        rownummin[n], colnummin[n] = divmod(ss0.num1, ncols)
        rownummax[n], colnummax[n] = divmod(ss0.num2, ncols)
        width[n] = np.sum(
                width_ratios[colnummin[n]:(colnummax[n] + 1)])
        height[n] = np.sum(
                height_ratios[rownummin[n]:(rownummax[n] + 1)])
    for nn, ax in enumerate(axs[:-1]):
        # now compare ax to all the axs:
        #
        # If the subplotspecs have the same colnumXmax, then line
        # up their right sides.  If they have the same min, then
        # line up their left sides (and vertical equivalents).
        rownum0min, colnum0min = rownummin[nn], colnummin[nn]
        rownum0max, colnum0max = rownummax[nn], colnummax[nn]
        width0, height0 = width[nn], height[nn]
        alignleft = False
        alignright = False
        alignbot = False
        aligntop = False
        alignheight = False
        alignwidth = False
        for mm in range(nn+1, len(axs)):
            axc = axs[mm]
            rownumCmin, colnumCmin = rownummin[mm], colnummin[mm]
            rownumCmax, colnumCmax = rownummax[mm], colnummax[mm]
            widthC, heightC = width[mm], height[mm]","[7, 8, 9, 10]"
"            # if expand is False, result should have the same name
            # as the original otherwise specified
            if name is None:
                name = getattr(result, ""name"", None)
            if name is None:
                # do not use logical or, _orig may be a DataFrame
                # which has ""name"" column
                name = self._orig.name
        # Wait until we are sure result is a Series or Index before
        # checking attributes (GH 12180)
        if isinstance(self._orig, ABCIndexClass):
            # if result is a boolean np.array, return the np.array
            # instead of wrapping it into a boolean Index (GH 8875)
            if is_bool_dtype(result):
                return result
            if expand:
                result = list(result)
                out = MultiIndex.from_tuples(result, names=name)
                if out.nlevels == 1:
                    # We had all tuples of length-one, which are
                    # better represented as a regular Index.
                    out = out.get_level_values(0)
                return out
            else:
                return Index(result, name=name)
        else:
            index = self._orig.index
            if expand:
                cons = self._orig._constructor_expanddim
                result = cons(result, columns=name, index=index, dtype=dtype)
            else:
                # Must be a Series
                cons = self._orig._constructor
                result = cons(result, name=name, index=index, dtype=dtype)
            return result
    def _get_series_list(self, others):
        """"""
        Auxiliary function for :meth:`str.cat`. Turn potentially mixed input
        into a list of Series (elements without an index must match the length
        of the calling Series/Index).
        Parameters
        ----------
        others : Series, DataFrame, np.ndarray, list-like or list-like of
            Objects that are either Series, Index or np.ndarray (1-dim).
        Returns
        -------
        list of Series
            Others transformed into list of Series.
        """"""
        from pandas import Series, DataFrame
        # self._orig is either Series or Index
        idx = self._orig if isinstance(self._orig, ABCIndexClass) else self._orig.index
        # Generally speaking, all objects without an index inherit the index
        # `idx` of the calling Series/Index - i.e. must have matching length.
        # Objects with an index (i.e. Series/Index/DataFrame) keep their own.
        if isinstance(others, ABCSeries):
            return [others]
        elif isinstance(others, ABCIndexClass):
            return [Series(others._values, index=others)]
        elif isinstance(others, ABCDataFrame):
            return [others[x] for x in others]
        elif isinstance(others, np.ndarray) and others.ndim == 2:
            others = DataFrame(others, index=idx)
            return [others[x] for x in others]
        elif is_list_like(others, allow_sets=False):
            others = list(others)  # ensure iterators do not get read twice etc
            # in case of list-like `others`, all elements must be
            # either Series/Index/np.ndarray (1-dim)...
            if all(
                isinstance(x, (ABCSeries, ABCIndexClass))
                or (isinstance(x, np.ndarray) and x.ndim == 1)
                for x in others
            ):
                los = []
                while others:  # iterate through list and append each element
                    los = los + self._get_series_list(others.pop(0))
                return los
            # ... or just strings
            elif all(not is_list_like(x) for x in others):
                return [Series(others, index=idx)]
        raise TypeError(
            ""others must be Series, Index, DataFrame, np.ndarrary ""
            ""or list-like (either containing only strings or ""
            ""containing only objects of type Series/Index/""
            ""np.ndarray[1-dim])""
        )
    @forbid_nonstring_types([""bytes"", ""mixed"", ""mixed-integer""])
    def cat(self, others=None, sep=None, na_rep=None, join=""left""):
        """"""
        Concatenate strings in the Series/Index with given separator.
        If `others` is specified, this function concatenates the Series/Index
        and elements of `others` element-wise.
        If `others` is not passed, then all values in the Series/Index are
        concatenated into a single string with a given `sep`.
        Parameters
        ----------
        others : Series, Index, DataFrame, np.ndarray or list-like
            Series, Index, DataFrame, np.ndarray (one- or two-dimensional) and
            other list-likes of strings must have the same length as the
            calling Series/Index, with the exception of indexed objects (i.e.
            Series/Index/DataFrame) if `join` is not None.
            If others is a list-like that contains a combination of Series,
            Index or np.ndarray (1-dim), then all elements will be unpacked and
            must satisfy the above criteria individually.
            If others is None, the method returns the concatenation of all
            strings in the calling Series/Index.
        sep : str, default ''
            The separator between the different elements/columns. By default
            the empty string `''` is used.
        na_rep : str or None, default None
            Representation that is inserted for all missing values:
            - If `na_rep` is None, and `others` is None, missing values in the
              Series/Index are omitted from the result.",[65]
"    # is this an index replacement?
    if (
        not any_callable
        and not any_arraylike
        and not any_groupers
        and match_axis_length
        and level is None
    ):
        if isinstance(obj, DataFrame):
            all_in_columns_index = all(
                g in obj.columns or g in obj.index.names for g in keys
            )
        else:
            assert isinstance(obj, Series)
            all_in_columns_index = all(g in obj.index.names for g in keys)
        if not all_in_columns_index:
            keys = [com.asarray_tuplesafe(keys)]
    if isinstance(level, (tuple, list)):
        if key is None:
            keys = [None] * len(level)
        levels = level
    else:
        levels = [level] * len(keys)
    groupings: List[Grouping] = []
    exclusions: List[Hashable] = []
    # if the actual grouper should be obj[key]
    def is_in_axis(key) -> bool:
        if not _is_label_like(key):
            items = obj._data.items
            try:
                items.get_loc(key)
            except (KeyError, TypeError):
                # TypeError shows up here if we pass e.g. Int64Index
                return False
        return True
    # if the grouper is obj[name]
    def is_in_obj(gpr) -> bool:
        if not hasattr(gpr, ""name""):
            return False
        try:
            return gpr is obj[gpr.name]
        except (KeyError, IndexError):
            return False
    for i, (gpr, level) in enumerate(zip(keys, levels)):
        if is_in_obj(gpr):  # df.groupby(df['name'])
            in_axis, name = True, gpr.name
            exclusions.append(name)
        elif is_in_axis(gpr):  # df.groupby('name')
            if gpr in obj:
                if validate:
                    obj._check_label_or_level_ambiguity(gpr, axis=axis)
                in_axis, name, gpr = True, gpr, obj[gpr]
                exclusions.append(name)
            elif obj._is_level_reference(gpr, axis=axis):
                in_axis, name, level, gpr = False, None, gpr, None
            else:
                raise KeyError(gpr)
        elif isinstance(gpr, Grouper) and gpr.key is not None:
            # Add key to exclusions
            exclusions.append(gpr.key)
            in_axis, name = False, None
        else:
            in_axis, name = False, None
        if is_categorical_dtype(gpr) and len(gpr) != obj.shape[axis]:
            raise ValueError(
                f""Length of grouper ({len(gpr)}) and axis ({obj.shape[axis]}) ""
                ""must be same length""
            )
        # create the Grouping
        # allow us to passing the actual Grouping as the gpr
        ping = (
            Grouping(
                group_axis,
                gpr,
                obj=obj,
                name=name,
                level=level,
                sort=sort,
                observed=observed,
                in_axis=in_axis,
            )
            if not isinstance(gpr, Grouping)
            else gpr
        )
        groupings.append(ping)
    if len(groupings) == 0 and len(obj):
        raise ValueError(""No group keys passed!"")
    elif len(groupings) == 0:
        groupings.append(Grouping(Index([], dtype=""int""), np.array([], dtype=np.intp)))
    # create the internals grouper
    grouper = ops.BaseGrouper(group_axis, groupings, sort=sort, mutated=mutated)
    return grouper, exclusions, obj

def _is_label_like(val) -> bool:
    return isinstance(val, (str, tuple)) or (val is not None and is_scalar(val))

def _convert_grouper(axis: Index, grouper):
    if isinstance(grouper, dict):
        return grouper.get
    elif isinstance(grouper, Series):
        if grouper.index.equals(axis):
            return grouper._values
        else:
            return grouper.reindex(axis)._values
    elif isinstance(grouper, (list, Series, Index, np.ndarray)):
        if len(grouper) != len(axis):
            raise ValueError(""Grouper and axis must be same length"")
        return grouper
    else:",[36]
"
class NPOIE(InfoExtractor):
    IE_NAME = 'npo.nl'
    _VALID_URL = r'https?://www\.npo\.nl/[^/]+/[^/]+/(?P<id>[^/?]+)'
    _TESTS = [
        {
            'url': 'http://www.npo.nl/nieuwsuur/22-06-2014/VPWON_1220719',
            'md5': '4b3f9c429157ec4775f2c9cb7b911016',
            'info_dict': {
                'id': 'VPWON_1220719',
                'ext': 'm4v',
                'title': 'Nieuwsuur',
                'description': 'Dagelijks tussen tien en elf: nieuws, sport en achtergronden.',
                'upload_date': '20140622',
            },
        },
        {
            'url': 'http://www.npo.nl/de-mega-mike-mega-thomas-show/27-02-2009/VARA_101191800',
            'md5': 'da50a5787dbfc1603c4ad80f31c5120b',
            'info_dict': {
                'id': 'VARA_101191800',
                'ext': 'm4v',
                'title': 'De Mega Mike & Mega Thomas show',
                'description': 'md5:3b74c97fc9d6901d5a665aac0e5400f4',
                'upload_date': '20090227',
                'duration': 2400,
            },
        },
        {
            'url': 'http://www.npo.nl/tegenlicht/25-02-2013/VPWON_1169289',
            'md5': 'f8065e4e5a7824068ed3c7e783178f2c',
            'info_dict': {
                'id': 'VPWON_1169289',
                'ext': 'm4v',
                'title': 'Tegenlicht',
                'description': 'md5:d6476bceb17a8c103c76c3b708f05dd1',
                'upload_date': '20130225',
                'duration': 3000,
            },
        }
    ]
    def _real_extract(self, url):
        mobj = re.match(self._VALID_URL, url)
        video_id = mobj.group('id')
        return self._get_info(video_id)
    def _get_info(self, video_id):
        metadata = self._download_json(
            'http://e.omroep.nl/metadata/aflevering/%s' % video_id,
            video_id,
            # We have to remove the javascript callback
            transform_source=lambda j: re.sub(r'parseMetadata\((.*?)\);\n//.*$', r'\1', j)
        )
        token_page = self._download_webpage(
            'http://ida.omroep.nl/npoplayer/i.js',
            video_id,
            note='Downloading token'
        )
        token = self._search_regex(r'npoplayer\.token = ""(.+?)""', token_page, 'token')
        formats = []
        quality = qualities(['adaptive', 'wmv_sb', 'h264_sb', 'wmv_bb', 'h264_bb', 'wvc1_std', 'h264_std'])
        for format_id in metadata['pubopties']:
            format_info = self._download_json(
                'http://ida.omroep.nl/odi/?prid=%s&puboptions=%s&adaptive=yes&token=%s' % (video_id, format_id, token),
                video_id, 'Downloading %s JSON' % format_id)
            if format_info.get('error_code', 0) or format_info.get('errorcode', 0):
                continue
            streams = format_info.get('streams')
            if streams:
                video_info = self._download_json(
                    streams[0] + '&type=json',
                    video_id, 'Downloading %s stream JSON' % format_id)
            else:
                video_info = format_info
            video_url = video_info.get('url')
            if not video_url:
                continue
            if format_id == 'adaptive':
                formats.extend(self._extract_m3u8_formats(video_url, video_id))
            else:
                formats.append({
                    'url': video_url,
                    'format_id': format_id,
                    'quality': quality(format_id),
                })
        self._sort_formats(formats)
        return {
            'id': video_id,
            'title': metadata['titel'],
            'description': metadata['info'],
            'thumbnail': metadata.get('images', [{'url': None}])[-1]['url'],
            'upload_date': unified_strdate(metadata.get('gidsdatum')),
            'duration': parse_duration(metadata.get('tijdsduur')),
            'formats': formats,
        }

class TegenlichtVproIE(NPOIE):
    IE_NAME = 'tegenlicht.vpro.nl'
    _VALID_URL = r'https?://tegenlicht\.vpro\.nl/afleveringen/.*?'
    _TESTS = [
        {
            'url': 'http://tegenlicht.vpro.nl/afleveringen/2012-2013/de-toekomst-komt-uit-afrika.html',
            'md5': 'f8065e4e5a7824068ed3c7e783178f2c',
            'info_dict': {
                'id': 'VPWON_1169289',
                'ext': 'm4v',
                'title': 'Tegenlicht',
                'description': 'md5:d6476bceb17a8c103c76c3b708f05dd1',
                'upload_date': '20130225',
            },
        },
    ]
    def _real_extract(self, url):
        name = url_basename(url)
        webpage = self._download_webpage(url, name)
        urn = self._html_search_meta('mediaurn', webpage)
        info_page = self._download_json(
            'http://rs.vpro.nl/v2/api/media/%s.json' % urn, name)",[54]
"    elif isinstance(obj, list):
        return _isna_ndarraylike(np.asarray(obj, dtype=object))
    elif hasattr(obj, ""__array__""):
        return _isna_ndarraylike(np.asarray(obj))
    else:
        return obj is None

def _isna_old(obj):
    """"""
    Detect missing values, treating None, NaN, INF, -INF as null.
    Parameters
    ----------
    arr: ndarray or object value
    Returns
    -------
    boolean ndarray or boolean
    """"""
    if is_scalar(obj):
        return libmissing.checknull_old(obj)
    # hack (for now) because MI registers as ndarray
    elif isinstance(obj, ABCMultiIndex):
        raise NotImplementedError(""isna is not defined for MultiIndex"")
    elif isinstance(obj, type):
        return False
    elif isinstance(obj, (ABCSeries, np.ndarray, ABCIndexClass)):
        return _isna_ndarraylike_old(obj)
    elif isinstance(obj, ABCGeneric):
        return obj._constructor(obj._data.isna(func=_isna_old))
    elif isinstance(obj, list):
        return _isna_ndarraylike_old(np.asarray(obj, dtype=object))
    elif hasattr(obj, ""__array__""):
        return _isna_ndarraylike_old(np.asarray(obj))
    else:
        return obj is None

_isna = _isna_new

def _use_inf_as_na(key):
    """"""
    Option change callback for na/inf behaviour.
    Choose which replacement for numpy.isnan / -numpy.isfinite is used.
    Parameters
    ----------
    flag: bool
        True means treat None, NaN, INF, -INF as null (old way),
        False means None and NaN are null, but INF, -INF are not null
        (new way).
    Notes
    -----
    This approach to setting global module values is discussed and
    approved here:
    * http://stackoverflow.com/questions/4859217/
      programmatically-creating-variables-in-python/4859312#4859312
    """"""
    flag = get_option(key)
    if flag:
        globals()[""_isna""] = _isna_old
    else:
        globals()[""_isna""] = _isna_new

def _isna_ndarraylike(obj):
    is_extension = is_extension_array_dtype(obj)
    if not is_extension:
        # Avoid accessing `.values` on things like
        # PeriodIndex, which may be expensive.
        values = getattr(obj, ""values"", obj)
    else:
        values = obj
    dtype = values.dtype
    if is_extension:
        if isinstance(obj, (ABCIndexClass, ABCSeries)):
            values = obj._values
        else:
            values = obj
        result = values.isna()
    elif isinstance(obj, ABCDatetimeArray):
        return obj.isna()
    elif is_string_dtype(dtype):
        # Working around NumPy ticket 1542
        shape = values.shape
        if is_string_like_dtype(dtype):
            # object array of strings
            result = np.zeros(values.shape, dtype=bool)
        else:
            # object array of non-strings
            result = np.empty(shape, dtype=bool)
            vec = libmissing.isnaobj(values.ravel())
            result[...] = vec.reshape(shape)
    elif needs_i8_conversion(dtype):
        # this is the NaT pattern
        result = values.view(""i8"") == iNaT
    else:
        result = np.isnan(values)
    # box
    if isinstance(obj, ABCSeries):
        result = obj._constructor(result, index=obj.index, name=obj.name, copy=False)
    return result

def _isna_ndarraylike_old(obj):
    values = getattr(obj, ""values"", obj)
    dtype = values.dtype
    if is_string_dtype(dtype):
        # Working around NumPy ticket 1542
        shape = values.shape
        if is_string_like_dtype(dtype):
            result = np.zeros(values.shape, dtype=bool)
        else:",[27]
"class Task(object):
    def __init__(self, task_id, status, deps, resources=None, priority=0, family='', module=None,
                 params=None, tracking_url=None, status_message=None, retry_policy='notoptional'):
        self.id = task_id
        self.stakeholders = set()  # workers ids that are somehow related to this task (i.e. don't prune while any of these workers are still active)
        self.workers = set()  # workers ids that can perform task - task is 'BROKEN' if none of these workers are active
        if deps is None:
            self.deps = set()
        else:
            self.deps = set(deps)
        self.status = status  # PENDING, RUNNING, FAILED or DONE
        self.time = time.time()  # Timestamp when task was first added
        self.updated = self.time
        self.retry = None
        self.remove = None
        self.worker_running = None  # the worker id that is currently running the task or None
        self.time_running = None  # Timestamp when picked up by worker
        self.expl = None
        self.priority = priority
        self.resources = _get_default(resources, {})
        self.family = family
        self.module = module
        self.params = _get_default(params, {})
        self.retry_policy = retry_policy
        self.failures = Failures(self.retry_policy.disable_window)
        self.tracking_url = tracking_url
        self.status_message = status_message
        self.scheduler_disable_time = None
        self.runnable = False
        self.batchable = False
        self.batch_id = None
    def __repr__(self):
        return ""Task(%r)"" % vars(self)
    # TODO(2017-08-10) replace this function with direct calls to batchable
    # this only exists for backward compatibility
    def is_batchable(self):
        try:
            return self.batchable
        except AttributeError:
            return False
    def add_failure(self):
        self.failures.add_failure()
    def has_excessive_failures(self):
        if self.failures.first_failure_time is not None:
            if (time.time() >= self.failures.first_failure_time + self.retry_policy.disable_hard_timeout):
                return True
        logger.debug('%s task num failures is %s and limit is %s', self.id, self.failures.num_failures(), self.retry_policy.retry_count)
        if self.failures.num_failures() >= self.retry_policy.retry_count:
            logger.debug('%s task num failures limit(%s) is exceeded', self.id, self.retry_policy.retry_count)
            return True
        return False
    @property
    def pretty_id(self):
        param_str = ', '.join('{}={}'.format(key, value) for key, value in self.params.items())
        return '{}({})'.format(self.family, param_str)

class Worker(object):
    """"""
    Structure for tracking worker activity and keeping their references.
    """"""
    def __init__(self, worker_id, last_active=None):
        self.id = worker_id
        self.reference = None  # reference to the worker in the real world. (Currently a dict containing just the host)
        self.last_active = last_active or time.time()  # seconds since epoch
        self.last_get_work = None
        self.started = time.time()  # seconds since epoch
        self.tasks = set()  # task objects
        self.info = {}
        self.disabled = False
    def add_info(self, info):
        self.info.update(info)
    def update(self, worker_reference, get_work=False):
        if worker_reference:
            self.reference = worker_reference
        self.last_active = time.time()
        if get_work:
            self.last_get_work = time.time()
    def prune(self, config):
        # Delete workers that haven't said anything for a while (probably killed)
        if self.last_active + config.worker_disconnect_delay < time.time():
            return True
    def get_pending_tasks(self, state):
        """"""
        Get PENDING (and RUNNING) tasks for this worker.
        You have to pass in the state for optimization reasons.
        """"""
        if len(self.tasks) < state.num_pending_tasks():
            return six.moves.filter(lambda task: task.status in [PENDING, RUNNING],
                                    self.tasks)
        else:
            return state.get_pending_tasks()
    def is_trivial_worker(self, state):
        """"""
        If it's not an assistant having only tasks that are without
        requirements.
        We have to pass the state parameter for optimization reasons.
        """"""
        if self.assistant:
            return False
        return all(not task.resources for task in self.get_pending_tasks(state))
    @property
    def assistant(self):
        return self.info.get('assistant', False)
    def __str__(self):
        return self.id
",[106]
"# Autogenerated by boilerplate.py.  Do not edit as changes will be lost.
@_copy_docstring_and_deprecators(Axes.vlines)
def vlines(
        x, ymin, ymax, colors='k', linestyles='solid', label='', *,
        data=None, **kwargs):
    return gca().vlines(
        x, ymin, ymax, colors=colors, linestyles=linestyles,
        label=label, **({""data"": data} if data is not None else {}),
        **kwargs)

# Autogenerated by boilerplate.py.  Do not edit as changes will be lost.
@_copy_docstring_and_deprecators(Axes.xcorr)
def xcorr(
        x, y, normed=True, detrend=mlab.detrend_none, usevlines=True,
        maxlags=10, *, data=None, **kwargs):
    return gca().xcorr(
        x, y, normed=normed, detrend=detrend, usevlines=usevlines,
        maxlags=maxlags,
        **({""data"": data} if data is not None else {}), **kwargs)

# Autogenerated by boilerplate.py.  Do not edit as changes will be lost.
@_copy_docstring_and_deprecators(Axes._sci)
def sci(im):
    return gca()._sci(im)

# Autogenerated by boilerplate.py.  Do not edit as changes will be lost.
@_copy_docstring_and_deprecators(Axes.set_title)
def title(label, fontdict=None, loc=None, pad=None, *, y=None, **kwargs):
    return gca().set_title(
        label, fontdict=fontdict, loc=loc, pad=pad, y=y, **kwargs)

# Autogenerated by boilerplate.py.  Do not edit as changes will be lost.
@_copy_docstring_and_deprecators(Axes.set_xlabel)
def xlabel(xlabel, fontdict=None, labelpad=None, *, loc=None, **kwargs):
    return gca().set_xlabel(
        xlabel, fontdict=fontdict, labelpad=labelpad, loc=loc,
        **kwargs)

# Autogenerated by boilerplate.py.  Do not edit as changes will be lost.
@_copy_docstring_and_deprecators(Axes.set_ylabel)
def ylabel(ylabel, fontdict=None, labelpad=None, *, loc=None, **kwargs):
    return gca().set_ylabel(
        ylabel, fontdict=fontdict, labelpad=labelpad, loc=loc,
        **kwargs)

# Autogenerated by boilerplate.py.  Do not edit as changes will be lost.
@_copy_docstring_and_deprecators(Axes.set_xscale)
def xscale(value, **kwargs):
    return gca().set_xscale(value, **kwargs)

# Autogenerated by boilerplate.py.  Do not edit as changes will be lost.
@_copy_docstring_and_deprecators(Axes.set_yscale)
def yscale(value, **kwargs):
    return gca().set_yscale(value, **kwargs)

# Autogenerated by boilerplate.py.  Do not edit as changes will be lost.
def autumn():
    """"""
    Set the colormap to ""autumn"".
    This changes the default colormap as well as the colormap of the current
    image if there is one. See ``help(colormaps)`` for more information.
    """"""
    set_cmap(""autumn"")

# Autogenerated by boilerplate.py.  Do not edit as changes will be lost.
def bone():
    """"""
    Set the colormap to ""bone"".
    This changes the default colormap as well as the colormap of the current
    image if there is one. See ``help(colormaps)`` for more information.
    """"""
    set_cmap(""bone"")

# Autogenerated by boilerplate.py.  Do not edit as changes will be lost.
def cool():
    """"""
    Set the colormap to ""cool"".
    This changes the default colormap as well as the colormap of the current
    image if there is one. See ``help(colormaps)`` for more information.
    """"""
    set_cmap(""cool"")

# Autogenerated by boilerplate.py.  Do not edit as changes will be lost.
def copper():
    """"""
    Set the colormap to ""copper"".
    This changes the default colormap as well as the colormap of the current
    image if there is one. See ``help(colormaps)`` for more information.
    """"""
    set_cmap(""copper"")

# Autogenerated by boilerplate.py.  Do not edit as changes will be lost.
def flag():
    """"""
    Set the colormap to ""flag"".
    This changes the default colormap as well as the colormap of the current
    image if there is one. See ``help(colormaps)`` for more information.
    """"""
    set_cmap(""flag"")

# Autogenerated by boilerplate.py.  Do not edit as changes will be lost.
def gray():
    """"""
    Set the colormap to ""gray"".
    This changes the default colormap as well as the colormap of the current
    image if there is one. See ``help(colormaps)`` for more information.
    """"""",[4]
"                if verbose == 1:
                    progbar.update(batch_end)
            if len(outs) == 1:
                return outs[0]
            return outs
    def _test_loop(self, f, ins, batch_size=None, verbose=0, steps=None):
        """"""Abstract method to loop over some data in batches.
        # Arguments
            f: Keras function returning a list of tensors.
            ins: list of tensors to be fed to `f`.
            batch_size: integer batch size or `None`.
            verbose: verbosity mode.
            steps: Total number of steps (batches of samples)
                before declaring predictions finished.
                Ignored with the default value of `None`.
        # Returns
            Scalar loss (if the model has a single output and no metrics)
            or list of scalars (if the model has multiple outputs
            and/or metrics). The attribute `model.metrics_names` will give you
            the display labels for the scalar outputs.
        """"""
        if hasattr(self, 'metrics'):
            for m in self.metrics:
                if isinstance(m, Layer) and m.stateful:
                    m.reset_states()
            stateful_metric_indices = [
                i for i, name in enumerate(self.metrics_names)
                if str(name) in self.stateful_metric_names]
        else:
            stateful_metric_indices = []
        num_samples = self._check_num_samples(ins, batch_size,
                                              steps,
                                              'steps')
        outs = []
        if verbose == 1:
            if steps is not None:
                progbar = Progbar(target=steps)
            else:
                progbar = Progbar(target=num_samples)
        # To prevent a slowdown, we find beforehand the arrays that need conversion.
        feed = self._feed_inputs + self._feed_targets + self._feed_sample_weights
        indices_for_conversion_to_dense = []
        for i in range(len(feed)):
            if issparse(ins[i]) and not K.is_sparse(feed[i]):
                indices_for_conversion_to_dense.append(i)
        if steps is not None:
            for step in range(steps):
                batch_outs = f(ins)
                if isinstance(batch_outs, list):
                    if step == 0:
                        for _ in enumerate(batch_outs):
                            outs.append(0.)
                    for i, batch_out in enumerate(batch_outs):
                        if i in stateful_metric_indices:
                            outs[i] = float(batch_out)
                        else:
                            outs[i] += batch_out
                else:
                    if step == 0:
                        outs.append(0.)
                    outs[0] += batch_outs
                if verbose == 1:
                    progbar.update(step + 1)
            for i in range(len(outs)):
                if i not in stateful_metric_indices:
                    outs[i] /= steps
        else:
            batches = _make_batches(num_samples, batch_size)
            index_array = np.arange(num_samples)
            for batch_index, (batch_start, batch_end) in enumerate(batches):
                batch_ids = index_array[batch_start:batch_end]
                if isinstance(ins[-1], float):
                    # Do not slice the training phase flag.
                    ins_batch = _slice_arrays(ins[:-1], batch_ids) + [ins[-1]]
                else:
                    ins_batch = _slice_arrays(ins, batch_ids)
                for i in indices_for_conversion_to_dense:
                    ins_batch[i] = ins_batch[i].toarray()
                batch_outs = f(ins_batch)
                if isinstance(batch_outs, list):
                    if batch_index == 0:
                        for batch_out in enumerate(batch_outs):
                            outs.append(0.)
                    for i, batch_out in enumerate(batch_outs):
                        if i in stateful_metric_indices:
                            outs[i] = batch_out
                        else:
                            outs[i] += batch_out * len(batch_ids)
                else:
                    if batch_index == 0:
                        outs.append(0.)
                    outs[0] += batch_outs * len(batch_ids)
                if verbose == 1:
                    progbar.update(batch_end)
            for i in range(len(outs)):
                if i not in stateful_metric_indices:
                    outs[i] /= num_samples
        if len(outs) == 1:
            return outs[0]
        return outs
    def _standardize_user_data(self, x, y,
                               sample_weight=None, class_weight=None,
                               check_array_lengths=True, batch_size=None):
        if not hasattr(self, 'optimizer'):
            raise RuntimeError('You must compile a model before '
                               'training/testing. '
                               'Use `model.compile(optimizer, loss)`.')
        output_shapes = []
        for output_shape, loss_fn in zip(self._feed_output_shapes, self._feed_loss_fns):
            if loss_fn is losses.sparse_categorical_crossentropy:
                output_shapes.append(output_shape[:-1] + (1,))
            elif (not hasattr(loss_fn, '__name__') or
                  getattr(losses, loss_fn.__name__, None) is None):
                # If `loss_fn` is not a function (e.g. callable class)
                # or if it not in the `losses` module, then
                # it is a user-defined loss and we make no assumptions","[26, 27, 28]"
"        """"""
        def instantiate():
            return super(Register, cls).__call__(*args, **kwargs)
        h = cls.__instance_cache
        if h is None:  # disabled
            return instantiate()
        params = cls.get_params()
        param_values = cls.get_param_values(params, args, kwargs)
        k = (cls, tuple(param_values))
        try:
            hash(k)
        except TypeError:
            logger.debug(""Not all parameter values are hashable so instance isn't coming from the cache"")
            return instantiate()  # unhashable types in parameters
        if k not in h:
            h[k] = instantiate()
        return h[k]
    @classmethod
    def clear_instance_cache(cls):
        """"""
        Clear/Reset the instance cache.
        """"""
        cls.__instance_cache = {}
    @classmethod
    def disable_instance_cache(cls):
        """"""
        Disables the instance cache.
        """"""
        cls.__instance_cache = None
    @property
    def task_family(cls):
        """"""
        The task family for the given class.
        If ``cls.task_namespace is None`` then it's the name of the class.
        Otherwise, ``<task_namespace>.`` is prefixed to the class name.
        """"""
        if cls.task_namespace is None:
            return cls.__name__
        else:
            return ""%s.%s"" % (cls.task_namespace, cls.__name__)
    @classmethod
    def __get_reg(cls):
        """"""Return all of the registered classes.
        :return:  an ``collections.OrderedDict`` of task_family -> class
        """"""
        # We have to do this on-demand in case task names have changed later
        # We return this in a topologically sorted list of inheritance: this is useful in some cases (#822)
        reg = OrderedDict()
        for cls in cls._reg:
            if cls.run == NotImplemented:
                continue
            name = cls.task_family
            if name in reg and reg[name] != cls and \
                    reg[name] != cls.AMBIGUOUS_CLASS and \
                    not issubclass(cls, reg[name]):
                # Registering two different classes - this means we can't instantiate them by name
                # The only exception is if one class is a subclass of the other. In that case, we
                # instantiate the most-derived class (this fixes some issues with decorator wrappers).
                reg[name] = cls.AMBIGUOUS_CLASS
            else:
                reg[name] = cls
        return reg
    @classmethod
    def task_names(cls):
        """"""
        List of task names as strings
        """"""
        return sorted(cls.__get_reg().keys())
    @classmethod
    def tasks_str(cls):
        """"""
        Human-readable register contents dump.
        """"""
        return ','.join(cls.task_names())
    @classmethod
    def get_task_cls(cls, name):
        """"""
        Returns an unambiguous class or raises an exception.
        """"""
        task_cls = cls.__get_reg().get(name)
        if not task_cls:
            raise TaskClassException('Task %r not found. Candidates are: %s' % (name, cls.tasks_str()))
        if task_cls == cls.AMBIGUOUS_CLASS:
            raise TaskClassException('Task %r is ambiguous' % name)
        return task_cls
    @classmethod
    def get_all_params(cls):
        """"""
        Compiles and returns all parameters for all :py:class:`Task`.
        :return: a generator of tuples (TODO: we should make this more elegant)
        """"""
        for task_name, task_cls in six.iteritems(cls.__get_reg()):
            if task_cls == cls.AMBIGUOUS_CLASS:
                continue
            for param_name, param_obj in task_cls.get_params():
                yield task_name, (not task_cls.use_cmdline_section), param_name, param_obj

def load_task(module, task_name, params_str):
    """"""
    Imports task dynamically given a module and a task name.
    """"""
    if module is not None:
        __import__(module)
    task_cls = Register.get_task_cls(task_name)","[62, 63]"
"import sys
from subprocess import Popen, PIPE
import unittest
from scrapy.utils.test import get_testenv
class CmdlineTest(unittest.TestCase):
    def setUp(self):
        self.env = get_testenv()
        self.env['SCRAPY_SETTINGS_MODULE'] = 'tests.test_cmdline.settings'
    def _execute(self, *new_args, **kwargs):
        args = (sys.executable, '-m', 'scrapy.cmdline') + new_args
        proc = Popen(args, stdout=PIPE, stderr=PIPE, env=self.env, **kwargs)
        comm = proc.communicate()
        return comm[0].strip()
    def test_default_settings(self):
        self.assertEqual(self._execute('settings', '--get', 'TEST1'), \
                         'default')
    def test_override_settings_using_set_arg(self):
        self.assertEqual(self._execute('settings', '--get', 'TEST1', '-s', 'TEST1=override'), \
                         'override')
    def test_override_settings_using_envvar(self):
        self.env['SCRAPY_TEST1'] = 'override'
        self.assertEqual(self._execute('settings', '--get', 'TEST1'), \
                         'override')
","[15, 16]"
"            i8self = Int64Index._simple_new(self.asi8, name=self.name)
            i8other = Int64Index._simple_new(other.asi8, name=other.name)
            i8result = i8self._union(i8other, sort=sort)
            result = type(self)(i8result, dtype=self.dtype, freq=""infer"")
            return result
    # --------------------------------------------------------------------
    # Join Methods
    _join_precedence = 10
    _inner_indexer = _join_i8_wrapper(libjoin.inner_join_indexer)
    _outer_indexer = _join_i8_wrapper(libjoin.outer_join_indexer)
    _left_indexer = _join_i8_wrapper(libjoin.left_join_indexer)
    _left_indexer_unique = _join_i8_wrapper(
        libjoin.left_join_indexer_unique, with_indexers=False
    )
    def join(
        self, other, how: str = ""left"", level=None, return_indexers=False, sort=False
    ):
        """"""
        See Index.join
        """"""
        if self._is_convertible_to_index_for_join(other):
            try:
                other = type(self)(other)
            except (TypeError, ValueError):
                pass
        this, other = self._maybe_utc_convert(other)
        return Index.join(
            this,
            other,
            how=how,
            level=level,
            return_indexers=return_indexers,
            sort=sort,
        )
    def _maybe_utc_convert(self, other):
        this = self
        if not hasattr(self, ""tz""):
            return this, other
        if isinstance(other, type(self)):
            if self.tz is not None:
                if other.tz is None:
                    raise TypeError(""Cannot join tz-naive with tz-aware DatetimeIndex"")
            elif other.tz is not None:
                raise TypeError(""Cannot join tz-naive with tz-aware DatetimeIndex"")
            if not timezones.tz_compare(self.tz, other.tz):
                this = self.tz_convert(""UTC"")
                other = other.tz_convert(""UTC"")
        return this, other
    @classmethod
    def _is_convertible_to_index_for_join(cls, other: Index) -> bool:
        """"""
        return a boolean whether I can attempt conversion to a
        DatetimeIndex/TimedeltaIndex
        """"""
        if isinstance(other, cls):
            return False
        elif len(other) > 0 and other.inferred_type not in (
            ""floating"",
            ""mixed-integer"",
            ""integer"",
            ""integer-na"",
            ""mixed-integer-float"",
            ""mixed"",
        ):
            return True
        return False
    # --------------------------------------------------------------------
    # List-Like Methods
    def insert(self, loc, item):
        """"""
        Make new Index inserting new item at location
        Parameters
        ----------
        loc : int
        item : object
            if not either a Python datetime or a numpy integer-like, returned
            Index dtype will be object rather than datetime.
        Returns
        -------
        new_index : Index
        """"""
        item = self._data._validate_insert_value(item)
        freq = None
        if isinstance(item, self._data._scalar_type) or item is NaT:
            self._data._check_compatible_with(item, setitem=True)
            # check freq can be preserved on edge cases
            if self.size and self.freq is not None:
                if item is NaT:
                    pass
                elif (loc == 0 or loc == -len(self)) and item + self.freq == self[0]:
                    freq = self.freq
                elif (loc == len(self)) and item - self.freq == self[-1]:
                    freq = self.freq
            elif self.freq is not None:
                # Adding a single item to an empty index may preserve freq
                if self.freq.is_on_offset(item):
                    freq = self.freq
            item = item.asm8
        try:
            new_i8s = np.concatenate(
                (self[:loc].asi8, [item.view(np.int64)], self[loc:].asi8)
            )
            arr = type(self._data)._simple_new(new_i8s, dtype=self.dtype, freq=freq)
            return type(self)._simple_new(arr, name=self.name)
        except (AttributeError, TypeError) as err:
            # fall back to object index
            if isinstance(item, str):
                return self.astype(object).insert(loc, item)
            raise TypeError(
                f""cannot insert {type(self).__name__} with incompatible label""","[96, 97, 98, 100, 107, 111, 113, 114, 115, 116, 117, 118, 119, 122, 123, 124, 125]"
"        if isinstance(key, slice):
            return
        if com.is_bool_indexer(key):
            return
        if not is_list_like_indexer(key):
            self._convert_scalar_indexer(key, axis)
    def _is_scalar_access(self, key: Tuple):
        # this is a shortcut accessor to both .loc and .iloc
        # that provide the equivalent access of .at and .iat
        # a) avoid getting things via sections and (to minimize dtype changes)
        # b) provide a performant path
        if len(key) != self.ndim:
            return False
        for i, k in enumerate(key):
            if not is_scalar(k):
                return False
            ax = self.obj.axes[i]
            if isinstance(ax, MultiIndex):
                return False
            if not ax.is_unique:
                return False
        return True
    def _getitem_scalar(self, key):
        # a fast-path to scalar access
        # if not, raise
        values = self.obj._get_value(*key)
        return values
    def _get_partial_string_timestamp_match_key(self, key, labels):
        """"""Translate any partial string timestamp matches in key, returning the
        new key (GH 10331)""""""
        if isinstance(labels, MultiIndex):
            if isinstance(key, str) and labels.levels[0].is_all_dates:
                # Convert key '2016-01-01' to
                # ('2016-01-01'[, slice(None, None, None)]+)
                key = tuple([key] + [slice(None)] * (len(labels.levels) - 1))
            if isinstance(key, tuple):
                # Convert (..., '2016-01-01', ...) in tuple to
                # (..., slice('2016-01-01', '2016-01-01', None), ...)
                new_key = []
                for i, component in enumerate(key):
                    if isinstance(component, str) and labels.levels[i].is_all_dates:
                        new_key.append(slice(component, component, None))
                    else:
                        new_key.append(component)
                key = tuple(new_key)
        return key
    def _getitem_axis(self, key, axis: int):
        key = item_from_zerodim(key)
        if is_iterator(key):
            key = list(key)
        labels = self.obj._get_axis(axis)
        key = self._get_partial_string_timestamp_match_key(key, labels)
        if isinstance(key, slice):
            self._validate_key(key, axis)
            return self._get_slice_axis(key, axis=axis)
        elif com.is_bool_indexer(key):
            return self._getbool_axis(key, axis=axis)
        elif is_list_like_indexer(key):
            # convert various list-like indexers
            # to a list of keys
            # we will use the *values* of the object
            # and NOT the index if its a PandasObject
            if isinstance(labels, MultiIndex):
                if isinstance(key, (ABCSeries, np.ndarray)) and key.ndim <= 1:
                    # Series, or 0,1 ndim ndarray
                    # GH 14730
                    key = list(key)
                elif isinstance(key, ABCDataFrame):
                    # GH 15438
                    raise NotImplementedError(
                        ""Indexing a MultiIndex with a ""
                        ""DataFrame key is not ""
                        ""implemented""
                    )
                elif hasattr(key, ""ndim"") and key.ndim > 1:
                    raise NotImplementedError(
                        ""Indexing a MultiIndex with a ""
                        ""multidimensional key is not ""
                        ""implemented""
                    )
                if (
                    not isinstance(key, tuple)
                    and len(key)
                    and not isinstance(key[0], tuple)
                ):
                    key = tuple([key])
            # an iterable multi-selection
            if not (isinstance(key, tuple) and isinstance(labels, MultiIndex)):
                if hasattr(key, ""ndim"") and key.ndim > 1:
                    raise ValueError(""Cannot index with multidimensional key"")
                return self._getitem_iterable(key, axis=axis)
            # nested tuple slicing
            if is_nested_tuple(key, labels):
                locs = labels.get_locs(key)
                indexer = [slice(None)] * self.ndim
                indexer[axis] = locs
                return self.obj.iloc[tuple(indexer)]
        # fall thru to straight lookup
        self._validate_key(key, axis)
        return self._get_label(key, axis=axis)

class _iLocIndexer(_LocationIndexer):
    """"""","[41, 51]"
"        if len(input_shape) != 4:
            raise ValueError('Inputs should have rank ' +
                             str(4) +
                             '; Received input shape:', str(input_shape))
        if self.data_format == 'channels_first':
            channel_axis = 1
        else:
            channel_axis = -1
        if input_shape[channel_axis] is None:
            raise ValueError('The channel dimension of the inputs '
                             'should be defined. Found `None`.')
        input_dim = input_shape[channel_axis]
        kernel_shape = self.kernel_size + (self.filters, input_dim)
        self.kernel = self.add_weight(shape=kernel_shape,
                                      initializer=self.kernel_initializer,
                                      name='kernel',
                                      regularizer=self.kernel_regularizer,
                                      constraint=self.kernel_constraint)
        if self.use_bias:
            self.bias = self.add_weight(shape=(self.filters,),
                                        initializer=self.bias_initializer,
                                        name='bias',
                                        regularizer=self.bias_regularizer,
                                        constraint=self.bias_constraint)
        else:
            self.bias = None
        # Set input spec.
        self.input_spec = InputSpec(ndim=4, axes={channel_axis: input_dim})
        self.built = True
    def call(self, inputs):
        input_shape = K.shape(inputs)
        batch_size = input_shape[0]
        if self.data_format == 'channels_first':
            h_axis, w_axis = 2, 3
        else:
            h_axis, w_axis = 1, 2
        height, width = input_shape[h_axis], input_shape[w_axis]
        kernel_h, kernel_w = self.kernel_size
        stride_h, stride_w = self.strides
        if self.output_padding is None:
            out_pad_h = out_pad_w = None
        else:
            out_pad_h, out_pad_w = self.output_padding
        # Infer the dynamic output shape:
        out_height = conv_utils.deconv_length(height,
                                              stride_h, kernel_h,
                                              self.padding,
                                              out_pad_h)
        out_width = conv_utils.deconv_length(width,
                                             stride_w, kernel_w,
                                             self.padding,
                                             out_pad_w)
        if self.data_format == 'channels_first':
            output_shape = (batch_size, self.filters, out_height, out_width)
        else:
            output_shape = (batch_size, out_height, out_width, self.filters)
        outputs = K.conv2d_transpose(
            inputs,
            self.kernel,
            output_shape,
            self.strides,
            padding=self.padding,
            data_format=self.data_format)
        if self.use_bias:
            outputs = K.bias_add(
                outputs,
                self.bias,
                data_format=self.data_format)
        if self.activation is not None:
            return self.activation(outputs)
        return outputs
    def compute_output_shape(self, input_shape):
        output_shape = list(input_shape)
        if self.data_format == 'channels_first':
            c_axis, h_axis, w_axis = 1, 2, 3
        else:
            c_axis, h_axis, w_axis = 3, 1, 2
        kernel_h, kernel_w = self.kernel_size
        stride_h, stride_w = self.strides
        if self.output_padding is None:
            out_pad_h = out_pad_w = None
        else:
            out_pad_h, out_pad_w = self.output_padding
        output_shape[c_axis] = self.filters
        output_shape[h_axis] = conv_utils.deconv_length(output_shape[h_axis],
                                                        stride_h,
                                                        kernel_h,
                                                        self.padding,
                                                        out_pad_h)
        output_shape[w_axis] = conv_utils.deconv_length(output_shape[w_axis],
                                                        stride_w,
                                                        kernel_w,
                                                        self.padding,
                                                        out_pad_w)
        return tuple(output_shape)
    def get_config(self):
        config = super(Conv2DTranspose, self).get_config()
        config.pop('dilation_rate')
        config['output_padding'] = self.output_padding
        return config

class Conv3DTranspose(Conv3D):
    """"""Transposed convolution layer (sometimes called Deconvolution).
    The need for transposed convolutions generally arises
    from the desire to use a transformation going in the opposite direction
    of a normal convolution, i.e., from something that has the shape of the
    output of some convolution to something that has the shape of its input
    while maintaining a connectivity pattern that is compatible with
    said convolution.
    When using this layer as the first layer in a model,
    provide the keyword argument `input_shape`
    (tuple of integers, does not include the sample axis),
    e.g. `input_shape=(128, 128, 128, 3)` for a 128x128x128 volume with 3 channels","[51, 55, 67, 98, 103, 108]"
"        target = np.asarray(target)
        left_distances = abs(self.values[left_indexer] - target)
        right_distances = abs(self.values[right_indexer] - target)
        op = operator.lt if self.is_monotonic_increasing else operator.le
        indexer = np.where(
            op(left_distances, right_distances) | (right_indexer == -1),
            left_indexer,
            right_indexer,
        )
        if tolerance is not None:
            indexer = self._filter_indexer_tolerance(target, indexer, tolerance)
        return indexer
    def _filter_indexer_tolerance(self, target, indexer, tolerance):
        distance = abs(self.values[indexer] - target)
        indexer = np.where(distance <= tolerance, indexer, -1)
        return indexer
    # --------------------------------------------------------------------
    # Indexer Conversion Methods
    _index_shared_docs[
        ""_convert_scalar_indexer""
    ] = """"""
        Convert a scalar indexer.
        Parameters
        ----------
        key : label of the slice bound
        kind : {'ix', 'loc', 'getitem', 'iloc'} or None
    """"""
    @Appender(_index_shared_docs[""_convert_scalar_indexer""])
    def _convert_scalar_indexer(self, key, kind=None):
        assert kind in [""ix"", ""loc"", ""getitem"", ""iloc"", None]
        if kind == ""iloc"":
            return self._validate_indexer(""positional"", key, kind)
        if len(self) and not isinstance(self, ABCMultiIndex):
            # we can raise here if we are definitive that this
            # is positional indexing (eg. .ix on with a float)
            # or label indexing if we are using a type able
            # to be represented in the index
            if kind in [""getitem"", ""ix""] and is_float(key):
                if not self.is_floating():
                    return self._invalid_indexer(""label"", key)
            elif kind in [""loc""] and is_float(key):
                # we want to raise KeyError on string/mixed here
                # technically we *could* raise a TypeError
                # on anything but mixed though
                if self.inferred_type not in [
                    ""floating"",
                    ""mixed-integer-float"",
                    ""integer-na"",
                    ""string"",
                    ""unicode"",
                    ""mixed"",
                ]:
                    self._invalid_indexer(""label"", key)
            elif kind in [""loc""] and is_integer(key):
                if not self.holds_integer():
                    self._invalid_indexer(""label"", key)
        return key
    _index_shared_docs[
        ""_convert_slice_indexer""
    ] = """"""
        Convert a slice indexer.
        By definition, these are labels unless 'iloc' is passed in.
        Floats are not allowed as the start, step, or stop of the slice.
        Parameters
        ----------
        key : label of the slice bound
        kind : {'ix', 'loc', 'getitem', 'iloc'} or None
    """"""
    @Appender(_index_shared_docs[""_convert_slice_indexer""])
    def _convert_slice_indexer(self, key: slice, kind=None):
        assert kind in [""ix"", ""loc"", ""getitem"", ""iloc"", None]
        # validate iloc
        if kind == ""iloc"":
            return slice(
                self._validate_indexer(""slice"", key.start, kind),
                self._validate_indexer(""slice"", key.stop, kind),
                self._validate_indexer(""slice"", key.step, kind),
            )
        # potentially cast the bounds to integers
        start, stop, step = key.start, key.stop, key.step
        # figure out if this is a positional indexer
        def is_int(v):
            return v is None or is_integer(v)
        is_null_slicer = start is None and stop is None
        is_index_slice = is_int(start) and is_int(stop)
        is_positional = is_index_slice and not self.is_integer()
        if kind == ""getitem"":
            """"""
            called from the getitem slicers, validate that we are in fact
            integers
            """"""
            if self.is_integer() or is_index_slice:
                return slice(
                    self._validate_indexer(""slice"", key.start, kind),
                    self._validate_indexer(""slice"", key.stop, kind),
                    self._validate_indexer(""slice"", key.step, kind),
                )
        # convert the slice to an indexer here
        # if we are mixed and have integers
        try:
            if is_positional and self.is_mixed():",[108]
"    def concat_same_type(self, to_concat, placement=None):
        """"""
        Concatenate list of single blocks of the same type.
        """"""
        values = self._concatenator(
            [blk.values for blk in to_concat], axis=self.ndim - 1
        )
        return self.make_block_same_class(
            values, placement=placement or slice(0, len(values), 1)
        )
    def iget(self, i):
        return self.values[i]
    def set(self, locs, values):
        """"""
        Modify Block in-place with new item value
        Returns
        -------
        None
        """"""
        self.values[locs] = values
    def delete(self, loc) -> None:
        """"""
        Delete given loc(-s) from block in-place.
        """"""
        self.values = np.delete(self.values, loc, 0)
        self.mgr_locs = self.mgr_locs.delete(loc)
    def apply(self, func, **kwargs) -> List[""Block""]:
        """"""
        apply the function to my values; return a block if we are not
        one
        """"""
        with np.errstate(all=""ignore""):
            result = func(self.values, **kwargs)
        return self._split_op_result(result)
    def _split_op_result(self, result) -> List[""Block""]:
        # See also: split_and_operate
        if is_extension_array_dtype(result) and result.ndim > 1:
            # if we get a 2D ExtensionArray, we need to split it into 1D pieces
            nbs = []
            for i, loc in enumerate(self.mgr_locs):
                vals = result[i]
                nv = _block_shape(vals, ndim=self.ndim)
                block = self.make_block(values=nv, placement=[loc])
                nbs.append(block)
            return nbs
        if not isinstance(result, Block):
            result = self.make_block(values=_block_shape(result, ndim=self.ndim))
        return [result]
    def fillna(
        self, value, limit=None, inplace: bool = False, downcast=None
    ) -> List[""Block""]:
        """"""
        fillna on the block with the value. If we fail, then convert to
        ObjectBlock and try again
        """"""
        inplace = validate_bool_kwarg(inplace, ""inplace"")
        mask = isna(self.values)
        if limit is not None:
            limit = libalgos._validate_limit(None, limit=limit)
            mask[mask.cumsum(self.ndim - 1) > limit] = False
        if not self._can_hold_na:
            if inplace:
                return [self]
            else:
                return [self.copy()]
        if self._can_hold_element(value):
            # equivalent: _try_coerce_args(value) would not raise
            blocks = self.putmask(mask, value, inplace=inplace)
            return self._maybe_downcast(blocks, downcast)
        # we can't process the value, but nothing to do
        if not mask.any():
            return [self] if inplace else [self.copy()]
        # operate column-by-column
        def f(mask, val, idx):
            block = self.coerce_to_target_dtype(value)
            # slice out our block
            if idx is not None:
                # i.e. self.ndim == 2
                block = block.getitem_block(slice(idx, idx + 1))
            return block.fillna(value, limit=limit, inplace=inplace, downcast=None)
        return self.split_and_operate(None, f, inplace)
    def split_and_operate(self, mask, f, inplace: bool) -> List[""Block""]:
        """"""
        split the block per-column, and apply the callable f
        per-column, return a new block for each. Handle
        masking which will not change a block unless needed.
        Parameters
        ----------
        mask : 2-d boolean mask
        f : callable accepting (1d-mask, 1d values, indexer)
        inplace : boolean
        Returns
        -------
        list of blocks
        """"""
        if mask is None:
            mask = np.broadcast_to(True, shape=self.shape)
        new_values = self.values
        def make_a_block(nv, ref_loc):
            if isinstance(nv, list):
                assert len(nv) == 1, nv
                assert isinstance(nv[0], Block)
                block = nv[0]
            else:","[17, 19, 20, 21]"
"                    # Special use case for galaxy.yml.j2 in our own default collection skeleton. We build the options
                    # dynamically which requires special options to be set.
                    # The templated data's keys must match the key name but the inject data contains collection_name
                    # instead of name. We just make a copy and change the key back to name for this file.
                    template_data = inject_data.copy()
                    template_data['name'] = template_data.pop('collection_name')
                    meta_value = GalaxyCLI._get_skeleton_galaxy_yml(os.path.join(root, rel_root, f), template_data)
                    b_dest_file = to_bytes(os.path.join(obj_path, rel_root, filename), errors='surrogate_or_strict')
                    with open(b_dest_file, 'wb') as galaxy_obj:
                        galaxy_obj.write(to_bytes(meta_value, errors='surrogate_or_strict'))
                elif ext == "".j2"" and not in_templates_dir:
                    src_template = os.path.join(rel_root, f)
                    dest_file = os.path.join(obj_path, rel_root, filename)
                    template_env.get_template(src_template).stream(inject_data).dump(dest_file, encoding='utf-8')
                else:
                    f_rel_path = os.path.relpath(os.path.join(root, f), obj_skeleton)
                    shutil.copyfile(os.path.join(root, f), os.path.join(obj_path, f_rel_path))
            for d in dirs:
                b_dir_path = to_bytes(os.path.join(obj_path, rel_root, d), errors='surrogate_or_strict')
                if not os.path.exists(b_dir_path):
                    os.makedirs(b_dir_path)
        display.display(""- %s was created successfully"" % obj_name)
    def execute_info(self):
        """"""
        prints out detailed information about an installed role as well as info available from the galaxy API.
        """"""
        roles_path = context.CLIARGS['roles_path']
        data = ''
        for role in context.CLIARGS['args']:
            role_info = {'path': roles_path}
            gr = GalaxyRole(self.galaxy, role)
            install_info = gr.install_info
            if install_info:
                if 'version' in install_info:
                    install_info['installed_version'] = install_info['version']
                    del install_info['version']
                role_info.update(install_info)
            remote_data = False
            if not context.CLIARGS['offline']:
                remote_data = self.api.lookup_role_by_name(role, False)
            if remote_data:
                role_info.update(remote_data)
            if gr.metadata:
                role_info.update(gr.metadata)
            req = RoleRequirement()
            role_spec = req.role_yaml_parse({'role': role})
            if role_spec:
                role_info.update(role_spec)
            data = self._display_role_info(role_info)
            # FIXME: This is broken in both 1.9 and 2.0 as
            # _display_role_info() always returns something
            if not data:
                data = u""\n- the role %s was not found"" % role
        self.pager(data)
    def execute_install(self):
        """"""
        Install one or more roles(``ansible-galaxy role install``), or one or more collections(``ansible-galaxy collection install``).
        You can pass in a list (roles or collections) or use the file
        option listed below (these are mutually exclusive). If you pass in a list, it
        can be a name (which will be downloaded via the galaxy API and github), or it can be a local tar archive file.
        """"""
        if context.CLIARGS['type'] == 'collection':
            collections = context.CLIARGS['args']
            force = context.CLIARGS['force']
            output_path = context.CLIARGS['collections_path']
            # TODO: use a list of server that have been configured in ~/.ansible_galaxy
            servers = [context.CLIARGS['api_server']]
            ignore_certs = context.CLIARGS['ignore_certs']
            ignore_errors = context.CLIARGS['ignore_errors']
            requirements_file = context.CLIARGS['requirements']
            no_deps = context.CLIARGS['no_deps']
            force_deps = context.CLIARGS['force_with_deps']
            if collections and requirements_file:
                raise AnsibleError(""The positional collection_name arg and --requirements-file are mutually exclusive."")
            elif not collections and not requirements_file:
                raise AnsibleError(""You must specify a collection name or a requirements file."")
            if requirements_file:
                requirements_file = GalaxyCLI._resolve_path(requirements_file)
                collection_requirements = parse_collections_requirements_file(requirements_file)
            else:
                collection_requirements = []
                for collection_input in collections:
                    name, dummy, requirement = collection_input.partition(':')
                    collection_requirements.append((name, requirement or '*', None))
            output_path = GalaxyCLI._resolve_path(output_path)
            collections_path = C.COLLECTIONS_PATHS
            if len([p for p in collections_path if p.startswith(output_path)]) == 0:
                display.warning(""The specified collections path '%s' is not part of the configured Ansible ""
                                ""collections paths '%s'. The installed collection won't be picked up in an Ansible ""
                                ""run."" % (to_text(output_path), to_text("":"".join(collections_path))))
            if os.path.split(output_path)[1] != 'ansible_collections':
                output_path = os.path.join(output_path, 'ansible_collections')
            b_output_path = to_bytes(output_path, errors='surrogate_or_strict')
            if not os.path.exists(b_output_path):
                os.makedirs(b_output_path)
            install_collections(collection_requirements, output_path, servers, (not ignore_certs), ignore_errors,
                                no_deps, force, force_deps)
            return 0
        role_file = context.CLIARGS['role_file']
        if not context.CLIARGS['args'] and role_file is None:
            # the user needs to specify one of either --role-file or specify a single user/role name",[25]
"        return default
    return d.get(key_or_keys, default)

def try_get(src, getter, expected_type=None):
    try:
        v = getter(src)
    except (AttributeError, KeyError, TypeError, IndexError):
        pass
    else:
        if expected_type is None or isinstance(v, expected_type):
            return v

def encode_compat_str(string, encoding=preferredencoding(), errors='strict'):
    return string if isinstance(string, compat_str) else compat_str(string, encoding, errors)

US_RATINGS = {
    'G': 0,
    'PG': 10,
    'PG-13': 13,
    'R': 16,
    'NC': 18,
}

TV_PARENTAL_GUIDELINES = {
    'TV-Y': 0,
    'TV-Y7': 7,
    'TV-G': 0,
    'TV-PG': 0,
    'TV-14': 14,
    'TV-MA': 17,
}

def parse_age_limit(s):
    if type(s) == int:
        return s if 0 <= s <= 21 else None
    if not isinstance(s, compat_basestring):
        return None
    m = re.match(r'^(?P<age>\d{1,2})\+?$', s)
    if m:
        return int(m.group('age'))
    if s in US_RATINGS:
        return US_RATINGS[s]
    return TV_PARENTAL_GUIDELINES.get(s)

def strip_jsonp(code):
    return re.sub(
        r'(?s)^[a-zA-Z0-9_.$]+\s*\(\s*(.*)\);?\s*?(?://[^\n]*)*$', r'\1', code)

def js_to_json(code):
    def fix_kv(m):
        v = m.group(0)
        if v in ('true', 'false', 'null'):
            return v
        elif v.startswith('/*') or v == ',':
            return """"
        if v[0] in (""'"", '""'):
            v = re.sub(r'(?s)\\.|""', lambda m: {
                '""': '\\""',
                ""\\'"": ""'"",
                '\\\n': '',
                '\\x': '\\u00',
            }.get(m.group(0), m.group(0)), v[1:-1])
        INTEGER_TABLE = (
            (r'^0[xX][0-9a-fA-F]+', 16),
            (r'^0+[0-7]+', 8),
        )
        for regex, base in INTEGER_TABLE:
            im = re.match(regex, v)
            if im:
                i = int(im.group(0), base)
                return '""%d"":' % i if v.endswith(':') else '%d' % i
        return '""%s""' % v
    return re.sub(r'''(?sx)
        ""(?:[^""\\]*(?:\\\\|\\['""nurtbfx/\n]))*[^""\\]*""|
        '(?:[^'\\]*(?:\\\\|\\['""nurtbfx/\n]))*[^'\\]*'|
        /\*.*?\*/|,(?=\s*[\]}])|
        [a-zA-Z_][.a-zA-Z_0-9]*|
        \b(?:0[xX][0-9a-fA-F]+|0+[0-7]+)(?:\s*:)?|
        [0-9]+(?=\s*:)
        ''', fix_kv, code)

def qualities(quality_ids):
    """""" Get a numeric quality value out of a list of possible values """"""
    def q(qid):
        try:
            return quality_ids.index(qid)
        except ValueError:
            return -1
    return q

DEFAULT_OUTTMPL = '%(title)s-%(id)s.%(ext)s'

def limit_length(s, length):
    """""" Add ellipses to overly long strings """"""
    if s is None:
        return None
    ELLIPSES = '...'
    if len(s) > length:
        return s[:length - len(ELLIPSES)] + ELLIPSES
    return s

def version_tuple(v):
    return tuple(int(e) for e in re.split(r'[-.]', v))

def is_outdated_version(version, limit, assume_new=True):
    if not version:
        return not assume_new
    try:
        return version_tuple(version) < version_tuple(limit)
    except ValueError:","[72, 73, 79]"
"    def __iter__(self):
        return iter(self.f)
    def write(self, *args):
        return self.f.write(*args)
    def read(self, *args):
        return self.f.read(*args)

def shell_quote(args):
    quoted_args = []
    encoding = sys.getfilesystemencoding()
    if encoding is None:
        encoding = 'utf-8'
    for a in args:
        if isinstance(a, bytes):
            # We may get a filename encoded with 'encodeFilename'
            a = a.decode(encoding)
        quoted_args.append(pipes.quote(a))
    return u' '.join(quoted_args)

def takewhile_inclusive(pred, seq):
    """""" Like itertools.takewhile, but include the latest evaluated element
        (the first element so that Not pred(e)) """"""
    for e in seq:
        yield e
        if not pred(e):
            return

def smuggle_url(url, data):
    """""" Pass additional data in a URL for internal use. """"""
    sdata = compat_urllib_parse.urlencode(
        {u'__youtubedl_smuggle': json.dumps(data)})
    return url + u'#' + sdata

def unsmuggle_url(smug_url):
    if not '#__youtubedl_smuggle' in smug_url:
        return smug_url, None
    url, _, sdata = smug_url.rpartition(u'#')
    jsond = compat_parse_qs(sdata)[u'__youtubedl_smuggle'][0]
    data = json.loads(jsond)
    return url, data

def format_bytes(bytes):
    if bytes is None:
        return u'N/A'
    if type(bytes) is str:
        bytes = float(bytes)
    if bytes == 0.0:
        exponent = 0
    else:
        exponent = int(math.log(bytes, 1024.0))
    suffix = [u'B', u'KiB', u'MiB', u'GiB', u'TiB', u'PiB', u'EiB', u'ZiB', u'YiB'][exponent]
    converted = float(bytes) / float(1024 ** exponent)
    return u'%.2f%s' % (converted, suffix)

def str_to_int(int_str):
    int_str = re.sub(r'[,\.]', u'', int_str)
    return int(int_str)

def get_term_width():
    columns = os.environ.get('COLUMNS', None)
    if columns:
        return int(columns)
    try:
        sp = subprocess.Popen(
            ['stty', 'size'],
            stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        out, err = sp.communicate()
        return int(out.split()[1])
    except:
        pass
    return None

def month_by_name(name):
    """""" Return the number of a month by (locale-independently) English name """"""
    ENGLISH_NAMES = [
        u'January', u'February', u'March', u'April', u'May', u'June',
        u'July', u'August', u'September', u'October', u'November', u'December']
    try:
        return ENGLISH_NAMES.index(name) + 1
    except ValueError:
        return None

def fix_xml_all_ampersand(xml_str):
    """"""Replace all the '&' by '&amp;' in XML""""""
    return xml_str.replace(u'&', u'&amp;')

def setproctitle(title):
    assert isinstance(title, type(u''))
    try:
        libc = ctypes.cdll.LoadLibrary(""libc.so.6"")
    except OSError:
        return
    title = title
    buf = ctypes.create_string_buffer(len(title) + 1)
    buf.value = title.encode('utf-8')
    try:
        libc.prctl(15, ctypes.byref(buf), 0, 0, 0)
    except AttributeError:
        return  # Strange libc, just skip this

def remove_start(s, start):
    if s.startswith(start):
        return s[len(start):]
    return s

def url_basename(url):
    m = re.match(r'(?:https?:|)//[^/]+/(?:[^/?#]+/)?([^/?#]+)/?(?:[?#]|$)', url)
    if not m:
        return u''",[123]
"        """"""
        if self.mode not in ['auto', 'min', 'max']:
            warnings.warn('Learning Rate Plateau Reducing mode %s is unknown, '
                          'fallback to auto mode.' % (self.mode),
                          RuntimeWarning)
            self.mode = 'auto'
        if (self.mode == 'min' or
           (self.mode == 'auto' and 'acc' not in self.monitor)):
            self.monitor_op = lambda a, b: np.less(a, b - self.min_delta)
            self.best = np.Inf
        else:
            self.monitor_op = lambda a, b: np.greater(a, b + self.min_delta)
            self.best = -np.Inf
        self.cooldown_counter = 0
        self.wait = 0
    def on_train_begin(self, logs=None):
        self._reset()
    def on_epoch_end(self, epoch, logs=None):
        logs = logs or {}
        logs['lr'] = K.get_value(self.model.optimizer.lr)
        current = logs.get(self.monitor)
        if current is None:
            warnings.warn(
                'Reduce LR on plateau conditioned on metric `%s` '
                'which is not available. Available metrics are: %s' %
                (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning
            )
        else:
            if self.in_cooldown():
                self.cooldown_counter -= 1
                self.wait = 0
            if self.monitor_op(current, self.best):
                self.best = current
                self.wait = 0
            elif not self.in_cooldown():
                self.wait += 1
                if self.wait >= self.patience:
                    old_lr = float(K.get_value(self.model.optimizer.lr))
                    if old_lr > self.min_lr:
                        new_lr = old_lr * self.factor
                        new_lr = max(new_lr, self.min_lr)
                        K.set_value(self.model.optimizer.lr, new_lr)
                        if self.verbose > 0:
                            print('\nEpoch %05d: ReduceLROnPlateau reducing learning '
                                  'rate to %s.' % (epoch + 1, new_lr))
                        self.cooldown_counter = self.cooldown
                        self.wait = 0
    def in_cooldown(self):
        return self.cooldown_counter > 0

class CSVLogger(Callback):
    """"""Callback that streams epoch results to a csv file.
    Supports all values that can be represented as a string,
    including 1D iterables such as np.ndarray.
    # Example
    ```python
    csv_logger = CSVLogger('training.log')
    model.fit(X_train, Y_train, callbacks=[csv_logger])
    ```
    # Arguments
        filename: filename of the csv file, e.g. 'run/log.csv'.
        separator: string used to separate elements in the csv file.
        append: True: append if file exists (useful for continuing
            training). False: overwrite existing file,
    """"""
    def __init__(self, filename, separator=',', append=False):
        self.sep = separator
        self.filename = filename
        self.append = append
        self.writer = None
        self.keys = None
        self.append_header = True
        self.file_flags = 'b' if six.PY2 and os.name == 'nt' else ''
        super(CSVLogger, self).__init__()
    def on_train_begin(self, logs=None):
        if self.append:
            if os.path.exists(self.filename):
                with open(self.filename, 'r' + self.file_flags) as f:
                    self.append_header = not bool(len(f.readline()))
            self.csv_file = open(self.filename, 'a' + self.file_flags)
        else:
            self.csv_file = open(self.filename, 'w' + self.file_flags)
    def on_epoch_end(self, epoch, logs=None):
        logs = logs or {}
        def handle_value(k):
            is_zero_dim_ndarray = isinstance(k, np.ndarray) and k.ndim == 0
            if isinstance(k, six.string_types):
                return k
            elif isinstance(k, Iterable) and not is_zero_dim_ndarray:
                return '""[%s]""' % (', '.join(map(str, k)))
            else:
                return k
        if self.keys is None:
            self.keys = sorted(logs.keys())
        if self.model.stop_training:
            # We set NA so that csv parsers do not fail for this last epoch.
            logs = dict([(k, logs[k]) if k in logs else (k, 'NA') for k in self.keys])
        if not self.writer:
            class CustomDialect(csv.excel):
                delimiter = self.sep
            self.writer = csv.DictWriter(self.csv_file,
                                         fieldnames=['epoch'] + self.keys, dialect=CustomDialect)
            if self.append_header:
                self.writer.writeheader()
        row_dict = OrderedDict({'epoch': epoch})
        row_dict.update((key, handle_value(logs[key])) for key in self.keys)
        self.writer.writerow(row_dict)
        self.csv_file.flush()","[83, 91, 93, 117, 119]"
"        else:
            return True
        # The loop was broken early, it does not meet all the requirements
        return False
    @staticmethod
    def from_tar(b_path, force, parent=None):
        if not tarfile.is_tarfile(b_path):
            raise AnsibleError(""Collection artifact at '%s' is not a valid tar file."" % to_native(b_path))
        info = {}
        with tarfile.open(b_path, mode='r') as collection_tar:
            for b_member_name, property_name in CollectionRequirement._FILE_MAPPING:
                n_member_name = to_native(b_member_name)
                try:
                    member = collection_tar.getmember(n_member_name)
                except KeyError:
                    raise AnsibleError(""Collection at '%s' does not contain the required file %s.""
                                       % (to_native(b_path), n_member_name))
                with _tarfile_extract(collection_tar, member) as member_obj:
                    try:
                        info[property_name] = json.loads(to_text(member_obj.read(), errors='surrogate_or_strict'))
                    except ValueError:
                        raise AnsibleError(""Collection tar file member %s does not contain a valid json string.""
                                           % n_member_name)
        meta = info['manifest_file']['collection_info']
        files = info['files_file']['files']
        namespace = meta['namespace']
        name = meta['name']
        version = meta['version']
        meta = CollectionVersionMetadata(namespace, name, version, None, None, meta['dependencies'])
        return CollectionRequirement(namespace, name, b_path, None, [version], version, force, parent=parent,
                                     metadata=meta, files=files)
    @staticmethod
    def from_path(b_path, force, parent=None):
        info = {}
        for b_file_name, property_name in CollectionRequirement._FILE_MAPPING:
            b_file_path = os.path.join(b_path, b_file_name)
            if not os.path.exists(b_file_path):
                continue
            with open(b_file_path, 'rb') as file_obj:
                try:
                    info[property_name] = json.loads(to_text(file_obj.read(), errors='surrogate_or_strict'))
                except ValueError:
                    raise AnsibleError(""Collection file at '%s' does not contain a valid json string.""
                                       % to_native(b_file_path))
        if 'manifest_file' in info:
            manifest = info['manifest_file']['collection_info']
            namespace = manifest['namespace']
            name = manifest['name']
            version = manifest['version']
            dependencies = manifest['dependencies']
        else:
            display.warning(""Collection at '%s' does not have a MANIFEST.json file, cannot detect version.""
                            % to_text(b_path))
            parent_dir, name = os.path.split(to_text(b_path, errors='surrogate_or_strict'))
            namespace = os.path.split(parent_dir)[1]
            version = '*'
            dependencies = {}
        meta = CollectionVersionMetadata(namespace, name, version, None, None, dependencies)
        files = info.get('files_file', {}).get('files', {})
        return CollectionRequirement(namespace, name, b_path, None, [version], version, force, parent=parent,
                                     metadata=meta, files=files, skip=True)
    @staticmethod
    def from_name(collection, apis, requirement, force, parent=None):
        namespace, name = collection.split('.', 1)
        galaxy_meta = None
        for api in apis:
            try:
                if not (requirement == '*' or requirement.startswith('<') or requirement.startswith('>') or
                        requirement.startswith('!=')):
                    if requirement.startswith('='):
                        requirement = requirement.lstrip('=')
                    resp = api.get_collection_version_metadata(namespace, name, requirement)
                    galaxy_meta = resp
                    versions = [resp.version]
                else:
                    resp = api.get_collection_versions(namespace, name)
                    # Galaxy supports semver but ansible-galaxy does not. We ignore any versions that don't match
                    # StrictVersion (x.y.z) and only support pre-releases if an explicit version was set (done above).
                    versions = [v for v in resp if StrictVersion.version_re.match(v)]
            except GalaxyError as err:
                if err.http_code == 404:
                    display.vvv(""Collection '%s' is not available from server %s %s""
                                % (collection, api.name, api.api_server))
                    continue
                raise
            display.vvv(""Collection '%s' obtained from server %s %s"" % (collection, api.name, api.api_server))
            break
        else:
            raise AnsibleError(""Failed to find collection %s:%s"" % (collection, requirement))
        req = CollectionRequirement(namespace, name, None, api, versions, requirement, force, parent=parent,
                                    metadata=galaxy_meta)
        return req

def build_collection(collection_path, output_path, force):
    """"""
    Creates the Ansible collection artifact in a .tar.gz file.
    :param collection_path: The path to the collection to build. This should be the directory that contains the
        galaxy.yml file.
    :param output_path: The path to create the collection build artifact. This should be a directory.
    :param force: Whether to overwrite an existing collection build artifact or fail.
    :return: The path to the collection build artifact.
    """"""
    b_collection_path = to_bytes(collection_path, errors='surrogate_or_strict')
    b_galaxy_path = os.path.join(b_collection_path, b'galaxy.yml')",[58]
"                video_webpage, count_name, default=None))
        like_count = _extract_count('like')
        dislike_count = _extract_count('dislike')
        if view_count is None:
            view_count = str_to_int(self._search_regex(
                r'<[^>]+class=[""\']watch-view-count[^>]+>\s*([\d,\s]+)', video_webpage,
                'view count', default=None))
        average_rating = (
            float_or_none(video_details.get('averageRating'))
            or try_get(video_info, lambda x: float_or_none(x['avg_rating'][0])))
        # subtitles
        video_subtitles = self.extract_subtitles(video_id, video_webpage)
        automatic_captions = self.extract_automatic_captions(video_id, video_webpage)
        video_duration = try_get(
            video_info, lambda x: int_or_none(x['length_seconds'][0]))
        if not video_duration:
            video_duration = int_or_none(video_details.get('lengthSeconds'))
        if not video_duration:
            video_duration = parse_duration(self._html_search_meta(
                'duration', video_webpage, 'video duration'))
        # annotations
        video_annotations = None
        if self._downloader.params.get('writeannotations', False):
            xsrf_token = self._search_regex(
                r'([\'""])XSRF_TOKEN\1\s*:\s*([\'""])(?P<xsrf_token>[A-Za-z0-9+/=]+)\2',
                video_webpage, 'xsrf token', group='xsrf_token', fatal=False)
            invideo_url = try_get(
                player_response, lambda x: x['annotations'][0]['playerAnnotationsUrlsRenderer']['invideoUrl'], compat_str)
            if xsrf_token and invideo_url:
                xsrf_field_name = self._search_regex(
                    r'([\'""])XSRF_FIELD_NAME\1\s*:\s*([\'""])(?P<xsrf_field_name>\w+)\2',
                    video_webpage, 'xsrf field name',
                    group='xsrf_field_name', default='session_token')
                video_annotations = self._download_webpage(
                    self._proto_relative_url(invideo_url),
                    video_id, note='Downloading annotations',
                    errnote='Unable to download video annotations', fatal=False,
                    data=urlencode_postdata({xsrf_field_name: xsrf_token}))
        chapters = self._extract_chapters(description_original, video_duration)
        # Look for the DASH manifest
        if self._downloader.params.get('youtube_include_dash_manifest', True):
            dash_mpd_fatal = True
            for mpd_url in dash_mpds:
                dash_formats = {}
                try:
                    def decrypt_sig(mobj):
                        s = mobj.group(1)
                        dec_s = self._decrypt_signature(s, video_id, player_url, age_gate)
                        return '/signature/%s' % dec_s
                    mpd_url = re.sub(r'/s/([a-fA-F0-9\.]+)', decrypt_sig, mpd_url)
                    for df in self._extract_mpd_formats(
                            mpd_url, video_id, fatal=dash_mpd_fatal,
                            formats_dict=self._formats):
                        if not df.get('filesize'):
                            df['filesize'] = _extract_filesize(df['url'])
                        # Do not overwrite DASH format found in some previous DASH manifest
                        if df['format_id'] not in dash_formats:
                            dash_formats[df['format_id']] = df
                        # Additional DASH manifests may end up in HTTP Error 403 therefore
                        # allow them to fail without bug report message if we already have
                        # some DASH manifest succeeded. This is temporary workaround to reduce
                        # burst of bug reports until we figure out the reason and whether it
                        # can be fixed at all.
                        dash_mpd_fatal = False
                except (ExtractorError, KeyError) as e:
                    self.report_warning(
                        'Skipping DASH manifest: %r' % e, video_id)
                if dash_formats:
                    # Remove the formats we found through non-DASH, they
                    # contain less info and it can be wrong, because we use
                    # fixed values (for example the resolution). See
                    # https://github.com/ytdl-org/youtube-dl/issues/5774 for an
                    # example.
                    formats = [f for f in formats if f['format_id'] not in dash_formats.keys()]
                    formats.extend(dash_formats.values())
        # Check for malformed aspect ratio
        stretched_m = re.search(
            r'<meta\s+property=""og:video:tag"".*?content=""yt:stretch=(?P<w>[0-9]+):(?P<h>[0-9]+)"">',
            video_webpage)
        if stretched_m:
            w = float(stretched_m.group('w'))
            h = float(stretched_m.group('h'))
            # yt:stretch may hold invalid ratio data (e.g. for Q39EVAstoRM ratio is 17:0).
            # We will only process correct ratios.
            if w > 0 and h > 0:
                ratio = w / h
                for f in formats:
                    if f.get('vcodec') != 'none':
                        f['stretched_ratio'] = ratio
        if not formats:
            if 'reason' in video_info:
                if 'The uploader has not made this video available in your country.' in video_info['reason']:
                    regions_allowed = self._html_search_meta(
                        'regionsAllowed', video_webpage, default=None)
                    countries = regions_allowed.split(',') if regions_allowed else None
                    self.raise_geo_restricted(
                        msg=video_info['reason'][0], countries=countries)
                reason = video_info['reason'][0]
                if 'Invalid parameters' in reason:
                    unavailable_message = extract_unavailable_message()
                    if unavailable_message:
                        reason = unavailable_message
                raise ExtractorError(
                    'YouTube said: %s' % reason,
                    expected=True, video_id=video_id)
            if video_info.get('license_info') or try_get(player_response, lambda x: x['streamingData']['licenseInfos']):
                raise ExtractorError('This video is DRM protected.', expected=True)
        self._sort_formats(formats)
        self.mark_watched(video_id, video_info, player_response)
        return {
            'id': video_id,
            'uploader': video_uploader,",[45]
"""""""
datetimelike delegation
""""""
import numpy as np
from pandas.core.dtypes.common import (
    is_categorical_dtype,
    is_datetime64_dtype,
    is_datetime64tz_dtype,
    is_datetime_arraylike,
    is_integer_dtype,
    is_list_like,
    is_period_arraylike,
    is_timedelta64_dtype,
)
from pandas.core.dtypes.generic import ABCSeries
from pandas.core.accessor import PandasDelegate, delegate_names
from pandas.core.algorithms import take_1d
from pandas.core.arrays import DatetimeArray, PeriodArray, TimedeltaArray
from pandas.core.base import NoNewAttributesMixin, PandasObject
from pandas.core.indexes.datetimes import DatetimeIndex
from pandas.core.indexes.timedeltas import TimedeltaIndex

class Properties(PandasDelegate, PandasObject, NoNewAttributesMixin):
    def __init__(self, data, orig):
        if not isinstance(data, ABCSeries):
            raise TypeError(
                ""cannot convert an object of type {0} to a ""
                ""datetimelike index"".format(type(data))
            )
        self._parent = data
        self.orig = orig
        self.name = getattr(data, ""name"", None)
        self._freeze()
    def _get_values(self):
        data = self._parent
        if is_datetime64_dtype(data.dtype):
            return DatetimeIndex(data, copy=False, name=self.name)
        elif is_datetime64tz_dtype(data.dtype):
            return DatetimeIndex(data, copy=False, name=self.name)
        elif is_timedelta64_dtype(data.dtype):
            return TimedeltaIndex(data, copy=False, name=self.name)
        else:
            if is_period_arraylike(data):
                # TODO: use to_period_array
                return PeriodArray(data, copy=False)
            if is_datetime_arraylike(data):
                return DatetimeIndex(data, copy=False, name=self.name)
        raise TypeError(
            ""cannot convert an object of type {0} to a ""
            ""datetimelike index"".format(type(data))
        )
    def _delegate_property_get(self, name):
        from pandas import Series
        values = self._get_values()
        result = getattr(values, name)
        # maybe need to upcast (ints)
        if isinstance(result, np.ndarray):
            if is_integer_dtype(result):
                result = result.astype(""int64"")
        elif not is_list_like(result):
            return result
        result = np.asarray(result)
        # blow up if we operate on categories
        if self.orig is not None:
            result = take_1d(result, self.orig.cat.codes)
            index = self.orig.index
        else:
            index = self._parent.index
        # return the result as a Series, which is by definition a copy
        result = Series(result, index=index, name=self.name)
        # setting this object will show a SettingWithCopyWarning/Error
        result._is_copy = (
            ""modifications to a property of a datetimelike ""
            ""object are not supported and are discarded. ""
            ""Change values on the original.""
        )
        return result
    def _delegate_property_set(self, name, value, *args, **kwargs):
        raise ValueError(
            ""modifications to a property of a datetimelike ""
            ""object are not supported. Change values on the ""
            ""original.""
        )
    def _delegate_method(self, name, *args, **kwargs):
        from pandas import Series
        values = self._get_values()
        method = getattr(values, name)
        result = method(*args, **kwargs)
        if not is_list_like(result):
            return result
        result = Series(result, index=self._parent.index, name=self.name)
        # setting this object will show a SettingWithCopyWarning/Error
        result._is_copy = (
            ""modifications to a method of a datetimelike ""
            ""object are not supported and are discarded. ""
            ""Change values on the original.""
        )
        return result

@delegate_names(
    delegate=DatetimeArray, accessors=DatetimeArray._datetimelike_ops, typ=""property""","[18, 79]"
"                if not reading_revision:
                    ids = id_regex.search(clean_line)
                    if ids:
                        article_id = ids[0]
                        if article_id in read_ids:
                            logger.info(
                                ""Found duplicate article ID"", article_id, clean_line
                            )  # This should never happen ...
                        read_ids.add(article_id)
                # read the title of this article (outside the revision portion of the document)
                if not reading_revision:
                    titles = title_regex.search(clean_line)
                    if titles:
                        article_title = titles[0].strip()
    logger.info(""Finished. Processed {} articles"".format(article_count))

def _process_wp_text(article_title, article_text, wp_to_id):
    # ignore meta Wikipedia pages
    if ns_regex.match(article_title):
        return None, None
    # remove the text tags
    text_search = text_regex.search(article_text)
    if text_search is None:
        return None, None
    text = text_search.group(0)
    # stop processing if this is a redirect page
    if text.startswith(""#REDIRECT""):
        return None, None
    # get the raw text without markup etc, keeping only interwiki links
    clean_text, entities = _remove_links(_get_clean_wp_text(text), wp_to_id)
    return clean_text, entities

def _get_clean_wp_text(article_text):
    clean_text = article_text.strip()
    # remove bolding & italic markup
    clean_text = clean_text.replace(""'''"", """")
    clean_text = clean_text.replace(""''"", """")
    # remove nested {{info}} statements by removing the inner/smallest ones first and iterating
    try_again = True
    previous_length = len(clean_text)
    while try_again:
        clean_text = info_regex.sub(
            """", clean_text
        )  # non-greedy match excluding a nested {
        if len(clean_text) < previous_length:
            try_again = True
        else:
            try_again = False
        previous_length = len(clean_text)
    # remove HTML comments
    clean_text = html_regex.sub("""", clean_text)
    # remove Category and File statements
    clean_text = category_regex.sub("""", clean_text)
    clean_text = file_regex.sub("""", clean_text)
    # remove multiple =
    while ""=="" in clean_text:
        clean_text = clean_text.replace(""=="", ""="")
    clean_text = clean_text.replace("". ="", ""."")
    clean_text = clean_text.replace("" = "", "". "")
    clean_text = clean_text.replace(""= "", ""."")
    clean_text = clean_text.replace("" ="", """")
    # remove refs (non-greedy match)
    clean_text = ref_regex.sub("""", clean_text)
    clean_text = ref_2_regex.sub("""", clean_text)
    # remove additional wikiformatting
    clean_text = re.sub(r""&lt;blockquote&gt;"", """", clean_text)
    clean_text = re.sub(r""&lt;/blockquote&gt;"", """", clean_text)
    # change special characters back to normal ones
    clean_text = clean_text.replace(r""&lt;"", ""<"")
    clean_text = clean_text.replace(r""&gt;"", "">"")
    clean_text = clean_text.replace(r""&quot;"", '""')
    clean_text = clean_text.replace(r""&amp;nbsp;"", "" "")
    clean_text = clean_text.replace(r""&amp;"", ""&"")
    # remove multiple spaces
    while ""  "" in clean_text:
        clean_text = clean_text.replace(""  "", "" "")
    return clean_text.strip()

def _remove_links(clean_text, wp_to_id):
    # read the text char by char to get the right offsets for the interwiki links
    entities = []
    final_text = """"
    open_read = 0
    reading_text = True
    reading_entity = False
    reading_mention = False
    reading_special_case = False
    entity_buffer = """"
    mention_buffer = """"
    for index, letter in enumerate(clean_text):
        if letter == ""["":
            open_read += 1
        elif letter == ""]"":
            open_read -= 1
        elif letter == ""|"":
            if reading_text:
                final_text += letter
            # switch from reading entity to mention in the [[entity|mention]] pattern
            elif reading_entity:
                reading_text = False
                reading_entity = False
                reading_mention = True
            else:
                reading_special_case = True
        else:
            if reading_entity:
                entity_buffer += letter
            elif reading_mention:
                mention_buffer += letter",[24]
"        ids, _, ngroup = self.grouper.group_info
        cast = self._transform_should_cast(func_nm)
        out = algorithms.take_1d(result._values, ids)
        if cast:
            out = self._try_cast(out, self.obj)
        return Series(out, index=self.obj.index, name=self.obj.name)
    def filter(self, func, dropna=True, *args, **kwargs):
        """"""
        Return a copy of a Series excluding elements from groups that
        do not satisfy the boolean criterion specified by func.
        Parameters
        ----------
        func : function
            To apply to each group. Should return True or False.
        dropna : Drop groups that do not pass the filter. True by default;
            if False, groups that evaluate False are filled with NaNs.
        Examples
        --------
        >>> df = pd.DataFrame({'A' : ['foo', 'bar', 'foo', 'bar',
        ...                           'foo', 'bar'],
        ...                    'B' : [1, 2, 3, 4, 5, 6],
        ...                    'C' : [2.0, 5., 8., 1., 2., 9.]})
        >>> grouped = df.groupby('A')
        >>> df.groupby('A').B.filter(lambda x: x.mean() > 3.)
        1    2
        3    4
        5    6
        Name: B, dtype: int64
        Returns
        -------
        filtered : Series
        """"""
        if isinstance(func, str):
            wrapper = lambda x: getattr(x, func)(*args, **kwargs)
        else:
            wrapper = lambda x: func(x, *args, **kwargs)
        # Interpret np.nan as False.
        def true_and_notna(x, *args, **kwargs) -> bool:
            b = wrapper(x, *args, **kwargs)
            return b and notna(b)
        try:
            indices = [
                self._get_index(name) for name, group in self if true_and_notna(group)
            ]
        except (ValueError, TypeError):
            raise TypeError(""the filter must return a boolean result"")
        filtered = self._apply_filter(indices, dropna)
        return filtered
    def nunique(self, dropna: bool = True) -> Series:
        """"""
        Return number of unique elements in the group.
        Returns
        -------
        Series
            Number of unique values within each group.
        """"""
        ids, _, _ = self.grouper.group_info
        val = self.obj._internal_get_values()
        # GH 27951
        # temporary fix while we wait for NumPy bug 12629 to be fixed
        val[isna(val)] = np.datetime64(""NaT"")
        try:
            sorter = np.lexsort((val, ids))
        except TypeError:  # catches object dtypes
            msg = f""val.dtype must be object, got {val.dtype}""
            assert val.dtype == object, msg
            val, _ = algorithms.factorize(val, sort=False)
            sorter = np.lexsort((val, ids))
            _isna = lambda a: a == -1
        else:
            _isna = isna
        ids, val = ids[sorter], val[sorter]
        # group boundaries are where group ids change
        # unique observations are where sorted values change
        idx = np.r_[0, 1 + np.nonzero(ids[1:] != ids[:-1])[0]]
        inc = np.r_[1, val[1:] != val[:-1]]
        # 1st item of each group is a new unique observation
        mask = _isna(val)
        if dropna:
            inc[idx] = 1
            inc[mask] = 0
        else:
            inc[mask & np.r_[False, mask[:-1]]] = 0
            inc[idx] = 1
        out = np.add.reduceat(inc, idx).astype(""int64"", copy=False)
        if len(ids):
            # NaN/NaT group exists if the head of ids is -1,
            # so remove it from res and exclude its index from idx
            if ids[0] == -1:
                res = out[1:]
                idx = idx[np.flatnonzero(idx)]
            else:
                res = out
        else:
            res = out[1:]
        ri = self.grouper.result_index
        # we might have duplications among the bins
        if len(res) != len(ri):
            res, out = np.zeros(len(ri), dtype=out.dtype), res
            res[ids[idx]] = out
        result = Series(res, index=ri, name=self._selection_name)
        return self._reindex_output(result, fill_value=0)
    @Appender(Series.describe.__doc__)
    def describe(self, **kwargs):
        result = self.apply(lambda x: x.describe(**kwargs))
        if self.axis == 1:
            return result.T
        return result.unstack()","[71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 89, 92]"
"        yield f""{'  ' * depth}{node.__class__.__name__}(""
        for field in sorted(node._fields):
            try:
                value = getattr(node, field)
            except AttributeError:
                continue
            yield f""{'  ' * (depth+1)}{field}=""
            if isinstance(value, list):
                for item in value:
                    if isinstance(item, ast.AST):
                        yield from _v(item, depth + 2)
            elif isinstance(value, ast.AST):
                yield from _v(value, depth + 2)
            else:
                yield f""{'  ' * (depth+2)}{value!r},  # {value.__class__.__name__}""
        yield f""{'  ' * depth})  # /{node.__class__.__name__}""
    try:
        src_ast = ast.parse(src)
    except Exception as exc:
        major, minor = sys.version_info[:2]
        raise AssertionError(
            f""cannot use --safe with this file; failed to parse source file ""
            f""with Python {major}.{minor}'s builtin AST. Re-run with --fast ""
            f""or stop using deprecated Python 2 syntax. AST error message: {exc}""
        )
    try:
        dst_ast = ast.parse(dst)
    except Exception as exc:
        log = dump_to_file("""".join(traceback.format_tb(exc.__traceback__)), dst)
        raise AssertionError(
            f""INTERNAL ERROR: Black produced invalid code: {exc}. ""
            f""Please report a bug on https://github.com/ambv/black/issues.  ""
            f""This invalid output might be helpful: {log}""
        ) from None
    src_ast_str = ""\n"".join(_v(src_ast))
    dst_ast_str = ""\n"".join(_v(dst_ast))
    if src_ast_str != dst_ast_str:
        log = dump_to_file(diff(src_ast_str, dst_ast_str, ""src"", ""dst""))
        raise AssertionError(
            f""INTERNAL ERROR: Black produced code that is not equivalent to ""
            f""the source.  ""
            f""Please report a bug on https://github.com/ambv/black/issues.  ""
            f""This diff might be helpful: {log}""
        ) from None

def assert_stable(src: str, dst: str, line_length: int) -> None:
    """"""Raise AssertionError if `dst` reformats differently the second time.""""""
    newdst = format_str(dst, line_length=line_length)
    if dst != newdst:
        log = dump_to_file(
            diff(src, dst, ""source"", ""first pass""),
            diff(dst, newdst, ""first pass"", ""second pass""),
        )
        raise AssertionError(
            f""INTERNAL ERROR: Black produced different code on the second pass ""
            f""of the formatter.  ""
            f""Please report a bug on https://github.com/ambv/black/issues.  ""
            f""This diff might be helpful: {log}""
        ) from None

def dump_to_file(*output: str) -> str:
    """"""Dump `output` to a temporary file. Return path to the file.""""""
    import tempfile
    with tempfile.NamedTemporaryFile(
        mode=""w"", prefix=""blk_"", suffix="".log"", delete=False
    ) as f:
        for lines in output:
            f.write(lines)
            if lines and lines[-1] != ""\n"":
                f.write(""\n"")
    return f.name

def diff(a: str, b: str, a_name: str, b_name: str) -> str:
    """"""Return a unified diff string between strings `a` and `b`.""""""
    import difflib
    a_lines = [line + ""\n"" for line in a.split(""\n"")]
    b_lines = [line + ""\n"" for line in b.split(""\n"")]
    return """".join(
        difflib.unified_diff(a_lines, b_lines, fromfile=a_name, tofile=b_name, n=5)
    )

def cancel(tasks: List[asyncio.Task]) -> None:
    """"""asyncio signal handler that cancels all `tasks` and reports to stderr.""""""
    err(""Aborted!"")
    for task in tasks:
        task.cancel()

def shutdown(loop: BaseEventLoop) -> None:
    """"""Cancel all pending tasks on `loop`, wait for them, and close the loop.""""""
    try:
        # This part is borrowed from asyncio/runners.py in Python 3.7b2.
        to_cancel = [task for task in asyncio.Task.all_tasks(loop) if not task.done()]
        if not to_cancel:
            return
        for task in to_cancel:
            task.cancel()
        loop.run_until_complete(
            asyncio.gather(*to_cancel, loop=loop, return_exceptions=True)
        )
    finally:
        # `concurrent.futures.Future` objects cannot be cancelled once they
        # are already running. There might be some when the `shutdown()` happened.
        # Silence their logger's spew about the event loop being closed.
        cf_logger = logging.getLogger(""concurrent.futures"")
        cf_logger.setLevel(logging.CRITICAL)
        loop.close()

if __name__ == ""__main__"":",[76]
"            )
        elif is_coroutine_callable(call):
            solved = await call(**sub_values)
        else:
            solved = await run_in_threadpool(call, **sub_values)
        if sub_dependant.name is not None:
            values[sub_dependant.name] = solved
        if sub_dependant.cache_key not in dependency_cache:
            dependency_cache[sub_dependant.cache_key] = solved
    path_values, path_errors = request_params_to_args(
        dependant.path_params, request.path_params
    )
    query_values, query_errors = request_params_to_args(
        dependant.query_params, request.query_params
    )
    header_values, header_errors = request_params_to_args(
        dependant.header_params, request.headers
    )
    cookie_values, cookie_errors = request_params_to_args(
        dependant.cookie_params, request.cookies
    )
    values.update(path_values)
    values.update(query_values)
    values.update(header_values)
    values.update(cookie_values)
    errors += path_errors + query_errors + header_errors + cookie_errors
    if dependant.body_params:
        (
            body_values,
            body_errors,
        ) = await request_body_to_args(  # body_params checked above
            required_params=dependant.body_params, received_body=body
        )
        values.update(body_values)
        errors.extend(body_errors)
    if dependant.request_param_name and isinstance(request, Request):
        values[dependant.request_param_name] = request
    elif dependant.websocket_param_name and isinstance(request, WebSocket):
        values[dependant.websocket_param_name] = request
    if dependant.background_tasks_param_name:
        if background_tasks is None:
            background_tasks = BackgroundTasks()
        values[dependant.background_tasks_param_name] = background_tasks
    if dependant.response_param_name:
        values[dependant.response_param_name] = response
    if dependant.security_scopes_param_name:
        values[dependant.security_scopes_param_name] = SecurityScopes(
            scopes=dependant.security_scopes
        )
    return values, errors, background_tasks, response, dependency_cache

def request_params_to_args(
    required_params: Sequence[ModelField],
    received_params: Union[Mapping[str, Any], QueryParams, Headers],
) -> Tuple[Dict[str, Any], List[ErrorWrapper]]:
    values = {}
    errors = []
    for field in required_params:
        if is_scalar_sequence_field(field) and isinstance(
            received_params, (QueryParams, Headers)
        ):
            value = received_params.getlist(field.alias) or field.default
        else:
            value = received_params.get(field.alias)
        field_info = get_field_info(field)
        assert isinstance(
            field_info, params.Param
        ), ""Params must be subclasses of Param""
        if value is None:
            if field.required:
                if PYDANTIC_1:
                    errors.append(
                        ErrorWrapper(
                            MissingError(), loc=(field_info.in_.value, field.alias)
                        )
                    )
                else:  # pragma: nocover
                    errors.append(
                        ErrorWrapper(  # type: ignore
                            MissingError(),
                            loc=(field_info.in_.value, field.alias),
                            config=BaseConfig,
                        )
                    )
            else:
                values[field.name] = deepcopy(field.default)
            continue
        v_, errors_ = field.validate(
            value, values, loc=(field_info.in_.value, field.alias)
        )
        if isinstance(errors_, ErrorWrapper):
            errors.append(errors_)
        elif isinstance(errors_, list):
            errors.extend(errors_)
        else:
            values[field.name] = v_
    return values, errors

async def request_body_to_args(
    required_params: List[ModelField],
    received_body: Optional[Union[Dict[str, Any], FormData]],
) -> Tuple[Dict[str, Any], List[ErrorWrapper]]:
    values = {}
    errors = []
    if required_params:
        field = required_params[0]
        field_info = get_field_info(field)
        embed = getattr(field_info, ""embed"", None)
        if len(required_params) == 1 and not embed:
            received_body = {field.alias: received_body}
        for field in required_params:
            value: Any = None
            if received_body is not None:
                if field.shape in sequence_shapes and isinstance(
                    received_body, FormData
                ):
                    value = received_body.getlist(field.alias)
                else:
                    value = received_body.get(field.alias)
            if (
                value is None
                or (isinstance(field_info, params.Form) and value == """")
                or (
                    isinstance(field_info, params.Form)
                    and field.shape in sequence_shapes","[115, 116, 117]"
"""""""
DataFrame
---------
An efficient 2D container for potentially mixed-type time series or other
labeled data series.
Similar to its R counterpart, data.frame, except providing automatic data
alignment and a host of useful data manipulation methods having to do with the
labeling information
""""""
import collections
from collections import abc
import datetime
from io import StringIO
import itertools
from textwrap import dedent
from typing import (
    IO,
    TYPE_CHECKING,
    Any,
    Dict,
    FrozenSet,
    Hashable,
    Iterable,
    Iterator,
    List,
    Optional,
    Sequence,
    Set,
    Tuple,
    Type,
    Union,
    cast,
)
import warnings
import numpy as np
import numpy.ma as ma
from pandas._config import get_option
from pandas._libs import algos as libalgos, lib, properties
from pandas._typing import (
    ArrayLike,
    Axes,
    Axis,
    Dtype,
    FilePathOrBuffer,
    IndexKeyFunc,
    Label,
    Level,
    Renamer,
    ValueKeyFunc,
)
from pandas.compat import PY37
from pandas.compat._optional import import_optional_dependency
from pandas.compat.numpy import function as nv
from pandas.util._decorators import (
    Appender,
    Substitution,
    deprecate_kwarg,
    doc,
    rewrite_axis_style_signature,
)
from pandas.util._validators import (
    validate_axis_style_args,
    validate_bool_kwarg,
    validate_percentile,
)
from pandas.core.dtypes.cast import (
    cast_scalar_to_array,
    coerce_to_dtypes,
    find_common_type,
    infer_dtype_from_scalar,
    invalidate_string_dtypes,
    maybe_cast_to_datetime,
    maybe_convert_platform,
    maybe_downcast_to_dtype,
    maybe_infer_to_datetimelike,
    maybe_upcast,
    maybe_upcast_putmask,
    validate_numeric_casting,
)
from pandas.core.dtypes.common import (
    ensure_float64,
    ensure_int64,
    ensure_platform_int,
    infer_dtype_from_object,
    is_bool_dtype,
    is_dataclass,
    is_datetime64_any_dtype,
    is_dict_like,
    is_dtype_equal,
    is_extension_array_dtype,
    is_float_dtype,
    is_hashable,
    is_integer,
    is_integer_dtype,
    is_iterator,
    is_list_like,
    is_named_tuple,
    is_object_dtype,
    is_scalar,
    is_sequence,
    needs_i8_conversion,
    pandas_dtype,
)
from pandas.core.dtypes.generic import (
    ABCDataFrame,
    ABCIndexClass,
    ABCMultiIndex,
    ABCSeries,
)
from pandas.core.dtypes.missing import isna, notna
from pandas.core import algorithms, common as com, nanops, ops
from pandas.core.accessor import CachedAccessor
from pandas.core.arrays import Categorical, ExtensionArray
from pandas.core.arrays.datetimelike import DatetimeLikeArrayMixin as DatetimeLikeArray
from pandas.core.arrays.sparse import SparseFrameAccessor
from pandas.core.generic import NDFrame, _shared_docs
from pandas.core.indexes import base as ibase
from pandas.core.indexes.api import Index, ensure_index, ensure_index_from_sequences
from pandas.core.indexes.datetimes import DatetimeIndex
from pandas.core.indexes.multi import MultiIndex, maybe_droplevels",[86]
"        return {
            'minval': self.minval,
            'maxval': self.maxval,
            'seed': self.seed,
        }

class TruncatedNormal(Initializer):
    """"""Initializer that generates a truncated normal distribution.
    These values are similar to values from a `RandomNormal`
    except that values more than two standard deviations from the mean
    are discarded and redrawn. This is the recommended initializer for
    neural network weights and filters.
    # Arguments
        mean: a python scalar or a scalar tensor. Mean of the random values
          to generate.
        stddev: a python scalar or a scalar tensor. Standard deviation of the
          random values to generate.
        seed: A Python integer. Used to seed the random generator.
    """"""
    def __init__(self, mean=0., stddev=0.05, seed=None):
        self.mean = mean
        self.stddev = stddev
        self.seed = seed
    def __call__(self, shape, dtype=None):
        return K.truncated_normal(shape, self.mean, self.stddev,
                                  dtype=dtype, seed=self.seed)
    def get_config(self):
        return {
            'mean': self.mean,
            'stddev': self.stddev,
            'seed': self.seed
        }

class VarianceScaling(Initializer):
    """"""Initializer capable of adapting its scale to the shape of weights.
    With `distribution=""normal""`, samples are drawn from a truncated normal
    distribution centered on zero, with `stddev = sqrt(scale / n)` where n is:
        - number of input units in the weight tensor, if mode = ""fan_in""
        - number of output units, if mode = ""fan_out""
        - average of the numbers of input and output units, if mode = ""fan_avg""
    With `distribution=""uniform""`,
    samples are drawn from a uniform distribution
    within [-limit, limit], with `limit = sqrt(3 * scale / n)`.
    # Arguments
        scale: Scaling factor (positive float).
        mode: One of ""fan_in"", ""fan_out"", ""fan_avg"".
        distribution: Random distribution to use. One of ""normal"", ""uniform"".
        seed: A Python integer. Used to seed the random generator.
    # Raises
        ValueError: In case of an invalid value for the ""scale"", mode"" or
          ""distribution"" arguments.
    """"""
    def __init__(self, scale=1.0,
                 mode='fan_in',
                 distribution='normal',
                 seed=None):
        if scale <= 0.:
            raise ValueError('`scale` must be a positive float. Got:', scale)
        mode = mode.lower()
        if mode not in {'fan_in', 'fan_out', 'fan_avg'}:
            raise ValueError('Invalid `mode` argument: '
                             'expected on of {""fan_in"", ""fan_out"", ""fan_avg""} '
                             'but got', mode)
        distribution = distribution.lower()
        if distribution not in {'normal', 'uniform'}:
            raise ValueError('Invalid `distribution` argument: '
                             'expected one of {""normal"", ""uniform""} '
                             'but got', distribution)
        self.scale = scale
        self.mode = mode
        self.distribution = distribution
        self.seed = seed
    def __call__(self, shape, dtype=None):
        fan_in, fan_out = _compute_fans(shape)
        scale = self.scale
        if self.mode == 'fan_in':
            scale /= max(1., fan_in)
        elif self.mode == 'fan_out':
            scale /= max(1., fan_out)
        else:
            scale /= max(1., float(fan_in + fan_out) / 2)
        if self.distribution == 'normal':
            # 0.879... = scipy.stats.truncnorm.std(a=-2, b=2, loc=0., scale=1.)
            stddev = np.sqrt(scale) / .87962566103423978
            return K.truncated_normal(shape, 0., stddev,
                                      dtype=dtype, seed=self.seed)
        else:
            limit = np.sqrt(3. * scale)
            return K.random_uniform(shape, -limit, limit,
                                    dtype=dtype, seed=self.seed)
    def get_config(self):
        return {
            'scale': self.scale,
            'mode': self.mode,
            'distribution': self.distribution,
            'seed': self.seed
        }

class Orthogonal(Initializer):
    """"""Initializer that generates a random orthogonal matrix.
    # Arguments
        gain: Multiplicative factor to apply to the orthogonal matrix.
        seed: A Python integer. Used to seed the random generator.
    # References
        - [Exact solutions to the nonlinear dynamics of learning in deep
           linear neural networks](http://arxiv.org/abs/1312.6120)
    """"""
    def __init__(self, gain=1., seed=None):","[29, 30, 98, 99, 102, 103]"
"        But it's better to avoid applymap in that case.
        >>> df ** 2
                   0          1
        0   1.000000   4.494400
        1  11.262736  20.857489
        """"""
        # if we have a dtype == 'M8[ns]', provide boxed values
        def infer(x):
            if x.empty:
                return lib.map_infer(x, func)
            return lib.map_infer(x.astype(object).values, func)
        return self.apply(infer)
    # ----------------------------------------------------------------------
    # Merging / joining methods
    def append(self, other, ignore_index=False, verify_integrity=False, sort=False):
        """"""
        Append rows of `other` to the end of caller, returning a new object.
        Columns in `other` that are not in the caller are added as new columns.
        Parameters
        ----------
        other : DataFrame or Series/dict-like object, or list of these
            The data to append.
        ignore_index : bool, default False
            If True, do not use the index labels.
        verify_integrity : bool, default False
            If True, raise ValueError on creating index with duplicates.
        sort : bool, default False
            Sort columns if the columns of `self` and `other` are not aligned.
            .. versionadded:: 0.23.0
            .. versionchanged:: 1.0.0
                Changed to not sort by default.
        Returns
        -------
        DataFrame
        See Also
        --------
        concat : General function to concatenate DataFrame or Series objects.
        Notes
        -----
        If a list of dict/series is passed and the keys are all contained in
        the DataFrame's index, the order of the columns in the resulting
        DataFrame will be unchanged.
        Iteratively appending rows to a DataFrame can be more computationally
        intensive than a single concatenate. A better solution is to append
        those rows to a list and then concatenate the list with the original
        DataFrame all at once.
        Examples
        --------
        >>> df = pd.DataFrame([[1, 2], [3, 4]], columns=list('AB'))
        >>> df
           A  B
        0  1  2
        1  3  4
        >>> df2 = pd.DataFrame([[5, 6], [7, 8]], columns=list('AB'))
        >>> df.append(df2)
           A  B
        0  1  2
        1  3  4
        0  5  6
        1  7  8
        With `ignore_index` set to True:
        >>> df.append(df2, ignore_index=True)
           A  B
        0  1  2
        1  3  4
        2  5  6
        3  7  8
        The following, while not recommended methods for generating DataFrames,
        show two ways to generate a DataFrame from multiple data sources.
        Less efficient:
        >>> df = pd.DataFrame(columns=['A'])
        >>> for i in range(5):
        ...     df = df.append({'A': i}, ignore_index=True)
        >>> df
           A
        0  0
        1  1
        2  2
        3  3
        4  4
        More efficient:
        >>> pd.concat([pd.DataFrame([i], columns=['A']) for i in range(5)],
        ...           ignore_index=True)
           A
        0  0
        1  1
        2  2
        3  3
        4  4
        """"""
        if isinstance(other, (Series, dict)):
            if isinstance(other, dict):
                other = Series(other)
            if other.name is None and not ignore_index:
                raise TypeError(
                    ""Can only append a Series if ignore_index=True""
                    "" or if the Series has a name""
                )
            if other.name is None:
                index = None
            else:
                # other must have the same index name as self, otherwise
                # index name will be reset
                index = Index([other.name], name=self.index.name)","[121, 122, 123, 126, 127]"
"        Other Parameters
        ----------------
        **kwargs :  `~matplotlib.collections.LineCollection` properties.
        See Also
        --------
        vlines : vertical lines
        axhline: horizontal line across the axes
        """"""
        # We do the conversion first since not all unitized data is uniform
        # process the unit information
        self._process_unit_info([xmin, xmax], y, kwargs=kwargs)
        y = self.convert_yunits(y)
        xmin = self.convert_xunits(xmin)
        xmax = self.convert_xunits(xmax)
        if not np.iterable(y):
            y = [y]
        if not np.iterable(xmin):
            xmin = [xmin]
        if not np.iterable(xmax):
            xmax = [xmax]
        y, xmin, xmax = cbook.delete_masked_points(y, xmin, xmax)
        y = np.ravel(y)
        xmin = np.resize(xmin, y.shape)
        xmax = np.resize(xmax, y.shape)
        verts = [((thisxmin, thisy), (thisxmax, thisy))
                 for thisxmin, thisxmax, thisy in zip(xmin, xmax, y)]
        lines = mcoll.LineCollection(verts, colors=colors,
                                     linestyles=linestyles, label=label)
        self.add_collection(lines, autolim=False)
        lines.update(kwargs)
        if len(y) > 0:
            minx = min(xmin.min(), xmax.min())
            maxx = max(xmin.max(), xmax.max())
            miny = y.min()
            maxy = y.max()
            corners = (minx, miny), (maxx, maxy)
            self.update_datalim(corners)
            self._request_autoscale_view()
        return lines
    @_preprocess_data(replace_names=[""x"", ""ymin"", ""ymax"", ""colors""],
                      label_namer=""x"")
    def vlines(self, x, ymin, ymax, colors='k', linestyles='solid',
               label='', **kwargs):
        """"""
        Plot vertical lines.
        Plot vertical lines at each *x* from *ymin* to *ymax*.
        Parameters
        ----------
        x : scalar or 1D array-like
            x-indexes where to plot the lines.
        ymin, ymax : scalar or 1D array-like
            Respective beginning and end of each line. If scalars are
            provided, all lines will have same length.
        colors : array-like of colors, default: 'k'
        linestyles : {'solid', 'dashed', 'dashdot', 'dotted'}, optional
        label : str, default: ''
        Returns
        -------
        `~matplotlib.collections.LineCollection`
        Other Parameters
        ----------------
        **kwargs : `~matplotlib.collections.LineCollection` properties.
        See Also
        --------
        hlines : horizontal lines
        axvline: vertical line across the axes
        """"""
        self._process_unit_info(xdata=x, ydata=[ymin, ymax], kwargs=kwargs)
        # We do the conversion first since not all unitized data is uniform
        x = self.convert_xunits(x)
        ymin = self.convert_yunits(ymin)
        ymax = self.convert_yunits(ymax)
        if not np.iterable(x):
            x = [x]
        if not np.iterable(ymin):
            ymin = [ymin]
        if not np.iterable(ymax):
            ymax = [ymax]
        x, ymin, ymax = cbook.delete_masked_points(x, ymin, ymax)
        x = np.ravel(x)
        ymin = np.resize(ymin, x.shape)
        ymax = np.resize(ymax, x.shape)
        verts = [((thisx, thisymin), (thisx, thisymax))
                 for thisx, thisymin, thisymax in zip(x, ymin, ymax)]
        lines = mcoll.LineCollection(verts, colors=colors,
                                     linestyles=linestyles, label=label)
        self.add_collection(lines, autolim=False)
        lines.update(kwargs)
        if len(x) > 0:
            minx = x.min()
            maxx = x.max()
            miny = min(ymin.min(), ymax.min())
            maxy = max(ymin.max(), ymax.max())
            corners = (minx, miny), (maxx, maxy)
            self.update_datalim(corners)
            self._request_autoscale_view()
        return lines
","[24, 25, 27, 28, 30, 31, 32, 102, 103, 105, 106, 108, 109, 110]"
"""""""
Scrapy Item
See documentation in docs/topics/item.rst
""""""
from pprint import pformat
from collections import MutableMapping
from abc import ABCMeta
import six
from scrapy.utils.trackref import object_ref

class BaseItem(object_ref):
    """"""Base class for all scraped items.""""""
    pass

class Field(dict):
    """"""Container of field metadata""""""

class ItemMeta(ABCMeta):
    def __new__(mcs, class_name, bases, attrs):
        new_bases = tuple(base._class for base in bases if hasattr(base, '_class'))
        _class = super(ItemMeta, mcs).__new__(mcs, 'x_' + class_name, new_bases, attrs)
        fields = {}
        new_attrs = {}
        for n in dir(_class):
            v = getattr(_class, n)
            if isinstance(v, Field):
                fields[n] = v
            elif n in attrs:
                new_attrs[n] = attrs[n]
        new_attrs['fields'] = fields
        new_attrs['_class'] = _class
        return super(ItemMeta, mcs).__new__(mcs, class_name, bases, new_attrs)

class DictItem(MutableMapping, BaseItem):
    fields = {}
    def __init__(self, *args, **kwargs):
        self._values = {}
        if args or kwargs:  # avoid creating dict for most common case
            for k, v in six.iteritems(dict(*args, **kwargs)):
                self[k] = v
    def __getitem__(self, key):
        return self._values[key]
    def __setitem__(self, key, value):
        if key in self.fields:
            self._values[key] = value
        else:
            raise KeyError(""%s does not support field: %s"" %
                (self.__class__.__name__, key))
    def __delitem__(self, key):
        del self._values[key]
    def __getattr__(self, name):
        if name in self.fields:
            raise AttributeError(""Use item[%r] to get field value"" % name)
        raise AttributeError(name)
    def __setattr__(self, name, value):
        if not name.startswith('_'):
            raise AttributeError(""Use item[%r] = %r to set field value"" %
                (name, value))
        super(DictItem, self).__setattr__(name, value)
    def __len__(self):
        return len(self._values)
    def __iter__(self):
        return iter(self._values)
    __hash__ = BaseItem.__hash__
    def keys(self):
        return self._values.keys()
    def __repr__(self):
        return pformat(dict(self))
    def copy(self):
        return self.__class__(self)

@six.add_metaclass(ItemMeta)
class Item(DictItem):
    pass",[29]
"from enum import Enum
from types import GeneratorType
from typing import Any, Set
from pydantic import BaseModel
from pydantic.json import ENCODERS_BY_TYPE

def jsonable_encoder(
    obj: Any,
    include: Set[str] = None,
    exclude: Set[str] = set(),
    by_alias: bool = False,
    include_none: bool = True,
    custom_encoder: dict = {},
) -> Any:
    if isinstance(obj, BaseModel):
        if not obj.Config.json_encoders:
            return jsonable_encoder(
                obj.dict(include=include, exclude=exclude, by_alias=by_alias),
                include_none=include_none,
            )
        else:
            return jsonable_encoder(
                obj.dict(include=include, exclude=exclude, by_alias=by_alias),
                include_none=include_none,
                custom_encoder=obj.Config.json_encoders,
            )
    if isinstance(obj, Enum):
        return obj.value
    if isinstance(obj, (str, int, float, type(None))):
        return obj
    if isinstance(obj, dict):
        return {
            jsonable_encoder(
                key,
                by_alias=by_alias,
                include_none=include_none,
                custom_encoder=custom_encoder,
            ): jsonable_encoder(
                value,
                by_alias=by_alias,
                include_none=include_none,
                custom_encoder=custom_encoder,
            )
            for key, value in obj.items()
            if value is not None or include_none
        }
    if isinstance(obj, (list, set, frozenset, GeneratorType, tuple)):
        return [
            jsonable_encoder(
                item,
                include=include,
                exclude=exclude,
                by_alias=by_alias,
                include_none=include_none,
                custom_encoder=custom_encoder,
            )
            for item in obj
        ]
    errors = []
    try:
        if custom_encoder and type(obj) in custom_encoder:
            encoder = custom_encoder[type(obj)]
        else:
            encoder = ENCODERS_BY_TYPE[type(obj)]
        return encoder(obj)
    except KeyError as e:
        errors.append(e)
        try:
            data = dict(obj)
        except Exception as e:
            errors.append(e)
            try:
                data = vars(obj)
            except Exception as e:
                errors.append(e)
                raise ValueError(errors)
    return jsonable_encoder(data, by_alias=by_alias, include_none=include_none)","[17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27]"
"                buf += tmp
                break
            else:
                fp_write(buf + tmp[:i + len(delim)])
                callback(1)  # n += 1
                buf = ''
                tmp = tmp[i + len(delim):]

# ((opt, type), ... )
RE_OPTS = re.compile(r'\n {8}(\S+)\s{2,}:\s*([^,]+)')
# better split method assuming no positional args
RE_SHLEX = re.compile(r'\s*--?([^\s=]+)(?:\s*|=|$)')
# TODO: add custom support for some of the following?
UNSUPPORTED_OPTS = ('iterable', 'gui', 'out', 'file')
# The 8 leading spaces are required for consistency
CLI_EXTRA_DOC = r""""""
        Extra CLI Options
        -----------------
        name  : type, optional
             TODO: find out why this is needed.
        delim  : chr, optional
            Delimiting character [default: '\n']. Use '\0' for null.
            N.B.: on Windows systems, Python converts '\n' to '\r\n'.
        buf_size  : int, optional
            String buffer size in bytes [default: 256]
            used when `delim` is specified.
        bytes  : bool, optional
            If true, will count bytes, ignore `delim`, and default
            `unit_scale` to True, `unit_divisor` to 1024, and `unit` to 'B'.
        log  : str, optional
            CRITICAL|FATAL|ERROR|WARN(ING)|[default: 'INFO']|DEBUG|NOTSET.
""""""

def main(fp=sys.stderr):
    """"""
    Paramters (internal use only)
    ---------
    fp  : file-like object for tqdm
    """"""
    try:
        log = sys.argv.index('--log')
    except ValueError:
        logLevel = 'INFO'
    else:
        # sys.argv.pop(log)
        # logLevel = sys.argv.pop(log)
        logLevel = sys.argv[log + 1]
    logging.basicConfig(level=getattr(logging, logLevel))
    log = logging.getLogger(__name__)
    d = tqdm.__init__.__doc__ + CLI_EXTRA_DOC
    opt_types = dict(RE_OPTS.findall(d))
    # opt_types['delim'] = 'chr'
    for o in UNSUPPORTED_OPTS:
        opt_types.pop(o)
    log.debug(sorted(opt_types.items()))
    # d = RE_OPTS.sub(r'  --\1=<\1>  : \2', d)
    split = RE_OPTS.split(d)
    opt_types_desc = zip(split[1::3], split[2::3], split[3::3])
    d = ''.join('\n  --{0}=<{0}>  : {1}{2}'.format(*otd)
                for otd in opt_types_desc if otd[0] not in UNSUPPORTED_OPTS)
    d = """"""Usage:
  tqdm [--help | options]
Options:
  -h, --help     Print this help and exit
  -v, --version  Print version and exit
"""""" + d.strip('\n') + '\n'
    # opts = docopt(d, version=__version__)
    if any(v in sys.argv for v in ('-v', '--version')):
        sys.stdout.write(__version__ + '\n')
        sys.exit(0)
    elif any(v in sys.argv for v in ('-h', '--help')):
        sys.stdout.write(d + '\n')
        sys.exit(0)
    argv = RE_SHLEX.split(' '.join([""tqdm""] + sys.argv[1:]))
    opts = dict(zip(argv[1::2], argv[2::2]))
    log.debug(opts)
    opts.pop('log', True)
    tqdm_args = {'file': fp}
    try:
        for (o, v) in opts.items():
            try:
                tqdm_args[o] = cast(v, opt_types[o])
            except KeyError as e:
                raise TqdmKeyError(str(e))
        log.debug('args:' + str(tqdm_args))
    except:
        fp.write('\nError:\nUsage:\n  tqdm [--help | options]\n')
        for i in sys.stdin:
            sys.stdout.write(i)
        raise
    else:
        buf_size = tqdm_args.pop('buf_size', 256)
        delim = tqdm_args.pop('delim', '\n')
        delim_per_char = tqdm_args.pop('bytes', False)
        if delim_per_char:
            tqdm_args.setdefault('unit', 'B')
            tqdm_args.setdefault('unit_scale', True)
            tqdm_args.setdefault('unit_divisor', 1024)
            log.debug(tqdm_args)
            with tqdm(**tqdm_args) as t:
                posix_pipe(sys.stdin, sys.stdout,
                           '', buf_size, t.update)
        elif delim == '\n':
            log.debug(tqdm_args)
            for i in tqdm(sys.stdin, **tqdm_args):
                sys.stdout.write(i)
        else:
            log.debug(tqdm_args)
            with tqdm(**tqdm_args) as t:
                posix_pipe(sys.stdin, sys.stdout,",[12]
"        if hasattr(select, ""kqueue""):
            # Python 2.6+ on BSD or Mac
            from tornado.platform.kqueue import KQueueIOLoop
            return KQueueIOLoop
        from tornado.platform.select import SelectIOLoop
        return SelectIOLoop
    def initialize(self, make_current=None):
        if make_current is None:
            if IOLoop.current(instance=False) is None:
                self.make_current()
        elif make_current:
            if IOLoop.current(instance=False) is None:
                raise RuntimeError(""current IOLoop already exists"")
            self.make_current()
    def close(self, all_fds=False):
        """"""Closes the `IOLoop`, freeing any resources used.
        If ``all_fds`` is true, all file descriptors registered on the
        IOLoop will be closed (not just the ones created by the
        `IOLoop` itself).
        Many applications will only use a single `IOLoop` that runs for the
        entire lifetime of the process.  In that case closing the `IOLoop`
        is not necessary since everything will be cleaned up when the
        process exits.  `IOLoop.close` is provided mainly for scenarios
        such as unit tests, which create and destroy a large number of
        ``IOLoops``.
        An `IOLoop` must be completely stopped before it can be closed.  This
        means that `IOLoop.stop()` must be called *and* `IOLoop.start()` must
        be allowed to return before attempting to call `IOLoop.close()`.
        Therefore the call to `close` will usually appear just after
        the call to `start` rather than near the call to `stop`.
        .. versionchanged:: 3.1
           If the `IOLoop` implementation supports non-integer objects
           for ""file descriptors"", those objects will have their
           ``close`` method when ``all_fds`` is true.
        """"""
        raise NotImplementedError()
    def add_handler(self, fd, handler, events):
        """"""Registers the given handler to receive the given events for ``fd``.
        The ``fd`` argument may either be an integer file descriptor or
        a file-like object with a ``fileno()`` method (and optionally a
        ``close()`` method, which may be called when the `IOLoop` is shut
        down).
        The ``events`` argument is a bitwise or of the constants
        ``IOLoop.READ``, ``IOLoop.WRITE``, and ``IOLoop.ERROR``.
        When an event occurs, ``handler(fd, events)`` will be run.
        .. versionchanged:: 4.0
           Added the ability to pass file-like objects in addition to
           raw file descriptors.
        """"""
        raise NotImplementedError()
    def update_handler(self, fd, events):
        """"""Changes the events we listen for ``fd``.
        .. versionchanged:: 4.0
           Added the ability to pass file-like objects in addition to
           raw file descriptors.
        """"""
        raise NotImplementedError()
    def remove_handler(self, fd):
        """"""Stop listening for events on ``fd``.
        .. versionchanged:: 4.0
           Added the ability to pass file-like objects in addition to
           raw file descriptors.
        """"""
        raise NotImplementedError()
    def set_blocking_signal_threshold(self, seconds, action):
        """"""Sends a signal if the `IOLoop` is blocked for more than
        ``s`` seconds.
        Pass ``seconds=None`` to disable.  Requires Python 2.6 on a unixy
        platform.
        The action parameter is a Python signal handler.  Read the
        documentation for the `signal` module for more information.
        If ``action`` is None, the process will be killed if it is
        blocked for too long.
        """"""
        raise NotImplementedError()
    def set_blocking_log_threshold(self, seconds):
        """"""Logs a stack trace if the `IOLoop` is blocked for more than
        ``s`` seconds.
        Equivalent to ``set_blocking_signal_threshold(seconds,
        self.log_stack)``
        """"""
        self.set_blocking_signal_threshold(seconds, self.log_stack)
    def log_stack(self, signal, frame):
        """"""Signal handler to log the stack trace of the current thread.
        For use with `set_blocking_signal_threshold`.
        """"""
        gen_log.warning('IOLoop blocked for %f seconds in\n%s',
                        self._blocking_signal_threshold,
                        ''.join(traceback.format_stack(frame)))
    def start(self):
        """"""Starts the I/O loop.
        The loop will run until one of the callbacks calls `stop()`, which
        will make the loop stop after the current event iteration completes.
        """"""
        raise NotImplementedError()
    def _setup_logging(self):
        """"""The IOLoop catches and logs exceptions, so it's
        important that log output be visible.  However, python's
        default behavior for non-root loggers (prior to python
        3.2) is to print an unhelpful ""no handlers could be
        found"" message rather than the actual log entry, so we
        must explicitly configure logging if we've made it this",[12]
"    from pandas import DataFrame
    df = DataFrame(data, index=common_idx)
    if values is None:
        df[""__dummy__""] = 0
        kwargs = {""aggfunc"": len, ""fill_value"": 0}
    else:
        df[""__dummy__""] = values
        kwargs = {""aggfunc"": aggfunc}
    table = df.pivot_table(
        ""__dummy__"",
        index=rownames,
        columns=colnames,
        margins=margins,
        margins_name=margins_name,
        dropna=dropna,
        **kwargs,
    )
    # Post-process
    if normalize is not False:
        table = _normalize(
            table, normalize=normalize, margins=margins, margins_name=margins_name
        )
    return table

def _normalize(table, normalize, margins: bool, margins_name=""All""):
    if not isinstance(normalize, (bool, str)):
        axis_subs = {0: ""index"", 1: ""columns""}
        try:
            normalize = axis_subs[normalize]
        except KeyError:
            raise ValueError(""Not a valid normalize argument"")
    if margins is False:
        # Actual Normalizations
        normalizers: Dict[Union[bool, str], Callable] = {
            ""all"": lambda x: x / x.sum(axis=1).sum(axis=0),
            ""columns"": lambda x: x / x.sum(),
            ""index"": lambda x: x.div(x.sum(axis=1), axis=0),
        }
        normalizers[True] = normalizers[""all""]
        try:
            f = normalizers[normalize]
        except KeyError:
            raise ValueError(""Not a valid normalize argument"")
        table = f(table)
        table = table.fillna(0)
    elif margins is True:
        # keep index and column of pivoted table
        table_index = table.index
        table_columns = table.columns
        # check if margin name is in (for MI cases) or equal to last
        # index/column and save the column and index margin
        if (margins_name not in table.iloc[-1, :].name) | (
            margins_name != table.iloc[:, -1].name
        ):
            raise ValueError(
                ""{mname} not in pivoted DataFrame"".format(mname=margins_name)
            )
        column_margin = table.iloc[:-1, -1]
        index_margin = table.iloc[-1, :-1]
        # keep the core table
        table = table.iloc[:-1, :-1]
        # Normalize core
        table = _normalize(table, normalize=normalize, margins=False)
        # Fix Margins
        if normalize == ""columns"":
            column_margin = column_margin / column_margin.sum()
            table = concat([table, column_margin], axis=1)
            table = table.fillna(0)
            table.columns = table_columns
        elif normalize == ""index"":
            index_margin = index_margin / index_margin.sum()
            table = table.append(index_margin)
            table = table.fillna(0)
            table.index = table_index
        elif normalize == ""all"" or normalize is True:
            column_margin = column_margin / column_margin.sum()
            index_margin = index_margin / index_margin.sum()
            index_margin.loc[margins_name] = 1
            table = concat([table, column_margin], axis=1)
            table = table.append(index_margin)
            table = table.fillna(0)
            table.index = table_index
            table.columns = table_columns
        else:
            raise ValueError(""Not a valid normalize argument"")
    else:
        raise ValueError(""Not a valid margins argument"")
    return table

def _get_names(arrs, names, prefix: str = ""row""):
    if names is None:
        names = []
        for i, arr in enumerate(arrs):
            if isinstance(arr, ABCSeries) and arr.name is not None:
                names.append(arr.name)
            else:
                names.append(""{prefix}_{i}"".format(prefix=prefix, i=i))
    else:
        if len(names) != len(arrs):
            raise AssertionError(""arrays and names must have the same length"")
        if not isinstance(names, list):
            names = list(names)
",[11]
"# Copyright 2019 Ram Rachum.
# This program is distributed under the MIT license.
import sys
import os
import inspect
import types
import datetime as datetime_module
import re
import collections
import decorator
from . import utils
from . import pycompat
from .tracer import Tracer

def get_write_function(output):
    if output is None:
        def write(s):
            stderr = sys.stderr
            stderr.write(s)
    elif isinstance(output, (pycompat.PathLike, str)):
        def write(s):
            with open(output_path, 'a') as output_file:
                output_file.write(s)
    else:
        assert isinstance(output, utils.WritableStream)
        def write(s):
            output.write(s)
    return write

def snoop(output=None, variables=(), depth=1, prefix=''):
    '''
    Snoop on the function, writing everything it's doing to stderr.
    This is useful for debugging.
    When you decorate a function with `@pysnooper.snoop()`, you'll get a log of
    every line that ran in the function and a play-by-play of every local
    variable that changed.
    If stderr is not easily accessible for you, you can redirect the output to
    a file::
        @pysnooper.snoop('/my/log/file.log')
    See values of some variables that aren't local variables::
        @pysnooper.snoop(variables=('foo.bar', 'self.whatever'))
    Show snoop lines for functions that your function calls::
        @pysnooper.snoop(depth=2)
    Start all snoop lines with a prefix, to grep for them easily::
        @pysnooper.snoop(prefix='ZZZ ')
    '''
    write = get_write_function(output)
    @decorator.decorator
    def decorate(function, *args, **kwargs):
        target_code_object = function.__code__
        with Tracer(target_code_object=target_code_object,
                    write=write, variables=variables,
                    depth=depth, prefix=prefix):
            return function(*args, **kwargs)
    return decorate
",[25]
"""""""Helper functions for working with signals""""""
import logging
from twisted.internet.defer import maybeDeferred, DeferredList, Deferred
from twisted.python.failure import Failure
from scrapy.xlib.pydispatch.dispatcher import Any, Anonymous, liveReceivers, \
    getAllReceivers, disconnect
from scrapy.xlib.pydispatch.robustapply import robustApply
logger = logging.getLogger(__name__)

def send_catch_log(signal=Any, sender=Anonymous, *arguments, **named):
    """"""Like pydispatcher.robust.sendRobust but it also logs errors and returns
    Failures instead of exceptions.
    """"""
    dont_log = named.pop('dont_log', None)
    spider = named.get('spider', None)
    responses = []
    for receiver in liveReceivers(getAllReceivers(sender, signal)):
        try:
            response = robustApply(receiver, signal=signal, sender=sender,
                *arguments, **named)
            if isinstance(response, Deferred):
                logger.error(""Cannot return deferreds from signal handler: %(receiver)s"",
                             {'receiver': receiver}, extra={'spider': spider})
        except dont_log:
            result = Failure()
        except Exception:
            result = Failure()
            logger.error(""Error caught on signal handler: %(receiver)s"",
                         {'receiver': receiver},
                         exc_info=True, extra={'spider': spider})
        else:
            result = response
        responses.append((receiver, result))
    return responses
def send_catch_log_deferred(signal=Any, sender=Anonymous, *arguments, **named):
    """"""Like send_catch_log but supports returning deferreds on signal handlers.
    Returns a deferred that gets fired once all signal handlers deferreds were
    fired.
    """"""
    def logerror(failure, recv):
        if dont_log is None or not isinstance(failure.value, dont_log):
            logger.error(""Error caught on signal handler: %(receiver)s"",
                         {'receiver': recv},
                         extra={'spider': spider, 'failure': failure})
        return failure
    dont_log = named.pop('dont_log', None)
    spider = named.get('spider', None)
    dfds = []
    for receiver in liveReceivers(getAllReceivers(sender, signal)):
        d = maybeDeferred(robustApply, receiver, signal=signal, sender=sender,
                *arguments, **named)
        d.addErrback(logerror, receiver)
        d.addBoth(lambda result: (receiver, result))
        dfds.append(d)
    d = DeferredList(dfds)
    d.addCallback(lambda out: [x[1] for x in out])
    return d
def disconnect_all(signal=Any, sender=Any):
    """"""Disconnect all signal handlers. Useful for cleaning up after running
    tests
    """"""
    for receiver in liveReceivers(getAllReceivers(sender, signal)):
        disconnect(receiver, signal=signal, sender=sender)",[49]
"# coding: utf-8
from __future__ import unicode_literals
from .common import InfoExtractor
from ..compat import (
    compat_urllib_parse,
    compat_urllib_request,
)
from ..utils import (
    int_or_none,
    parse_filesize,
)

class MinhatecaIE(InfoExtractor):
    _VALID_URL = r'https?://minhateca\.com\.br/[^?#]+,(?P<id>[0-9]+)\.'
    _TEST = {
        'url': 'http://minhateca.com.br/pereba/misc/youtube-dl+test+video,125848331.mp4(video)',
        'info_dict': {
            'id': '125848331',
            'ext': 'mp4',
            'title': 'youtube-dl test video',
            'thumbnail': 're:^https?://.*\.jpg$',
            'filesize_approx': 1530000,
            'duration': 9,
            'view_count': int,
        }
    }
    def _real_extract(self, url):
        video_id = self._match_id(url)
        webpage = self._download_webpage(url, video_id)
        token = self._html_search_regex(
            r'<input name=""__RequestVerificationToken"".*?value=""([^""]+)""',
            webpage, 'request token')
        token_data = [
            ('fileId', video_id),
            ('__RequestVerificationToken', token),
        ]
        req = compat_urllib_request.Request(
            'http://minhateca.com.br/action/License/Download',
            data=compat_urllib_parse.urlencode(token_data))
        req.add_header('Content-Type', 'application/x-www-form-urlencoded')
        data = self._download_json(
            req, video_id, note='Downloading metadata')
        video_url = data['redirectUrl']
        title_str = self._html_search_regex(
            r'<h1.*?>(.*?)</h1>', webpage, 'title')
        title, _, ext = title_str.rpartition('.')
        filesize_approx = parse_filesize(self._html_search_regex(
            r'<p class=""fileSize"">(.*?)</p>',
            webpage, 'file size approximation', fatal=False))
        duration = int_or_none(self._html_search_regex(
            r'(?s)<p class=""fileLeng[ht][th]"">.*?([0-9]+)\s*s',
            webpage, 'duration', fatal=False))
        view_count = int_or_none(self._html_search_regex(
            r'<p class=""downloadsCounter"">([0-9]+)</p>',
            webpage, 'view count', fatal=False))
        return {
            'id': video_id,
            'url': video_url,
            'title': title,
            'ext': ext,
            'filesize_approx': filesize_approx,
            'duration': duration,
            'view_count': view_count,
            'thumbnail': self._og_search_thumbnail(webpage),
        }","[54, 55]"
"        path = self.file_path(request, info=info)
        dfd = defer.maybeDeferred(self.store.stat_file, path, info)
        dfd.addCallbacks(_onsuccess, lambda _: None)
        dfd.addErrback(
            lambda f:
            logger.error(self.__class__.__name__ + '.store.stat_file',
                         extra={'spider': info.spider, 'failure': f})
        )
        return dfd
    def media_failed(self, failure, request, info):
        if not isinstance(failure.value, IgnoreRequest):
            referer = request.headers.get('Referer')
            logger.warning(
                'File (unknown-error): Error downloading %(medianame)s from '
                '%(request)s referred in <%(referer)s>: %(exception)s',
                {'medianame': self.MEDIA_NAME, 'request': request,
                 'referer': referer, 'exception': failure.value},
                extra={'spider': info.spider}
            )
        raise FileException
    def media_downloaded(self, response, request, info):
        referer = request.headers.get('Referer')
        if response.status != 200:
            logger.warning(
                'File (code: %(status)s): Error downloading file from '
                '%(request)s referred in <%(referer)s>',
                {'status': response.status,
                 'request': request, 'referer': referer},
                extra={'spider': info.spider}
            )
            raise FileException('download-error')
        if not response.body:
            logger.warning(
                'File (empty-content): Empty file from %(request)s referred '
                'in <%(referer)s>: no-content',
                {'request': request, 'referer': referer},
                extra={'spider': info.spider}
            )
            raise FileException('empty-content')
        status = 'cached' if 'cached' in response.flags else 'downloaded'
        logger.debug(
            'File (%(status)s): Downloaded file from %(request)s referred in '
            '<%(referer)s>',
            {'status': status, 'request': request, 'referer': referer},
            extra={'spider': info.spider}
        )
        self.inc_stats(info.spider, status)
        try:
            path = self.file_path(request, response=response, info=info)
            checksum = self.file_downloaded(response, request, info)
        except FileException as exc:
            logger.warning(
                'File (error): Error processing file from %(request)s '
                'referred in <%(referer)s>: %(errormsg)s',
                {'request': request, 'referer': referer, 'errormsg': str(exc)},
                extra={'spider': info.spider}, exc_info=True
            )
            raise
        except Exception as exc:
            logger.error(
                'File (unknown-error): Error processing file from %(request)s '
                'referred in <%(referer)s>',
                {'request': request, 'referer': referer},
                exc_info=True, extra={'spider': info.spider}
            )
            raise FileException(str(exc))
        return {'url': request.url, 'path': path, 'checksum': checksum}
    def inc_stats(self, spider, status):
        spider.crawler.stats.inc_value('file_count', spider=spider)
        spider.crawler.stats.inc_value('file_status_count/%s' % status, spider=spider)
    ### Overridable Interface
    def get_media_requests(self, item, info):
        return [Request(x) for x in item.get(self.FILES_URLS_FIELD, [])]
    def file_downloaded(self, response, request, info):
        path = self.file_path(request, response=response, info=info)
        buf = BytesIO(response.body)
        self.store.persist_file(path, buf, info)
        checksum = md5sum(buf)
        return checksum
    def item_completed(self, results, item, info):
        if isinstance(item, dict) or self.FILES_RESULT_FIELD in item.fields:
            item[self.FILES_RESULT_FIELD] = [x for ok, x in results if ok]
        return item
    def file_path(self, request, response=None, info=None):
        ## start of deprecation warning block (can be removed in the future)
        def _warn():
            from scrapy.exceptions import ScrapyDeprecationWarning
            import warnings
            warnings.warn('FilesPipeline.file_key(url) method is deprecated, please use '
                          'file_path(request, response=None, info=None) instead',
                          category=ScrapyDeprecationWarning, stacklevel=1)
        # check if called from file_key with url as first argument
        if not isinstance(request, Request):
            _warn()
            url = request
        else:
            url = request.url
        # detect if file_key() method has been overridden
        if not hasattr(self.file_key, '_base'):
            _warn()
            return self.file_key(url)
        ## end of deprecation warning block
        media_guid = hashlib.sha1(url).hexdigest()  # change to request.url after deprecation
        media_ext = os.path.splitext(url)[1]  # change to request.url after deprecation
        return 'full/%s%s' % (media_guid, media_ext)
    # deprecated
    def file_key(self, url):
        return self.file_path(url)",[7]
"import re
from .common import InfoExtractor
from ..utils import (
    find_xpath_attr,
    fix_xml_all_ampersand,
)

class ClipsyndicateIE(InfoExtractor):
    _VALID_URL = r'http://www\.clipsyndicate\.com/video/play(list/\d+)?/(?P<id>\d+)'
    _TEST = {
        u'url': u'http://www.clipsyndicate.com/video/play/4629301/brick_briscoe',
        u'md5': u'4d7d549451bad625e0ff3d7bd56d776c',
        u'info_dict': {
            u'id': u'4629301',
            u'ext': u'mp4',
            u'title': u'Brick Briscoe',
            u'duration': 612,
        },
    }
    def _real_extract(self, url):
        mobj = re.match(self._VALID_URL, url)
        video_id = mobj.group('id')
        js_player = self._download_webpage(
            'http://eplayer.clipsyndicate.com/embed/player.js?va_id=%s' % video_id,
            video_id, u'Downlaoding player')
        # it includes a required token
        flvars = self._search_regex(r'flvars: ""(.*?)""', js_player, u'flvars')
        pdoc = self._download_xml(
            'http://eplayer.clipsyndicate.com/osmf/playlist?%s' % flvars,
            video_id, u'Downloading video info',
            transform_source=fix_xml_all_ampersand) 
        track_doc = pdoc.find('trackList/track')
        def find_param(name):
            node = find_xpath_attr(track_doc, './/param', 'name', name)
            if node is not None:
                return node.attrib['value']
        return {
            'id': video_id,
            'title': find_param('title'),
            'url': track_doc.find('location').text,
            'thumbnail': find_param('thumbnail'),
            'duration': int(find_param('duration')),
        }","[5, 35]"
"        before, after = text.split(""e"")
        sign = """"
        if after.startswith(""-""):
            after = after[1:]
            sign = ""-""
        elif after.startswith(""+""):
            after = after[1:]
        before = format_float_or_int_string(before)
        text = f""{before}e{sign}{after}""
    elif text.endswith((""j"", ""l"")):
        number = text[:-1]
        suffix = text[-1]
        # Capitalize in ""2L"" because ""l"" looks too similar to ""1"".
        if suffix == ""l"":
            suffix = ""L""
        text = f""{format_float_or_int_string(number)}{suffix}""
    else:
        text = format_float_or_int_string(text)
    leaf.value = text

def format_float_or_int_string(text: str) -> str:
    """"""Formats a float string like ""1.0"".""""""
    if ""."" not in text:
        return text
    before, after = text.split(""."")
    return f""{before or 0}.{after or 0}""

def normalize_invisible_parens(node: Node, parens_after: Set[str]) -> None:
    """"""Make existing optional parentheses invisible or create new ones.
    `parens_after` is a set of string leaf values immeditely after which parens
    should be put.
    Standardizes on visible parentheses for single-element tuples, and keeps
    existing visible parentheses for other tuples and generator expressions.
    """"""
    for pc in list_comments(node.prefix, is_endmarker=False):
        if pc.value in FMT_OFF:
            # This `node` has a prefix with `# fmt: off`, don't mess with parens.
            return
    check_lpar = False
    for index, child in enumerate(list(node.children)):
        if check_lpar:
            if child.type == syms.atom:
                if maybe_make_parens_invisible_in_atom(child, parent=node):
                    lpar = Leaf(token.LPAR, """")
                    rpar = Leaf(token.RPAR, """")
                    index = child.remove() or 0
                    node.insert_child(index, Node(syms.atom, [lpar, child, rpar]))
            elif is_one_tuple(child):
                # wrap child in visible parentheses
                lpar = Leaf(token.LPAR, ""("")
                rpar = Leaf(token.RPAR, "")"")
                child.remove()
                node.insert_child(index, Node(syms.atom, [lpar, child, rpar]))
            elif node.type == syms.import_from:
                # ""import from"" nodes store parentheses directly as part of
                # the statement
                if child.type == token.LPAR:
                    # make parentheses invisible
                    child.value = """"  # type: ignore
                    node.children[-1].value = """"  # type: ignore
                elif child.type != token.STAR:
                    # insert invisible parentheses
                    node.insert_child(index, Leaf(token.LPAR, """"))
                    node.append_child(Leaf(token.RPAR, """"))
                break
            elif not (isinstance(child, Leaf) and is_multiline_string(child)):
                # wrap child in invisible parentheses
                lpar = Leaf(token.LPAR, """")
                rpar = Leaf(token.RPAR, """")
                index = child.remove() or 0
                node.insert_child(index, Node(syms.atom, [lpar, child, rpar]))
        check_lpar = isinstance(child, Leaf) and child.value in parens_after

def normalize_fmt_off(node: Node) -> None:
    """"""Convert content between `# fmt: off`/`# fmt: on` into standalone comments.""""""
    try_again = True
    while try_again:
        try_again = convert_one_fmt_off_pair(node)

def convert_one_fmt_off_pair(node: Node) -> bool:
    """"""Convert content of a single `# fmt: off`/`# fmt: on` into a standalone comment.
    Returns True if a pair was converted.
    """"""
    for leaf in node.leaves():
        previous_consumed = 0
        for comment in list_comments(leaf.prefix, is_endmarker=False):
            if comment.value in FMT_OFF:
                # We only want standalone comments. If there's no previous leaf or
                # the previous leaf is indentation, it's a standalone comment in
                # disguise.
                if comment.type != STANDALONE_COMMENT:
                    prev = preceding_leaf(leaf)
                    if prev and prev.type not in WHITESPACE:
                        continue
                ignored_nodes = list(generate_ignored_nodes(leaf))
                if not ignored_nodes:
                    continue
                first = ignored_nodes[0]  # Can be a container node with the `leaf`.
                parent = first.parent
                prefix = first.prefix
                first.prefix = prefix[comment.consumed :]
                hidden_value = (
                    comment.value + ""\n"" + """".join(str(n) for n in ignored_nodes)
                )
                if hidden_value.endswith(""\n""):
                    # That happens when one of the `ignored_nodes` ended with a NEWLINE
                    # leaf (possibly followed by a DEDENT).
                    hidden_value = hidden_value[:-1]
                first_idx = None
                for ignored in ignored_nodes:
                    index = ignored.remove()
                    if first_idx is None:
                        first_idx = index
                assert parent is not None, ""INTERNAL ERROR: fmt: on/off handling (1)""",[77]
"# encoding: utf-8
from __future__ import unicode_literals
import re
from .common import InfoExtractor
from ..utils import (
    ExtractorError,
    find_xpath_attr,
    unified_strdate,
    determine_ext,
    get_element_by_id,
    compat_str,
    get_element_by_attribute,
)
# There are different sources of video in arte.tv, the extraction process 
# is different for each one. The videos usually expire in 7 days, so we can't
# add tests.

class ArteTvIE(InfoExtractor):
    _VALID_URL = r'http://videos\.arte\.tv/(?P<lang>fr|de)/.*-(?P<id>.*?)\.html'
    IE_NAME = 'arte.tv'
    def _real_extract(self, url):
        mobj = re.match(self._VALID_URL, url)
        lang = mobj.group('lang')
        video_id = mobj.group('id')
        ref_xml_url = url.replace('/videos/', '/do_delegate/videos/')
        ref_xml_url = ref_xml_url.replace('.html', ',view,asPlayerXml.xml')
        ref_xml_doc = self._download_xml(
            ref_xml_url, video_id, note='Downloading metadata')
        config_node = find_xpath_attr(ref_xml_doc, './/video', 'lang', lang)
        config_xml_url = config_node.attrib['ref']
        config = self._download_xml(
            config_xml_url, video_id, note='Downloading configuration')
        formats = [{
            'forma_id': q.attrib['quality'],
            # The playpath starts at 'mp4:', if we don't manually
            # split the url, rtmpdump will incorrectly parse them
            'url': q.text.split('mp4:', 1)[0],
            'play_path': 'mp4:' + q.text.split('mp4:', 1)[1],
            'ext': 'flv',
            'quality': 2 if q.attrib['quality'] == 'hd' else 1,
        } for q in config.findall('./urls/url')]
        self._sort_formats(formats)
        title = config.find('.//name').text
        thumbnail = config.find('.//firstThumbnailUrl').text
        return {
            'id': video_id,
            'title': title,
            'thumbnail': thumbnail,
            'formats': formats,
        }

class ArteTVPlus7IE(InfoExtractor):
    IE_NAME = 'arte.tv:+7'
    _VALID_URL = r'https?://(?:www\.)?arte\.tv/guide/(?P<lang>fr|de)/(?:(?:sendungen|emissions)/)?(?P<id>.*?)/(?P<name>.*?)(\?.*)?'
    @classmethod
    def _extract_url_info(cls, url):
        mobj = re.match(cls._VALID_URL, url)
        lang = mobj.group('lang')
        # This is not a real id, it can be for example AJT for the news
        # http://www.arte.tv/guide/fr/emissions/AJT/arte-journal
        video_id = mobj.group('id')
        return video_id, lang
    def _real_extract(self, url):
        video_id, lang = self._extract_url_info(url)
        webpage = self._download_webpage(url, video_id)
        return self._extract_from_webpage(webpage, video_id, lang)
    def _extract_from_webpage(self, webpage, video_id, lang):
        json_url = self._html_search_regex(
            [r'arte_vp_url=[""\'](.*?)[""\']', r'data-url=[""\']([^""]+)[""\']'],
            webpage, 'json vp url')
        return self._extract_from_json_url(json_url, video_id, lang)
    def _extract_from_json_url(self, json_url, video_id, lang):
        info = self._download_json(json_url, video_id)
        player_info = info['videoJsonPlayer']
        info_dict = {
            'id': player_info['VID'],
            'title': player_info['VTI'],
            'description': player_info.get('VDE'),
            'upload_date': unified_strdate(player_info.get('VDA', '').split(' ')[0]),
            'thumbnail': player_info.get('programImage') or player_info.get('VTU', {}).get('IUR'),
        }
        all_formats = player_info['VSR'].values()
        # Some formats use the m3u8 protocol
        all_formats = list(filter(lambda f: f.get('videoFormat') != 'M3U8', all_formats))
        def _match_lang(f):
            if f.get('versionCode') is None:
                return True
            # Return true if that format is in the language of the url
            if lang == 'fr':
                l = 'F'
            elif lang == 'de':
                l = 'A'
            else:
                l = lang
            regexes = [r'VO?%s' % l, r'VO?.-ST%s' % l]
            return any(re.match(r, f['versionCode']) for r in regexes)
        # Some formats may not be in the same language as the url
        # TODO: Might want not to drop videos that does not match requested language
        # but to process those formats with lower precedence
        formats = filter(_match_lang, all_formats)
        formats = list(formats)  # in python3 filter returns an iterator
        if not formats:
            # Some videos are only available in the 'Originalversion'
            # they aren't tagged as being in French or German
            # Sometimes there are neither videos of requested lang code
            # nor original version videos available
            # For such cases we just take all_formats as is
            formats = all_formats
            if not formats:
                raise ExtractorError('The formats list is empty')
        if re.match(r'[A-Z]Q', formats[0]['quality']) is not None:",[92]
"    Extract NER tags if available and convert them so that they follow
    BILUO and the Wikipedia scheme
    """"""
    # by @dvsrepo, via #11 explosion/spacy-dev-resources
    # by @katarkor
    docs = []
    sentences = []
    conll_tuples = read_conllx(input_data, use_morphology=use_morphology)
    checked_for_ner = False
    has_ner_tags = False
    for i, (raw_text, tokens) in enumerate(conll_tuples):
        sentence, brackets = tokens[0]
        if not checked_for_ner:
            has_ner_tags = is_ner(sentence[5][0])
            checked_for_ner = True
        sentences.append(generate_sentence(sentence, has_ner_tags))
        # Real-sized documents could be extracted using the comments on the
        # conluu document
        if len(sentences) % n_sents == 0:
            doc = create_doc(sentences, i)
            docs.append(doc)
            sentences = []
    if sentences:
        doc = create_doc(sentences, i)
        docs.append(doc)
    return docs

def is_ner(tag):
    """"""
    Check the 10th column of the first token to determine if the file contains
    NER tags
    """"""
    tag_match = re.match(""([A-Z_]+)-([A-Z_]+)"", tag)
    if tag_match:
        return True
    elif tag == ""O"":
        return True
    else:
        return False

def read_conllx(input_data, use_morphology=False, n=0):
    i = 0
    for sent in input_data.strip().split(""\n\n""):
        lines = sent.strip().split(""\n"")
        if lines:
            while lines[0].startswith(""#""):
                lines.pop(0)
            tokens = []
            for line in lines:
                parts = line.split(""\t"")
                id_, word, lemma, pos, tag, morph, head, dep, _1, iob = parts
                if ""-"" in id_ or ""."" in id_:
                    continue
                try:
                    id_ = int(id_) - 1
                    head = (int(head) - 1) if head != ""0"" else id_
                    dep = ""ROOT"" if dep == ""root"" else dep
                    tag = pos if tag == ""_"" else tag
                    tag = tag + ""__"" + morph if use_morphology else tag
                    iob = iob if iob else ""O""
                    tokens.append((id_, word, tag, head, dep, iob))
                except:  # noqa: E722
                    print(line)
                    raise
            tuples = [list(t) for t in zip(*tokens)]
            yield (None, [[tuples, []]])
            i += 1
            if n >= 1 and i >= n:
                break

def simplify_tags(iob):
    """"""
    Simplify tags obtained from the dataset in order to follow Wikipedia
    scheme (PER, LOC, ORG, MISC). 'PER', 'LOC' and 'ORG' keep their tags, while
    'GPE_LOC' is simplified to 'LOC', 'GPE_ORG' to 'ORG' and all remaining tags to
    'MISC'.
    """"""
    new_iob = []
    for tag in iob:
        tag_match = re.match(""([A-Z_]+)-([A-Z_]+)"", tag)
        if tag_match:
            prefix = tag_match.group(1)
            suffix = tag_match.group(2)
            if suffix == ""GPE_LOC"":
                suffix = ""LOC""
            elif suffix == ""GPE_ORG"":
                suffix = ""ORG""
            elif suffix != ""PER"" and suffix != ""LOC"" and suffix != ""ORG"":
                suffix = ""MISC""
            tag = prefix + ""-"" + suffix
        new_iob.append(tag)
    return new_iob

def generate_sentence(sent, has_ner_tags):
    (id_, word, tag, head, dep, iob) = sent
    sentence = {}
    tokens = []
    if has_ner_tags:
        iob = simplify_tags(iob)
        biluo = iob_to_biluo(iob)
    for i, id in enumerate(id_):
        token = {}
        token[""id""] = id
        token[""orth""] = word[i]
        token[""tag""] = tag[i]
        token[""head""] = head[i] - id
        token[""dep""] = dep[i]
        if has_ner_tags:
            token[""ner""] = biluo[i]
        tokens.append(token)
    sentence[""tokens""] = tokens
    return sentence

def create_doc(sentences, id):
    doc = {}
    paragraph = {}
    doc[""id""] = id
    doc[""paragraphs""] = []
    paragraph[""sentences""] = sentences
    doc[""paragraphs""].append(paragraph)",[58]
"    write_back: WriteBack,
    report: ""Report"",
    loop: BaseEventLoop,
    executor: Executor,
) -> None:
    """"""Run formatting of `sources` in parallel using the provided `executor`.
    (Use ProcessPoolExecutors for actual parallelism.)
    `line_length`, `write_back`, and `fast` options are passed to
    :func:`format_file_in_place`.
    """"""
    cache: Cache = {}
    if write_back != WriteBack.DIFF:
        cache = read_cache()
        sources, cached = filter_cached(cache, sources)
        for src in cached:
            report.done(src, Changed.CACHED)
    cancelled = []
    formatted = []
    if sources:
        lock = None
        if write_back == WriteBack.DIFF:
            # For diff output, we need locks to ensure we don't interleave output
            # from different processes.
            manager = Manager()
            lock = manager.Lock()
        tasks = {
            src: loop.run_in_executor(
                executor, format_file_in_place, src, line_length, fast, write_back, lock
            )
            for src in sources
        }
        _task_values = list(tasks.values())
        try:
            loop.add_signal_handler(signal.SIGINT, cancel, _task_values)
            loop.add_signal_handler(signal.SIGTERM, cancel, _task_values)
        except NotImplementedError:
            # There are no good alternatives for these on Windows
            pass
        await asyncio.wait(_task_values)
        for src, task in tasks.items():
            if not task.done():
                report.failed(src, ""timed out, cancelling"")
                task.cancel()
                cancelled.append(task)
            elif task.cancelled():
                cancelled.append(task)
            elif task.exception():
                report.failed(src, str(task.exception()))
            else:
                formatted.append(src)
                report.done(src, Changed.YES if task.result() else Changed.NO)
    if cancelled:
        await asyncio.gather(*cancelled, loop=loop, return_exceptions=True)
    if write_back != WriteBack.DIFF and formatted:
        write_cache(cache, formatted)

def format_file_in_place(
    src: Path,
    line_length: int,
    fast: bool,
    write_back: WriteBack = WriteBack.NO,
    lock: Any = None,  # multiprocessing.Manager().Lock() is some crazy proxy
) -> bool:
    """"""Format file under `src` path. Return True if changed.
    If `write_back` is True, write reformatted code back to stdout.
    `line_length` and `fast` options are passed to :func:`format_file_contents`.
    """"""
    with tokenize.open(src) as src_buffer:
        src_contents = src_buffer.read()
    try:
        dst_contents = format_file_contents(
            src_contents, line_length=line_length, fast=fast
        )
    except NothingChanged:
        return False
    if write_back == write_back.YES:
        with open(src, ""w"", encoding=src_buffer.encoding) as f:
            f.write(dst_contents)
    elif write_back == write_back.DIFF:
        src_name = f""{src.name}  (original)""
        dst_name = f""{src.name}  (formatted)""
        diff_contents = diff(src_contents, dst_contents, src_name, dst_name)
        if lock:
            lock.acquire()
        try:
            sys.stdout.write(diff_contents)
        finally:
            if lock:
                lock.release()
    return True

def format_stdin_to_stdout(
    line_length: int, fast: bool, write_back: WriteBack = WriteBack.NO
) -> bool:
    """"""Format file on stdin. Return True if changed.
    If `write_back` is True, write reformatted code back to stdout.
    `line_length` and `fast` arguments are passed to :func:`format_file_contents`.
    """"""
    src = sys.stdin.read()
    dst = src
    try:
        dst = format_file_contents(src, line_length=line_length, fast=fast)
        return True
    except NothingChanged:
        return False
    finally:
        if write_back == WriteBack.YES:
            sys.stdout.write(dst)
        elif write_back == WriteBack.DIFF:
            src_name = ""<stdin>  (original)""
            dst_name = ""<stdin>  (formatted)""
            sys.stdout.write(diff(src, dst, src_name, dst_name))

def format_file_contents(
    src_contents: str, line_length: int, fast: bool","[86, 87]"
"        if leaves:
            # Since body is a new indent level, remove spurious leading whitespace.
            normalize_prefix(leaves[0], inside_brackets=True)
            # Ensure a trailing comma for imports, but be careful not to add one after
            # any comments.
            if original.is_import:
                for i in range(len(leaves) - 1, -1, -1):
                    if leaves[i].type == STANDALONE_COMMENT:
                        continue
                    elif leaves[i].type == token.COMMA:
                        break
                    else:
                        leaves.insert(i + 1, Leaf(token.COMMA, "",""))
                        break
    # Populate the line
    for leaf in leaves:
        result.append(leaf, preformatted=True)
        for comment_after in original.comments_after(leaf):
            result.append(comment_after, preformatted=True)
    if is_body:
        result.should_explode = should_explode(result, opening_bracket)
    return result

def dont_increase_indentation(split_func: SplitFunc) -> SplitFunc:
    """"""Normalize prefix of the first leaf in every line returned by `split_func`.
    This is a decorator over relevant split functions.
    """"""
    @wraps(split_func)
    def split_wrapper(line: Line, features: Collection[Feature] = ()) -> Iterator[Line]:
        for l in split_func(line, features):
            normalize_prefix(l.leaves[0], inside_brackets=True)
            yield l
    return split_wrapper

@dont_increase_indentation
def delimiter_split(line: Line, features: Collection[Feature] = ()) -> Iterator[Line]:
    """"""Split according to delimiters of the highest priority.
    If the appropriate Features are given, the split will add trailing commas
    also in function signatures and calls that contain `*` and `**`.
    """"""
    try:
        last_leaf = line.leaves[-1]
    except IndexError:
        raise CannotSplit(""Line empty"")
    bt = line.bracket_tracker
    try:
        delimiter_priority = bt.max_delimiter_priority(exclude={id(last_leaf)})
    except ValueError:
        raise CannotSplit(""No delimiters found"")
    if delimiter_priority == DOT_PRIORITY:
        if bt.delimiter_count_with_priority(delimiter_priority) == 1:
            raise CannotSplit(""Splitting a single attribute from its owner looks wrong"")
    current_line = Line(depth=line.depth, inside_brackets=line.inside_brackets)
    lowest_depth = sys.maxsize
    trailing_comma_safe = True
    def append_to_line(leaf: Leaf) -> Iterator[Line]:
        """"""Append `leaf` to current line or to new line if appending impossible.""""""
        nonlocal current_line
        try:
            current_line.append_safe(leaf, preformatted=True)
        except ValueError:
            yield current_line
            current_line = Line(depth=line.depth, inside_brackets=line.inside_brackets)
            current_line.append(leaf)
    for leaf in line.leaves:
        yield from append_to_line(leaf)
        for comment_after in line.comments_after(leaf):
            yield from append_to_line(comment_after)
        lowest_depth = min(lowest_depth, leaf.bracket_depth)
        if leaf.bracket_depth == lowest_depth:
            if is_vararg(leaf, within={syms.typedargslist}):
                trailing_comma_safe = (
                    trailing_comma_safe and Feature.TRAILING_COMMA_IN_DEF in features
                )
            elif is_vararg(leaf, within={syms.arglist, syms.argument}):
                trailing_comma_safe = (
                    trailing_comma_safe and Feature.TRAILING_COMMA_IN_CALL in features
                )
        leaf_priority = bt.delimiters.get(id(leaf))
        if leaf_priority == delimiter_priority:
            yield current_line
            current_line = Line(depth=line.depth, inside_brackets=line.inside_brackets)
    if current_line:
        if (
            trailing_comma_safe
            and delimiter_priority == COMMA_PRIORITY
            and current_line.leaves[-1].type != token.COMMA
            and current_line.leaves[-1].type != STANDALONE_COMMENT
        ):
            current_line.append(Leaf(token.COMMA, "",""))
        yield current_line

@dont_increase_indentation
def standalone_comment_split(
    line: Line, features: Collection[Feature] = ()
) -> Iterator[Line]:
    """"""Split standalone comments from the rest of the line.""""""
    if not line.contains_standalone_comments(0):
        raise CannotSplit(""Line does not have any standalone comments"")
    current_line = Line(depth=line.depth, inside_brackets=line.inside_brackets)
    def append_to_line(leaf: Leaf) -> Iterator[Line]:
        """"""Append `leaf` to current line or to new line if appending impossible.""""""
        nonlocal current_line
        try:
            current_line.append_safe(leaf, preformatted=True)
        except ValueError:
            yield current_line
",[5]
"    def __len__(self):
        return self.total if self.iterable is None else \
            (self.iterable.shape[0] if hasattr(self.iterable, ""shape"")
             else len(self.iterable) if hasattr(self.iterable, ""__len__"")
             else self.total)
    def __enter__(self):
        return self
    def __exit__(self, *exc):
        self.close()
        return False
    def __del__(self):
        self.close()
    def __repr__(self, elapsed=None):
        return self.format_meter(
            self.n, self.total,
            elapsed if elapsed is not None else self._time() - self.start_t,
            self.dynamic_ncols(self.fp) if self.dynamic_ncols else self.ncols,
            self.desc, self.ascii, self.unit,
            self.unit_scale, 1 / self.avg_time if self.avg_time else None,
            self.bar_format, self.postfix, self.unit_divisor)
    def __lt__(self, other):
        return abs(self.pos) < abs(other.pos)
    def __le__(self, other):
        return (self < other) or (self == other)
    def __eq__(self, other):
        return abs(self.pos) == abs(other.pos)
    def __ne__(self, other):
        return not (self == other)
    def __gt__(self, other):
        return not (self <= other)
    def __ge__(self, other):
        return not (self < other)
    def __hash__(self):
        return id(self)
    def __iter__(self):
        """"""Backward-compatibility to use: for x in tqdm(iterable)""""""
        # Inlining instance variables as locals (speed optimisation)
        iterable = self.iterable
        # If the bar is disabled, then just walk the iterable
        # (note: keep this check outside the loop for performance)
        if self.disable:
            for obj in iterable:
                yield obj
        else:
            mininterval = self.mininterval
            maxinterval = self.maxinterval
            miniters = self.miniters
            dynamic_miniters = self.dynamic_miniters
            last_print_t = self.last_print_t
            last_print_n = self.last_print_n
            n = self.n
            smoothing = self.smoothing
            avg_time = self.avg_time
            _time = self._time
            try:
                sp = self.sp
            except AttributeError:
                raise TqdmDeprecationWarning(""""""\
Please use `tqdm_gui(...)` instead of `tqdm(..., gui=True)`
"""""", fp_write=getattr(self.fp, 'write', sys.stderr.write))
            for obj in iterable:
                yield obj
                # Update and possibly print the progressbar.
                # Note: does not call self.update(1) for speed optimisation.
                n += 1
                # check counter first to avoid calls to time()
                if n - last_print_n >= self.miniters:
                    miniters = self.miniters  # watch monitoring thread changes
                    delta_t = _time() - last_print_t
                    if delta_t >= mininterval:
                        cur_t = _time()
                        delta_it = n - last_print_n
                        # EMA (not just overall average)
                        if smoothing and delta_t and delta_it:
                            avg_time = delta_t / delta_it \
                                if avg_time is None \
                                else smoothing * delta_t / delta_it + \
                                (1 - smoothing) * avg_time
                        self.n = n
                        with self._lock:
                            if self.pos:
                                self.moveto(abs(self.pos))
                            # Print bar update
                            sp(self.__repr__())
                            if self.pos:
                                self.moveto(-abs(self.pos))
                        # If no `miniters` was specified, adjust automatically
                        # to the max iteration rate seen so far between 2 prints
                        if dynamic_miniters:
                            if maxinterval and delta_t >= maxinterval:
                                # Adjust miniters to time interval by rule of 3
                                if mininterval:
                                    # Set miniters to correspond to mininterval
                                    miniters = delta_it * mininterval / delta_t
                                else:
                                    # Set miniters to correspond to maxinterval
                                    miniters = delta_it * maxinterval / delta_t
                            elif smoothing:
                                # EMA-weight miniters to converge
                                # towards the timeframe of mininterval
                                miniters = smoothing * delta_it * \
                                    (mininterval / delta_t
                                     if mininterval and delta_t else 1) + \
                                    (1 - smoothing) * miniters
                            else:
                                # Maximum nb of iterations between 2 prints
                                miniters = max(miniters, delta_it)
                        # Store old values for next call",[4]
"            raise ValueError(""create_table() found no columns for %r""
                             % self.table)
    def run(self):
        """"""
        If the target table doesn't exist, self.create_table
        will be called to attempt to create the table.
        """"""
        if not (self.table):
            raise Exception(""table need to be specified"")
        path = self.s3_load_path()
        output = self.output()
        connection = output.connect()
        cursor = connection.cursor()
        self.init_copy(connection)
        self.copy(cursor, path)
        self.post_copy(cursor)
        # update marker table
        output.touch(connection)
        connection.commit()
        # commit and clean up
        connection.close()
    def copy(self, cursor, f):
        """"""
        Defines copying from s3 into redshift.
        If both key-based and role-based credentials are provided, role-based will be used.
        """"""
        # format the credentials string dependent upon which type of credentials were provided
        if self.aws_account_id and self.aws_arn_role_name:
            cred_str = 'aws_iam_role=arn:aws:iam::{id}:role/{role}'.format(
                id=self.aws_account_id,
                role=self.aws_arn_role_name
            )
        elif self.aws_access_key_id and self.aws_secret_access_key:
            cred_str = 'aws_access_key_id={key};aws_secret_access_key={secret}{opt}'.format(
                key=self.aws_access_key_id,
                secret=self.aws_secret_access_key,
                opt=';token={}'.format(self.aws_session_token) if self.aws_session_token else ''
            )
        else:
            raise NotImplementedError(""Missing Credentials. ""
                                      ""Override one of the following pairs of auth-args: ""
                                      ""'aws_access_key_id' AND 'aws_secret_access_key' OR ""
                                      ""'aws_account_id' AND 'aws_arn_role_name'"")
        logger.info(""Inserting file: %s"", f)
        cursor.execute(""""""
         COPY {table} from '{source}'
         CREDENTIALS '{creds}'
         {options}
         ;"""""".format(
            table=self.table,
            source=f,
            creds=cred_str,
            options=self.copy_options)
        )
    def output(self):
        """"""
        Returns a RedshiftTarget representing the inserted dataset.
        Normally you don't override this.
        """"""
        return RedshiftTarget(
            host=self.host,
            database=self.database,
            user=self.user,
            password=self.password,
            table=self.table,
            update_id=self.update_id)
    def does_table_exist(self, connection):
        """"""
        Determine whether the table already exists.
        """"""
        if '.' in self.table:
            query = (""select 1 as table_exists ""
                     ""from information_schema.tables ""
                     ""where table_schema = %s and table_name = %s limit 1"")
        else:
            query = (""select 1 as table_exists ""
                     ""from pg_table_def ""
                     ""where tablename = %s limit 1"")
        cursor = connection.cursor()
        try:
            cursor.execute(query, tuple(self.table.split('.')))
            result = cursor.fetchone()
            return bool(result)
        finally:
            cursor.close()
    def init_copy(self, connection):
        """"""
        Perform pre-copy sql - such as creating table, truncating, or removing data older than x.
        """"""
        if not self.does_table_exist(connection):
            logger.info(""Creating table %s"", self.table)
            connection.reset()
            self.create_table(connection)
        if self.do_truncate_table:
            logger.info(""Truncating table %s"", self.table)
            self.truncate_table(connection)
        if self.do_prune():
            logger.info(""Removing %s older than %s from %s"", self.prune_column, self.prune_date, self.prune_table)
            self.prune(connection)
    def post_copy(self, cursor):
        """"""
        Performs post-copy sql - such as cleansing data, inserting into production table (if copied to temp table), etc.
        """"""
        logger.info('Executing post copy queries')
        for query in self.queries:
            cursor.execute(query)

class S3CopyJSONToTable(S3CopyToTable):
    """"""
    Template task for inserting a JSON data set into Redshift from s3.","[85, 89]"
"from thefuck.utils import replace_argument
from thefuck.specific.git import git_support

@git_support
def match(command):
    return ('push' in command.script
            and '! [rejected]' in command.stderr
            and 'failed to push some refs to' in command.stderr
            and 'Updates were rejected because the tip of your current branch is behind' in command.stderr)

@git_support
def get_new_command(command):
    return replace_argument(command.script, 'push', 'push --force')

enabled_by_default = False",[14]
"
# TODO: remove when removing support for Pydantic < 1.0.0
def warning_response_model_skip_defaults_deprecated() -> None:
    logger.warning(  # pragma: nocover
        ""response_model_skip_defaults has been deprecated in favor of ""
        ""response_model_exclude_unset to keep in line with Pydantic v1, support for ""
        ""it will be removed soon.""
    )

def get_flat_models_from_routes(routes: Sequence[BaseRoute]) -> Set[Type[BaseModel]]:
    body_fields_from_routes: List[ModelField] = []
    responses_from_routes: List[ModelField] = []
    callback_flat_models: Set[Type[BaseModel]] = set()
    for route in routes:
        if getattr(route, ""include_in_schema"", None) and isinstance(
            route, routing.APIRoute
        ):
            if route.body_field:
                assert isinstance(
                    route.body_field, ModelField
                ), ""A request body must be a Pydantic Field""
                body_fields_from_routes.append(route.body_field)
            if route.response_field:
                responses_from_routes.append(route.response_field)
            if route.response_fields:
                responses_from_routes.extend(route.response_fields.values())
            if route.callbacks:
                callback_flat_models |= get_flat_models_from_routes(route.callbacks)
    flat_models = callback_flat_models | get_flat_models_from_fields(
        body_fields_from_routes + responses_from_routes, known_models=set()
    )
    return flat_models

def get_model_definitions(
    *, flat_models: Set[Type[BaseModel]], model_name_map: Dict[Type[BaseModel], str]
) -> Dict[str, Any]:
    definitions: Dict[str, Dict] = {}
    for model in flat_models:
        m_schema, m_definitions, m_nested_models = model_process_schema(
            model, model_name_map=model_name_map, ref_prefix=REF_PREFIX
        )
        definitions.update(m_definitions)
        model_name = model_name_map[model]
        definitions[model_name] = m_schema
    return definitions

def get_path_param_names(path: str) -> Set[str]:
    return {item.strip(""{}"") for item in re.findall(""{[^}]*}"", path)}

def create_cloned_field(field: ModelField) -> ModelField:
    original_type = field.type_
    if is_dataclass(original_type) and hasattr(original_type, ""__pydantic_model__""):
        original_type = original_type.__pydantic_model__  # type: ignore
    use_type = original_type
    if lenient_issubclass(original_type, BaseModel):
        original_type = cast(Type[BaseModel], original_type)
        use_type = create_model(
            original_type.__name__, __config__=original_type.__config__
        )
        for f in original_type.__fields__.values():
            use_type.__fields__[f.name] = f
        use_type.__validators__ = original_type.__validators__
    if PYDANTIC_1:
        new_field = ModelField(
            name=field.name,
            type_=use_type,
            class_validators={},
            default=None,
            required=False,
            model_config=BaseConfig,
            field_info=FieldInfo(None),
        )
    else:  # pragma: nocover
        new_field = ModelField(  # type: ignore
            name=field.name,
            type_=use_type,
            class_validators={},
            default=None,
            required=False,
            model_config=BaseConfig,
            schema=FieldInfo(None),
        )
    new_field.has_alias = field.has_alias
    new_field.alias = field.alias
    new_field.class_validators = field.class_validators
    new_field.default = field.default
    new_field.required = field.required
    new_field.model_config = field.model_config
    if PYDANTIC_1:
        new_field.field_info = field.field_info
    else:  # pragma: nocover
        new_field.schema = field.schema  # type: ignore
    new_field.allow_none = field.allow_none
    new_field.validate_always = field.validate_always
    if field.sub_fields:
        new_field.sub_fields = [
            create_cloned_field(sub_field) for sub_field in field.sub_fields
        ]
    if field.key_field:
        new_field.key_field = create_cloned_field(field.key_field)
    new_field.validators = field.validators
    if PYDANTIC_1:
        new_field.pre_validators = field.pre_validators
        new_field.post_validators = field.post_validators
    else:  # pragma: nocover
        new_field.whole_pre_validators = field.whole_pre_validators  # type: ignore
        new_field.whole_post_validators = field.whole_post_validators  # type: ignore
    new_field.parse_json = field.parse_json
    new_field.shape = field.shape
    try:
        new_field.populate_validators()
    except AttributeError:  # pragma: nocover
        # TODO: remove when removing support for Pydantic < 1.0.0
        new_field._populate_validators()  # type: ignore
    return new_field

def generate_operation_id_for_path(*, name: str, path: str, method: str) -> str:
    operation_id = name + path
    operation_id = re.sub(""[^0-9a-zA-Z_]"", ""_"", operation_id)
    operation_id = operation_id + ""_"" + method.lower()",[65]
"                output = tf.where(tiled_mask_t, output, states[0])
                new_states = [tf.where(tiled_mask_t, new_states[i], states[i]) for i in range(len(states))]
                output_ta_t = output_ta_t.write(time, output)
                return (time + 1, output_ta_t) + tuple(new_states)
        else:
            def _step(time, output_ta_t, *states):
                """"""RNN step function.
                # Arguments
                    time: Current timestep value.
                    output_ta_t: TensorArray.
                    *states: List of states.
                # Returns
                    Tuple: `(time + 1,output_ta_t) + tuple(new_states)`
                """"""
                current_input = input_ta.read(time)
                output, new_states = step_function(current_input,
                                                   tuple(states) +
                                                   tuple(constants))
                if getattr(output, '_uses_learning_phase', False):
                    global uses_learning_phase
                    uses_learning_phase = True
                for state, new_state in zip(states, new_states):
                    new_state.set_shape(state.get_shape())
                output_ta_t = output_ta_t.write(time, output)
                return (time + 1, output_ta_t) + tuple(new_states)
        final_outputs = control_flow_ops.while_loop(
            cond=lambda time, *_: time < time_steps,
            body=_step,
            loop_vars=(time, output_ta) + states,
            parallel_iterations=32,
            swap_memory=True)
        last_time = final_outputs[0]
        output_ta = final_outputs[1]
        new_states = final_outputs[2:]
        outputs = output_ta.stack()
        last_output = output_ta.read(last_time - 1)
    axes = [1, 0] + list(range(2, len(outputs.get_shape())))
    outputs = tf.transpose(outputs, axes)
    last_output._uses_learning_phase = uses_learning_phase
    return last_output, outputs, new_states

def switch(condition, then_expression, else_expression):
    """"""Switches between two operations depending on a scalar value.
    Note that both `then_expression` and `else_expression`
    should be symbolic tensors of the *same shape*.
    # Arguments
        condition: tensor (`int` or `bool`).
        then_expression: either a tensor, or a callable that returns a tensor.
        else_expression: either a tensor, or a callable that returns a tensor.
    # Returns
        The selected tensor.
    # Raises
        ValueError: If rank of `condition` is greater than rank of expressions.
    """"""
    if condition.dtype != tf.bool:
        condition = tf.cast(condition, 'bool')
    cond_ndim = ndim(condition)
    if not cond_ndim:
        if not callable(then_expression):
            def then_expression_fn():
                return then_expression
        else:
            then_expression_fn = then_expression
        if not callable(else_expression):
            def else_expression_fn():
                return else_expression
        else:
            else_expression_fn = else_expression
        x = tf.cond(condition,
                    then_expression_fn,
                    else_expression_fn)
    else:
        # tf.where needs its condition tensor
        # to be the same shape as its two
        # result tensors
        if callable(then_expression):
            then_expression = then_expression()
        if callable(else_expression):
            else_expression = else_expression()
        expr_ndim = ndim(then_expression)
        if cond_ndim > expr_ndim:
            raise ValueError('Rank of `condition` should be less than or'
                             ' equal to rank of `then_expression` and '
                             '`else_expression`. ndim(condition)=' +
                             str(cond_ndim) + ', ndim(then_expression)'
                             '=' + str(expr_ndim))
        if cond_ndim > 1:
            ndim_diff = expr_ndim - cond_ndim
            cond_shape = tf.concat([tf.shape(condition), [1] * ndim_diff], axis=0)
            condition = tf.reshape(condition, cond_shape)
            expr_shape = tf.shape(then_expression)
            shape_diff = expr_shape - cond_shape
            tile_shape = tf.where(shape_diff > 0, expr_shape, tf.ones_like(expr_shape))
            condition = tf.tile(condition, tile_shape)
        x = tf.where(condition, then_expression, else_expression)
    return x

def in_train_phase(x, alt, training=None):
    """"""Selects `x` in train phase, and `alt` otherwise.
    Note that `alt` should have the *same shape* as `x`.
    # Arguments
        x: What to return in train phase
            (tensor or callable that returns a tensor).
        alt: What to return otherwise
            (tensor or callable that returns a tensor).
        training: Optional scalar tensor
            (or Python boolean, or Python integer)
            specifying the learning phase.
    # Returns
        Either `x` or `alt` based on the `training` flag.
        the `training` flag defaults to `K.learning_phase()`.
    """"""
    if training is None:",[1]
"        assert isinstance(values, np.ndarray), type(values)
        super().__init__(values, placement=placement, ndim=ndim)
    @property
    def _holder(self):
        return TimedeltaArray
    def _can_hold_element(self, element: Any) -> bool:
        tipo = maybe_infer_dtype_type(element)
        if tipo is not None:
            return issubclass(tipo.type, np.timedelta64)
        elif element is NaT:
            return True
        elif isinstance(element, (timedelta, np.timedelta64)):
            return True
        return is_valid_nat_for_dtype(element, self.dtype)
    def fillna(self, value, **kwargs):
        # allow filling with integers to be
        # interpreted as nanoseconds
        if is_integer(value):
            # Deprecation GH#24694, GH#19233
            raise TypeError(
                ""Passing integers to fillna for timedelta64[ns] dtype is no ""
                ""longer supported.  To obtain the old behavior, pass ""
                ""`pd.Timedelta(seconds=n)` instead.""
            )
        return super().fillna(value, **kwargs)
    def should_store(self, value) -> bool:
        return is_timedelta64_dtype(value.dtype)
    def to_native_types(self, slicer=None, na_rep=None, quoting=None, **kwargs):
        """""" convert to our native types format, slicing if desired """"""
        values = self.values
        if slicer is not None:
            values = values[:, slicer]
        mask = isna(values)
        rvalues = np.empty(values.shape, dtype=object)
        if na_rep is None:
            na_rep = ""NaT""
        rvalues[mask] = na_rep
        imask = (~mask).ravel()
        # FIXME:
        # should use the formats.format.Timedelta64Formatter here
        # to figure what format to pass to the Timedelta
        # e.g. to not show the decimals say
        rvalues.flat[imask] = np.array(
            [Timedelta(val)._repr_base(format=""all"") for val in values.ravel()[imask]],
            dtype=object,
        )
        return rvalues
    def external_values(self):
        return np.asarray(self.values.astype(""timedelta64[ns]"", copy=False))
    def array_values(self) -> ExtensionArray:
        return TimedeltaArray._simple_new(self.values)

class BoolBlock(NumericBlock):
    __slots__ = ()
    is_bool = True
    _can_hold_na = False
    def _can_hold_element(self, element: Any) -> bool:
        tipo = maybe_infer_dtype_type(element)
        if tipo is not None:
            return issubclass(tipo.type, np.bool_)
        return isinstance(element, (bool, np.bool_))
    def should_store(self, value) -> bool:
        return issubclass(value.dtype.type, np.bool_) and not is_extension_array_dtype(
            value
        )
    def replace(
        self, to_replace, value, inplace=False, filter=None, regex=False, convert=True
    ):
        inplace = validate_bool_kwarg(inplace, ""inplace"")
        to_replace_values = np.atleast_1d(to_replace)
        if not np.can_cast(to_replace_values, bool):
            return self
        return super().replace(
            to_replace,
            value,
            inplace=inplace,
            filter=filter,
            regex=regex,
            convert=convert,
        )

class ObjectBlock(Block):
    __slots__ = ()
    is_object = True
    _can_hold_na = True
    def __init__(self, values, placement=None, ndim=2):
        if issubclass(values.dtype.type, str):
            values = np.array(values, dtype=object)
        super().__init__(values, ndim=ndim, placement=placement)
    @property
    def is_bool(self):
        """"""
        we can be a bool if we have only bool values but are of type
        object
        """"""
        return lib.is_bool_array(self.values.ravel())
    def convert(
        self,
        copy: bool = True,
        datetime: bool = True,
        numeric: bool = True,
        timedelta: bool = True,
        coerce: bool = False,
    ):
        """"""
        attempt to coerce any object types to better types return a copy of
        the block (if copy = True) by definition we ARE an ObjectBlock!!!!!
","[30, 31, 32, 74]"
"        param_a = luigi.Parameter()
        def output(self):
            return luigi.LocalTarget('/tmp/log-{t.param_a}'.format(t=self))
    @requires(TaskA)
    class TaskB(luigi.Task):
        param_b = luigi.Parameter()
        # The class decorator does this for me!
        # def requires(self):
        #     return self.clone(TaskA)
Use these helper functions effectively to avoid unnecessary
repetition and dodge a few potentially nasty workflow pitfalls at the same
time. Brilliant!
""""""
import datetime
import logging
from luigi import six
from luigi import task
from luigi import parameter
if six.PY3:
    xrange = range
logger = logging.getLogger('luigi-interface')

def common_params(task_instance, task_cls):
    """"""
    Grab all the values in task_instance that are found in task_cls.
    """"""
    if not isinstance(task_cls, task.Register):
        raise TypeError(""task_cls must be an uninstantiated Task"")
    task_instance_param_names = dict(task_instance.get_params()).keys()
    task_cls_params_dict = dict(task_cls.get_params())
    task_cls_param_names = task_cls_params_dict.keys()
    common_param_names = set(task_instance_param_names).intersection(set(task_cls_param_names))
    common_param_vals = [(key, task_cls_params_dict[key]) for key in common_param_names]
    common_kwargs = dict((key, task_instance.param_kwargs[key]) for key in common_param_names)
    vals = dict(task_instance.get_param_values(common_param_vals, [], common_kwargs))
    return vals

class inherits(object):
    """"""
    Task inheritance.
    Usage:
    .. code-block:: python
        class AnotherTask(luigi.Task):
            n = luigi.IntParameter()
            # ...
        @inherits(AnotherTask):
        class MyTask(luigi.Task):
            def requires(self):
               return self.clone_parent()
            def run(self):
               print self.n # this will be defined
               # ...
    """"""
    def __init__(self, task_to_inherit):
        super(inherits, self).__init__()
        self.task_to_inherit = task_to_inherit
    def __call__(self, task_that_inherits):
        for param_name, param_obj in self.task_to_inherit.get_params():
            if not hasattr(task_that_inherits, param_name):
                setattr(task_that_inherits, param_name, param_obj)
        # Modify task_that_inherits by subclassing it and adding methods
        @task._task_wraps(task_that_inherits)
        class Wrapped(task_that_inherits):
            def clone_parent(_self, **args):
                return _self.clone(cls=self.task_to_inherit, **args)
        return Wrapped

class requires(object):
    """"""
    Same as @inherits, but also auto-defines the requires method.
    """"""
    def __init__(self, task_to_require):
        super(requires, self).__init__()
        self.inherit_decorator = inherits(task_to_require)
    def __call__(self, task_that_requires):
        task_that_requires = self.inherit_decorator(task_that_requires)
        # Modify task_that_requres by subclassing it and adding methods
        @task._task_wraps(task_that_requires)
        class Wrapped(task_that_requires):
            def requires(_self):
                return _self.clone_parent()
        return Wrapped

class copies(object):
    """"""
    Auto-copies a task.
    Usage:
    .. code-block:: python
        @copies(MyTask):
        class CopyOfMyTask(luigi.Task):
            def output(self):
               return LocalTarget(self.date.strftime('/var/xyz/report-%Y-%m-%d'))
    """"""
    def __init__(self, task_to_copy):","[81, 82, 84, 85, 86, 87, 103, 104, 106, 107, 108, 109]"
"        self.executor.join()

class GeneratorEnqueuer(SequenceEnqueuer):
    """"""Builds a queue out of a data generator.
    The provided generator can be finite in which case the class will throw
    a `StopIteration` exception.
    Used in `fit_generator`, `evaluate_generator`, `predict_generator`.
    # Arguments
        generator: a generator function which yields data
        use_multiprocessing: use multiprocessing if True, otherwise threading
        wait_time: time to sleep in-between calls to `put()`
        random_seed: Initial seed for workers,
            will be incremented by one for each worker.
    """"""
    def __init__(self, generator,
                 use_multiprocessing=False,
                 wait_time=0.05,
                 seed=None):
        self.wait_time = wait_time
        self._generator = generator
        self._use_multiprocessing = use_multiprocessing
        self._threads = []
        self._stop_event = None
        self.queue = None
        self.seed = seed
    def start(self, workers=1, max_queue_size=10):
        """"""Kicks off threads which add data from the generator into the queue.
        # Arguments
            workers: number of worker threads
            max_queue_size: queue size
                (when full, threads could block on `put()`)
        """"""
        def data_generator_task():
            while not self._stop_event.is_set():
                try:
                    if self._use_multiprocessing or self.queue.qsize() < max_queue_size:
                        generator_output = next(self._generator)
                        self.queue.put(generator_output)
                    else:
                        time.sleep(self.wait_time)
                except StopIteration:
                    break
                except Exception:
                    self._stop_event.set()
                    raise
        try:
            if self._use_multiprocessing:
                self.queue = multiprocessing.Queue(maxsize=max_queue_size)
                self._stop_event = multiprocessing.Event()
            else:
                self.queue = queue.Queue()
                self._stop_event = threading.Event()
            for _ in range(workers):
                if self._use_multiprocessing:
                    # Reset random seed else all children processes
                    # share the same seed
                    np.random.seed(self.seed)
                    thread = multiprocessing.Process(target=data_generator_task)
                    thread.daemon = True
                    if self.seed is not None:
                        self.seed += 1
                else:
                    thread = threading.Thread(target=data_generator_task)
                self._threads.append(thread)
                thread.start()
        except:
            self.stop()
            raise
    def is_running(self):
        return self._stop_event is not None and not self._stop_event.is_set()
    def stop(self, timeout=None):
        """"""Stops running threads and wait for them to exit, if necessary.
        Should be called by the same thread which called `start()`.
        # Arguments
            timeout: maximum time to wait on `thread.join()`.
        """"""
        if self.is_running():
            self._stop_event.set()
        for thread in self._threads:
            if thread.is_alive():
                if self._use_multiprocessing:
                    thread.terminate()
                else:
                    thread.join(timeout)
        if self._use_multiprocessing:
            if self.queue is not None:
                self.queue.close()
        self._threads = []
        self._stop_event = None
        self.queue = None
    def get(self):
        """"""Creates a generator to extract data from the queue.
        Skip the data if it is `None`.
        # Returns
            A generator
        """"""
        while self.is_running():
            if not self.queue.empty():
                inputs = self.queue.get()
                if inputs is not None:
                    yield inputs
            else:
                all_finished = all([not thread.is_alive() for thread in self._threads])
                if all_finished and self.queue.empty():
                    raise StopIteration()
                else:","[45, 50, 52, 56, 100, 101, 102, 118, 119, 120]"
"    ndim : int
    loc : object
    Returns
    -------
    tuple
    """"""
    tup = [slice(None, None) for _ in range(ndim)]
    tup[0] = loc
    return tuple(tup)

def convert_to_index_sliceable(obj, key):
    """"""
    if we are index sliceable, then return my slicer, otherwise return None
    """"""
    idx = obj.index
    if isinstance(key, slice):
        return idx._convert_slice_indexer(key, kind=""getitem"")
    elif isinstance(key, str):
        # we are an actual column
        if key in obj._data.items:
            return None
        # We might have a datetimelike string that we can translate to a
        # slice here via partial string indexing
        if idx.is_all_dates:
            try:
                return idx._get_string_slice(key)
            except (KeyError, ValueError, NotImplementedError):
                return None
    return None

def check_bool_indexer(index: Index, key) -> np.ndarray:
    """"""
    Check if key is a valid boolean indexer for an object with such index and
    perform reindexing or conversion if needed.
    This function assumes that is_bool_indexer(key) == True.
    Parameters
    ----------
    index : Index
        Index of the object on which the indexing is done
    key : list-like
        Boolean indexer to check
    Returns
    -------
    result: np.array
        Resulting key
    Raises
    ------
    IndexError
        If the key does not have the same length as index
    IndexingError
        If the index of the key is unalignable to index
    """"""
    result = key
    if isinstance(key, ABCSeries) and not key.index.equals(index):
        result = result.reindex(index)
        mask = isna(result._values)
        if mask.any():
            raise IndexingError(
                ""Unalignable boolean Series provided as ""
                ""indexer (index of the boolean Series and of ""
                ""the indexed object do not match).""
            )
        result = result.astype(bool)._values
    else:
        if is_sparse(result):
            result = result.to_dense()
        result = np.asarray(result, dtype=bool)
        # GH26658
        if len(result) != len(index):
            raise IndexError(
                ""Item wrong length {} instead of {}."".format(len(result), len(index))
            )
    return result

def convert_missing_indexer(indexer):
    """"""
    reverse convert a missing indexer, which is a dict
    return the scalar indexer and a boolean indicating if we converted
    """"""
    if isinstance(indexer, dict):
        # a missing key (but not a tuple indexer)
        indexer = indexer[""key""]
        if isinstance(indexer, bool):
            raise KeyError(""cannot use a single bool to index into setitem"")
        return indexer, True
    return indexer, False

def convert_from_missing_indexer_tuple(indexer, axes):
    """"""
    create a filtered indexer that doesn't have any missing indexers
    """"""
    def get_indexer(_i, _idx):
        return axes[_i].get_loc(_idx[""key""]) if isinstance(_idx, dict) else _idx
    return tuple(get_indexer(_i, _idx) for _i, _idx in enumerate(indexer))

def maybe_convert_ix(*args):
    """"""
    We likely want to take the cross-product
    """"""
    ixify = True
    for arg in args:
        if not isinstance(arg, (np.ndarray, list, ABCSeries, Index)):",[28]
"    @property
    def _cookies(self):
        return self.jar._cookies
    def clear_session_cookies(self, *args, **kwargs):
        return self.jar.clear_session_cookies(*args, **kwargs)
    def clear(self):
        return self.jar.clear()
    def __iter__(self):
        return iter(self.jar)
    def __len__(self):
        return len(self.jar)
    def set_policy(self, pol):
        return self.jar.set_policy(pol)
    def make_cookies(self, response, request):
        wreq = WrappedRequest(request)
        wrsp = WrappedResponse(response)
        return self.jar.make_cookies(wrsp, wreq)
    def set_cookie(self, cookie):
        self.jar.set_cookie(cookie)
    def set_cookie_if_ok(self, cookie, request):
        self.jar.set_cookie_if_ok(cookie, WrappedRequest(request))

def potential_domain_matches(domain):
    """"""Potential domain matches for a cookie
    >>> potential_domain_matches('www.example.com')
    ['www.example.com', 'example.com', '.www.example.com', '.example.com']
    """"""
    matches = [domain]
    try:
        start = domain.index('.') + 1
        end = domain.rindex('.')
        while start < end:
            matches.append(domain[start:])
            start = domain.index('.', start) + 1
    except ValueError:
        pass
    return matches + ['.' + d for d in matches]

class _DummyLock(object):
    def acquire(self):
        pass
    def release(self):
        pass

class WrappedRequest(object):
    """"""Wraps a scrapy Request class with methods defined by urllib2.Request class to interact with CookieJar class
    see http://docs.python.org/library/urllib2.html#urllib2.Request
    """"""
    def __init__(self, request):
        self.request = request
    def get_full_url(self):
        return self.request.url
    def get_host(self):
        return urlparse_cached(self.request).netloc
    def get_type(self):
        return urlparse_cached(self.request).scheme
    def is_unverifiable(self):
        """"""Unverifiable should indicate whether the request is unverifiable, as defined by RFC 2965.
        It defaults to False. An unverifiable request is one whose URL the user did not have the
        option to approve. For example, if the request is for an image in an
        HTML document, and the user had no option to approve the automatic
        fetching of the image, this should be true.
        """"""
        return self.request.meta.get('is_unverifiable', False)
    # python3 uses request.unverifiable
    @property
    def unverifiable(self):
        return self.is_unverifiable()
    def get_origin_req_host(self):
        return urlparse_cached(self.request).hostname
    def has_header(self, name):
        return name in self.request.headers
    def get_header(self, name, default=None):
        return to_native_str(self.request.headers.get(name, default),
                             errors='replace')
    def header_items(self):
        return [
            (to_native_str(k, errors='replace'),
             [to_native_str(x, errors='replace') for x in v])
            for k, v in self.request.headers.items()
        ]
    def add_unredirected_header(self, name, value):
        self.request.headers.appendlist(name, value)

class WrappedResponse(object):
    def __init__(self, response):
        self.response = response
    def info(self):
        return self
    # python3 cookiejars calls get_all
    def get_all(self, name, default=None):
        return [to_native_str(v, errors='replace')
                for v in self.response.headers.getlist(name)]
    # python2 cookiejars calls getheaders","[92, 93]"
"class FacebookIE(InfoExtractor):
    _VALID_URL = r'''(?x)
        https?://(?:\w+\.)?facebook\.com/
        (?:[^#]*?\#!/)?
        (?:video/video\.php|photo\.php|video\.php|video/embed)\?(?:.*?)
        (?:v|video_id)=(?P<id>[0-9]+)
        (?:.*)'''
    _LOGIN_URL = 'https://www.facebook.com/login.php?next=http%3A%2F%2Ffacebook.com%2Fhome.php&login_attempt=1'
    _CHECKPOINT_URL = 'https://www.facebook.com/checkpoint/?next=http%3A%2F%2Ffacebook.com%2Fhome.php&_fb_noscript=1'
    _NETRC_MACHINE = 'facebook'
    IE_NAME = 'facebook'
    _TESTS = [{
        'url': 'https://www.facebook.com/video.php?v=637842556329505&fref=nf',
        'md5': '6a40d33c0eccbb1af76cf0485a052659',
        'info_dict': {
            'id': '637842556329505',
            'ext': 'mp4',
            'duration': 38,
            'title': 'Did you know Kei Nishikori is the first Asian man to ever reach a Grand Slam fin...',
        }
    }, {
        'url': 'https://www.facebook.com/video.php?v=10204634152394104',
        'only_matching': True,
    }]
    def _login(self):
        (useremail, password) = self._get_login_info()
        if useremail is None:
            return
        login_page_req = compat_urllib_request.Request(self._LOGIN_URL)
        login_page_req.add_header('Cookie', 'locale=en_US')
        login_page = self._download_webpage(login_page_req, None,
            note='Downloading login page',
            errnote='Unable to download login page')
        lsd = self._search_regex(
            r'<input type=""hidden"" name=""lsd"" value=""([^""]*)""',
            login_page, 'lsd')
        lgnrnd = self._search_regex(r'name=""lgnrnd"" value=""([^""]*?)""', login_page, 'lgnrnd')
        login_form = {
            'email': useremail,
            'pass': password,
            'lsd': lsd,
            'lgnrnd': lgnrnd,
            'next': 'http://facebook.com/home.php',
            'default_persistent': '0',
            'legacy_return': '1',
            'timezone': '-60',
            'trynum': '1',
            }
        request = compat_urllib_request.Request(self._LOGIN_URL, urlencode_postdata(login_form))
        request.add_header('Content-Type', 'application/x-www-form-urlencoded')
        try:
            login_results = self._download_webpage(request, None,
                note='Logging in', errnote='unable to fetch login page')
            if re.search(r'<form(.*)name=""login""(.*)</form>', login_results) is not None:
                self._downloader.report_warning('unable to log in: bad username/password, or exceded login rate limit (~3/min). Check credentials or wait.')
                return
            check_form = {
                'fb_dtsg': self._search_regex(r'name=""fb_dtsg"" value=""(.+?)""', login_results, 'fb_dtsg'),
                'h': self._search_regex(
                    r'name=""h""\s+(?:\w+=""[^""]+""\s+)*?value=""([^""]+)""', login_results, 'h'),
                'name_action_selected': 'dont_save',
            }
            check_req = compat_urllib_request.Request(self._CHECKPOINT_URL, urlencode_postdata(check_form))
            check_req.add_header('Content-Type', 'application/x-www-form-urlencoded')
            check_response = self._download_webpage(check_req, None,
                note='Confirming login')
            if re.search(r'id=""checkpointSubmitButton""', check_response) is not None:
                self._downloader.report_warning('Unable to confirm login, you have to login in your brower and authorize the login.')
        except (compat_urllib_error.URLError, compat_http_client.HTTPException, socket.error) as err:
            self._downloader.report_warning('unable to log in: %s' % compat_str(err))
            return
    def _real_initialize(self):
        self._login()
    def _real_extract(self, url):
        mobj = re.match(self._VALID_URL, url)
        video_id = mobj.group('id')
        url = 'https://www.facebook.com/video/video.php?v=%s' % video_id
        webpage = self._download_webpage(url, video_id)
        BEFORE = '{swf.addParam(param[0], param[1]);});\n'
        AFTER = '.forEach(function(variable) {swf.addVariable(variable[0], variable[1]);});'
        m = re.search(re.escape(BEFORE) + '(.*?)' + re.escape(AFTER), webpage)
        if not m:
            m_msg = re.search(r'class=""[^""]*uiInterstitialContent[^""]*""><div>(.*?)</div>', webpage)
            if m_msg is not None:
                raise ExtractorError(
                    'The video is not available, Facebook said: ""%s""' % m_msg.group(1),
                    expected=True)
            else:
                raise ExtractorError('Cannot parse data')
        data = dict(json.loads(m.group(1)))
        params_raw = compat_urllib_parse.unquote(data['params'])
        params = json.loads(params_raw)
        video_data = params['video_data'][0]
        video_url = video_data.get('hd_src')
        if not video_url:
            video_url = video_data['sd_src']
        if not video_url:
            raise ExtractorError('Cannot find video URL')
        video_title = self._html_search_regex(
            r'<h2 class=""uiHeaderTitle"">([^<]*)</h2>', webpage, 'title',
            fatal=False)
        if not video_title:
            video_title = self._html_search_regex(
                r'(?s)<span class=""fbPhotosPhotoCaption"".*?id=""fbPhotoPageCaption""><span class=""hasCaption"">(.*?)</span>',
                webpage, 'alternative title', default=None)
            if len(video_title) > 80 + 3:
                video_title = video_title[:80] + '...'
        if not video_title:
            video_title = 'Facebook video #%s' % video_id
        return {
            'id': video_id,
            'title': video_title,
            'url': video_url,
            'duration': int(video_data['video_duration']),
            'thumbnail': video_data['thumbnail_src'],","[115, 116]"
"            return self
        return super().replace(
            to_replace,
            value,
            inplace=inplace,
            filter=filter,
            regex=regex,
            convert=convert,
        )

class ObjectBlock(Block):
    __slots__ = ()
    is_object = True
    _can_hold_na = True
    def __init__(self, values, placement=None, ndim=2):
        if issubclass(values.dtype.type, str):
            values = np.array(values, dtype=object)
        super().__init__(values, ndim=ndim, placement=placement)
    @property
    def is_bool(self):
        """"""
        we can be a bool if we have only bool values but are of type
        object
        """"""
        return lib.is_bool_array(self.values.ravel())
    def convert(
        self,
        copy: bool = True,
        datetime: bool = True,
        numeric: bool = True,
        timedelta: bool = True,
        coerce: bool = False,
    ):
        """"""
        attempt to coerce any object types to better types return a copy of
        the block (if copy = True) by definition we ARE an ObjectBlock!!!!!
        can return multiple blocks!
        """"""
        # operate column-by-column
        def f(mask, val, idx):
            shape = val.shape
            values = soft_convert_objects(
                val.ravel(),
                datetime=datetime,
                numeric=numeric,
                timedelta=timedelta,
                coerce=coerce,
                copy=copy,
            )
            if isinstance(values, np.ndarray):
                # TODO: allow EA once reshape is supported
                values = values.reshape(shape)
            values = _block_shape(values, ndim=self.ndim)
            return values
        if self.ndim == 2:
            blocks = self.split_and_operate(None, f, False)
        else:
            values = f(None, self.values.ravel(), None)
            blocks = [make_block(values, ndim=self.ndim, placement=self.mgr_locs)]
        return blocks
    def _maybe_downcast(self, blocks: List[""Block""], downcast=None) -> List[""Block""]:
        if downcast is not None:
            return blocks
        # split and convert the blocks
        return _extend_blocks([b.convert(datetime=True, numeric=False) for b in blocks])
    def _can_hold_element(self, element: Any) -> bool:
        return True
    def should_store(self, value) -> bool:
        return not (
            issubclass(
                value.dtype.type,
                (np.integer, np.floating, np.complexfloating, np.datetime64, np.bool_),
            )
            or is_extension_array_dtype(value)
        )
    def replace(
        self, to_replace, value, inplace=False, filter=None, regex=False, convert=True
    ):
        to_rep_is_list = is_list_like(to_replace)
        value_is_list = is_list_like(value)
        both_lists = to_rep_is_list and value_is_list
        either_list = to_rep_is_list or value_is_list
        result_blocks = []
        blocks = [self]
        if not either_list and is_re(to_replace):
            return self._replace_single(
                to_replace,
                value,
                inplace=inplace,
                filter=filter,
                regex=True,
                convert=convert,
            )
        elif not (either_list or regex):
            return super().replace(
                to_replace,
                value,
                inplace=inplace,
                filter=filter,
                regex=regex,
                convert=convert,
            )
        elif both_lists:
            for to_rep, v in zip(to_replace, value):
                result_blocks = []
                for b in blocks:
                    result = b._replace_single(
                        to_rep,
                        v,
                        inplace=inplace,",[81]
"        else:
            # scalar callable may return tuple
            key = com.apply_if_callable(key, self.obj)
        if not isinstance(key, tuple):
            key = _tuplify(self.ndim, key)
        if len(key) != self.ndim:
            raise ValueError(""Not enough indexers for scalar access (setting)!"")
        key = list(self._convert_key(key, is_setter=True))
        self.obj._set_value(*key, value=value, takeable=self._takeable)

@Appender(IndexingMixin.at.__doc__)
class _AtIndexer(_ScalarAccessIndexer):
    _takeable = False
    def _convert_key(self, key, is_setter: bool = False):
        """"""
        Require they keys to be the same type as the index. (so we don't
        fallback)
        """"""
        # allow arbitrary setting
        if is_setter:
            return list(key)
        for ax, i in zip(self.obj.axes, key):
            if ax.is_integer():
                if not is_integer(i):
                    raise ValueError(
                        ""At based indexing on an integer index ""
                        ""can only have integer indexers""
                    )
            else:
                if is_integer(i) and not ax.holds_integer():
                    raise ValueError(
                        ""At based indexing on an non-integer ""
                        ""index can only have non-integer ""
                        ""indexers""
                    )
        return key

@Appender(IndexingMixin.iat.__doc__)
class _iAtIndexer(_ScalarAccessIndexer):
    _takeable = True
    def _convert_key(self, key, is_setter: bool = False):
        """"""
        Require integer args. (and convert to label arguments)
        """"""
        for a, i in zip(self.obj.axes, key):
            if not is_integer(i):
                raise ValueError(""iAt based indexing can only have integer indexers"")
        return key

def _tuplify(ndim: int, loc: Hashable) -> Tuple[Union[Hashable, slice], ...]:
    """"""
    Given an indexer for the first dimension, create an equivalent tuple
    for indexing over all dimensions.
    Parameters
    ----------
    ndim : int
    loc : object
    Returns
    -------
    tuple
    """"""
    _tup: List[Union[Hashable, slice]]
    _tup = [slice(None, None) for _ in range(ndim)]
    _tup[0] = loc
    return tuple(_tup)

def convert_to_index_sliceable(obj, key):
    """"""
    If we are index sliceable, then return my slicer, otherwise return None.
    """"""
    idx = obj.index
    if isinstance(key, slice):
        return idx._convert_slice_indexer(key, kind=""getitem"")
    elif isinstance(key, str):
        # we are an actual column
        if key in obj._data.items:
            return None
        # We might have a datetimelike string that we can translate to a
        # slice here via partial string indexing
        if idx._supports_partial_string_indexing:
            try:
                return idx._get_string_slice(key)
            except (KeyError, ValueError, NotImplementedError):
                return None
    return None

def check_bool_indexer(index: Index, key) -> np.ndarray:
    """"""
    Check if key is a valid boolean indexer for an object with such index and
    perform reindexing or conversion if needed.
    This function assumes that is_bool_indexer(key) == True.
    Parameters
    ----------
    index : Index
        Index of the object on which the indexing is done.
    key : list-like
        Boolean indexer to check.
    Returns
    -------
    np.array
        Resulting key.
    Raises
    ------
    IndexError
        If the key does not have the same length as index.
    IndexingError
        If the index of the key is unalignable to index.
    """"""",[33]
"def reformat_many(
    sources: Set[Path], fast: bool, write_back: WriteBack, mode: Mode, report: ""Report""
) -> None:
    """"""Reformat multiple files using a ProcessPoolExecutor.""""""
    loop = asyncio.get_event_loop()
    worker_count = os.cpu_count()
    if sys.platform == ""win32"":
        # Work around https://bugs.python.org/issue26903
        worker_count = min(worker_count, 61)
    executor = ProcessPoolExecutor(max_workers=worker_count)
    try:
        loop.run_until_complete(
            schedule_formatting(
                sources=sources,
                fast=fast,
                write_back=write_back,
                mode=mode,
                report=report,
                loop=loop,
                executor=executor,
            )
        )
    finally:
        shutdown(loop)
        executor.shutdown()

async def schedule_formatting(
    sources: Set[Path],
    fast: bool,
    write_back: WriteBack,
    mode: Mode,
    report: ""Report"",
    loop: asyncio.AbstractEventLoop,
    executor: Executor,
) -> None:
    """"""Run formatting of `sources` in parallel using the provided `executor`.
    (Use ProcessPoolExecutors for actual parallelism.)
    `write_back`, `fast`, and `mode` options are passed to
    :func:`format_file_in_place`.
    """"""
    cache: Cache = {}
    if write_back != WriteBack.DIFF:
        cache = read_cache(mode)
        sources, cached = filter_cached(cache, sources)
        for src in sorted(cached):
            report.done(src, Changed.CACHED)
    if not sources:
        return
    cancelled = []
    sources_to_cache = []
    lock = None
    if write_back == WriteBack.DIFF:
        # For diff output, we need locks to ensure we don't interleave output
        # from different processes.
        manager = Manager()
        lock = manager.Lock()
    tasks = {
        asyncio.ensure_future(
            loop.run_in_executor(
                executor, format_file_in_place, src, fast, mode, write_back, lock
            )
        ): src
        for src in sorted(sources)
    }
    pending: Iterable[""asyncio.Future[bool]""] = tasks.keys()
    try:
        loop.add_signal_handler(signal.SIGINT, cancel, pending)
        loop.add_signal_handler(signal.SIGTERM, cancel, pending)
    except NotImplementedError:
        # There are no good alternatives for these on Windows.
        pass
    while pending:
        done, _ = await asyncio.wait(pending, return_when=asyncio.FIRST_COMPLETED)
        for task in done:
            src = tasks.pop(task)
            if task.cancelled():
                cancelled.append(task)
            elif task.exception():
                report.failed(src, str(task.exception()))
            else:
                changed = Changed.YES if task.result() else Changed.NO
                # If the file was written back or was successfully checked as
                # well-formatted, store this information in the cache.
                if write_back is WriteBack.YES or (
                    write_back is WriteBack.CHECK and changed is Changed.NO
                ):
                    sources_to_cache.append(src)
                report.done(src, changed)
    if cancelled:
        await asyncio.gather(*cancelled, loop=loop, return_exceptions=True)
    if sources_to_cache:
        write_cache(cache, sources_to_cache, mode)

def format_file_in_place(
    src: Path,
    fast: bool,
    mode: Mode,
    write_back: WriteBack = WriteBack.NO,
    lock: Any = None,  # multiprocessing.Manager().Lock() is some crazy proxy
) -> bool:
    """"""Format file under `src` path. Return True if changed.
    If `write_back` is DIFF, write a diff to stdout. If it is YES, write reformatted
    code to the file.
    `mode` and `fast` options are passed to :func:`format_file_contents`.
    """"""
    if src.suffix == "".pyi"":
        mode = replace(mode, is_pyi=True)
    then = datetime.utcfromtimestamp(src.stat().st_mtime)
    with open(src, ""rb"") as buf:
        src_contents, encoding, newline = decode_bytes(buf.read())
    try:
        dst_contents = format_file_contents(src_contents, fast=fast, mode=mode)
    except NothingChanged:
        return False
    if write_back == WriteBack.YES:
        with open(src, ""w"", encoding=encoding, newline=newline) as f:
            f.write(dst_contents)
    elif write_back in (WriteBack.DIFF, WriteBack.COLOR_DIFF):
        now = datetime.utcnow()","[9, 24, 34]"
"""""""
Thin wrappers around common functions.
Subpackages contain potentially unstable extensions.
""""""
from tqdm import tqdm
from tqdm.auto import tqdm as tqdm_auto
from tqdm.utils import ObjectWrapper
from copy import deepcopy
import functools
import sys
__author__ = {""github.com/"": [""casperdcl""]}
__all__ = ['tenumerate', 'tzip', 'tmap']

class DummyTqdmFile(ObjectWrapper):
    """"""Dummy file-like that will write to tqdm""""""
    def write(self, x, nolock=False):
        # Avoid print() second call (useless \n)
        if len(x.rstrip()) > 0:
            tqdm.write(x, file=self._wrapped, nolock=nolock)

def tenumerate(iterable, start=0, total=None, tqdm_class=tqdm_auto,
               **tqdm_kwargs):
    """"""
    Equivalent of `numpy.ndenumerate` or builtin `enumerate`.
    Parameters
    ----------
    tqdm_class  : [default: tqdm.auto.tqdm].
    """"""
    try:
        import numpy as np
    except ImportError:
        pass
    else:
        if isinstance(iterable, np.ndarray):
            return tqdm_class(np.ndenumerate(iterable),
                              total=total or len(iterable), **tqdm_kwargs)
    return enumerate(tqdm_class(iterable, start, **tqdm_kwargs))

def _tzip(iter1, *iter2plus, **tqdm_kwargs):
    """"""
    Equivalent of builtin `zip`.
    Parameters
    ----------
    tqdm_class  : [default: tqdm.auto.tqdm].
    """"""
    kwargs = deepcopy(tqdm_kwargs)
    tqdm_class = kwargs.pop(""tqdm_class"", tqdm_auto)
    for i in zip(tqdm_class(iter1, **tqdm_kwargs), *iter2plus):
        yield i

def _tmap(function, *sequences, **tqdm_kwargs):
    """"""
    Equivalent of builtin `map`.
    Parameters
    ----------
    tqdm_class  : [default: tqdm.auto.tqdm].
    """"""
    for i in _tzip(*sequences, **tqdm_kwargs):
        yield function(*i)

if sys.version_info[:1] < (3,):
    @functools.wraps(_tzip)
    def tzip(*args, **kwargs):
        return list(_tzip(*args, **kwargs))
    @functools.wraps(_tmap)
    def tmap(*args, **kwargs):
        return list(_tmap(*args, **kwargs))
else:
    tzip = _tzip
    tmap = _tmap",[40]
"        warnings.warn(
            (
                ""The 'set_value' method is deprecated, and ""
                ""will be removed in a future version.""
            ),
            FutureWarning,
            stacklevel=2,
        )
        loc = self._engine.get_loc(key)
        validate_numeric_casting(arr.dtype, value)
        arr[loc] = value
    _index_shared_docs[
        ""get_indexer_non_unique""
    ] = """"""
        Compute indexer and mask for new index given the current index. The
        indexer should be then used as an input to ndarray.take to align the
        current data to the new index.
        Parameters
        ----------
        target : %(target_klass)s
        Returns
        -------
        indexer : ndarray of int
            Integers from 0 to n - 1 indicating that the index at these
            positions matches the corresponding target values. Missing values
            in the target are marked by -1.
        missing : ndarray of int
            An indexer into the target of the values not found.
            These correspond to the -1 in the indexer array.
        """"""
    @Appender(_index_shared_docs[""get_indexer_non_unique""] % _index_doc_kwargs)
    def get_indexer_non_unique(self, target):
        target = ensure_index(target)
        pself, ptarget = self._maybe_promote(target)
        if pself is not self or ptarget is not target:
            return pself.get_indexer_non_unique(ptarget)
        if is_categorical(target):
            tgt_values = np.asarray(target)
        elif self.is_all_dates and target.is_all_dates:  # GH 30399
            tgt_values = target.asi8
        else:
            tgt_values = target._get_engine_target()
        indexer, missing = self._engine.get_indexer_non_unique(tgt_values)
        return ensure_platform_int(indexer), missing
    def get_indexer_for(self, target, **kwargs):
        """"""
        Guaranteed return of an indexer even when non-unique.
        This dispatches to get_indexer or get_indexer_non_unique
        as appropriate.
        Returns
        -------
        numpy.ndarray
            List of indices.
        """"""
        if self.is_unique:
            return self.get_indexer(target, **kwargs)
        indexer, _ = self.get_indexer_non_unique(target, **kwargs)
        return indexer
    def _maybe_promote(self, other):
        # A hack, but it works
        if self.inferred_type == ""date"" and isinstance(other, ABCDatetimeIndex):
            return type(other)(self), other
        elif self.inferred_type == ""boolean"":
            if not is_object_dtype(self.dtype):
                return self.astype(""object""), other.astype(""object"")
        return self, other
    def groupby(self, values) -> PrettyDict[Hashable, np.ndarray]:
        """"""
        Group the index labels by a given array of values.
        Parameters
        ----------
        values : array
            Values used to determine the groups.
        Returns
        -------
        dict
            {group name -> group labels}
        """"""
        # TODO: if we are a MultiIndex, we can do better
        # that converting to tuples
        if isinstance(values, ABCMultiIndex):
            values = values.values
        values = ensure_categorical(values)
        result = values._reverse_indexer()
        # map to the label
        result = {k: self.take(v) for k, v in result.items()}
        return PrettyDict(result)
    def map(self, mapper, na_action=None):
        """"""
        Map values using input correspondence (a dict, Series, or function).
        Parameters
        ----------
        mapper : function, dict, or Series
            Mapping correspondence.
        na_action : {None, 'ignore'}
            If 'ignore', propagate NA values, without passing them to the
            mapping correspondence.
        Returns
        -------
        applied : Union[Index, MultiIndex], inferred
            The output of the mapping function applied to the index.
            If the function returns a tuple with more than one element
            a MultiIndex will be returned.
        """"""
        from pandas.core.indexes.multi import MultiIndex
        new_values = super()._map_values(mapper, na_action=na_action)
","[41, 43, 44]"
"                    suse_facts['distribution_release'] = release
                # Starting with SLES4SAP12 SP3 NAME reports 'SLES' instead of 'SLES_SAP'
                # According to SuSe Support (SR101182877871) we should use the CPE_NAME to detect SLES4SAP
                if re.search(""^CPE_NAME=.*sles_sap.*$"", line):
                    suse_facts['distribution'] = 'SLES_SAP'
        elif path == '/etc/SuSE-release':
            if 'open' in data.lower():
                data = data.splitlines()
                distdata = get_file_content(path).splitlines()[0]
                suse_facts['distribution'] = distdata.split()[0]
                for line in data:
                    release = re.search('CODENAME *= *([^\n]+)', line)
                    if release:
                        suse_facts['distribution_release'] = release.groups()[0].strip()
            elif 'enterprise' in data.lower():
                lines = data.splitlines()
                distribution = lines[0].split()[0]
                if ""Server"" in data:
                    suse_facts['distribution'] = ""SLES""
                elif ""Desktop"" in data:
                    suse_facts['distribution'] = ""SLED""
                for line in lines:
                    release = re.search('PATCHLEVEL = ([0-9]+)', line)  # SLES doesn't got funny release names
                    if release:
                        suse_facts['distribution_release'] = release.group(1)
                        suse_facts['distribution_version'] = collected_facts['distribution_version'] + '.' + release.group(1)
        return True, suse_facts
    def parse_distribution_file_Debian(self, name, data, path, collected_facts):
        debian_facts = {}
        if 'Debian' in data or 'Raspbian' in data:
            debian_facts['distribution'] = 'Debian'
            release = re.search(r""PRETTY_NAME=[^(]+ \(?([^)]+?)\)"", data)
            if release:
                debian_facts['distribution_release'] = release.groups()[0]
            # Last resort: try to find release from tzdata as either lsb is missing or this is very old debian
            if collected_facts['distribution_release'] == 'NA' and 'Debian' in data:
                dpkg_cmd = self.module.get_bin_path('dpkg')
                if dpkg_cmd:
                    cmd = ""%s --status tzdata|grep Provides|cut -f2 -d'-'"" % dpkg_cmd
                    rc, out, err = self.module.run_command(cmd)
                    if rc == 0:
                        debian_facts['distribution_release'] = out.strip()
        elif 'Ubuntu' in data:
            debian_facts['distribution'] = 'Ubuntu'
            # nothing else to do, Ubuntu gets correct info from python functions
        elif 'SteamOS' in data:
            debian_facts['distribution'] = 'SteamOS'
            # nothing else to do, SteamOS gets correct info from python functions
        elif path == '/etc/lsb-release' and 'Kali' in data:
            debian_facts['distribution'] = 'Kali'
            release = re.search('DISTRIB_RELEASE=(.*)', data)
            if release:
                debian_facts['distribution_release'] = release.groups()[0]
        elif 'Devuan' in data:
            debian_facts['distribution'] = 'Devuan'
            release = re.search(r""PRETTY_NAME=\""?[^(\""]+ \(?([^) \""]+)\)?"", data)
            if release:
                debian_facts['distribution_release'] = release.groups()[0]
            version = re.search(r""VERSION_ID=\""(.*)\"""", data)
            if version:
                debian_facts['distribution_version'] = version.group(1)
                debian_facts['distribution_major_version'] = version.group(1)
        elif 'Cumulus' in data:
            debian_facts['distribution'] = 'Cumulus Linux'
            version = re.search(r""VERSION_ID=(.*)"", data)
            if version:
                major, _minor, _dummy_ver = version.group(1).split(""."")
                debian_facts['distribution_version'] = version.group(1)
                debian_facts['distribution_major_version'] = major
            release = re.search(r'VERSION=""(.*)""', data)
            if release:
                debian_facts['distribution_release'] = release.groups()[0]
        elif ""Mint"" in data:
            debian_facts['distribution'] = 'Linux Mint'
            version = re.search(r""VERSION_ID=\""(.*)\"""", data)
            if version:
                debian_facts['distribution_version'] = version.group(1)
                debian_facts['distribution_major_version'] = version.group(1).split('.')[0]
        else:
            return False, debian_facts
        return True, debian_facts
    def parse_distribution_file_Mandriva(self, name, data, path, collected_facts):
        mandriva_facts = {}
        if 'Mandriva' in data:
            mandriva_facts['distribution'] = 'Mandriva'
            version = re.search('DISTRIB_RELEASE=""(.*)""', data)
            if version:
                mandriva_facts['distribution_version'] = version.groups()[0]
            release = re.search('DISTRIB_CODENAME=""(.*)""', data)
            if release:
                mandriva_facts['distribution_release'] = release.groups()[0]
            mandriva_facts['distribution'] = name
        else:
            return False, mandriva_facts
        return True, mandriva_facts
    def parse_distribution_file_NA(self, name, data, path, collected_facts):
        na_facts = {}
        for line in data.splitlines():
            distribution = re.search(""^NAME=(.*)"", line)
            if distribution and name == 'NA':
                na_facts['distribution'] = distribution.group(1).strip('""')
            version = re.search(""^VERSION=(.*)"", line)
            if version and collected_facts['distribution_version'] == 'NA':
                na_facts['distribution_version'] = version.group(1).strip('""')
        return True, na_facts
    def parse_distribution_file_Coreos(self, name, data, path, collected_facts):
        coreos_facts = {}
        # FIXME: pass in ro copy of facts for this kind of thing
        distro = get_distribution()
        if distro.lower() == 'coreos':
            if not data:
                # include fix from #15230, #15228
                # TODO: verify this is ok for above bugs
                return False, coreos_facts
            release = re.search(""^GROUP=(.*)"", data)
            if release:
                coreos_facts['distribution_release'] = release.group(1).strip('""')",[51]
"        dst = format_file_contents(src, fast=fast, mode=mode)
        return True
    except NothingChanged:
        return False
    finally:
        f = io.TextIOWrapper(
            sys.stdout.buffer, encoding=encoding, newline=newline, write_through=True
        )
        if write_back == WriteBack.YES:
            f.write(dst)
        elif write_back == WriteBack.DIFF:
            now = datetime.utcnow()
            src_name = f""STDIN\t{then} +0000""
            dst_name = f""STDOUT\t{now} +0000""
            f.write(diff(src, dst, src_name, dst_name))
        f.detach()

def format_file_contents(
    src_contents: str, *, fast: bool, mode: FileMode
) -> FileContent:
    """"""Reformat contents a file and return new contents.
    If `fast` is False, additionally confirm that the reformatted code is
    valid by calling :func:`assert_equivalent` and :func:`assert_stable` on it.
    `line_length` is passed to :func:`format_str`.
    """"""
    if src_contents.strip() == """":
        raise NothingChanged
    dst_contents = format_str(src_contents, mode=mode)
    if src_contents == dst_contents:
        raise NothingChanged
    if not fast:
        assert_equivalent(src_contents, dst_contents)
        assert_stable(src_contents, dst_contents, mode=mode)
    return dst_contents

def format_str(src_contents: str, *, mode: FileMode) -> FileContent:
    """"""Reformat a string and return new contents.
    `line_length` determines how many characters per line are allowed.
    """"""
    src_node = lib2to3_parse(src_contents.lstrip(), mode.target_versions)
    dst_contents = """"
    future_imports = get_future_imports(src_node)
    if mode.target_versions:
        versions = mode.target_versions
    else:
        versions = detect_target_versions(src_node)
    normalize_fmt_off(src_node)
    lines = LineGenerator(
        remove_u_prefix=""unicode_literals"" in future_imports
        or supports_feature(versions, Feature.UNICODE_LITERALS),
        is_pyi=mode.is_pyi,
        normalize_strings=mode.string_normalization,
    )
    elt = EmptyLineTracker(is_pyi=mode.is_pyi)
    empty_line = Line()
    after = 0
    split_line_features = {
        feature
        for feature in {Feature.TRAILING_COMMA_IN_CALL, Feature.TRAILING_COMMA_IN_DEF}
        if supports_feature(versions, feature)
    }
    for current_line in lines.visit(src_node):
        for _ in range(after):
            dst_contents += str(empty_line)
        before, after = elt.maybe_empty_lines(current_line)
        for _ in range(before):
            dst_contents += str(empty_line)
        for line in split_line(
            current_line, line_length=mode.line_length, features=split_line_features
        ):
            dst_contents += str(line)
    return dst_contents

def decode_bytes(src: bytes) -> Tuple[FileContent, Encoding, NewLine]:
    """"""Return a tuple of (decoded_contents, encoding, newline).
    `newline` is either CRLF or LF but `decoded_contents` is decoded with
    universal newlines (i.e. only contains LF).
    """"""
    srcbuf = io.BytesIO(src)
    encoding, lines = tokenize.detect_encoding(srcbuf.readline)
    if not lines:
        return """", encoding, ""\n""
    newline = ""\r\n"" if b""\r\n"" == lines[0][-2:] else ""\n""
    srcbuf.seek(0)
    with io.TextIOWrapper(srcbuf, encoding) as tiow:
        return tiow.read(), encoding, newline

def get_grammars(target_versions: Set[TargetVersion]) -> List[Grammar]:
    if not target_versions:
        # No target_version specified, so try all grammars.
        return [
            pygram.python_grammar_no_print_statement_no_exec_statement,
            pygram.python_grammar_no_print_statement,
            pygram.python_grammar,
        ]
    elif all(version.is_python2() for version in target_versions):
        # Python 2-only code, so try Python 2 grammars.
        return [pygram.python_grammar_no_print_statement, pygram.python_grammar]
    else:
        # Python 3-compatible code, so only try Python 3 grammar.
        return [pygram.python_grammar_no_print_statement_no_exec_statement]

def lib2to3_parse(src_txt: str, target_versions: Iterable[TargetVersion] = ()) -> Node:
    """"""Given a string with source, return the lib2to3 Node.""""""
    if src_txt[-1:] != ""\n"":
        src_txt += ""\n""
    for grammar in get_grammars(set(target_versions)):
        drv = driver.Driver(grammar, pytree.convert)
        try:
            result = drv.parse_string(src_txt, True)
            break
        except ParseError as pe:","[99, 103, 104, 105, 109, 112, 120, 121]"
"
def unified_strdate(date_str):
    """"""Return a string with the date in the format YYYYMMDD""""""
    upload_date = None
    #Replace commas
    date_str = date_str.replace(',',' ')
    # %z (UTC offset) is only supported in python>=3.2
    date_str = re.sub(r' ?(\+|-)[0-9:]*$', '', date_str)
    format_expressions = [
        '%d %B %Y',
        '%B %d %Y',
        '%b %d %Y',
        '%Y-%m-%d',
        '%d/%m/%Y',
        '%Y/%m/%d %H:%M:%S',
        '%Y-%m-%d %H:%M:%S',
        '%d.%m.%Y %H:%M',
        '%Y-%m-%dT%H:%M:%SZ',
        '%Y-%m-%dT%H:%M:%S.%fZ',
        '%Y-%m-%dT%H:%M:%S.%f0Z',
        '%Y-%m-%dT%H:%M:%S',
        '%Y-%m-%dT%H:%M',
    ]
    for expression in format_expressions:
        try:
            upload_date = datetime.datetime.strptime(date_str, expression).strftime('%Y%m%d')
        except ValueError:
            pass
    if upload_date is None:
        timetuple = email.utils.parsedate_tz(date_str)
        if timetuple:
            upload_date = datetime.datetime(*timetuple[:6]).strftime('%Y%m%d')
    return upload_date
def determine_ext(url, default_ext=u'unknown_video'):
    guess = url.partition(u'?')[0].rpartition(u'.')[2]
    if re.match(r'^[A-Za-z0-9]+$', guess):
        return guess
    else:
        return default_ext
def subtitles_filename(filename, sub_lang, sub_format):
    return filename.rsplit('.', 1)[0] + u'.' + sub_lang + u'.' + sub_format
def date_from_str(date_str):
    """"""
    Return a datetime object from a string in the format YYYYMMDD or
    (now|today)[+-][0-9](day|week|month|year)(s)?""""""
    today = datetime.date.today()
    if date_str == 'now'or date_str == 'today':
        return today
    match = re.match('(now|today)(?P<sign>[+-])(?P<time>\d+)(?P<unit>day|week|month|year)(s)?', date_str)
    if match is not None:
        sign = match.group('sign')
        time = int(match.group('time'))
        if sign == '-':
            time = -time
        unit = match.group('unit')
        #A bad aproximation?
        if unit == 'month':
            unit = 'day'
            time *= 30
        elif unit == 'year':
            unit = 'day'
            time *= 365
        unit += 's'
        delta = datetime.timedelta(**{unit: time})
        return today + delta
    return datetime.datetime.strptime(date_str, ""%Y%m%d"").date()
    
def hyphenate_date(date_str):
    """"""
    Convert a date in 'YYYYMMDD' format to 'YYYY-MM-DD' format""""""
    match = re.match(r'^(\d\d\d\d)(\d\d)(\d\d)$', date_str)
    if match is not None:
        return '-'.join(match.groups())
    else:
        return date_str
class DateRange(object):
    """"""Represents a time interval between two dates""""""
    def __init__(self, start=None, end=None):
        """"""start and end must be strings in the format accepted by date""""""
        if start is not None:
            self.start = date_from_str(start)
        else:
            self.start = datetime.datetime.min.date()
        if end is not None:
            self.end = date_from_str(end)
        else:
            self.end = datetime.datetime.max.date()
        if self.start > self.end:
            raise ValueError('Date range: ""%s"" , the start date must be before the end date' % self)
    @classmethod
    def day(cls, day):
        """"""Returns a range that only contains the given day""""""
        return cls(day,day)
    def __contains__(self, date):
        """"""Check if the date is in the range""""""
        if not isinstance(date, datetime.date):
            date = date_from_str(date)
        return self.start <= date <= self.end
    def __str__(self):
        return '%s - %s' % ( self.start.isoformat(), self.end.isoformat())

def platform_name():
    """""" Returns the platform name as a compat_str """"""
    res = platform.platform()
    if isinstance(res, bytes):
        res = res.decode(preferredencoding())
    assert isinstance(res, compat_str)
    return res

def write_string(s, out=None):
    if out is None:
        out = sys.stderr
    assert type(s) == compat_str
    if ('b' in getattr(out, 'mode', '') or
            sys.version_info[0] < 3):  # Python 2 lies about mode of sys.stderr
        s = s.encode(preferredencoding(), 'ignore')
    try:
        out.write(s)","[6, 8]"
"import os
import json
import argparse
import re
from collections import namedtuple
from . import pretty
from . import __doc__ as doc
from . import __version__ as version

SEP_COMMON = ':'
SEP_HEADERS = SEP_COMMON
SEP_DATA = '='
SEP_DATA_RAW_JSON = ':='
SEP_FILES = '@'
PRETTIFY_STDOUT_TTY_ONLY = object()
OUT_REQUEST_HEADERS = 'H'
OUT_REQUEST_BODY = 'B'
OUT_RESPONSE_HEADERS = 'h'
OUT_RESPONSE_BODY = 'b'
OUTPUT_OPTIONS = [OUT_REQUEST_HEADERS,
                  OUT_REQUEST_BODY,
                  OUT_RESPONSE_HEADERS,
                  OUT_RESPONSE_BODY]

class ParseError(Exception):
    pass

KeyValue = namedtuple('KeyValue', ['key', 'value', 'sep', 'orig'])
class KeyValueType(object):
    """"""A type used with `argparse`.""""""
    def __init__(self, *separators):
        self.separators = separators
    def __call__(self, string):
        found = {}
        for sep in self.separators:
            regex = '[^\\\\]' + sep
            match = re.search(regex, string)
            if match:
                found[match.start() + 1] = sep
        if not found:
            #noinspection PyExceptionInherit
            raise argparse.ArgumentTypeError(
                '""%s"" is not a valid value' % string)
        # split the string at the earliest non-escaped separator.
        seploc = min(found.keys())
        sep = found[seploc]
        key = string[:seploc]
        value = string[seploc + len(sep):]
        # remove escape chars
        for sepstr in self.separators:
            key = key.replace('\\' + sepstr, sepstr)
            value = value.replace('\\' + sepstr, sepstr)
        return KeyValue(key=key, value=value, sep=sep, orig=string)

def parse_items(items, data=None, headers=None, files=None):
    """"""Parse `KeyValueType` `items` into `data`, `headers` and `files`.""""""
    if headers is None:
        headers = {}
    if data is None:
        data = {}
    if files is None:
        files = {}
    for item in items:
        value = item.value
        key = item.key
        if item.sep == SEP_HEADERS:
            target = headers
        elif item.sep == SEP_FILES:
            try:
                value = open(os.path.expanduser(item.value), 'r')
            except IOError as e:
                raise ParseError(
                    'Invalid argument %r. %s' % (item.orig, e))
            if not key:
                key = os.path.basename(value.name)
            target = files
        elif item.sep in [SEP_DATA, SEP_DATA_RAW_JSON]:
            if item.sep == SEP_DATA_RAW_JSON:
                try:
                    value = json.loads(item.value)
                except ValueError:
                    raise ParseError('%s is not valid JSON' % item.orig)
            target = data
        else:
            raise ParseError('%s is not valid item' % item.orig)
        if key in target:
            ParseError('duplicate item %s (%s)' % (item.key, item.orig))
        target[key] = value
    return headers, data, files

def _(text):
    """"""Normalize white space.""""""
    return ' '.join(text.strip().split())

class HTTPieArgumentParser(argparse.ArgumentParser):
    def parse_args(self, args=None, namespace=None):
        args = super(HTTPieArgumentParser, self).parse_args(args, namespace)
        self._validate_output_options(args)
        self._validate_auth_options(args)
        return args
    def _validate_output_options(self, args):
        unknown_output_options = set(args.output_options) - set(OUTPUT_OPTIONS)
        if unknown_output_options:
            self.error('Unknown output options: %s' % ','.join(unknown_output_options))
    def _validate_auth_options(self, args):
        if args.auth_type and not args.auth:
            self.error('--auth-type can only be used with --auth')
","[42, 43, 44, 45]"
"import atexit
import os
import pickle
import re
import shelve
import sys
import six
from decorator import decorator
from difflib import get_close_matches as difflib_get_close_matches
from functools import wraps
from .logs import warn, exception
from .conf import settings
from .system import Path
DEVNULL = open(os.devnull, 'w')
if six.PY2:
    import anydbm
    shelve_open_error = anydbm.error
else:
    import dbm
    shelve_open_error = dbm.error

def memoize(fn):
    """"""Caches previous calls to the function.""""""
    memo = {}
    @wraps(fn)
    def wrapper(*args, **kwargs):
        if not memoize.disabled:
            key = pickle.dumps((args, kwargs))
            if key not in memo:
                memo[key] = fn(*args, **kwargs)
            value = memo[key]
        else:
            # Memoize is disabled, call the function
            value = fn(*args, **kwargs)
        return value
    return wrapper

memoize.disabled = False

@memoize
def which(program):
    """"""Returns `program` path or `None`.""""""
    try:
        from shutil import which
        return which(program)
    except ImportError:
        def is_exe(fpath):
            return os.path.isfile(fpath) and os.access(fpath, os.X_OK)
        fpath, fname = os.path.split(program)
        if fpath:
            if is_exe(program):
                return program
        else:
            for path in os.environ[""PATH""].split(os.pathsep):
                path = path.strip('""')
                exe_file = os.path.join(path, program)
                if is_exe(exe_file):
                    return exe_file
        return None

def default_settings(params):
    """"""Adds default values to settings if it not presented.
    Usage:
        @default_settings({'apt': '/usr/bin/apt'})
        def match(command):
            print(settings.apt)
    """"""
    def _default_settings(fn, command):
        for k, w in params.items():
            settings.setdefault(k, w)
        return fn(command)
    return decorator(_default_settings)

def get_closest(word, possibilities, cutoff=0.6, fallback_to_first=True):
    """"""Returns closest match or just first from possibilities.""""""
    possibilities = list(possibilities)
    try:
        return difflib_get_close_matches(word, possibilities, 1, cutoff)[0]
    except IndexError:
        if fallback_to_first:
            return possibilities[0]

def get_close_matches(word, possibilities, n=None, cutoff=0.6):
    """"""Overrides `difflib.get_close_match` to controle argument `n`.""""""
    if n is None:
        n = settings.num_close_matches
    return difflib_get_close_matches(word, possibilities, n, cutoff)

@memoize
def get_all_executables():
    from thefuck.shells import shell
    def _safe(fn, fallback):
        try:
            return fn()
        except OSError:
            return fallback
    tf_alias = get_alias()
    tf_entry_points = ['thefuck', 'fuck']
    bins = [exe.name.decode('utf8') if six.PY2 else exe.name
            for path in os.environ.get('PATH', '').split(':')
            for exe in _safe(lambda: list(Path(path).iterdir()), [])
            if not _safe(exe.is_dir, True)
            and exe.name not in tf_entry_points]
    aliases = [alias.decode('utf8') if six.PY2 else alias
               for alias in shell.get_aliases() if alias != tf_alias]
",[120]
"        want = param_list_to_dict(want, ""vlan_id"", remove_key=False)
        have = param_list_to_dict(have, ""vlan_id"", remove_key=False)
        if state == 'overridden':
            commands = self._state_overridden(want, have)
        elif state == 'deleted':
            commands = self._state_deleted(want, have)
        elif state == 'merged':
            commands = self._state_merged(want, have)
        elif state == 'replaced':
            commands = self._state_replaced(want, have)
        return commands
    @staticmethod
    def _state_replaced(want, have):
        """""" The command generator when state is replaced
        :rtype: A list
        :returns: the commands necessary to migrate the current configuration
                  to the desired configuration
        """"""
        commands = []
        for vlan_id, desired in want.items():
            if vlan_id in have:
                extant = have[vlan_id]
            else:
                extant = dict()
            add_config = dict_diff(extant, desired)
            del_config = dict_diff(desired, extant)
            commands.extend(generate_commands(vlan_id, add_config, del_config))
        return commands
    @staticmethod
    def _state_overridden(want, have):
        """""" The command generator when state is overridden
        :rtype: A list
        :returns: the commands necessary to migrate the current configuration
                  to the desired configuration
        """"""
        commands = []
        for vlan_id, extant in have.items():
            if vlan_id in want:
                desired = want[vlan_id]
            else:
                desired = dict()
            add_config = dict_diff(extant, desired)
            del_config = dict_diff(desired, extant)
            commands.extend(generate_commands(vlan_id, add_config, del_config))
        # Handle vlans not already in config
        new_vlans = [vlan_id for vlan_id in want if vlan_id not in have]
        for vlan_id in new_vlans:
            desired = want[vlan_id]
            extant = dict(vlan_id=vlan_id)
            add_config = dict_diff(extant, desired)
            commands.extend(generate_commands(vlan_id, add_config, {}))
        return commands
    @staticmethod
    def _state_merged(want, have):
        """""" The command generator when state is merged
        :rtype: A list
        :returns: the commands necessary to merge the provided into
                  the current configuration
        """"""
        commands = []
        for vlan_id, desired in want.items():
            if vlan_id in have:
                extant = have[vlan_id]
            else:
                extant = dict()
            add_config = dict_diff(extant, desired)
            commands.extend(generate_commands(vlan_id, add_config, {}))
        return commands
    @staticmethod
    def _state_deleted(want, have):
        """""" The command generator when state is deleted
        :rtype: A list
        :returns: the commands necessary to remove the current configuration
                  of the provided objects
        """"""
        commands = []
        for vlan_id in want:
            desired = dict()
            if vlan_id in have:
                extant = have[vlan_id]
            else:
                continue
            del_config = dict_diff(desired, extant)
            commands.extend(generate_commands(vlan_id, {}, del_config))
        return commands

def generate_commands(vlan_id, to_set, to_remove):
    commands = []
    if ""vlan_id"" in to_remove:
        return [""no vlan {0}"".format(vlan_id)]
    for key, value in to_set.items():
        if key == ""vlan_id"" or value is None:
            continue
        commands.append(""{0} {1}"".format(key, value))
    for key in to_remove:
        commands.append(""no {0}"".format(key))
    if commands:
        commands.insert(0, ""vlan {0}"".format(vlan_id))
","[120, 121, 122, 125]"
"from __future__ import unicode_literals
import json
import re
import socket
from .common import InfoExtractor
from ..utils import (
    compat_http_client,
    compat_str,
    compat_urllib_error,
    compat_urllib_parse,
    compat_urllib_request,
    urlencode_postdata,
    ExtractorError,
)

class FacebookIE(InfoExtractor):
    _VALID_URL = r'''(?x)
        https?://(?:\w+\.)?facebook\.com/
        (?:[^#]*?\#!/)?
        (?:video/video\.php|photo\.php|video\.php|video/embed)\?(?:.*?)
        (?:v|video_id)=(?P<id>[0-9]+)
        (?:.*)'''
    _LOGIN_URL = 'https://www.facebook.com/login.php?next=http%3A%2F%2Ffacebook.com%2Fhome.php&login_attempt=1'
    _CHECKPOINT_URL = 'https://www.facebook.com/checkpoint/?next=http%3A%2F%2Ffacebook.com%2Fhome.php&_fb_noscript=1'
    _NETRC_MACHINE = 'facebook'
    IE_NAME = 'facebook'
    _TESTS = [{
        'url': 'https://www.facebook.com/video.php?v=637842556329505&fref=nf',
        'md5': '6a40d33c0eccbb1af76cf0485a052659',
        'info_dict': {
            'id': '637842556329505',
            'ext': 'mp4',
            'duration': 38,
            'title': 'Did you know Kei Nishikori is the first Asian man to ever reach a Grand Slam fin...',
        }
    }, {
        'url': 'https://www.facebook.com/video.php?v=10204634152394104',
        'only_matching': True,
    }]
    def _login(self):
        (useremail, password) = self._get_login_info()
        if useremail is None:
            return
        login_page_req = compat_urllib_request.Request(self._LOGIN_URL)
        login_page_req.add_header('Cookie', 'locale=en_US')
        login_page = self._download_webpage(login_page_req, None,
            note='Downloading login page',
            errnote='Unable to download login page')
        lsd = self._search_regex(
            r'<input type=""hidden"" name=""lsd"" value=""([^""]*)""',
            login_page, 'lsd')
        lgnrnd = self._search_regex(r'name=""lgnrnd"" value=""([^""]*?)""', login_page, 'lgnrnd')
        login_form = {
            'email': useremail,
            'pass': password,
            'lsd': lsd,
            'lgnrnd': lgnrnd,
            'next': 'http://facebook.com/home.php',
            'default_persistent': '0',
            'legacy_return': '1',
            'timezone': '-60',
            'trynum': '1',
            }
        request = compat_urllib_request.Request(self._LOGIN_URL, urlencode_postdata(login_form))
        request.add_header('Content-Type', 'application/x-www-form-urlencoded')
        try:
            login_results = self._download_webpage(request, None,
                note='Logging in', errnote='unable to fetch login page')
            if re.search(r'<form(.*)name=""login""(.*)</form>', login_results) is not None:
                self._downloader.report_warning('unable to log in: bad username/password, or exceded login rate limit (~3/min). Check credentials or wait.')
                return
            check_form = {
                'fb_dtsg': self._search_regex(r'name=""fb_dtsg"" value=""(.+?)""', login_results, 'fb_dtsg'),
                'h': self._search_regex(
                    r'name=""h""\s+(?:\w+=""[^""]+""\s+)*?value=""([^""]+)""', login_results, 'h'),
                'name_action_selected': 'dont_save',
            }
            check_req = compat_urllib_request.Request(self._CHECKPOINT_URL, urlencode_postdata(check_form))
            check_req.add_header('Content-Type', 'application/x-www-form-urlencoded')
            check_response = self._download_webpage(check_req, None,
                note='Confirming login')
            if re.search(r'id=""checkpointSubmitButton""', check_response) is not None:
                self._downloader.report_warning('Unable to confirm login, you have to login in your brower and authorize the login.')
        except (compat_urllib_error.URLError, compat_http_client.HTTPException, socket.error) as err:
            self._downloader.report_warning('unable to log in: %s' % compat_str(err))
            return
    def _real_initialize(self):
        self._login()
    def _real_extract(self, url):
        mobj = re.match(self._VALID_URL, url)
        video_id = mobj.group('id')
        url = 'https://www.facebook.com/video/video.php?v=%s' % video_id
        webpage = self._download_webpage(url, video_id)
        BEFORE = '{swf.addParam(param[0], param[1]);});\n'
        AFTER = '.forEach(function(variable) {swf.addVariable(variable[0], variable[1]);});'
        m = re.search(re.escape(BEFORE) + '(.*?)' + re.escape(AFTER), webpage)
        if not m:
            m_msg = re.search(r'class=""[^""]*uiInterstitialContent[^""]*""><div>(.*?)</div>', webpage)
            if m_msg is not None:
                raise ExtractorError(
                    'The video is not available, Facebook said: ""%s""' % m_msg.group(1),
                    expected=True)
            else:
                raise ExtractorError('Cannot parse data')
        data = dict(json.loads(m.group(1)))
        params_raw = compat_urllib_parse.unquote(data['params'])
        params = json.loads(params_raw)
        video_data = params['video_data'][0]
        video_url = video_data.get('hd_src')
        if not video_url:
            video_url = video_data['sd_src']
        if not video_url:
            raise ExtractorError('Cannot find video URL')
        video_title = self._html_search_regex(",[14]
"""""""
Base and utility classes for tseries type pandas objects.
""""""
from datetime import datetime, timedelta
from typing import Any, List, Optional, Union, cast
import numpy as np
from pandas._libs import NaT, iNaT, join as libjoin, lib
from pandas._libs.tslibs import timezones
from pandas._typing import Label
from pandas.compat.numpy import function as nv
from pandas.errors import AbstractMethodError
from pandas.util._decorators import Appender, cache_readonly, doc
from pandas.core.dtypes.common import (
    ensure_int64,
    is_bool_dtype,
    is_datetime64_any_dtype,
    is_dtype_equal,
    is_integer,
    is_list_like,
    is_object_dtype,
    is_period_dtype,
    is_scalar,
    is_timedelta64_dtype,
)
from pandas.core.dtypes.concat import concat_compat
from pandas.core.dtypes.generic import ABCIndex, ABCIndexClass, ABCSeries
from pandas.core.dtypes.missing import isna
from pandas.core import algorithms
from pandas.core.arrays import DatetimeArray, PeriodArray, TimedeltaArray
from pandas.core.arrays.datetimelike import DatetimeLikeArrayMixin
from pandas.core.base import IndexOpsMixin
import pandas.core.indexes.base as ibase
from pandas.core.indexes.base import Index, _index_shared_docs
from pandas.core.indexes.extension import (
    ExtensionIndex,
    inherit_names,
    make_wrapped_arith_op,
)
from pandas.core.indexes.numeric import Int64Index
from pandas.core.ops import get_op_result_name
from pandas.core.tools.timedeltas import to_timedelta
from pandas.tseries.frequencies import DateOffset, to_offset
from pandas.tseries.offsets import Tick
_index_doc_kwargs = dict(ibase._index_doc_kwargs)

def _join_i8_wrapper(joinf, with_indexers: bool = True):
    """"""
    Create the join wrapper methods.
    """"""
    @staticmethod  # type: ignore
    def wrapper(left, right):
        if isinstance(left, (np.ndarray, ABCIndex, ABCSeries, DatetimeLikeArrayMixin)):
            left = left.view(""i8"")
        if isinstance(right, (np.ndarray, ABCIndex, ABCSeries, DatetimeLikeArrayMixin)):
            right = right.view(""i8"")
        results = joinf(left, right)
        if with_indexers:
            # dtype should be timedelta64[ns] for TimedeltaIndex
            #  and datetime64[ns] for DatetimeIndex
            dtype = left.dtype.base
            join_index, left_indexer, right_indexer = results
            join_index = join_index.view(dtype)
            return join_index, left_indexer, right_indexer
        return results
    return wrapper

def _make_wrapped_arith_op_with_freq(opname: str):
    """"""
    Dispatch the operation to the underlying ExtensionArray, and infer
    the appropriate frequency for the result.
    """"""
    meth = make_wrapped_arith_op(opname)
    def wrapped(self, other):
        result = meth(self, other)
        if result is NotImplemented:
            return NotImplemented
        new_freq = self._get_addsub_freq(other, result)
        result._freq = new_freq
        return result
    wrapped.__name__ = opname
    return wrapped

@inherit_names(
    [""inferred_freq"", ""_isnan"", ""_resolution"", ""resolution""],
    DatetimeLikeArrayMixin,
    cache=True,
)
@inherit_names(
    [""mean"", ""asi8"", ""_box_func""], DatetimeLikeArrayMixin,
)
class DatetimeIndexOpsMixin(ExtensionIndex):
    """"""
    Common ops mixin to support a unified interface datetimelike Index.
    """"""
    _data: Union[DatetimeArray, TimedeltaArray, PeriodArray]
    freq: Optional[DateOffset]
    freqstr: Optional[str]
    _resolution: int
    _bool_ops: List[str] = []
    _field_ops: List[str] = []
    hasnans = cache_readonly(DatetimeLikeArrayMixin._hasnans.fget)  # type: ignore
    _hasnans = hasnans  # for index / array -agnostic code
    @property
    def is_all_dates(self) -> bool:
        return True
    # ------------------------------------------------------------------------
    # Abstract data attributes",[46]
"            return aware
        values = self.view(""i8"")
        result = round_nsint64(values, mode, freq)
        result = self._maybe_mask_results(result, fill_value=NaT)
        return self._simple_new(result, dtype=self.dtype)
    @Appender((_round_doc + _round_example).format(op=""round""))
    def round(self, freq, ambiguous=""raise"", nonexistent=""raise""):
        return self._round(freq, RoundTo.NEAREST_HALF_EVEN, ambiguous, nonexistent)
    @Appender((_round_doc + _floor_example).format(op=""floor""))
    def floor(self, freq, ambiguous=""raise"", nonexistent=""raise""):
        return self._round(freq, RoundTo.MINUS_INFTY, ambiguous, nonexistent)
    @Appender((_round_doc + _ceil_example).format(op=""ceil""))
    def ceil(self, freq, ambiguous=""raise"", nonexistent=""raise""):
        return self._round(freq, RoundTo.PLUS_INFTY, ambiguous, nonexistent)
    def _with_freq(self, freq):
        """"""
        Helper to set our freq in-place, returning self to allow method chaining.
        Parameters
        ----------
        freq : DateOffset, None, or ""infer""
        Returns
        -------
        self
        """"""
        # GH#29843
        if freq is None:
            # Always valid
            pass
        elif len(self) == 0 and isinstance(freq, DateOffset):
            # Always valid.  In the TimedeltaArray case, we assume this
            #  is a Tick offset.
            pass
        else:
            # As an internal method, we can ensure this assertion always holds
            assert freq == ""infer""
            freq = frequencies.to_offset(self.inferred_freq)
        self._freq = freq
        return self

class DatetimeLikeArrayMixin(
    ExtensionOpsMixin, AttributesMixin, NDArrayBackedExtensionArray
):
    """"""
    Shared Base/Mixin class for DatetimeArray, TimedeltaArray, PeriodArray
    Assumes that __new__/__init__ defines:
        _data
        _freq
    and that the inheriting class has methods:
        _generate_range
    """"""
    # ------------------------------------------------------------------
    # NDArrayBackedExtensionArray compat
    @property
    def _ndarray(self) -> np.ndarray:
        # NB: A bunch of Interval tests fail if we use ._data
        return self.asi8
    def _from_backing_data(self: _T, arr: np.ndarray) -> _T:
        # Note: we do not retain `freq`
        return type(self)(arr, dtype=self.dtype)  # type: ignore
    # ------------------------------------------------------------------
    @property
    def _box_func(self):
        """"""
        box function to get object from internal representation
        """"""
        raise AbstractMethodError(self)
    def _box_values(self, values):
        """"""
        apply box func to passed values
        """"""
        return lib.map_infer(values, self._box_func)
    def __iter__(self):
        return (self._box_func(v) for v in self.asi8)
    @property
    def asi8(self) -> np.ndarray:
        """"""
        Integer representation of the values.
        Returns
        -------
        ndarray
            An ndarray with int64 dtype.
        """"""
        # do not cache or you'll create a memory leak
        return self._data.view(""i8"")
    # ----------------------------------------------------------------
    # Rendering Methods
    def _format_native_types(self, na_rep=""NaT"", date_format=None):
        """"""
        Helper method for astype when converting to strings.
        Returns
        -------
        ndarray[str]
        """"""
        raise AbstractMethodError(self)
    def _formatter(self, boxed=False):
        # TODO: Remove Datetime & DatetimeTZ formatters.
        return ""'{}'"".format
    # ----------------------------------------------------------------
    # Array-Like / EA-Interface Methods
    def __array__(self, dtype=None) -> np.ndarray:
        # used for Timedelta/DatetimeArray, overwritten by PeriodArray","[21, 29, 44, 45]"
"
@Appender(IndexingMixin.iloc.__doc__)
class _iLocIndexer(_LocationIndexer):
    _valid_types = (
        ""integer, integer slice (START point is INCLUDED, END ""
        ""point is EXCLUDED), listlike of integers, boolean array""
    )
    _takeable = True
    # -------------------------------------------------------------------
    # Key Checks
    def _validate_key(self, key, axis: int):
        if com.is_bool_indexer(key):
            if hasattr(key, ""index"") and isinstance(key.index, Index):
                if key.index.inferred_type == ""integer"":
                    raise NotImplementedError(
                        ""iLocation based boolean ""
                        ""indexing on an integer type ""
                        ""is not available""
                    )
                raise ValueError(
                    ""iLocation based boolean indexing cannot use ""
                    ""an indexable as a mask""
                )
            return
        if isinstance(key, slice):
            return
        elif is_integer(key):
            self._validate_integer(key, axis)
        elif isinstance(key, tuple):
            # a tuple should already have been caught by this point
            # so don't treat a tuple as a valid indexer
            raise IndexingError(""Too many indexers"")
        elif is_list_like_indexer(key):
            arr = np.array(key)
            len_axis = len(self.obj._get_axis(axis))
            # check that the key has a numeric dtype
            if not is_numeric_dtype(arr.dtype):
                raise IndexError(f"".iloc requires numeric indexers, got {arr}"")
            # check that the key does not exceed the maximum size of the index
            if len(arr) and (arr.max() >= len_axis or arr.min() < -len_axis):
                raise IndexError(""positional indexers are out-of-bounds"")
        else:
            raise ValueError(f""Can only index by location with a [{self._valid_types}]"")
    def _has_valid_setitem_indexer(self, indexer):
        self._has_valid_positional_setitem_indexer(indexer)
    def _has_valid_positional_setitem_indexer(self, indexer) -> bool:
        """"""
        Validate that a positional indexer cannot enlarge its target
        will raise if needed, does not modify the indexer externally.
        Returns
        -------
        bool
        """"""
        if isinstance(indexer, dict):
            raise IndexError(f""{self.name} cannot enlarge its target object"")
        else:
            if not isinstance(indexer, tuple):
                indexer = _tuplify(self.ndim, indexer)
            for ax, i in zip(self.obj.axes, indexer):
                if isinstance(i, slice):
                    # should check the stop slice?
                    pass
                elif is_list_like_indexer(i):
                    # should check the elements?
                    pass
                elif is_integer(i):
                    if i >= len(ax):
                        raise IndexError(
                            f""{self.name} cannot enlarge its target object""
                        )
                elif isinstance(i, dict):
                    raise IndexError(f""{self.name} cannot enlarge its target object"")
        return True
    def _is_scalar_access(self, key: Tuple) -> bool:
        """"""
        Returns
        -------
        bool
        """"""
        # this is a shortcut accessor to both .loc and .iloc
        # that provide the equivalent access of .at and .iat
        # a) avoid getting things via sections and (to minimize dtype changes)
        # b) provide a performant path
        if len(key) != self.ndim:
            return False
        for i, k in enumerate(key):
            if not is_integer(k):
                return False
            ax = self.obj.axes[i]
            if not ax.is_unique:
                return False
        return True
    def _validate_integer(self, key: int, axis: int) -> None:
        """"""
        Check that 'key' is a valid position in the desired axis.
        Parameters
        ----------
        key : int
            Requested position.
        axis : int
            Desired axis.
        Raises
        ------
        IndexError
            If 'key' is not a valid position in axis 'axis'.
        """"""
        len_axis = len(self.obj._get_axis(axis))
        if key >= len_axis or key < -len_axis:
            raise IndexError(""single positional indexer is out-of-bounds"")
","[101, 102, 103, 104]"
"""""""
==================================
Colormap Normalizations Symlognorm
==================================
Demonstration of using norm to map colormaps onto data in non-linear ways.
""""""
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.colors as colors
""""""
SymLogNorm: two humps, one negative and one positive, The positive
with 5-times the amplitude. Linearly, you cannot see detail in the
negative hump.  Here we logarithmically scale the positive and
negative data separately.
Note that colorbar labels do not come out looking very good.
""""""
N = 100
X, Y = np.mgrid[-3:3:complex(0, N), -2:2:complex(0, N)]
Z1 = np.exp(-X**2 - Y**2)
Z2 = np.exp(-(X - 1)**2 - (Y - 1)**2)
Z = (Z1 - Z2) * 2
fig, ax = plt.subplots(2, 1)
pcm = ax[0].pcolormesh(X, Y, Z,
                       norm=colors.SymLogNorm(linthresh=0.03, linscale=0.03,
                                              vmin=-1.0, vmax=1.0),
                       cmap='RdBu_r')
fig.colorbar(pcm, ax=ax[0], extend='both')
pcm = ax[1].pcolormesh(X, Y, Z, cmap='RdBu_r', vmin=-np.max(Z))
fig.colorbar(pcm, ax=ax[1], extend='both')
plt.show()",[31]
"    def start(self, workers=1, max_queue_size=10):
        """"""Start the handler's workers.
        # Arguments
            workers: number of worker threads
            max_queue_size: queue size
                (when full, workers could block on `put()`)
        """"""
        if self.use_multiprocessing:
            self.executor = multiprocessing.Pool(workers)
        else:
            self.executor = ThreadPool(workers)
        self.workers = workers
        self.queue = queue.Queue(max_queue_size)
        self.stop_signal = threading.Event()
        self.run_thread = threading.Thread(target=self._run)
        self.run_thread.daemon = True
        self.run_thread.start()
    def _wait_queue(self):
        """"""Wait for the queue to be empty.""""""
        while True:
            time.sleep(0.1)
            if self.queue.unfinished_tasks == 0 or self.stop_signal.is_set():
                return
    def _run(self):
        """"""Function to submit request to the executor and queue the `Future` objects.""""""
        sequence = list(range(len(self.sequence)))
        self._send_sequence()  # Share the initial sequence
        while True:
            if self.shuffle:
                random.shuffle(sequence)
            for i in sequence:
                if self.stop_signal.is_set():
                    return
                self.queue.put(
                    self.executor.apply_async(get_index, (self.uid, i)), block=True)
            # Done with the current epoch, waiting for the final batches
            self._wait_queue()
            if self.stop_signal.is_set():
                # We're done
                return
            # Call the internal on epoch end.
            self.sequence.on_epoch_end()
            self._send_sequence()  # Update the pool
    def get(self):
        """"""Creates a generator to extract data from the queue.
        Skip the data if it is `None`.
        # Returns
            Generator yielding tuples (inputs, targets)
                or (inputs, targets, sample_weights)
        """"""
        try:
            while self.is_running():
                inputs = self.queue.get(block=True).get()
                self.queue.task_done()
                if inputs is not None:
                    yield inputs
        except Exception as e:
            self.stop()
            raise StopIteration(e)
    def _send_sequence(self):
        """"""Send current Sequence to all workers.""""""
        global _SHARED_SEQUENCES
        _SHARED_SEQUENCES[self.uid] = self.sequence  # For new processes that may spawn
        self._close_pool()
        if self.use_multiprocessing:
            self.executor = multiprocessing.Pool(self.workers)
        else:
            self.executor = ThreadPool(self.workers)
    def stop(self, timeout=None):
        """"""Stops running threads and wait for them to exit, if necessary.
        Should be called by the same thread which called `start()`.
        # Arguments
            timeout: maximum time to wait on `thread.join()`
        """"""
        global _SHARED_SEQUENCES
        self.stop_signal.set()
        with self.queue.mutex:
            self.queue.queue.clear()
            self.queue.unfinished_tasks = 0
            self.queue.not_full.notify()
        self._close_pool()
        self.run_thread.join(timeout)
        _SHARED_SEQUENCES[self.uid] = None
    def _close_pool(self):
        self.executor.close()
        self.executor.join()

class GeneratorEnqueuer(SequenceEnqueuer):
    """"""Builds a queue out of a data generator.
    The provided generator can be finite in which case the class will throw
    a `StopIteration` exception.
    Used in `fit_generator`, `evaluate_generator`, `predict_generator`.
    # Arguments
        generator: a generator function which yields data
        use_multiprocessing: use multiprocessing if True, otherwise threading
        wait_time: time to sleep in-between calls to `put()`
        random_seed: Initial seed for workers,
            will be incremented by one for each worker.
    """"""
    def __init__(self, generator,
                 use_multiprocessing=False,
                 wait_time=0.05,
                 seed=None):
        self.wait_time = wait_time
        self._generator = generator
        self._use_multiprocessing = use_multiprocessing
        self._threads = []",[67]
"from datetime import timedelta
import operator
from typing import Any, Callable, List, Optional, Sequence, Union
import numpy as np
from pandas._libs.tslibs import (
    NaT,
    NaTType,
    frequencies as libfrequencies,
    iNaT,
    period as libperiod,
)
from pandas._libs.tslibs.fields import isleapyear_arr
from pandas._libs.tslibs.period import (
    DIFFERENT_FREQ,
    IncompatibleFrequency,
    Period,
    get_period_field_arr,
    period_asfreq_arr,
)
from pandas._libs.tslibs.timedeltas import Timedelta, delta_to_nanoseconds
import pandas.compat as compat
from pandas.util._decorators import Appender, cache_readonly
from pandas.core.dtypes.common import (
    _TD_DTYPE,
    ensure_object,
    is_datetime64_dtype,
    is_float_dtype,
    is_list_like,
    is_object_dtype,
    is_period_dtype,
    pandas_dtype,
)
from pandas.core.dtypes.dtypes import PeriodDtype
from pandas.core.dtypes.generic import (
    ABCIndexClass,
    ABCPeriodArray,
    ABCPeriodIndex,
    ABCSeries,
)
from pandas.core.dtypes.missing import isna, notna
from pandas.core import ops
import pandas.core.algorithms as algos
from pandas.core.arrays import datetimelike as dtl
import pandas.core.common as com
from pandas.core.ops.common import unpack_zerodim_and_defer
from pandas.core.ops.invalid import invalid_comparison
from pandas.tseries import frequencies
from pandas.tseries.offsets import DateOffset, Tick, _delta_to_tick

def _field_accessor(name, alias, docstring=None):
    def f(self):
        base, mult = libfrequencies.get_freq_code(self.freq)
        result = get_period_field_arr(alias, self.asi8, base)
        return result
    f.__name__ = name
    f.__doc__ = docstring
    return property(f)

def _period_array_cmp(cls, op):
    """"""
    Wrap comparison operations to convert Period-like to PeriodDtype
    """"""
    opname = f""__{op.__name__}__""
    nat_result = opname == ""__ne__""
    @unpack_zerodim_and_defer(opname)
    def wrapper(self, other):
        ordinal_op = getattr(self.asi8, opname)
        if isinstance(other, str):
            try:
                other = self._scalar_from_string(other)
            except ValueError:
                # string that can't be parsed as Period
                return invalid_comparison(self, other, op)
        elif isinstance(other, int):
            # TODO: sure we want to allow this?  we dont for DTA/TDA
            #  2 tests rely on this
            other = Period(other, freq=self.freq)
            result = ordinal_op(other.ordinal)
        if isinstance(other, self._recognized_scalars) or other is NaT:
            other = self._scalar_type(other)
            self._check_compatible_with(other)
            other_i8 = self._unbox_scalar(other)
            result = op(self.view(""i8""), other_i8)
            if isna(other):
                result.fill(nat_result)
        elif not is_list_like(other):
            return invalid_comparison(self, other, op)
        elif len(other) != len(self):
            raise ValueError(""Lengths must match"")
        else:
            if isinstance(other, list):
                # TODO: could use pd.Index to do inference?
                other = np.array(other)
            if not isinstance(other, (np.ndarray, cls)):
                return invalid_comparison(self, other, op)
            if is_object_dtype(other):
                with np.errstate(all=""ignore""):
                    result = ops.comp_method_OBJECT_ARRAY(
                        op, self.astype(object), other
                    )
                o_mask = isna(other)
            elif not cls._is_recognized_dtype(other.dtype):
                # e.g. is_timedelta64_dtype(other)
                return invalid_comparison(self, other, op)
            else:
                assert isinstance(other, cls), type(other)
","[75, 83, 86, 87]"
"# Copyright 2019 Ram Rachum and collaborators.
# This program is distributed under the MIT license.
import functools
import inspect
import opcode
import sys
import re
import collections
import datetime as datetime_module
import itertools
import threading
import traceback
from .variables import CommonVariable, Exploding, BaseVariable
from . import utils, pycompat

ipython_filename_pattern = re.compile('^<ipython-input-([0-9]+)-.*>$')

def get_local_reprs(frame, watch=()):
    code = frame.f_code
    vars_order = code.co_varnames + code.co_cellvars + code.co_freevars + tuple(frame.f_locals.keys())
    result_items = [(key, utils.get_shortish_repr(value)) for key, value in frame.f_locals.items()]
    result_items.sort(key=lambda key_value: vars_order.index(key_value[0]))
    result = collections.OrderedDict(result_items)
    for variable in watch:
        result.update(sorted(variable.items(frame)))
    return result

class UnavailableSource(object):
    def __getitem__(self, i):
        return u'SOURCE IS UNAVAILABLE'

source_cache = {}

def get_source_from_frame(frame):
    globs = frame.f_globals or {}
    module_name = globs.get('__name__')
    file_name = frame.f_code.co_filename
    cache_key = (module_name, file_name)
    try:
        return source_cache[cache_key]
    except KeyError:
        pass
    loader = globs.get('__loader__')
    source = None
    if hasattr(loader, 'get_source'):
        try:
            source = loader.get_source(module_name)
        except ImportError:
            pass
        if source is not None:
            source = source.splitlines()
    if source is None:
        ipython_filename_match = ipython_filename_pattern.match(file_name)
        if ipython_filename_match:
            entry_number = int(ipython_filename_match.group(1))
            try:
                import IPython
                ipython_shell = IPython.get_ipython()
                ((_, _, source_chunk),) = ipython_shell.history_manager. \
                                  get_range(0, entry_number, entry_number + 1)
                source = source_chunk.splitlines()
            except Exception:
                pass
        else:
            try:
                with open(file_name, 'rb') as fp:
                    source = fp.read().splitlines()
            except utils.file_reading_errors:
                pass
    if source is None:
        source = UnavailableSource()
    # If we just read the source from a file, or if the loader did not
    # apply tokenize.detect_encoding to decode the source into a
    # string, then we should do that ourselves.
    if isinstance(source[0], bytes):
        encoding = 'ascii'
        for line in source[:2]:
            # File coding may be specified. Match pattern from PEP-263
            # (https://www.python.org/dev/peps/pep-0263/)
            match = re.search(br'coding[:=]\s*([-\w.]+)', line)
            if match:
                encoding = match.group(1).decode('ascii')
                break
        source = [pycompat.text_type(sline, encoding, 'replace') for sline in
                  source]
    source_cache[cache_key] = source
    return source

def get_write_function(output, overwrite):
    is_path = isinstance(output, (pycompat.PathLike, str))
    if overwrite and not is_path:
        raise Exception('`overwrite=True` can only be used when writing '
                        'content to file.')
    if output is None:
        def write(s):
            stderr = sys.stderr
            try:
                stderr.write(s)
            except UnicodeEncodeError:
                # God damn Python 2
                stderr.write(utils.shitcode(s))
    elif is_path:
        return FileWriter(output, overwrite).write
    elif callable(output):
        write = output
    else:
        assert isinstance(output, utils.WritableStream)
        def write(s):
            output.write(s)
    return write

class FileWriter(object):",[86]
"        nv.validate_groupby_func(""std"", args, kwargs)
        return np.sqrt(self.var(ddof=ddof, **kwargs))
    @Substitution(name=""groupby"")
    @Appender(_common_see_also)
    def var(self, ddof: int = 1, *args, **kwargs):
        """"""
        Compute variance of groups, excluding missing values.
        For multiple groupings, the result index will be a MultiIndex.
        Parameters
        ----------
        ddof : int, default 1
            Degrees of freedom.
        Returns
        -------
        Series or DataFrame
            Variance of values within each group.
        """"""
        nv.validate_groupby_func(""var"", args, kwargs)
        if ddof == 1:
            return self._cython_agg_general(
                ""var"", alt=lambda x, axis: Series(x).var(ddof=ddof, **kwargs), **kwargs
            )
        else:
            f = lambda x: x.var(ddof=ddof, **kwargs)
            with _group_selection_context(self):
                return self._python_agg_general(f)
    @Substitution(name=""groupby"")
    @Appender(_common_see_also)
    def sem(self, ddof: int = 1):
        """"""
        Compute standard error of the mean of groups, excluding missing values.
        For multiple groupings, the result index will be a MultiIndex.
        Parameters
        ----------
        ddof : int, default 1
            Degrees of freedom.
        Returns
        -------
        Series or DataFrame
            Standard error of the mean of values within each group.
        """"""
        return self.std(ddof=ddof) / np.sqrt(self.count())
    @Substitution(name=""groupby"")
    @Appender(_common_see_also)
    def size(self):
        """"""
        Compute group sizes.
        Returns
        -------
        Series
            Number of rows in each group.
        """"""
        result = self.grouper.size()
        if isinstance(self.obj, Series):
            result.name = self.obj.name
        return result
    @classmethod
    def _add_numeric_operations(cls):
        """"""
        Add numeric operations to the GroupBy generically.
        """"""
        def groupby_function(
            name: str,
            alias: str,
            npfunc,
            numeric_only: bool = True,
            min_count: int = -1,
        ):
            _local_template = """"""
            Compute %(f)s of group values.
            Returns
            -------
            Series or DataFrame
                Computed %(f)s of values within each group.
            """"""
            @Substitution(name=""groupby"", f=name)
            @Appender(_common_see_also)
            @Appender(_local_template)
            def f(self, **kwargs):
                if ""numeric_only"" not in kwargs:
                    kwargs[""numeric_only""] = numeric_only
                if ""min_count"" not in kwargs:
                    kwargs[""min_count""] = min_count
                self._set_group_selection()
                # try a cython aggregation if we can
                try:
                    return self._cython_agg_general(alias, alt=npfunc, **kwargs)
                except DataError:
                    pass
                except NotImplementedError as err:
                    if ""function is not implemented for this dtype"" in str(err):
                        # raised in _get_cython_function, in some cases can
                        #  be trimmed by implementing cython funcs for more dtypes
                        pass
                    else:
                        raise
                # apply a non-cython aggregation
                result = self.aggregate(lambda x: npfunc(x, axis=self.axis))
                return result
            set_function_name(f, name, cls)
            return f
        def first_compat(x, axis=0):
            def first(x):
                x = x.to_numpy()
",[66]
"        margin_right = 1 - _right if _right else None
        margin_top = 1 - _top if _top else None
    vspaces = np.zeros((rows + 1, cols))
    hspaces = np.zeros((rows, cols + 1))
    if ax_bbox_list is None:
        ax_bbox_list = [
            Bbox.union([ax.get_position(original=True) for ax in subplots])
            for subplots in subplot_list]
    for subplots, ax_bbox, (num1, num2) in zip(subplot_list,
                                               ax_bbox_list,
                                               num1num2_list):
        if all(not ax.get_visible() for ax in subplots):
            continue
        bb = []
        for ax in subplots:
            if ax.get_visible():
                try:
                    bb += [ax.get_tightbbox(renderer, for_layout_only=True)]
                except TypeError:
                    bb += [ax.get_tightbbox(renderer)]
        tight_bbox_raw = Bbox.union(bb)
        tight_bbox = TransformedBbox(tight_bbox_raw,
                                     fig.transFigure.inverted())
        row1, col1 = divmod(num1, cols)
        if num2 is None:
            num2 = num1
        row2, col2 = divmod(num2, cols)
        for row_i in range(row1, row2 + 1):
            hspaces[row_i, col1] += ax_bbox.xmin - tight_bbox.xmin  # left
            hspaces[row_i, col2 + 1] += tight_bbox.xmax - ax_bbox.xmax  # right
        for col_i in range(col1, col2 + 1):
            vspaces[row1, col_i] += tight_bbox.ymax - ax_bbox.ymax  # top
            vspaces[row2 + 1, col_i] += ax_bbox.ymin - tight_bbox.ymin  # bot.
    fig_width_inch, fig_height_inch = fig.get_size_inches()
    # margins can be negative for axes with aspect applied, so use max(, 0) to
    # make them nonnegative.
    if not margin_left:
        margin_left = (max(hspaces[:, 0].max(), 0)
                       + pad_inches / fig_width_inch)
    if not margin_right:
        margin_right = (max(hspaces[:, -1].max(), 0)
                        + pad_inches / fig_width_inch)
    if not margin_top:
        margin_top = (max(vspaces[0, :].max(), 0)
                      + pad_inches / fig_height_inch)
        suptitle = fig._suptitle
        if suptitle and suptitle.get_in_layout():
            rel_suptitle_height = fig.transFigure.inverted().transform_bbox(
                suptitle.get_window_extent(renderer)).height
            margin_top += rel_suptitle_height + pad_inches / fig_height_inch
    if not margin_bottom:
        margin_bottom = (max(vspaces[-1, :].max(), 0)
                         + pad_inches / fig_height_inch)
    if margin_left + margin_right >= 1:
        cbook._warn_external('Tight layout not applied. The left and right '
                             'margins cannot be made large enough to '
                             'accommodate all axes decorations. ')
        return None
    if margin_bottom + margin_top >= 1:
        cbook._warn_external('Tight layout not applied. The bottom and top '
                             'margins cannot be made large enough to '
                             'accommodate all axes decorations. ')
        return None
    kwargs = dict(left=margin_left,
                  right=1 - margin_right,
                  bottom=margin_bottom,
                  top=1 - margin_top)
    if cols > 1:
        hspace = hspaces[:, 1:-1].max() + hpad_inches / fig_width_inch
        # axes widths:
        h_axes = (1 - margin_right - margin_left - hspace * (cols - 1)) / cols
        if h_axes < 0:
            cbook._warn_external('Tight layout not applied. tight_layout '
                                 'cannot make axes width small enough to '
                                 'accommodate all axes decorations')
            return None
        else:
            kwargs[""wspace""] = hspace / h_axes
    if rows > 1:
        vspace = vspaces[1:-1, :].max() + vpad_inches / fig_height_inch
        v_axes = (1 - margin_top - margin_bottom - vspace * (rows - 1)) / rows
        if v_axes < 0:
            cbook._warn_external('Tight layout not applied. tight_layout '
                                 'cannot make axes height small enough to '
                                 'accommodate all axes decorations')
            return None
        else:
            kwargs[""hspace""] = vspace / v_axes
    return kwargs

def get_renderer(fig):
    if fig._cachedRenderer:
        return fig._cachedRenderer
    else:
        canvas = fig.canvas
        if canvas and hasattr(canvas, ""get_renderer""):
            return canvas.get_renderer()
        else:
            from . import backend_bases
            return backend_bases._get_renderer(fig, draw_disabled=True)

def get_subplotspec_list(axes_list, grid_spec=None):
    """"""
    Return a list of subplotspec from the given list of axes.
    For an instance of axes that does not support subplotspec, None is inserted
    in the list.
    If grid_spec is given, None is inserted for those not from the given
    grid_spec.
    """"""
    subplotspec_list = []",[113]
"# the exponential labels using a norm.
N = 100
X, Y = np.mgrid[-3:3:complex(0, N), -2:2:complex(0, N)]
# A low hump with a spike coming out of the top.  Needs to have
# z/colour axis on a log scale so we see both hump and spike.  linear
# scale only shows the spike.
Z1 = np.exp(-X**2 - Y**2)
Z2 = np.exp(-(X * 10)**2 - (Y * 10)**2)
Z = Z1 + 50 * Z2
fig, ax = plt.subplots(2, 1)
pcm = ax[0].pcolor(X, Y, Z,
                   norm=colors.LogNorm(vmin=Z.min(), vmax=Z.max()),
                   cmap='PuBu_r')
fig.colorbar(pcm, ax=ax[0], extend='max')
pcm = ax[1].pcolor(X, Y, Z, cmap='PuBu_r')
fig.colorbar(pcm, ax=ax[1], extend='max')

###############################################################################
# PowerNorm: Here a power-law trend in X partially obscures a rectified
# sine wave in Y. We can remove the power law using a PowerNorm.
X, Y = np.mgrid[0:3:complex(0, N), 0:2:complex(0, N)]
Z1 = (1 + np.sin(Y * 10.)) * X**2
fig, ax = plt.subplots(2, 1)
pcm = ax[0].pcolormesh(X, Y, Z1, norm=colors.PowerNorm(gamma=1. / 2.),
                       cmap='PuBu_r')
fig.colorbar(pcm, ax=ax[0], extend='max')
pcm = ax[1].pcolormesh(X, Y, Z1, cmap='PuBu_r')
fig.colorbar(pcm, ax=ax[1], extend='max')
###############################################################################
# SymLogNorm: two humps, one negative and one positive, The positive
# with 5-times the amplitude. Linearly, you cannot see detail in the
# negative hump.  Here we logarithmically scale the positive and
# negative data separately.
#
# Note that colorbar labels do not come out looking very good.
X, Y = np.mgrid[-3:3:complex(0, N), -2:2:complex(0, N)]
Z1 = 5 * np.exp(-X**2 - Y**2)
Z2 = np.exp(-(X - 1)**2 - (Y - 1)**2)
Z = (Z1 - Z2) * 2
fig, ax = plt.subplots(2, 1)
pcm = ax[0].pcolormesh(X, Y, Z1,
                       norm=colors.SymLogNorm(linthresh=0.03, linscale=0.03,
                                              vmin=-1.0, vmax=1.0),
                       cmap='RdBu_r')
fig.colorbar(pcm, ax=ax[0], extend='both')
pcm = ax[1].pcolormesh(X, Y, Z1, cmap='RdBu_r', vmin=-np.max(Z1))
fig.colorbar(pcm, ax=ax[1], extend='both')

###############################################################################
# Custom Norm: An example with a customized normalization.  This one
# uses the example above, and normalizes the negative data differently
# from the positive.
X, Y = np.mgrid[-3:3:complex(0, N), -2:2:complex(0, N)]
Z1 = np.exp(-X**2 - Y**2)
Z2 = np.exp(-(X - 1)**2 - (Y - 1)**2)
Z = (Z1 - Z2) * 2
# Example of making your own norm.  Also see matplotlib.colors.
# From Joe Kington: This one gives two different linear ramps:

class MidpointNormalize(colors.Normalize):
    def __init__(self, vmin=None, vmax=None, midpoint=None, clip=False):
        self.midpoint = midpoint
        colors.Normalize.__init__(self, vmin, vmax, clip)
    def __call__(self, value, clip=None):
        # I'm ignoring masked values and all kinds of edge cases to make a
        # simple example...
        x, y = [self.vmin, self.midpoint, self.vmax], [0, 0.5, 1]
        return np.ma.masked_array(np.interp(value, x, y))

#####
fig, ax = plt.subplots(2, 1)
pcm = ax[0].pcolormesh(X, Y, Z,
                       norm=MidpointNormalize(midpoint=0.),
                       cmap='RdBu_r')
fig.colorbar(pcm, ax=ax[0], extend='both')
pcm = ax[1].pcolormesh(X, Y, Z, cmap='RdBu_r', vmin=-np.max(Z))
fig.colorbar(pcm, ax=ax[1], extend='both')
###############################################################################
# BoundaryNorm: For this one you provide the boundaries for your colors,
# and the Norm puts the first color in between the first pair, the
# second color between the second pair, etc.
fig, ax = plt.subplots(3, 1, figsize=(8, 8))
ax = ax.flatten()
# even bounds gives a contour-like effect
bounds = np.linspace(-1, 1, 10)
norm = colors.BoundaryNorm(boundaries=bounds, ncolors=256)
pcm = ax[0].pcolormesh(X, Y, Z,
                       norm=norm,
                       cmap='RdBu_r')
fig.colorbar(pcm, ax=ax[0], extend='both', orientation='vertical')
# uneven bounds changes the colormapping:
bounds = np.array([-0.25, -0.125, 0, 0.5, 1])
norm = colors.BoundaryNorm(boundaries=bounds, ncolors=256)
pcm = ax[1].pcolormesh(X, Y, Z, norm=norm, cmap='RdBu_r')
fig.colorbar(pcm, ax=ax[1], extend='both', orientation='vertical')
pcm = ax[2].pcolormesh(X, Y, Z, cmap='RdBu_r', vmin=-np.max(Z1))
fig.colorbar(pcm, ax=ax[2], extend='both', orientation='vertical')
",[57]
"        return self.__dict[key]
    def __iter__(self):
        return iter(self.__dict)
    def __len__(self):
        return len(self.__dict)
    def __repr__(self):
        return '<FrozenOrderedDict %s>' % repr(self.__dict)
    def __hash__(self):
        if self.__hash is None:
            hashes = map(hash, self.items())
            self.__hash = functools.reduce(operator.xor, hashes, 0)
        return self.__hash
    def get_wrapped(self):
        return self.__dict

def _recursively_freeze(value):
    """"""
    Recursively walks ``Mapping``s and ``list``s and converts them to ``_FrozenOrderedDict`` and ``tuples``, respectively.
    """"""
    if isinstance(value, Mapping):
        return _FrozenOrderedDict(((k, _recursively_freeze(v)) for k, v in value.items()))
    elif isinstance(value, list):
        return tuple(_recursively_freeze(v) for v in value)
    return value

class DictParameter(Parameter):
    """"""
    Parameter whose value is a ``dict``.
    In the task definition, use
    .. code-block:: python
        class MyTask(luigi.Task):
          tags = luigi.DictParameter()
            def run(self):
                logging.info(""Find server with role: %s"", self.tags['role'])
                server = aws.ec2.find_my_resource(self.tags)

    At the command line, use
    .. code-block:: console
        $ luigi --module my_tasks MyTask --tags <JSON string>
    Simple example with two tags:
    .. code-block:: console
        $ luigi --module my_tasks MyTask --tags '{""role"": ""web"", ""env"": ""staging""}'
    It can be used to define dynamic parameters, when you do not know the exact list of your parameters (e.g. list of
    tags, that are dynamically constructed outside Luigi), or you have a complex parameter containing logically related
    values (like a database connection config).
    """"""
    class _DictParamEncoder(JSONEncoder):
        """"""
        JSON encoder for :py:class:`~DictParameter`, which makes :py:class:`~_FrozenOrderedDict` JSON serializable.
        """"""
        def default(self, obj):
            if isinstance(obj, _FrozenOrderedDict):
                return obj.get_wrapped()
            return json.JSONEncoder.default(self, obj)
    def normalize(self, value):
        """"""
        Ensure that dictionary parameter is converted to a _FrozenOrderedDict so it can be hashed.
        """"""
        return _recursively_freeze(value)
    def parse(self, s):
        """"""
        Parses an immutable and ordered ``dict`` from a JSON string using standard JSON library.
        We need to use an immutable dictionary, to create a hashable parameter and also preserve the internal structure
        of parsing. The traversal order of standard ``dict`` is undefined, which can result various string
        representations of this parameter, and therefore a different task id for the task containing this parameter.
        This is because task id contains the hash of parameters' JSON representation.
        :param s: String to be parse
        """"""
        return json.loads(s, object_pairs_hook=_FrozenOrderedDict)
    def serialize(self, x):
        return json.dumps(x, cls=DictParameter._DictParamEncoder)

class ListParameter(Parameter):
    """"""
    Parameter whose value is a ``list``.
    In the task definition, use
    .. code-block:: python
        class MyTask(luigi.Task):
          grades = luigi.ListParameter()
            def run(self):
                sum = 0
                for element in self.grades:
                    sum += element
                avg = sum / len(self.grades)

    At the command line, use
    .. code-block:: console
        $ luigi --module my_tasks MyTask --grades <JSON string>
    Simple example with two grades:
    .. code-block:: console
        $ luigi --module my_tasks MyTask --grades '[100,70]'","[28, 65, 66, 68, 70, 71, 72, 73, 74, 95]"
"        self.pipeline[self.pipe_names.index(name)] = (name, component)
        if ENABLE_PIPELINE_ANALYSIS:
            analyze_all_pipes(self.pipeline)
    def rename_pipe(self, old_name, new_name):
        """"""Rename a pipeline component.
        old_name (unicode): Name of the component to rename.
        new_name (unicode): New name of the component.
        DOCS: https://spacy.io/api/language#rename_pipe
        """"""
        if old_name not in self.pipe_names:
            raise ValueError(Errors.E001.format(name=old_name, opts=self.pipe_names))
        if new_name in self.pipe_names:
            raise ValueError(Errors.E007.format(name=new_name, opts=self.pipe_names))
        i = self.pipe_names.index(old_name)
        self.pipeline[i] = (new_name, self.pipeline[i][1])
    def remove_pipe(self, name):
        """"""Remove a component from the pipeline.
        name (unicode): Name of the component to remove.
        RETURNS (tuple): A `(name, component)` tuple of the removed component.
        DOCS: https://spacy.io/api/language#remove_pipe
        """"""
        if name not in self.pipe_names:
            raise ValueError(Errors.E001.format(name=name, opts=self.pipe_names))
        if ENABLE_PIPELINE_ANALYSIS:
            analyze_all_pipes(self.pipeline)
        return self.pipeline.pop(self.pipe_names.index(name))
    def __call__(self, text, disable=[], component_cfg=None):
        """"""Apply the pipeline to some text. The text can span multiple sentences,
        and can contain arbtrary whitespace. Alignment into the original string
        is preserved.
        text (unicode): The text to be processed.
        disable (list): Names of the pipeline components to disable.
        component_cfg (dict): An optional dictionary with extra keyword arguments
            for specific components.
        RETURNS (Doc): A container for accessing the annotations.
        DOCS: https://spacy.io/api/language#call
        """"""
        if len(text) > self.max_length:
            raise ValueError(
                Errors.E088.format(length=len(text), max_length=self.max_length)
            )
        doc = self.make_doc(text)
        if component_cfg is None:
            component_cfg = {}
        for name, proc in self.pipeline:
            if name in disable:
                continue
            if not hasattr(proc, ""__call__""):
                raise ValueError(Errors.E003.format(component=type(proc), name=name))
            doc = proc(doc, **component_cfg.get(name, {}))
            if doc is None:
                raise ValueError(Errors.E005.format(name=name))
        return doc
    def disable_pipes(self, *names):
        """"""Disable one or more pipeline components. If used as a context
        manager, the pipeline will be restored to the initial state at the end
        of the block. Otherwise, a DisabledPipes object is returned, that has
        a `.restore()` method you can use to undo your changes.
        DOCS: https://spacy.io/api/language#disable_pipes
        """"""
        if len(names) == 1 and isinstance(names[0], (list, tuple)):
            names = names[0]  # support list of names instead of spread
        return DisabledPipes(self, *names)
    def make_doc(self, text):
        return self.tokenizer(text)
    def _format_docs_and_golds(self, docs, golds):
        """"""Format golds and docs before update models.""""""
        expected_keys = (""words"", ""tags"", ""heads"", ""deps"", ""entities"", ""cats"", ""links"")
        gold_objs = []
        doc_objs = []
        for doc, gold in zip(docs, golds):
            if isinstance(doc, basestring_):
                doc = self.make_doc(doc)
            if not isinstance(gold, GoldParse):
                unexpected = [k for k in gold if k not in expected_keys]
                if unexpected:
                    err = Errors.E151.format(unexp=unexpected, exp=expected_keys)
                    raise ValueError(err)
                gold = GoldParse(doc, **gold)
            doc_objs.append(doc)
            gold_objs.append(gold)
        return doc_objs, gold_objs
    def update(self, docs, golds, drop=0.0, sgd=None, losses=None, component_cfg=None):
        """"""Update the models in the pipeline.
        docs (iterable): A batch of `Doc` objects.
        golds (iterable): A batch of `GoldParse` objects.
        drop (float): The dropout rate.
        sgd (callable): An optimizer.
        losses (dict): Dictionary to update with the loss, keyed by component.
        component_cfg (dict): Config parameters for specific pipeline
            components, keyed by component name.
        DOCS: https://spacy.io/api/language#update
        """"""
        if len(docs) != len(golds):
            raise IndexError(Errors.E009.format(n_docs=len(docs), n_golds=len(golds)))
        if len(docs) == 0:
            return
        if sgd is None:
            if self._optimizer is None:
                self._optimizer = create_default_optimizer(Model.ops)
            sgd = self._optimizer
        # Allow dict of args to GoldParse, instead of GoldParse objects.
        docs, golds = self._format_docs_and_golds(docs, golds)
        grads = {}
        def get_grads(W, dW, key=None):
            grads[key] = (W, dW)
        get_grads.alpha = sgd.alpha
        get_grads.b1 = sgd.b1",[31]
"from collections import namedtuple
from traceback import format_stack
from .logs import debug
Command = namedtuple('Command', ('script', 'stdout', 'stderr'))
CorrectedCommand = namedtuple('CorrectedCommand', ('script', 'side_effect', 'priority'))
Rule = namedtuple('Rule', ('name', 'match', 'get_new_command',
                           'enabled_by_default', 'side_effect',
                           'priority', 'requires_output'))

class RulesNamesList(list):
    """"""Wrapper a top of list for storing rules names.""""""
    def __contains__(self, item):
        return super(RulesNamesList, self).__contains__(item.name)

class Settings(dict):
    def __getattr__(self, item):
        return self.get(item)
    def update(self, **kwargs):
        """"""
        Returns new settings with values from `kwargs` for unset settings.
        """"""
        conf = dict(kwargs)
        conf.update(self)
        return Settings(conf)

class SortedCorrectedCommandsSequence(object):
    """"""List-like collection/wrapper around generator, that:
    - immediately gives access to the first commands through [];
    - realises generator and sorts commands on first access to other
      commands through [], or when len called.
    """"""
    def __init__(self, commands, settings):
        self._settings = settings
        self._commands = commands
        self._cached = self._get_first_two_unique()
        self._realised = False
    def _get_first_two_unique(self):
        """"""Returns first two unique commands.""""""
        try:
            first = next(self._commands)
        except StopIteration:
            return []
        for command in self._commands:
            if command.script != first.script or \
                            command.side_effect != first.side_effect:
                return [first, command]
        return [first]
    def _remove_duplicates(self, corrected_commands):
        """"""Removes low-priority duplicates.""""""
        commands = {(command.script, command.side_effect): command
                    for command in sorted(corrected_commands,
                                          key=lambda command: -command.priority)
                    if command.script != self._cached[0].script
                    or command.side_effect != self._cached[0].side_effect}
        return commands.values()
    def _realise(self):
        """"""Realises generator, removes duplicates and sorts commands.""""""
        commands = self._cached[1:] + list(self._commands)
        commands = self._remove_duplicates(commands)
        self._cached = [self._cached[0]] + sorted(
            commands, key=lambda corrected_command: corrected_command.priority)
        self._realised = True
        debug('SortedCommandsSequence was realised with: {}, after: {}'.format(
            self._cached, '\n'.join(format_stack())), self._settings)
    def __getitem__(self, item):
        if item != 0 and not self._realised:
            self._realise()
        return self._cached[item]
    def __bool__(self):
        return bool(self._cached)
    def __len__(self):
        if not self._realised:
            self._realise()
        return len(self._cached)
    def __iter__(self):
        if not self._realised:
            self._realise()
        return iter(self._cached)
    @property
    def is_multiple(self):
        return len(self._cached) > 1","[6, 7, 56, 57, 63, 66, 67, 68]"
"import numpy as np
from pandas.core.dtypes.common import is_list_like
import pandas.core.common as com

def cartesian_product(X):
    """"""
    Numpy version of itertools.product.
    Sometimes faster (for large inputs)...
    Parameters
    ----------
    X : list-like of list-likes
    Returns
    -------
    product : list of ndarrays
    Examples
    --------
    >>> cartesian_product([list('ABC'), [1, 2]])
    [array(['A', 'A', 'B', 'B', 'C', 'C'], dtype='|S1'),
    array([1, 2, 1, 2, 1, 2])]
    See Also
    --------
    itertools.product : Cartesian product of input iterables.  Equivalent to
        nested for-loops.
    """"""
    msg = ""Input must be a list-like of list-likes""
    if not is_list_like(X):
        raise TypeError(msg)
    for x in X:
        if not is_list_like(x):
            raise TypeError(msg)
    if len(X) == 0:
        return []
    lenX = np.fromiter((len(x) for x in X), dtype=np.intp)
    cumprodX = np.cumproduct(lenX)
    a = np.roll(cumprodX, 1)
    a[0] = 1
    if cumprodX[-1] != 0:
        b = cumprodX[-1] / cumprodX
    else:
        # if any factor is empty, the cartesian product is empty
        b = np.zeros_like(cumprodX)
    return [
        np.tile(
            np.repeat(np.asarray(com.values_from_object(x)), b[i]), np.product(a[i])
        )
        for i, x in enumerate(X)
    ]","[4, 5, 53, 54, 55, 56, 57]"
"        Return the day names of the DateTimeIndex with specified locale.
        .. versionadded:: 0.23.0
        Parameters
        ----------
        locale : str, optional
            Locale determining the language in which to return the day name.
            Default is English locale.
        Returns
        -------
        Index
            Index of day names.
        Examples
        --------
        >>> idx = pd.date_range(start='2018-01-01', freq='D', periods=3)
        >>> idx
        DatetimeIndex(['2018-01-01', '2018-01-02', '2018-01-03'],
                      dtype='datetime64[ns]', freq='D')
        >>> idx.day_name()
        Index(['Monday', 'Tuesday', 'Wednesday'], dtype='object')
        """"""
        if self.tz is not None and not timezones.is_utc(self.tz):
            values = self._local_timestamps()
        else:
            values = self.asi8
        result = fields.get_date_name_field(values, ""day_name"", locale=locale)
        result = self._maybe_mask_results(result, fill_value=None)
        return result
    @property
    def time(self):
        """"""
        Returns numpy array of datetime.time. The time part of the Timestamps.
        """"""
        # If the Timestamps have a timezone that is not UTC,
        # convert them into their i8 representation while
        # keeping their timezone and not using UTC
        if self.tz is not None and not timezones.is_utc(self.tz):
            timestamps = self._local_timestamps()
        else:
            timestamps = self.asi8
        return tslib.ints_to_pydatetime(timestamps, box=""time"")
    @property
    def timetz(self):
        """"""
        Returns numpy array of datetime.time also containing timezone
        information. The time part of the Timestamps.
        """"""
        return tslib.ints_to_pydatetime(self.asi8, self.tz, box=""time"")
    @property
    def date(self):
        """"""
        Returns numpy array of python datetime.date objects (namely, the date
        part of Timestamps without timezone information).
        """"""
        # If the Timestamps have a timezone that is not UTC,
        # convert them into their i8 representation while
        # keeping their timezone and not using UTC
        if self.tz is not None and not timezones.is_utc(self.tz):
            timestamps = self._local_timestamps()
        else:
            timestamps = self.asi8
        return tslib.ints_to_pydatetime(timestamps, box=""date"")
    def isocalendar(self):
        """"""
        Returns a DataFrame with the year, week, and day calculated according to
        the ISO 8601 standard.
        .. versionadded:: 1.1.0
        Returns
        -------
        DataFrame
            with columns year, week and day
        See Also
        --------
        Timestamp.isocalendar
        datetime.date.isocalendar
        Examples
        --------
        >>> idx = pd.date_range(start='2019-12-29', freq='D', periods=4)
        >>> idx.isocalendar()
           year  week  day
        0  2019    52    7
        1  2020     1    1
        2  2020     1    2
        3  2020     1    3
        >>> idx.isocalendar().week
        0    52
        1     1
        2     1
        3     1
        Name: week, dtype: UInt32
        """"""
        from pandas import DataFrame
        sarray = fields.build_isocalendar_sarray(self.asi8)
        iso_calendar_df = DataFrame(
            sarray, columns=[""year"", ""week"", ""day""], dtype=""UInt32""
        )
        if self._hasnans:
            iso_calendar_df.iloc[self._isnan] = None
        return iso_calendar_df
    year = _field_accessor(
        ""year"",
        ""Y"",
        """"""
        The year of the datetime.
        Examples
        --------
        >>> datetime_series = pd.Series(
        ...     pd.date_range(""2000-01-01"", periods=3, freq=""Y"")
        ... )
        >>> datetime_series",[107]
"        mods = walk_modules('tests.test_utils_misc.test_walk_modules')
        expected = [
            'tests.test_utils_misc.test_walk_modules',
            'tests.test_utils_misc.test_walk_modules.mod',
            'tests.test_utils_misc.test_walk_modules.mod.mod0',
            'tests.test_utils_misc.test_walk_modules.mod1',
        ]
        self.assertEqual(set([m.__name__ for m in mods]), set(expected))
        mods = walk_modules('tests.test_utils_misc.test_walk_modules.mod')
        expected = [
            'tests.test_utils_misc.test_walk_modules.mod',
            'tests.test_utils_misc.test_walk_modules.mod.mod0',
        ]
        self.assertEqual(set([m.__name__ for m in mods]), set(expected))
        mods = walk_modules('tests.test_utils_misc.test_walk_modules.mod1')
        expected = [
            'tests.test_utils_misc.test_walk_modules.mod1',
        ]
        self.assertEqual(set([m.__name__ for m in mods]), set(expected))
        self.assertRaises(ImportError, walk_modules, 'nomodule999')
    def test_walk_modules_egg(self):
        egg = os.path.join(os.path.dirname(__file__), 'test.egg')
        sys.path.append(egg)
        try:
            mods = walk_modules('testegg')
            expected = [
                'testegg.spiders',
                'testegg.spiders.a',
                'testegg.spiders.b',
                'testegg'
            ]
            self.assertEqual(set([m.__name__ for m in mods]), set(expected))
        finally:
            sys.path.remove(egg)
    def test_arg_to_iter(self):
        class TestItem(Item):
            name = Field()
        assert hasattr(arg_to_iter(None), '__iter__')
        assert hasattr(arg_to_iter(100), '__iter__')
        assert hasattr(arg_to_iter('lala'), '__iter__')
        assert hasattr(arg_to_iter([1, 2, 3]), '__iter__')
        assert hasattr(arg_to_iter(l for l in 'abcd'), '__iter__')
        self.assertEqual(list(arg_to_iter(None)), [])
        self.assertEqual(list(arg_to_iter('lala')), ['lala'])
        self.assertEqual(list(arg_to_iter(100)), [100])
        self.assertEqual(list(arg_to_iter(l for l in 'abc')), ['a', 'b', 'c'])
        self.assertEqual(list(arg_to_iter([1, 2, 3])), [1, 2, 3])
        self.assertEqual(list(arg_to_iter({'a': 1})), [{'a': 1}])
        self.assertEqual(list(arg_to_iter(TestItem(name=""john""))), [TestItem(name=""john"")])
    def test_create_instance(self):
        settings = mock.MagicMock()
        crawler = mock.MagicMock(spec_set=['settings'])
        args = (True, 100.)
        kwargs = {'key': 'val'}
        def _test_with_settings(mock, settings):
            create_instance(mock, settings, None, *args, **kwargs)
            if hasattr(mock, 'from_crawler'):
                self.assertEqual(mock.from_crawler.call_count, 0)
            if hasattr(mock, 'from_settings'):
                mock.from_settings.assert_called_once_with(settings, *args,
                                                           **kwargs)
                self.assertEqual(mock.call_count, 0)
            else:
                mock.assert_called_once_with(*args, **kwargs)
        def _test_with_crawler(mock, settings, crawler):
            create_instance(mock, settings, crawler, *args, **kwargs)
            if hasattr(mock, 'from_crawler'):
                mock.from_crawler.assert_called_once_with(crawler, *args,
                                                          **kwargs)
                if hasattr(mock, 'from_settings'):
                    self.assertEqual(mock.from_settings.call_count, 0)
                self.assertEqual(mock.call_count, 0)
            elif hasattr(mock, 'from_settings'):
                mock.from_settings.assert_called_once_with(settings, *args,
                                                           **kwargs)
                self.assertEqual(mock.call_count, 0)
            else:
                mock.assert_called_once_with(*args, **kwargs)
        # Check usage of correct constructor using four mocks:
        #   1. with no alternative constructors
        #   2. with from_settings() constructor
        #   3. with from_crawler() constructor
        #   4. with from_settings() and from_crawler() constructor
        spec_sets = ([], ['from_settings'], ['from_crawler'],
                     ['from_settings', 'from_crawler'])
        for specs in spec_sets:
            m = mock.MagicMock(spec_set=specs)
            _test_with_settings(m, settings)
            m.reset_mock()
            _test_with_crawler(m, settings, crawler)
        # Check adoption of crawler settings
        m = mock.MagicMock(spec_set=['from_settings'])
        create_instance(m, None, crawler, *args, **kwargs)
        m.from_settings.assert_called_once_with(crawler.settings, *args,
                                                **kwargs)
        with self.assertRaises(ValueError):
            create_instance(m, None, None)
    def test_set_environ(self):
        assert os.environ.get('some_test_environ') is None
        with set_environ(some_test_environ='test_value'):
            assert os.environ.get('some_test_environ') == 'test_value'
        assert os.environ.get('some_test_environ') is None
        os.environ['some_test_environ'] = 'test'
        assert os.environ.get('some_test_environ') == 'test'
        with set_environ(some_test_environ='test_value'):
            assert os.environ.get('some_test_environ') == 'test_value'
        assert os.environ.get('some_test_environ') == 'test'

if __name__ == ""__main__"":","[95, 96, 104]"
"    x = _preprocess_conv2d_input(x, data_format)
    kernel = _preprocess_conv2d_kernel(kernel, data_format)
    th_padding = _preprocess_padding(padding)
    conv_out = T.nnet.conv2d(x, kernel,
                             border_mode=th_padding,
                             subsample=strides,
                             input_shape=image_shape,
                             filter_shape=kernel_shape,
                             filter_dilation=dilation_rate)
    conv_out = _postprocess_conv2d_output(conv_out, x, padding,
                                          kernel_shape, strides, data_format)
    return conv_out

def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),
                     padding='valid', data_format=None):
    """"""2D deconvolution (transposed convolution).
    # Arguments
        kernel: kernel tensor.
        output_shape: desired dimensions of output.
        strides: strides tuple.
        padding: string, ""same"" or ""valid"".
        data_format: ""channels_last"" or ""channels_first"".
            Whether to use Theano or TensorFlow data format
        in inputs/kernels/outputs.
    # Raises
        ValueError: if using an even kernel size with padding 'same'.
    """"""
    flip_filters = False
    data_format = normalize_data_format(data_format)
    if data_format == 'channels_last':
        output_shape = (output_shape[0],
                        output_shape[3],
                        output_shape[1],
                        output_shape[2])
    kernel_shape = int_shape(kernel)
    if kernel_shape is None:
        kernel_shape = kernel.eval().shape  # in case of a shared variable
    if padding == 'same' and kernel_shape[0] % 2 == 0:
        raise ValueError('In `Conv2DTranspose`, with padding mode `same`, '
                         'even kernel sizes are not supported with Theano. '
                         'You can set `kernel_size` to an odd number.')
    kernel_shape = _preprocess_conv2d_filter_shape(kernel_shape, data_format)
    x = _preprocess_conv2d_input(x, data_format)
    kernel = _preprocess_conv2d_kernel(kernel, data_format)
    th_padding = _preprocess_padding(padding)
    op = T.nnet.abstract_conv.AbstractConv2d_gradInputs(imshp=None,
                                                        kshp=kernel_shape,
                                                        subsample=strides,
                                                        border_mode=th_padding,
                                                        filter_flip=not flip_filters)
    conv_out = op(kernel, x, output_shape[2:])
    conv_out = _postprocess_conv2d_output(conv_out, x, padding,
                                          kernel_shape, strides, data_format)
    return conv_out

def separable_conv1d(x, depthwise_kernel, pointwise_kernel, strides=1,
                     padding='valid', data_format=None, dilation_rate=1):
    """"""1D convolution with separable filters.
    # Arguments
        x: input tensor
        depthwise_kernel: convolution kernel for the depthwise convolution.
        pointwise_kernel: kernel for the 1x1 convolution.
        strides: strides integer.
        padding: string, `""same""` or `""valid""`.
        data_format: string, `""channels_last""` or `""channels_first""`.
        dilation_rate: integer dilation rate.
    # Returns
        Output tensor.
    # Raises
        ValueError: if `data_format` is neither `""channels_last""` or `""channels_first""`.
    """"""
    data_format = normalize_data_format(data_format)
    if isinstance(strides, int):
        strides = (strides,)
    if isinstance(dilation_rate, int):
        dilation_rate = (dilation_rate,)
    if data_format == 'channels_last':
        spatial_start_dim = 2
    else:
        spatial_start_dim = 3
    x = expand_dims(x, spatial_start_dim)
    depthwise_kernel = expand_dims(depthwise_kernel, 1)
    pointwise_kernel = expand_dims(pointwise_kernel, 1)
    strides = strides + (1,)
    dilation_rate = dilation_rate + (1,)
    image_shape = _preprocess_conv2d_image_shape(int_shape(x), data_format)
    depthwise_kernel_shape = int_shape(depthwise_kernel)
    if depthwise_kernel_shape is None:
        depthwise_kernel_shape = depthwise_kernel.eval().shape  # in case of a shared variable
    depthwise_kernel_shape = _preprocess_conv2d_depthwise_filter_shape(depthwise_kernel_shape, data_format)
    pointwise_kernel_shape = int_shape(pointwise_kernel)
    if pointwise_kernel_shape is None:
        pointwise_kernel_shape = pointwise_kernel.eval().shape  # in case of a shared variable
    pointwise_kernel_shape = _preprocess_conv2d_filter_shape(pointwise_kernel_shape, data_format)
    x = _preprocess_conv2d_input(x, data_format)
    depthwise_kernel = _preprocess_conv2d_depthwise_kernel(depthwise_kernel, depthwise_kernel_shape, data_format)
    pointwise_kernel = _preprocess_conv2d_kernel(pointwise_kernel, data_format)
    th_padding = _preprocess_padding(padding)
    conv_out = T.nnet.conv2d(x, depthwise_kernel,
                             border_mode=th_padding,
                             subsample=strides,
                             input_shape=image_shape,
                             filter_shape=depthwise_kernel_shape,
                             filter_dilation=dilation_rate,
                             num_groups=image_shape[1])
    conv_out = T.nnet.conv2d(conv_out, pointwise_kernel,
                             border_mode=th_padding,
                             subsample=(1, 1),","[17, 27, 60]"
"        Parameters
        ----------
        filename : str or path-like or file-like
            The file where the figure is saved.
        dpi : float, default: :rc:`savefig.dpi`
            The dots per inch to save the figure in.
        facecolor : color or 'auto', default: :rc:`savefig.facecolor`
            The facecolor of the figure.  If 'auto', use the current figure
            facecolor.
        edgecolor : color or 'auto', default: :rc:`savefig.edgecolor`
            The edgecolor of the figure.  If 'auto', use the current figure
            edgecolor.
        orientation : {'landscape', 'portrait'}, default: 'portrait'
            Only currently applies to PostScript printing.
        format : str, optional
            Force a specific file format. If not given, the format is inferred
            from the *filename* extension, and if that fails from
            :rc:`savefig.format`.
        bbox_inches : 'tight' or `.Bbox`, default: :rc:`savefig.bbox`
            Bounding box in inches: only the given portion of the figure is
            saved.  If 'tight', try to figure out the tight bbox of the figure.
        pad_inches : float, default: :rc:`savefig.pad_inches`
            Amount of padding around the figure when *bbox_inches* is 'tight'.
        bbox_extra_artists : list of `~matplotlib.artist.Artist`, optional
            A list of extra artists that will be considered when the
            tight bbox is calculated.
        backend : str, optional
            Use a non-default backend to render the file, e.g. to render a
            png file with the ""cairo"" backend rather than the default ""agg"",
            or a pdf file with the ""pgf"" backend rather than the default
            ""pdf"".  Note that the default backend is normally sufficient.  See
            :ref:`the-builtin-backends` for a list of valid backends for each
            file format.  Custom backends can be referenced as ""module://..."".
        """"""
        if format is None:
            # get format from filename, or from backend's default filetype
            if isinstance(filename, os.PathLike):
                filename = os.fspath(filename)
            if isinstance(filename, str):
                format = os.path.splitext(filename)[1][1:]
            if format is None or format == '':
                format = self.get_default_filetype()
                if isinstance(filename, str):
                    filename = filename.rstrip('.') + '.' + format
        format = format.lower()
        # get canvas object and print method for format
        canvas = self._get_output_canvas(backend, format)
        print_method = getattr(canvas, 'print_%s' % format)
        if dpi is None:
            dpi = rcParams['savefig.dpi']
        if dpi == 'figure':
            dpi = getattr(self.figure, '_original_dpi', self.figure.dpi)
        # Remove the figure manager, if any, to avoid resizing the GUI widget.
        # Some code (e.g. Figure.show) differentiates between having *no*
        # manager and a *None* manager, which should be fixed at some point,
        # but this should be fine.
        with cbook._setattr_cm(self, _is_saving=True, manager=None), \
                cbook._setattr_cm(self.figure, dpi=dpi):
            origfacecolor = self.figure.get_facecolor()
            origedgecolor = self.figure.get_edgecolor()
            if facecolor is None:
                facecolor = rcParams['savefig.facecolor']
            if cbook._str_equal(facecolor, 'auto'):
                facecolor = origfacecolor
            if edgecolor is None:
                edgecolor = rcParams['savefig.edgecolor']
            if cbook._str_equal(edgecolor, 'auto'):
                edgecolor = origedgecolor
            self.figure.set_facecolor(facecolor)
            self.figure.set_edgecolor(edgecolor)
            if bbox_inches is None:
                bbox_inches = rcParams['savefig.bbox']
            if bbox_inches:
                if bbox_inches == ""tight"":
                    renderer = _get_renderer(
                        self.figure,
                        functools.partial(
                            print_method, orientation=orientation),
                        draw_disabled=True)
                    self.figure.draw(renderer)
                    bbox_inches = self.figure.get_tightbbox(
                        renderer, bbox_extra_artists=bbox_extra_artists)
                    if pad_inches is None:
                        pad_inches = rcParams['savefig.pad_inches']
                    bbox_inches = bbox_inches.padded(pad_inches)
                # call adjust_bbox to save only the given area
                restore_bbox = tight_bbox.adjust_bbox(self.figure, bbox_inches,
                                                      canvas.fixed_dpi)
                _bbox_inches_restore = (bbox_inches, restore_bbox)
            else:
                _bbox_inches_restore = None
            try:
                result = print_method(
                    filename,
                    dpi=dpi,
                    facecolor=facecolor,
                    edgecolor=edgecolor,
                    orientation=orientation,
                    bbox_inches_restore=_bbox_inches_restore,
                    **kwargs)
            finally:
                if bbox_inches and restore_bbox:
                    restore_bbox()
                self.figure.set_facecolor(origfacecolor)
                self.figure.set_edgecolor(origedgecolor)
                self.figure.set_canvas(self)
            return result","[93, 94, 95]"
"        # always sort time groupers
        kwargs[""sort""] = True
        super().__init__(freq=freq, axis=axis, **kwargs)
    def _get_resampler(self, obj, kind=None):
        """"""
        Return my resampler or raise if we have an invalid axis.
        Parameters
        ----------
        obj : input object
        kind : string, optional
            'period','timestamp','timedelta' are valid
        Returns
        -------
        a Resampler
        Raises
        ------
        TypeError if incompatible axis
        """"""
        self._set_grouper(obj)
        ax = self.ax
        if isinstance(ax, DatetimeIndex):
            return DatetimeIndexResampler(obj, groupby=self, kind=kind, axis=self.axis)
        elif isinstance(ax, PeriodIndex) or kind == ""period"":
            return PeriodIndexResampler(obj, groupby=self, kind=kind, axis=self.axis)
        elif isinstance(ax, TimedeltaIndex):
            return TimedeltaIndexResampler(obj, groupby=self, axis=self.axis)
        raise TypeError(
            ""Only valid with DatetimeIndex, ""
            ""TimedeltaIndex or PeriodIndex, ""
            f""but got an instance of '{type(ax).__name__}'""
        )
    def _get_grouper(self, obj, validate: bool = True):
        # create the resampler and return our binner
        r = self._get_resampler(obj)
        r._set_binner()
        return r.binner, r.grouper, r.obj
    def _get_time_bins(self, ax):
        if not isinstance(ax, DatetimeIndex):
            raise TypeError(
                ""axis must be a DatetimeIndex, but got ""
                f""an instance of {type(ax).__name__}""
            )
        if len(ax) == 0:
            binner = labels = DatetimeIndex(data=[], freq=self.freq, name=ax.name)
            return binner, [], labels
        first, last = _get_timestamp_range_edges(
            ax.min(), ax.max(), self.freq, closed=self.closed, base=self.base
        )
        # GH #12037
        # use first/last directly instead of call replace() on them
        # because replace() will swallow the nanosecond part
        # thus last bin maybe slightly before the end if the end contains
        # nanosecond part and lead to `Values falls after last bin` error
        binner = labels = date_range(
            freq=self.freq,
            start=first,
            end=last,
            tz=ax.tz,
            name=ax.name,
            ambiguous=""infer"",
            nonexistent=""shift_forward"",
        )
        ax_values = ax.asi8
        binner, bin_edges = self._adjust_bin_edges(binner, ax_values)
        # general version, knowing nothing about relative frequencies
        bins = lib.generate_bins_dt64(
            ax_values, bin_edges, self.closed, hasnans=ax.hasnans
        )
        if self.closed == ""right"":
            labels = binner
            if self.label == ""right"":
                labels = labels[1:]
        elif self.label == ""right"":
            labels = labels[1:]
        if ax.hasnans:
            binner = binner.insert(0, NaT)
            labels = labels.insert(0, NaT)
        # if we end up with more labels than bins
        # adjust the labels
        # GH4076
        if len(bins) < len(labels):
            labels = labels[: len(bins)]
        return binner, bins, labels
    def _adjust_bin_edges(self, binner, ax_values):
        # Some hacks for > daily data, see #1471, #1458, #1483
        if self.freq != ""D"" and is_superperiod(self.freq, ""D""):
            if self.closed == ""right"":
                # GH 21459, GH 9119: Adjust the bins relative to the wall time
                bin_edges = binner.tz_localize(None)
                bin_edges = bin_edges + timedelta(1) - Nano(1)
                bin_edges = bin_edges.tz_localize(binner.tz).asi8
            else:
                bin_edges = binner.asi8
            # intraday values on last day
            if bin_edges[-2] > ax_values.max():
                bin_edges = bin_edges[:-1]
                binner = binner[:-1]
        else:
            bin_edges = binner.asi8
        return binner, bin_edges
    def _get_time_delta_bins(self, ax):
        if not isinstance(ax, TimedeltaIndex):
            raise TypeError(
                ""axis must be a TimedeltaIndex, but got """,[72]
"    Returns a Series indexed like the original Series.
    Raises TypeError if the Series does not contain datetimelike values.
    """"""
    def to_pytimedelta(self):
        """"""
        Return an array of native `datetime.timedelta` objects.
        Python's standard `datetime` library uses a different representation
        timedelta's. This method converts a Series of pandas Timedeltas
        to `datetime.timedelta` format with the same length as the original
        Series.
        Returns
        -------
        a : numpy.ndarray
            Array of 1D containing data with `datetime.timedelta` type.
        See Also
        --------
        datetime.timedelta
        Examples
        --------
        >>> s = pd.Series(pd.to_timedelta(np.arange(5), unit='d'))
        >>> s
        0   0 days
        1   1 days
        2   2 days
        3   3 days
        4   4 days
        dtype: timedelta64[ns]
        >>> s.dt.to_pytimedelta()
        array([datetime.timedelta(0), datetime.timedelta(1),
               datetime.timedelta(2), datetime.timedelta(3),
               datetime.timedelta(4)], dtype=object)
        """"""
        return self._get_values().to_pytimedelta()
    @property
    def components(self):
        """"""
        Return a Dataframe of the components of the Timedeltas.
        Returns
        -------
        DataFrame
        Examples
        --------
        >>> s = pd.Series(pd.to_timedelta(np.arange(5), unit='s'))
        >>> s
        0   00:00:00
        1   00:00:01
        2   00:00:02
        3   00:00:03
        4   00:00:04
        dtype: timedelta64[ns]
        >>> s.dt.components
           days  hours  minutes  seconds  milliseconds  microseconds  nanoseconds
        0     0      0        0        0             0             0            0
        1     0      0        0        1             0             0            0
        2     0      0        0        2             0             0            0
        3     0      0        0        3             0             0            0
        4     0      0        0        4             0             0            0
        """"""  # noqa: E501
        return self._get_values().components.set_index(self._parent.index)
    @property
    def freq(self):
        return self._get_values().inferred_freq

@delegate_names(
    delegate=PeriodArray, accessors=PeriodArray._datetimelike_ops, typ=""property""
)
@delegate_names(
    delegate=PeriodArray, accessors=PeriodArray._datetimelike_methods, typ=""method""
)
class PeriodProperties(Properties):
    """"""
    Accessor object for datetimelike properties of the Series values.
    Examples
    --------
    >>> s.dt.hour
    >>> s.dt.second
    >>> s.dt.quarter
    Returns a Series indexed like the original Series.
    Raises TypeError if the Series does not contain datetimelike values.
    """"""

class CombinedDatetimelikeProperties(
    DatetimeProperties, TimedeltaProperties, PeriodProperties
):
    def __new__(cls, data):
        # CombinedDatetimelikeProperties isn't really instantiated. Instead
        # we need to choose which parent (datetime or timedelta) is
        # appropriate. Since we're checking the dtypes anyway, we'll just
        # do all the validation here.
        from pandas import Series
        if not isinstance(data, ABCSeries):
            raise TypeError(
                ""cannot convert an object of type {0} to a ""
                ""datetimelike index"".format(type(data))
            )
        orig = data if is_categorical_dtype(data) else None
        if orig is not None:
            data = Series(orig.values.categories, name=orig.name, copy=False)
        if is_datetime64_dtype(data.dtype):
            return DatetimeProperties(data, orig)
        elif is_datetime64tz_dtype(data.dtype):
            return DatetimeProperties(data, orig)
        elif is_timedelta64_dtype(data.dtype):
            return TimedeltaProperties(data, orig)
        elif is_period_arraylike(data):
            return PeriodProperties(data, orig)
        elif is_datetime_arraylike(data):
            return DatetimeProperties(data, orig)
",[113]
"        return cls(**kwargs)
    @property
    def rule_code(self) -> str:
        month = ccalendar.MONTH_ALIASES[self.startingMonth]
        return f""{self._prefix}-{month}""
    @apply_wraps
    def apply(self, other):
        # months_since: find the calendar quarter containing other.month,
        # e.g. if other.month == 8, the calendar quarter is [Jul, Aug, Sep].
        # Then find the month in that quarter containing an is_on_offset date for
        # self.  `months_since` is the number of months to shift other.month
        # to get to this on-offset month.
        months_since = other.month % 3 - self.startingMonth % 3
        qtrs = liboffsets.roll_qtrday(
            other, self.n, self.startingMonth, day_opt=self._day_opt, modby=3
        )
        months = qtrs * 3 - months_since
        return shift_month(other, months, self._day_opt)
    def is_on_offset(self, dt: datetime) -> bool:
        if self.normalize and not _is_normalized(dt):
            return False
        mod_month = (dt.month - self.startingMonth) % 3
        return mod_month == 0 and dt.day == self._get_offset_day(dt)
    @apply_index_wraps
    def apply_index(self, dtindex):
        shifted = liboffsets.shift_quarters(
            dtindex.asi8, self.n, self.startingMonth, self._day_opt
        )
        # TODO: going through __new__ raises on call to _validate_frequency;
        #  are we passing incorrect freq?
        return type(dtindex)._simple_new(
            shifted, freq=dtindex.freq, dtype=dtindex.dtype
        )

class BQuarterEnd(QuarterOffset):
    """"""
    DateOffset increments between business Quarter dates.
    startingMonth = 1 corresponds to dates like 1/31/2007, 4/30/2007, ...
    startingMonth = 2 corresponds to dates like 2/28/2007, 5/31/2007, ...
    startingMonth = 3 corresponds to dates like 3/30/2007, 6/29/2007, ...
    """"""
    _outputName = ""BusinessQuarterEnd""
    _default_startingMonth = 3
    _from_name_startingMonth = 12
    _prefix = ""BQ""
    _day_opt = ""business_end""

# TODO: This is basically the same as BQuarterEnd
class BQuarterBegin(QuarterOffset):
    _outputName = ""BusinessQuarterBegin""
    # I suspect this is wrong for *all* of them.
    _default_startingMonth = 3
    _from_name_startingMonth = 1
    _prefix = ""BQS""
    _day_opt = ""business_start""

class QuarterEnd(QuarterOffset):
    """"""
    DateOffset increments between business Quarter dates.
    startingMonth = 1 corresponds to dates like 1/31/2007, 4/30/2007, ...
    startingMonth = 2 corresponds to dates like 2/28/2007, 5/31/2007, ...
    startingMonth = 3 corresponds to dates like 3/31/2007, 6/30/2007, ...
    """"""
    _outputName = ""QuarterEnd""
    _default_startingMonth = 3
    _prefix = ""Q""
    _day_opt = ""end""

class QuarterBegin(QuarterOffset):
    _outputName = ""QuarterBegin""
    _default_startingMonth = 3
    _from_name_startingMonth = 1
    _prefix = ""QS""
    _day_opt = ""start""

# ---------------------------------------------------------------------
# Year-Based Offset Classes

class YearOffset(DateOffset):
    """"""
    DateOffset that just needs a month.
    """"""
    _adjust_dst = True
    _attributes = frozenset([""n"", ""normalize"", ""month""])
    def _get_offset_day(self, other: datetime) -> int:
        # override BaseOffset method to use self.month instead of other.month
        # TODO: there may be a more performant way to do this
        return liboffsets.get_day_of_month(
            other.replace(month=self.month), self._day_opt
        )
    @apply_wraps
    def apply(self, other):
        years = roll_yearday(other, self.n, self.month, self._day_opt)
        months = years * 12 + (self.month - other.month)
        return shift_month(other, months, self._day_opt)
    @apply_index_wraps
    def apply_index(self, dtindex):
        shifted = liboffsets.shift_quarters(
            dtindex.asi8, self.n, self.month, self._day_opt, modby=12
        )
        # TODO: going through __new__ raises on call to _validate_frequency;
        #  are we passing incorrect freq?
        return type(dtindex)._simple_new(
            shifted, freq=dtindex.freq, dtype=dtindex.dtype
        )
    def is_on_offset(self, dt: datetime) -> bool:
        if self.normalize and not _is_normalized(dt):
            return False","[34, 35, 36, 120, 121, 122]"
"import asyncio
import inspect
from typing import Any, Callable, Dict, List, Optional, Sequence, Set, Type, Union
from fastapi import params
from fastapi.dependencies.models import Dependant
from fastapi.dependencies.utils import (
    get_body_field,
    get_dependant,
    get_parameterless_sub_dependant,
    solve_dependencies,
)
from fastapi.encoders import DictIntStrAny, SetIntStr, jsonable_encoder
from fastapi.exceptions import RequestValidationError, WebSocketRequestValidationError
from fastapi.logger import logger
from fastapi.openapi.constants import STATUS_CODES_WITH_NO_BODY
from fastapi.utils import (
    PYDANTIC_1,
    create_cloned_field,
    create_response_field,
    generate_operation_id_for_path,
    get_field_info,
    warning_response_model_skip_defaults_deprecated,
)
from pydantic import BaseModel
from pydantic.error_wrappers import ErrorWrapper, ValidationError
from starlette import routing
from starlette.concurrency import run_in_threadpool
from starlette.exceptions import HTTPException
from starlette.requests import Request
from starlette.responses import JSONResponse, Response
from starlette.routing import Mount  # noqa
from starlette.routing import (
    compile_path,
    get_name,
    request_response,
    websocket_session,
)
from starlette.status import WS_1008_POLICY_VIOLATION
from starlette.types import ASGIApp
from starlette.websockets import WebSocket
try:
    from pydantic.fields import FieldInfo, ModelField
except ImportError:  # pragma: nocover
    # TODO: remove when removing support for Pydantic < 1.0.0
    from pydantic import Schema as FieldInfo  # type: ignore
    from pydantic.fields import Field as ModelField  # type: ignore

async def serialize_response(
    *,
    field: ModelField = None,
    response_content: Any,
    include: Union[SetIntStr, DictIntStrAny] = None,
    exclude: Union[SetIntStr, DictIntStrAny] = set(),
    by_alias: bool = True,
    exclude_unset: bool = False,
    is_coroutine: bool = True,
) -> Any:
    if field:
        errors = []
        if exclude_unset and isinstance(response_content, BaseModel):
            if PYDANTIC_1:
                response_content = response_content.dict(exclude_unset=exclude_unset)
            else:
                response_content = response_content.dict(
                    skip_defaults=exclude_unset
                )  # pragma: nocover
        if is_coroutine:
            value, errors_ = field.validate(response_content, {}, loc=(""response"",))
        else:
            value, errors_ = await run_in_threadpool(
                field.validate, response_content, {}, loc=(""response"",)
            )
        if isinstance(errors_, ErrorWrapper):
            errors.append(errors_)
        elif isinstance(errors_, list):
            errors.extend(errors_)
        if errors:
            raise ValidationError(errors, field.type_)
        return jsonable_encoder(
            value,
            include=include,
            exclude=exclude,
            by_alias=by_alias,
            exclude_unset=exclude_unset,
        )
    else:
        return jsonable_encoder(response_content)

async def run_endpoint_function(
    *, dependant: Dependant, values: Dict[str, Any], is_coroutine: bool
) -> Any:
    # Only called by get_request_handler. Has been split into its own function to
    # facilitate profiling endpoints, since inner functions are harder to profile.
    assert dependant.call is not None, ""dependant.call must be a function""
    if is_coroutine:
        return await dependant.call(**values)
    else:
        return await run_in_threadpool(dependant.call, **values)

def get_request_handler(
    dependant: Dependant,
    body_field: ModelField = None,
    status_code: int = 200,
    response_class: Type[Response] = JSONResponse,
    response_field: ModelField = None,
    response_model_include: Union[SetIntStr, DictIntStrAny] = None,
    response_model_exclude: Union[SetIntStr, DictIntStrAny] = set(),
    response_model_by_alias: bool = True,
    response_model_exclude_unset: bool = False,
    dependency_overrides_provider: Any = None,
) -> Callable:
    assert dependant.call is not None, ""dependant.call must be a function""
    is_coroutine = asyncio.iscoroutinefunction(dependant.call)
    is_body_form = body_field and isinstance(get_field_info(body_field), params.Form)
    async def app(request: Request) -> Response:
        try:
            body = None
            if body_field:
                if is_body_form:
                    body = await request.form()","[62, 63, 64, 65, 66, 67, 68]"
"        return newb
    def convert(
        self,
        copy: bool = True,
        datetime: bool = True,
        numeric: bool = True,
        timedelta: bool = True,
        coerce: bool = False,
    ):
        """""" attempt to coerce any object types to better types return a copy
        of the block (if copy = True) by definition we are not an ObjectBlock
        here!
        """"""
        return self.copy() if copy else self
    def _can_hold_element(self, element: Any) -> bool:
        """""" require the same dtype as ourselves """"""
        dtype = self.values.dtype.type
        tipo = maybe_infer_dtype_type(element)
        if tipo is not None:
            return issubclass(tipo.type, dtype)
        return isinstance(element, dtype)
    def _try_coerce_args(self, other):
        """""" provide coercion to our input arguments """"""
        if np.any(notna(other)) and not self._can_hold_element(other):
            # coercion issues
            # let higher levels handle
            raise TypeError(
                ""cannot convert {} to an {}"".format(
                    type(other).__name__,
                    type(self).__name__.lower().replace(""Block"", """"),
                )
            )
        if np.any(isna(other)) and not self._can_hold_na:
            raise TypeError(
                ""cannot convert {} to an {}"".format(
                    type(other).__name__,
                    type(self).__name__.lower().replace(""Block"", """"),
                )
            )
        return other
    def to_native_types(self, slicer=None, na_rep=""nan"", quoting=None, **kwargs):
        """""" convert to our native types format, slicing if desired """"""
        values = self.get_values()
        if slicer is not None:
            values = values[:, slicer]
        mask = isna(values)
        if not self.is_object and not quoting:
            values = values.astype(str)
        else:
            values = np.array(values, dtype=""object"")
        values[mask] = na_rep
        return values
    # block actions #
    def copy(self, deep=True):
        """""" copy constructor """"""
        values = self.values
        if deep:
            values = values.copy()
        return self.make_block_same_class(values, ndim=self.ndim)
    def replace(
        self, to_replace, value, inplace=False, filter=None, regex=False, convert=True
    ):
        """"""replace the to_replace value with value, possible to create new
        blocks here this is just a call to putmask. regex is not used here.
        It is used in ObjectBlocks.  It is here for API compatibility.
        """"""
        inplace = validate_bool_kwarg(inplace, ""inplace"")
        original_to_replace = to_replace
        # If we cannot replace with own dtype, convert to ObjectBlock and
        # retry
        if not self._can_hold_element(to_replace):
            if not isinstance(to_replace, list):
                if inplace:
                    return [self]
                return [self.copy()]
            to_replace = [x for x in to_replace if self._can_hold_element(x)]
            if not len(to_replace):
                # GH#28084 avoid costly checks since we can infer
                #  that there is nothing to replace in this block
                if inplace:
                    return [self]
                return [self.copy()]
            if len(to_replace) == 1:
                # _can_hold_element checks have reduced this back to the
                #  scalar case and we can avoid a costly object cast
                return self.replace(
                    to_replace[0],
                    value,
                    inplace=inplace,
                    filter=filter,
                    regex=regex,
                    convert=convert,
                )
            # GH 22083, TypeError or ValueError occurred within error handling
            # causes infinite loop. Cast and retry only if not objectblock.
            if is_object_dtype(self):
                raise AssertionError
            # try again with a compatible block
            block = self.astype(object)
            return block.replace(
                to_replace=to_replace,
                value=value,
                inplace=inplace,
                filter=filter,
                regex=regex,
                convert=convert,
            )
",[57]
"# -*- coding: utf-8 -*-
""""""Utilities for text input preprocessing.
""""""
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function
import string
import sys
import warnings
from collections import OrderedDict
from hashlib import md5
import numpy as np
from six.moves import range
from six.moves import zip
if sys.version_info < (3,):
    maketrans = string.maketrans
else:
    maketrans = str.maketrans

def text_to_word_sequence(text,
                          filters='!""#$%&()*+,-./:;<=>?@[\\]^_`{|}~\t\n',
                          lower=True, split="" ""):
    """"""Converts a text to a sequence of words (or tokens).
    # Arguments
        text: Input text (string).
        filters: Sequence of characters to filter out.
        lower: Whether to convert the input to lowercase.
        split: Sentence split marker (string).
    # Returns
        A list of words (or tokens).
    """"""
    if lower:
        text = text.lower()
    if sys.version_info < (3,) and isinstance(text, unicode):
        translate_map = dict((ord(c), unicode(split)) for c in filters)
    else:
        translate_map = maketrans(filters, split * len(filters))
    text = text.translate(translate_map)
    seq = text.split(split)
    return [i for i in seq if i]

def one_hot(text, n,
            filters='!""#$%&()*+,-./:;<=>?@[\\]^_`{|}~\t\n',
            lower=True,
            split=' '):
    """"""One-hot encodes a text into a list of word indexes of size n.
    This is a wrapper to the `hashing_trick` function using `hash` as the
    hashing function; unicity of word to index mapping non-guaranteed.
    # Arguments
        text: Input text (string).
        n: Dimension of the hashing space.
        filters: Sequence of characters to filter out.
        lower: Whether to convert the input to lowercase.
        split: Sentence split marker (string).
    # Returns
        A list of integer word indices (unicity non-guaranteed).
    """"""
    return hashing_trick(text, n,
                         hash_function=hash,
                         filters=filters,
                         lower=lower,
                         split=split)

def hashing_trick(text, n,
                  hash_function=None,
                  filters='!""#$%&()*+,-./:;<=>?@[\\]^_`{|}~\t\n',
                  lower=True,
                  split=' '):
    """"""Converts a text to a sequence of indexes in a fixed-size hashing space.
    # Arguments
        text: Input text (string).
        n: Dimension of the hashing space.
        hash_function: if `None` uses python `hash` function, can be 'md5' or
            any function that takes in input a string and returns a int.
            Note that `hash` is not a stable hashing function, so
            it is not consistent across different runs, while 'md5'
            is a stable hashing function.
        filters: Sequence of characters to filter out.
        lower: Whether to convert the input to lowercase.
        split: Sentence split marker (string).
    # Returns
        A list of integer word indices (unicity non-guaranteed).
    `0` is a reserved index that won't be assigned to any word.
    Two or more words may be assigned to the same index, due to possible
    collisions by the hashing function.
    The [probability](https://en.wikipedia.org/wiki/Birthday_problem#Probability_table)
    of a collision is in relation to the dimension of the hashing space and
    the number of distinct objects.
    """"""
    if hash_function is None:
        hash_function = hash
    elif hash_function == 'md5':
        hash_function = lambda w: int(md5(w.encode()).hexdigest(), 16)
    seq = text_to_word_sequence(text,
                                filters=filters,
                                lower=lower,
                                split=split)
    return [(hash_function(w) % (n - 1) + 1) for w in seq]

class Tokenizer(object):
    """"""Text tokenization utility class.
    This class allows to vectorize a text corpus, by turning each
    text into either a sequence of integers (each integer being the index
    of a token in a dictionary) or into a vector where the coefficient
    for each token could be binary, based on word count, based on tf-idf...
    # Arguments","[40, 41, 43, 45]"
"# Copyright 2004-2005 Elemental Security, Inc. All Rights Reserved.
# Licensed to PSF under a Contributor Agreement.
# Modifications:
# Copyright 2006 Google, Inc. All Rights Reserved.
# Licensed to PSF under a Contributor Agreement.
""""""Parser driver.
This provides a high-level interface to parse a file into a syntax tree.
""""""
__author__ = ""Guido van Rossum <guido@python.org>""
__all__ = [""Driver"", ""load_grammar""]
# Python imports
import codecs
import io
import os
import logging
import pkgutil
import sys
# Pgen imports
from . import grammar, parse, token, tokenize, pgen

class Driver(object):
    def __init__(self, grammar, convert=None, logger=None):
        self.grammar = grammar
        if logger is None:
            logger = logging.getLogger(__name__)
        self.logger = logger
        self.convert = convert
    def parse_tokens(self, tokens, debug=False):
        """"""Parse a series of tokens and return the syntax tree.""""""
        # XXX Move the prefix computation into a wrapper around tokenize.
        p = parse.Parser(self.grammar, self.convert)
        p.setup()
        lineno = 1
        column = 0
        indent_columns = []
        type = value = start = end = line_text = None
        prefix = """"
        for quintuple in tokens:
            type, value, start, end, line_text = quintuple
            if start != (lineno, column):
                assert (lineno, column) <= start, ((lineno, column), start)
                s_lineno, s_column = start
                if lineno < s_lineno:
                    prefix += ""\n"" * (s_lineno - lineno)
                    lineno = s_lineno
                    column = 0
                if column < s_column:
                    prefix += line_text[column:s_column]
                    column = s_column
            if type in (tokenize.COMMENT, tokenize.NL):
                prefix += value
                lineno, column = end
                if value.endswith(""\n""):
                    lineno += 1
                    column = 0
                continue
            if type == token.OP:
                type = grammar.opmap[value]
            if debug:
                self.logger.debug(""%s %r (prefix=%r)"",
                                  token.tok_name[type], value, prefix)
            if type == token.INDENT:
                indent_columns.append(len(value))
                _prefix = prefix + value
                prefix = """"
                value = """"
            elif type == token.DEDENT:
                _indent_col = indent_columns.pop()
                prefix, _prefix = self._partially_consume_prefix(prefix, _indent_col)
            if p.addtoken(type, value, (prefix, start)):
                if debug:
                    self.logger.debug(""Stop."")
                break
            prefix = """"
            if type in {token.INDENT, token.DEDENT}:
                prefix = _prefix
            lineno, column = end
            if value.endswith(""\n""):
                lineno += 1
                column = 0
        else:
            # We never broke out -- EOF is too soon (how can this happen???)
            raise parse.ParseError(""incomplete input"",
                                   type, value, (prefix, start))
        return p.rootnode
    def parse_stream_raw(self, stream, debug=False):
        """"""Parse a stream and return the syntax tree.""""""
        tokens = tokenize.generate_tokens(stream.readline)
        return self.parse_tokens(tokens, debug)
    def parse_stream(self, stream, debug=False):
        """"""Parse a stream and return the syntax tree.""""""
        return self.parse_stream_raw(stream, debug)
    def parse_file(self, filename, encoding=None, debug=False):
        """"""Parse a file and return the syntax tree.""""""
        with io.open(filename, ""r"", encoding=encoding) as stream:
            return self.parse_stream(stream, debug)
    def parse_string(self, text, debug=False):
        """"""Parse a string and return the syntax tree.""""""
        tokens = tokenize.generate_tokens(io.StringIO(text).readline)
        return self.parse_tokens(tokens, debug)
    def _partially_consume_prefix(self, prefix, column):
        lines = []
        current_line = """"
        current_column = 0
        wait_for_nl = False
        for char in prefix:
            current_line += char
            if wait_for_nl:
                if char == '\n':
                    if current_line.strip() and current_column < column:
                        res = ''.join(lines)","[31, 99, 113]"
"    def _validate_searchsorted_value(self, value):
        if isinstance(value, str):
            try:
                value = self._scalar_from_string(value)
            except ValueError as err:
                raise TypeError(
                    ""searchsorted requires compatible dtype or scalar""
                ) from err
        elif is_valid_nat_for_dtype(value, self.dtype):
            value = NaT
        elif isinstance(value, self._recognized_scalars):
            value = self._scalar_type(value)
        elif is_list_like(value) and not isinstance(value, type(self)):
            value = array(value)
            if not type(self)._is_recognized_dtype(value):
                raise TypeError(
                    ""searchsorted requires compatible dtype or scalar, ""
                    f""not {type(value).__name__}""
                )
        if not (isinstance(value, (self._scalar_type, type(self))) or (value is NaT)):
            raise TypeError(f""Unexpected type for 'value': {type(value)}"")
        if isinstance(value, type(self)):
            self._check_compatible_with(value)
            value = value.asi8
        else:
            value = self._unbox_scalar(value)
        return value
    def _validate_setitem_value(self, value):
        if lib.is_scalar(value) and not isna(value):
            value = com.maybe_box_datetimelike(value)
        if is_list_like(value):
            value = type(self)._from_sequence(value, dtype=self.dtype)
            self._check_compatible_with(value, setitem=True)
            value = value.asi8
        elif isinstance(value, self._scalar_type):
            self._check_compatible_with(value, setitem=True)
            value = self._unbox_scalar(value)
        elif is_valid_nat_for_dtype(value, self.dtype):
            value = iNaT
        else:
            msg = (
                f""'value' should be a '{self._scalar_type.__name__}', 'NaT', ""
                f""or array of those. Got '{type(value).__name__}' instead.""
            )
            raise TypeError(msg)
        return value
    def _validate_insert_value(self, value):
        if isinstance(value, self._recognized_scalars):
            value = self._scalar_type(value)
        elif is_valid_nat_for_dtype(value, self.dtype):
            # GH#18295
            value = NaT
        elif lib.is_scalar(value) and isna(value):
            raise TypeError(
                f""cannot insert {type(self).__name__} with incompatible label""
            )
        return value
    def _validate_where_value(self, other):
        if is_valid_nat_for_dtype(other, self.dtype):
            other = NaT
        elif isinstance(other, self._recognized_scalars):
            other = self._scalar_type(other)
            self._check_compatible_with(other, setitem=True)
        elif not is_list_like(other):
            raise TypeError(f""Where requires matching dtype, not {type(other)}"")
        else:
            # Do type inference if necessary up front
            # e.g. we passed PeriodIndex.values and got an ndarray of Periods
            other = array(other)
            other = extract_array(other, extract_numpy=True)
            if is_categorical_dtype(other.dtype):
                # e.g. we have a Categorical holding self.dtype
                if is_dtype_equal(other.categories.dtype, self.dtype):
                    other = other._internal_get_values()
            if not type(self)._is_recognized_dtype(other.dtype):
                raise TypeError(f""Where requires matching dtype, not {other.dtype}"")
            self._check_compatible_with(other, setitem=True)
        if lib.is_scalar(other):
            other = self._unbox_scalar(other)
        else:
            other = other.view(""i8"")
        return other
    # ------------------------------------------------------------------
    # Additional array methods
    #  These are not part of the EA API, but we implement them because
    #  pandas assumes they're there.
    def searchsorted(self, value, side=""left"", sorter=None):
        """"""
        Find indices where elements should be inserted to maintain order.
        Find the indices into a sorted array `self` such that, if the
        corresponding elements in `value` were inserted before the indices,
        the order of `self` would be preserved.
        Parameters
        ----------
        value : array_like
            Values to insert into `self`.
        side : {'left', 'right'}, optional
            If 'left', the index of the first suitable location found is given.
            If 'right', return the last such index.  If there is no suitable
            index, return either 0 or N (where N is the length of `self`).
        sorter : 1-D array_like, optional
            Optional array of integer indices that sort `self` into ascending
            order. They are typically the result of ``np.argsort``.
",[64]
"# -*- coding: utf-8 -*-
""""""Recurrent layers and their base classes.
""""""
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function
import numpy as np
import warnings
from .. import backend as K
from .. import activations
from .. import initializers
from .. import regularizers
from .. import constraints
from ..engine import Layer
from ..engine import InputSpec
from ..utils.generic_utils import has_arg
# Legacy support.
from ..legacy.layers import Recurrent
from ..legacy import interfaces

class StackedRNNCells(Layer):
    """"""Wrapper allowing a stack of RNN cells to behave as a single cell.
    Used to implement efficient stacked RNNs.
    # Arguments
        cells: List of RNN cell instances.
    # Examples
    ```python
        cells = [
            keras.layers.LSTMCell(output_dim),
            keras.layers.LSTMCell(output_dim),
            keras.layers.LSTMCell(output_dim),
        ]
        inputs = keras.Input((timesteps, input_dim))
        x = keras.layers.RNN(cells)(inputs)
    ```
    """"""
    def __init__(self, cells, **kwargs):
        for cell in cells:
            if not hasattr(cell, 'call'):
                raise ValueError('All cells must have a `call` method. '
                                 'received cells:', cells)
            if not hasattr(cell, 'state_size'):
                raise ValueError('All cells must have a '
                                 '`state_size` attribute. '
                                 'received cells:', cells)
        self.cells = cells
        super(StackedRNNCells, self).__init__(**kwargs)
    @property
    def state_size(self):
        # States are a flat list
        # in reverse order of the cell stack.
        # This allows to preserve the requirement
        # `stack.state_size[0] == output_dim`.
        # e.g. states of a 2-layer LSTM would be
        # `[h2, c2, h1, c1]`
        # (assuming one LSTM has states [h, c])
        state_size = []
        for cell in self.cells[::-1]:
            if hasattr(cell.state_size, '__len__'):
                state_size += list(cell.state_size)
            else:
                state_size.append(cell.state_size)
        return tuple(state_size)
    def call(self, inputs, states, **kwargs):
        # Recover per-cell states.
        nested_states = []
        for cell in self.cells[::-1]:
            if hasattr(cell.state_size, '__len__'):
                nested_states.append(states[:len(cell.state_size)])
                states = states[len(cell.state_size):]
            else:
                nested_states.append([states[0]])
                states = states[1:]
        nested_states = nested_states[::-1]
        # Call the cells in order and store the returned states.
        new_nested_states = []
        for cell, states in zip(self.cells, nested_states):
            inputs, states = cell.call(inputs, states, **kwargs)
            new_nested_states.append(states)
        # Format the new states as a flat list
        # in reverse cell order.
        states = []
        for cell_states in new_nested_states[::-1]:
            states += cell_states
        return inputs, states
    def build(self, input_shape):
        for cell in self.cells:
            if isinstance(cell, Layer):
                cell.build(input_shape)
            if hasattr(cell.state_size, '__len__'):
                output_dim = cell.state_size[0]
            else:
                output_dim = cell.state_size
            input_shape = (input_shape[0], input_shape[1], output_dim)
        self.built = True
    def get_config(self):
        cells = []
        for cell in self.cells:
            cells.append({'class_name': cell.__class__.__name__,
                          'config': cell.get_config()})
        config = {'cells': cells}
        base_config = super(StackedRNNCells, self).get_config()
        return dict(list(base_config.items()) + list(config.items()))
    @classmethod
    def from_config(cls, config, custom_objects=None):
        from . import deserialize as deserialize_layer
        cells = []
        for cell_config in config.pop('cells'):
            cells.append(deserialize_layer(cell_config,
                                           custom_objects=custom_objects))",[108]
"            # For fields playlist_index and autonumber convert all occurrences
            # of %(field)s to %(field)0Nd for backward compatibility
            field_size_compat_map = {
                'playlist_index': len(str(template_dict['n_entries'])),
                'autonumber': autonumber_size,
            }
            FIELD_SIZE_COMPAT_RE = r'(?<!%)%\((?P<field>autonumber|playlist_index)\)s'
            mobj = re.search(FIELD_SIZE_COMPAT_RE, outtmpl)
            if mobj:
                outtmpl = re.sub(
                    FIELD_SIZE_COMPAT_RE,
                    r'%%(\1)0%dd' % field_size_compat_map[mobj.group('field')],
                    outtmpl)
            # Missing numeric fields used together with integer presentation types
            # in format specification will break the argument substitution since
            # string 'NA' is returned for missing fields. We will patch output
            # template for missing fields to meet string presentation type.
            for numeric_field in self._NUMERIC_FIELDS:
                if numeric_field not in template_dict:
                    # As of [1] format syntax is:
                    #  %[mapping_key][conversion_flags][minimum_width][.precision][length_modifier]type
                    # 1. https://docs.python.org/2/library/stdtypes.html#string-formatting
                    FORMAT_RE = r'''(?x)
                        (?<!%)
                        %
                        \({0}\)  # mapping key
                        (?:[#0\-+ ]+)?  # conversion flags (optional)
                        (?:\d+)?  # minimum field width (optional)
                        (?:\.\d+)?  # precision (optional)
                        [hlL]?  # length modifier (optional)
                        [diouxXeEfFgGcrs%]  # conversion type
                    '''
                    outtmpl = re.sub(
                        FORMAT_RE.format(numeric_field),
                        r'%({0})s'.format(numeric_field), outtmpl)
            filename = expand_path(outtmpl % template_dict)
            # Temporary fix for #4787
            # 'Treat' all problem characters by passing filename through preferredencoding
            # to workaround encoding issues with subprocess on python2 @ Windows
            if sys.version_info < (3, 0) and sys.platform == 'win32':
                filename = encodeFilename(filename, True).decode(preferredencoding())
            return sanitize_path(filename)
        except ValueError as err:
            self.report_error('Error in output template: ' + str(err) + ' (encoding: ' + repr(preferredencoding()) + ')')
            return None
    def _match_entry(self, info_dict, incomplete):
        """""" Returns None iff the file should be downloaded """"""
        video_title = info_dict.get('title', info_dict.get('id', 'video'))
        if 'title' in info_dict:
            # This can happen when we're just evaluating the playlist
            title = info_dict['title']
            matchtitle = self.params.get('matchtitle', False)
            if matchtitle:
                if not re.search(matchtitle, title, re.IGNORECASE):
                    return '""' + title + '"" title did not match pattern ""' + matchtitle + '""'
            rejecttitle = self.params.get('rejecttitle', False)
            if rejecttitle:
                if re.search(rejecttitle, title, re.IGNORECASE):
                    return '""' + title + '"" title matched reject pattern ""' + rejecttitle + '""'
        date = info_dict.get('upload_date')
        if date is not None:
            dateRange = self.params.get('daterange', DateRange())
            if date not in dateRange:
                return '%s upload date is not in range %s' % (date_from_str(date).isoformat(), dateRange)
        view_count = info_dict.get('view_count')
        if view_count is not None:
            min_views = self.params.get('min_views')
            if min_views is not None and view_count < min_views:
                return 'Skipping %s, because it has not reached minimum view count (%d/%d)' % (video_title, view_count, min_views)
            max_views = self.params.get('max_views')
            if max_views is not None and view_count > max_views:
                return 'Skipping %s, because it has exceeded the maximum view count (%d/%d)' % (video_title, view_count, max_views)
        if age_restricted(info_dict.get('age_limit'), self.params.get('age_limit')):
            return 'Skipping ""%s"" because it is age restricted' % video_title
        if self.in_download_archive(info_dict):
            return '%s has already been recorded in archive' % video_title
        if not incomplete:
            match_filter = self.params.get('match_filter')
            if match_filter is not None:
                ret = match_filter(info_dict)
                if ret is not None:
                    return ret
        return None
    @staticmethod
    def add_extra_info(info_dict, extra_info):
        '''Set the keys from extra_info in info dict if they are missing'''
        for key, value in extra_info.items():
            info_dict.setdefault(key, value)
    def extract_info(self, url, download=True, ie_key=None, extra_info={},
                     process=True, force_generic_extractor=False):
        '''
        Returns a list with a dictionary for each video we find.
        If 'download', also downloads the videos.
        extra_info is a dict containing the extra values to add to each result
        '''
        if not ie_key and force_generic_extractor:
            ie_key = 'Generic'
        if ie_key:
            ies = [self.get_info_extractor(ie_key)]
        else:
            ies = self._ies
        for ie in ies:
            if not ie.suitable(url):
                continue
            ie = self.get_info_extractor(ie.ie_key())
            if not ie.working():
                self.report_warning('The program functionality for this site has been marked as broken, '
                                    'and will probably not work.')
            try:
                ie_result = ie.extract(url)
                if ie_result is None:  # Finished already (backwards compatibility; listformats and friends should be moved here)
                    break
                if isinstance(ie_result, list):",[38]
"""""""
Base class for Scrapy spiders
See documentation in docs/topics/spiders.rst
""""""
import logging
import warnings
from scrapy import signals
from scrapy.http import Request
from scrapy.utils.trackref import object_ref
from scrapy.utils.url import url_is_from_spider
from scrapy.utils.deprecate import create_deprecated_class
from scrapy.exceptions import ScrapyDeprecationWarning

class Spider(object_ref):
    """"""Base class for scrapy spiders. All spiders must inherit from this
    class.
    """"""
    name = None
    custom_settings = None
    def __init__(self, name=None, **kwargs):
        if name is not None:
            self.name = name
        elif not getattr(self, 'name', None):
            raise ValueError(""%s must have a name"" % type(self).__name__)
        self.__dict__.update(kwargs)
        if not hasattr(self, 'start_urls'):
            self.start_urls = []
    @property
    def logger(self):
        logger = logging.getLogger(self.name)
        return logging.LoggerAdapter(logger, {'spider': self})
    def log(self, message, level=logging.DEBUG, **kw):
        """"""Log the given message at the given log level
        This helper wraps a log call to the logger within the spider, but you
        can use it directly (e.g. Spider.logger.info('msg')) or use any other
        Python logger too.
        """"""
        self.logger.log(level, message, **kw)
    @classmethod
    def from_crawler(cls, crawler, *args, **kwargs):
        spider = cls(*args, **kwargs)
        spider._set_crawler(crawler)
        return spider
    def set_crawler(self, crawler):
        warnings.warn(""set_crawler is deprecated, instantiate and bound the ""
                      ""spider to this crawler with from_crawler method ""
                      ""instead."",
                      category=ScrapyDeprecationWarning, stacklevel=2)
        assert not hasattr(self, 'crawler'), ""Spider already bounded to a "" \
                                             ""crawler""
        self._set_crawler(crawler)
    def _set_crawler(self, crawler):
        self.crawler = crawler
        self.settings = crawler.settings
        crawler.signals.connect(self.close, signals.spider_closed)
    def start_requests(self):
        if self.make_requests_from_url is not Spider.make_requests_from_url:
            warnings.warn(
                ""Spider.make_requests_from_url method is deprecated; ""
                ""it won't be called in future Scrapy releases. ""
                ""Please override start_requests method instead.""
            )
            for url in self.start_urls:
                yield self.make_requests_from_url(url)
        else:
            for url in self.start_urls:
                yield Request(url, dont_filter=True)
    def make_requests_from_url(self, url):
        """""" This method is deprecated. """"""
        return Request(url, dont_filter=True)
    def parse(self, response):
        raise NotImplementedError
    @classmethod
    def update_settings(cls, settings):
        settings.setdict(cls.custom_settings or {}, priority='spider')
    @classmethod
    def handles_request(cls, request):
        return url_is_from_spider(request.url, cls)
    @staticmethod
    def close(spider, reason):
        closed = getattr(spider, 'closed', None)
        if callable(closed):
            return closed(reason)
    def __str__(self):
        return ""<%s %r at 0x%0x>"" % (type(self).__name__, self.name, id(self))
    __repr__ = __str__

BaseSpider = create_deprecated_class('BaseSpider', Spider)

class ObsoleteClass(object):
    def __init__(self, message):
        self.message = message
    def __getattr__(self, name):
        raise AttributeError(self.message)
spiders = ObsoleteClass(
    '""from scrapy.spider import spiders"" no longer works - use '
    '""from scrapy.spiderloader import SpiderLoader"" and instantiate '
    'it with your project settings""'
)
# Top-level imports
from scrapy.spiders.crawl import CrawlSpider, Rule
from scrapy.spiders.feed import XMLFeedSpider, CSVFeedSpider
from scrapy.spiders.sitemap import SitemapSpider","[68, 70, 71, 72]"
"                ""implemented"".format(action=self._window_type, dtype=values.dtype)
            )
        else:
            try:
                values = ensure_float64(values)
            except (ValueError, TypeError):
                raise TypeError(""cannot handle this type -> {0}"".format(values.dtype))
        # Always convert inf to nan
        values[np.isinf(values)] = np.NaN
        return values
    def _wrap_result(self, result, block=None, obj=None):
        """"""
        Wrap a single result.
        """"""
        if obj is None:
            obj = self._selected_obj
        index = obj.index
        if isinstance(result, np.ndarray):
            # coerce if necessary
            if block is not None:
                if is_timedelta64_dtype(block.values.dtype):
                    # TODO: do we know what result.dtype is at this point?
                    #  i.e. can we just do an astype?
                    from pandas import to_timedelta
                    result = to_timedelta(result.ravel(), unit=""ns"").values.reshape(
                        result.shape
                    )
            if result.ndim == 1:
                from pandas import Series
                return Series(result, index, name=obj.name)
            return type(obj)(result, index=index, columns=block.columns)
        return result
    def _wrap_results(self, results, blocks, obj, exclude=None) -> FrameOrSeries:
        """"""
        Wrap the results.
        Parameters
        ----------
        results : list of ndarrays
        blocks : list of blocks
        obj : conformed data (may be resampled)
        exclude: list of columns to exclude, default to None
        """"""
        from pandas import Series, concat
        final = []
        for result, block in zip(results, blocks):
            result = self._wrap_result(result, block=block, obj=obj)
            if result.ndim == 1:
                return result
            final.append(result)
        # if we have an 'on' column
        # we want to put it back into the results
        # in the same location
        columns = self._selected_obj.columns
        if self.on is not None and not self._on.equals(obj.index):
            name = self._on.name
            final.append(Series(self._on, index=obj.index, name=name))
            if self._selection is not None:
                selection = ensure_index(self._selection)
                # need to reorder to include original location of
                # the on column (if its not already there)
                if name not in selection:
                    columns = self.obj.columns
                    indexer = columns.get_indexer(selection.tolist() + [name])
                    columns = columns.take(sorted(indexer))
        # exclude nuisance columns so that they are not reindexed
        if exclude is not None and exclude:
            columns = [c for c in columns if c not in exclude]
            if not columns:
                raise DataError(""No numeric types to aggregate"")
        if not len(final):
            return obj.astype(""float64"")
        return concat(final, axis=1).reindex(columns=columns, copy=False)
    def _center_window(self, result, window) -> np.ndarray:
        """"""
        Center the result in the window.
        """"""
        if self.axis > result.ndim - 1:
            raise ValueError(""Requested axis is larger then no. of argument dimensions"")
        offset = _offset(window, True)
        if offset > 0:
            if isinstance(result, (ABCSeries, ABCDataFrame)):
                result = result.slice_shift(-offset, axis=self.axis)
            else:
                lead_indexer = [slice(None)] * result.ndim
                lead_indexer[self.axis] = slice(offset, None)
                result = np.copy(result[tuple(lead_indexer)])
        return result
    def _get_roll_func(
        self, cfunc: Callable, check_minp: Callable, index: np.ndarray, **kwargs
    ) -> Callable:
        """"""
        Wrap rolling function to check values passed.
        Parameters
        ----------
        cfunc : callable
            Cython function used to calculate rolling statistics
        check_minp : callable
            function to check minimum period parameter
        index : ndarray
            used for variable window",[9]
"                        access = yield self.get_authenticated_user(
                            redirect_uri='http://your.site.com/auth/google',
                            code=self.get_argument('code'))
                        user = yield self.oauth2_request(
                            ""https://www.googleapis.com/oauth2/v1/userinfo"",
                            access_token=access[""access_token""])
                        # Save the user and access token with
                        # e.g. set_secure_cookie.
                    else:
                        yield self.authorize_redirect(
                            redirect_uri='http://your.site.com/auth/google',
                            client_id=self.settings['google_oauth']['key'],
                            scope=['profile', 'email'],
                            response_type='code',
                            extra_params={'approval_prompt': 'auto'})
        .. testoutput::
           :hide:
        """"""
        http = self.get_auth_http_client()
        body = urllib_parse.urlencode({
            ""redirect_uri"": redirect_uri,
            ""code"": code,
            ""client_id"": self.settings[self._OAUTH_SETTINGS_KEY]['key'],
            ""client_secret"": self.settings[self._OAUTH_SETTINGS_KEY]['secret'],
            ""grant_type"": ""authorization_code"",
        })
        http.fetch(self._OAUTH_ACCESS_TOKEN_URL,
                   functools.partial(self._on_access_token, callback),
                   method=""POST"", headers={'Content-Type': 'application/x-www-form-urlencoded'}, body=body)
    def _on_access_token(self, future, response):
        """"""Callback function for the exchange to the access token.""""""
        if response.error:
            future.set_exception(AuthError('Google auth error: %s' % str(response)))
            return
        args = escape.json_decode(response.body)
        future.set_result(args)

class FacebookGraphMixin(OAuth2Mixin):
    """"""Facebook authentication using the new Graph API and OAuth2.""""""
    _OAUTH_ACCESS_TOKEN_URL = ""https://graph.facebook.com/oauth/access_token?""
    _OAUTH_AUTHORIZE_URL = ""https://www.facebook.com/dialog/oauth?""
    _OAUTH_NO_CALLBACKS = False
    _FACEBOOK_BASE_URL = ""https://graph.facebook.com""
    @_auth_return_future
    def get_authenticated_user(self, redirect_uri, client_id, client_secret,
                               code, callback, extra_fields=None):
        """"""Handles the login for the Facebook user, returning a user object.
        Example usage:
        .. testcode::
            class FacebookGraphLoginHandler(tornado.web.RequestHandler,
                                            tornado.auth.FacebookGraphMixin):
              @tornado.gen.coroutine
              def get(self):
                  if self.get_argument(""code"", False):
                      user = yield self.get_authenticated_user(
                          redirect_uri='/auth/facebookgraph/',
                          client_id=self.settings[""facebook_api_key""],
                          client_secret=self.settings[""facebook_secret""],
                          code=self.get_argument(""code""))
                      # Save the user with e.g. set_secure_cookie
                  else:
                      yield self.authorize_redirect(
                          redirect_uri='/auth/facebookgraph/',
                          client_id=self.settings[""facebook_api_key""],
                          extra_params={""scope"": ""read_stream,offline_access""})
        .. testoutput::
           :hide:
        """"""
        http = self.get_auth_http_client()
        args = {
            ""redirect_uri"": redirect_uri,
            ""code"": code,
            ""client_id"": client_id,
            ""client_secret"": client_secret,
        }
        fields = set(['id', 'name', 'first_name', 'last_name',
                      'locale', 'picture', 'link'])
        if extra_fields:
            fields.update(extra_fields)
        http.fetch(self._oauth_request_token_url(**args),
                   functools.partial(self._on_access_token, redirect_uri, client_id,
                                     client_secret, callback, fields))
    def _on_access_token(self, redirect_uri, client_id, client_secret,
                         future, fields, response):
        if response.error:
            future.set_exception(AuthError('Facebook auth error: %s' % str(response)))
            return
        args = escape.parse_qs_bytes(escape.native_str(response.body))
        session = {
            ""access_token"": args[""access_token""][-1],
            ""expires"": args.get(""expires"")
        }
        self.facebook_request(
            path=""/me"",
            callback=functools.partial(
                self._on_get_user_info, future, session, fields),
            access_token=session[""access_token""],
            fields="","".join(fields)
        )
    def _on_get_user_info(self, future, session, fields, user):
        if user is None:
            future.set_result(None)
            return
        fieldmap = {}
        for field in fields:
            fieldmap[field] = user.get(field)
        fieldmap.update({""access_token"": session[""access_token""], ""session_expires"": session.get(""expires"")})",[103]
"            # e.g. ""[False] is an invalid key""
            if is_scalar(key):
                raise IndexError(key)
            raise InvalidIndexError(key)
    def set_value(self, arr, key, value):
        """"""
        Fast lookup of value from 1-dimensional ndarray.
        .. deprecated:: 1.0
        Notes
        -----
        Only use this if you know what you're doing.
        """"""
        warnings.warn(
            (
                ""The 'set_value' method is deprecated, and ""
                ""will be removed in a future version.""
            ),
            FutureWarning,
            stacklevel=2,
        )
        self._engine.set_value(
            com.values_from_object(arr), com.values_from_object(key), value
        )
    _index_shared_docs[
        ""get_indexer_non_unique""
    ] = """"""
        Compute indexer and mask for new index given the current index. The
        indexer should be then used as an input to ndarray.take to align the
        current data to the new index.
        Parameters
        ----------
        target : %(target_klass)s
        Returns
        -------
        indexer : ndarray of int
            Integers from 0 to n - 1 indicating that the index at these
            positions matches the corresponding target values. Missing values
            in the target are marked by -1.
        missing : ndarray of int
            An indexer into the target of the values not found.
            These correspond to the -1 in the indexer array.
        """"""
    @Appender(_index_shared_docs[""get_indexer_non_unique""] % _index_doc_kwargs)
    def get_indexer_non_unique(self, target):
        target = ensure_index(target)
        pself, ptarget = self._maybe_promote(target)
        if pself is not self or ptarget is not target:
            return pself.get_indexer_non_unique(ptarget)
        if is_categorical(target):
            tgt_values = np.asarray(target)
        elif self.is_all_dates:
            tgt_values = target.asi8
        else:
            tgt_values = target._ndarray_values
        indexer, missing = self._engine.get_indexer_non_unique(tgt_values)
        return ensure_platform_int(indexer), missing
    def get_indexer_for(self, target, **kwargs):
        """"""
        Guaranteed return of an indexer even when non-unique.
        This dispatches to get_indexer or get_indexer_non_unique
        as appropriate.
        Returns
        -------
        numpy.ndarray
            List of indices.
        """"""
        if self.is_unique:
            return self.get_indexer(target, **kwargs)
        indexer, _ = self.get_indexer_non_unique(target, **kwargs)
        return indexer
    def _maybe_promote(self, other):
        # A hack, but it works
        if self.inferred_type == ""date"" and isinstance(other, ABCDatetimeIndex):
            return type(other)(self), other
        elif self.inferred_type == ""boolean"":
            if not is_object_dtype(self.dtype):
                return self.astype(""object""), other.astype(""object"")
        return self, other
    def groupby(self, values):
        """"""
        Group the index labels by a given array of values.
        Parameters
        ----------
        values : array
            Values used to determine the groups.
        Returns
        -------
        groups : dict
            {group name -> group labels}
        """"""
        # TODO: if we are a MultiIndex, we can do better
        # that converting to tuples
        if isinstance(values, ABCMultiIndex):
            values = values.values
        values = ensure_categorical(values)
        result = values._reverse_indexer()
        # map to the label
        result = {k: self.take(v) for k, v in result.items()}
        return result
    def map(self, mapper, na_action=None):
        """"""
        Map values using input correspondence (a dict, Series, or function).
        Parameters
        ----------
        mapper : function, dict, or Series",[58]
"        # We are called by `union`, which is responsible for this validation
        assert isinstance(other, TimedeltaIndex)
        this, other = self, other
        if this._can_fast_union(other):
            return this._fast_union(other)
        else:
            result = Index._union(this, other, sort=sort)
            if isinstance(result, TimedeltaIndex):
                if result.freq is None:
                    result._set_freq(""infer"")
            return result
    def _fast_union(self, other):
        if len(other) == 0:
            return self.view(type(self))
        if len(self) == 0:
            return other.view(type(self))
        # to make our life easier, ""sort"" the two ranges
        if self[0] <= other[0]:
            left, right = self, other
        else:
            left, right = other, self
        left_end = left[-1]
        right_end = right[-1]
        # concatenate
        if left_end < right_end:
            loc = right.searchsorted(left_end, side=""right"")
            right_chunk = right.values[loc:]
            dates = concat_compat((left.values, right_chunk))
            return self._shallow_copy(dates)
        else:
            return left
    def _maybe_promote(self, other):
        if other.inferred_type == ""timedelta"":
            other = TimedeltaIndex(other)
        return self, other
    def get_value(self, series, key):
        """"""
        Fast lookup of value from 1-dimensional ndarray. Only use this if you
        know what you're doing
        """"""
        if _is_convertible_to_td(key):
            key = Timedelta(key)
            return self.get_value_maybe_box(series, key)
        try:
            value = Index.get_value(self, series, key)
        except KeyError:
            try:
                loc = self._get_string_slice(key)
                return series[loc]
            except (TypeError, ValueError, KeyError):
                pass
            try:
                return self.get_value_maybe_box(series, key)
            except (TypeError, ValueError, KeyError):
                raise KeyError(key)
        else:
            return com.maybe_box(self, value, series, key)
    def get_value_maybe_box(self, series, key: Timedelta):
        values = self._engine.get_value(com.values_from_object(series), key)
        return com.maybe_box(self, values, series, key)
    def get_loc(self, key, method=None, tolerance=None):
        """"""
        Get integer location for requested label
        Returns
        -------
        loc : int
        """"""
        if is_list_like(key) or (isinstance(key, datetime) and key is not NaT):
            # GH#20464 datetime check here is to ensure we don't allow
            #   datetime objects to be incorrectly treated as timedelta
            #   objects; NaT is a special case because it plays a double role
            #   as Not-A-Timedelta
            raise TypeError
        if isna(key):
            key = NaT
        if tolerance is not None:
            # try converting tolerance now, so errors don't get swallowed by
            # the try/except clauses below
            tolerance = self._convert_tolerance(tolerance, np.asarray(key))
        if _is_convertible_to_td(key) or key is NaT:
            key = Timedelta(key)
            return Index.get_loc(self, key, method, tolerance)
        try:
            return Index.get_loc(self, key, method, tolerance)
        except (KeyError, ValueError, TypeError):
            try:
                return self._get_string_slice(key)
            except (TypeError, KeyError, ValueError):
                pass
            try:
                stamp = Timedelta(key)
                return Index.get_loc(self, stamp, method, tolerance)
            except (KeyError, ValueError):
                raise KeyError(key)
    def _maybe_cast_slice_bound(self, label, side, kind):
        """"""
        If label is a string, cast it to timedelta according to resolution.
        Parameters
        ----------
        label : object
        side : {'left', 'right'}
        kind : {'ix', 'loc', 'getitem'}
        Returns
        -------","[6, 14]"
"            indexer = self._get_fill_indexer(target, method, limit, tolerance)
        elif method == ""nearest"":
            indexer = self._get_nearest_indexer(target, limit, tolerance)
        else:
            if tolerance is not None:
                raise ValueError(
                    ""tolerance argument only valid if doing pad, ""
                    ""backfill or nearest reindexing""
                )
            if limit is not None:
                raise ValueError(
                    ""limit argument only valid if doing pad, ""
                    ""backfill or nearest reindexing""
                )
            indexer = self._engine.get_indexer(target._ndarray_values)
        return ensure_platform_int(indexer)
    def _convert_tolerance(self, tolerance, target):
        # override this method on subclasses
        tolerance = np.asarray(tolerance)
        if target.size != tolerance.size and tolerance.size > 1:
            raise ValueError(""list-like tolerance size must match target index size"")
        return tolerance
    def _get_fill_indexer(
        self, target: ""Index"", method: str_t, limit=None, tolerance=None
    ) -> np.ndarray:
        if self.is_monotonic_increasing and target.is_monotonic_increasing:
            engine_method = (
                self._engine.get_pad_indexer
                if method == ""pad""
                else self._engine.get_backfill_indexer
            )
            indexer = engine_method(target._ndarray_values, limit)
        else:
            indexer = self._get_fill_indexer_searchsorted(target, method, limit)
        if tolerance is not None:
            indexer = self._filter_indexer_tolerance(
                target._ndarray_values, indexer, tolerance
            )
        return indexer
    def _get_fill_indexer_searchsorted(
        self, target: ""Index"", method: str_t, limit=None
    ) -> np.ndarray:
        """"""
        Fallback pad/backfill get_indexer that works for monotonic decreasing
        indexes and non-monotonic targets.
        """"""
        if limit is not None:
            raise ValueError(
                f""limit argument for {repr(method)} method only well-defined ""
                ""if index and target are monotonic""
            )
        side = ""left"" if method == ""pad"" else ""right""
        # find exact matches first (this simplifies the algorithm)
        indexer = self.get_indexer(target)
        nonexact = indexer == -1
        indexer[nonexact] = self._searchsorted_monotonic(target[nonexact], side)
        if side == ""left"":
            # searchsorted returns ""indices into a sorted array such that,
            # if the corresponding elements in v were inserted before the
            # indices, the order of a would be preserved"".
            # Thus, we need to subtract 1 to find values to the left.
            indexer[nonexact] -= 1
            # This also mapped not found values (values of 0 from
            # np.searchsorted) to -1, which conveniently is also our
            # sentinel for missing values
        else:
            # Mark indices to the right of the largest value as not found
            indexer[indexer == len(self)] = -1
        return indexer
    def _get_nearest_indexer(self, target: ""Index"", limit, tolerance) -> np.ndarray:
        """"""
        Get the indexer for the nearest index labels; requires an index with
        values that can be subtracted from each other (e.g., not strings or
        tuples).
        """"""
        left_indexer = self.get_indexer(target, ""pad"", limit=limit)
        right_indexer = self.get_indexer(target, ""backfill"", limit=limit)
        target = np.asarray(target)
        left_distances = abs(self.values[left_indexer] - target)
        right_distances = abs(self.values[right_indexer] - target)
        op = operator.lt if self.is_monotonic_increasing else operator.le
        indexer = np.where(
            op(left_distances, right_distances) | (right_indexer == -1),
            left_indexer,
            right_indexer,
        )
        if tolerance is not None:
            indexer = self._filter_indexer_tolerance(target, indexer, tolerance)
        return indexer
    def _filter_indexer_tolerance(
        self, target: ""Index"", indexer: np.ndarray, tolerance
    ) -> np.ndarray:
        distance = abs(self.values[indexer] - target)
        indexer = np.where(distance <= tolerance, indexer, -1)
        return indexer
    # --------------------------------------------------------------------
    # Indexer Conversion Methods
    def _convert_scalar_indexer(self, key, kind=None):
        """"""
        Convert a scalar indexer.
        Parameters
        ----------
        key : label of the slice bound
        kind : {'loc', 'getitem', 'iloc'} or None
        """"""
        assert kind in [""loc"", ""getitem"", ""iloc"", None]
        if kind == ""iloc"":
            self._validate_indexer(""positional"", key, ""iloc"")
            return key
        if len(self) and not isinstance(self, ABCMultiIndex):
","[86, 87, 88]"
"""""""
Functions for defining unary operations.
""""""
from typing import Any
from pandas._typing import ArrayLike
from pandas.core.dtypes.common import (
    is_datetime64_dtype,
    is_integer_dtype,
    is_object_dtype,
    is_timedelta64_dtype,
)
from pandas.core.dtypes.generic import ABCExtensionArray

def should_extension_dispatch(left: ArrayLike, right: Any) -> bool:
    """"""
    Identify cases where Series operation should dispatch to ExtensionArray method.
    Parameters
    ----------
    left : np.ndarray or ExtensionArray
    right : object
    Returns
    -------
    bool
    """"""
    return isinstance(left, ABCExtensionArray) or isinstance(right, ABCExtensionArray)

def should_series_dispatch(left, right, op):
    """"""
    Identify cases where a DataFrame operation should dispatch to its
    Series counterpart.
    Parameters
    ----------
    left : DataFrame
    right : DataFrame or Series
    op : binary operator
    Returns
    -------
    override : bool
    """"""
    if left._is_mixed_type or right._is_mixed_type:
        return True
    if op.__name__.strip(""_"") in [""and"", ""or"", ""xor"", ""rand"", ""ror"", ""rxor""]:
        # TODO: GH references for what this fixes
        # Note: this check must come before the check for nonempty columns.
        return True
    if right.ndim == 1:
        # operating with Series, short-circuit checks that would fail
        #  with AttributeError.
        return False
    if not len(left.columns) or not len(right.columns):
        # ensure obj.dtypes[0] exists for each obj
        return False
    ldtype = left.dtypes.iloc[0]
    rdtype = right.dtypes.iloc[0]
    if (is_timedelta64_dtype(ldtype) and is_integer_dtype(rdtype)) or (
        is_timedelta64_dtype(rdtype) and is_integer_dtype(ldtype)
    ):
        # numpy integer dtypes as timedelta64 dtypes in this scenario
        return True
    if is_datetime64_dtype(ldtype) and is_object_dtype(rdtype):
        # in particular case where right is an array of DateOffsets
        return True
    return False",[73]
"class InvalidInput(ValueError):
    """"""Raised when input source code fails all parse attempts.""""""

class WriteBack(Enum):
    NO = 0
    YES = 1
    DIFF = 2
    CHECK = 3
    @classmethod
    def from_configuration(cls, *, check: bool, diff: bool) -> ""WriteBack"":
        if check and not diff:
            return cls.CHECK
        return cls.DIFF if diff else cls.YES

class Changed(Enum):
    NO = 0
    CACHED = 1
    YES = 2

class TargetVersion(Enum):
    PY27 = 2
    PY33 = 3
    PY34 = 4
    PY35 = 5
    PY36 = 6
    PY37 = 7
    PY38 = 8
    def is_python2(self) -> bool:
        return self is TargetVersion.PY27

PY36_VERSIONS = {TargetVersion.PY36, TargetVersion.PY37, TargetVersion.PY38}

class Feature(Enum):
    # All string literals are unicode
    UNICODE_LITERALS = 1
    F_STRINGS = 2
    NUMERIC_UNDERSCORES = 3
    TRAILING_COMMA_IN_CALL = 4
    TRAILING_COMMA_IN_DEF = 5

VERSION_TO_FEATURES: Dict[TargetVersion, Set[Feature]] = {
    TargetVersion.PY27: set(),
    TargetVersion.PY33: {Feature.UNICODE_LITERALS},
    TargetVersion.PY34: {Feature.UNICODE_LITERALS},
    TargetVersion.PY35: {Feature.UNICODE_LITERALS, Feature.TRAILING_COMMA_IN_CALL},
    TargetVersion.PY36: {
        Feature.UNICODE_LITERALS,
        Feature.F_STRINGS,
        Feature.NUMERIC_UNDERSCORES,
        Feature.TRAILING_COMMA_IN_CALL,
        Feature.TRAILING_COMMA_IN_DEF,
    },
    TargetVersion.PY37: {
        Feature.UNICODE_LITERALS,
        Feature.F_STRINGS,
        Feature.NUMERIC_UNDERSCORES,
        Feature.TRAILING_COMMA_IN_CALL,
        Feature.TRAILING_COMMA_IN_DEF,
    },
    TargetVersion.PY38: {
        Feature.UNICODE_LITERALS,
        Feature.F_STRINGS,
        Feature.NUMERIC_UNDERSCORES,
        Feature.TRAILING_COMMA_IN_CALL,
        Feature.TRAILING_COMMA_IN_DEF,
    },
}

@dataclass
class FileMode:
    target_versions: Set[TargetVersion] = Factory(set)
    line_length: int = DEFAULT_LINE_LENGTH
    string_normalization: bool = True
    is_pyi: bool = False
    def get_cache_key(self) -> str:
        if self.target_versions:
            version_str = "","".join(
                str(version.value)
                for version in sorted(self.target_versions, key=lambda v: v.value)
            )
        else:
            version_str = ""-""
        parts = [
            version_str,
            str(self.line_length),
            str(int(self.string_normalization)),
            str(int(self.is_pyi)),
        ]
        return ""."".join(parts)

def supports_feature(target_versions: Set[TargetVersion], feature: Feature) -> bool:
    return all(feature in VERSION_TO_FEATURES[version] for version in target_versions)

def read_pyproject_toml(
    ctx: click.Context, param: click.Parameter, value: Union[str, int, bool, None]
) -> Optional[str]:
    """"""Inject Black configuration from ""pyproject.toml"" into defaults in `ctx`.
    Returns the path to a successfully found and read configuration file, None
    otherwise.
    """"""
    assert not isinstance(value, (int, bool)), ""Invalid parameter type passed""
    if not value:
        root = find_project_root(ctx.params.get(""src"", ()))
        path = root / ""pyproject.toml""
        if path.is_file():
            value = str(path)
        else:
            return None
    try:
        pyproject_toml = toml.load(value)
        config = pyproject_toml.get(""tool"", {}).get(""black"", {})
    except (toml.TomlDecodeError, OSError) as e:","[50, 51, 52, 53]"
"from time import time
import os
from ..conf import settings
from ..utils import memoize
from .generic import Generic

class Zsh(Generic):
    def app_alias(self, alias_name):
        alias = ""alias {0}='TF_ALIAS={0}"" \
                "" PYTHONIOENCODING=utf-8"" \
                ' TF_SHELL_ALIASES=$(alias)' \
                "" TF_CMD=$(thefuck $(fc -ln -1 | tail -n 1)) &&"" \
                "" eval $TF_CMD"".format(alias_name)
        if settings.alter_history:
            return alias + "" && print -s $TF_CMD'""
        else:
            return alias + ""'""
    def _parse_alias(self, alias):
        name, value = alias.split('=', 1)
        if value[0] == value[-1] == '""' or value[0] == value[-1] == ""'"":
            value = value[1:-1]
        return name, value
    @memoize
    def get_aliases(self):
        raw_aliases = os.environ.get('TF_SHELL_ALIASES', '').split('\n')
        return dict(self._parse_alias(alias)
                    for alias in raw_aliases if alias and '=' in alias)
    def _get_history_file_name(self):
        return os.environ.get(""HISTFILE"",
                              os.path.expanduser('~/.zsh_history'))
    def _get_history_line(self, command_script):
        return u': {}:0;{}\n'.format(int(time()), command_script)
    def _script_from_history(self, line):
        if ';' in line:
            return line.split(';', 1)[1]
        else:
            return ''
    def how_to_configure(self):
        return 'eval $(thefuck --alias)', '~/.zshrc'","[9, 11, 12]"
"""""""
This is the Scrapy engine which controls the Scheduler, Downloader and Spiders.
For more information see docs/topics/architecture.rst
""""""
import logging
from time import time
from twisted.internet import defer
from twisted.python.failure import Failure
from scrapy import signals
from scrapy.core.scraper import Scraper
from scrapy.exceptions import DontCloseSpider
from scrapy.http import Response, Request
from scrapy.utils.misc import load_object
from scrapy.utils.reactor import CallLaterOnce
from scrapy.utils.log import logformatter_adapter
logger = logging.getLogger(__name__)

class Slot(object):
    def __init__(self, start_requests, close_if_idle, nextcall, scheduler):
        self.closing = False
        self.inprogress = set() # requests in progress
        self.start_requests = iter(start_requests)
        self.close_if_idle = close_if_idle
        self.nextcall = nextcall
        self.scheduler = scheduler
    def add_request(self, request):
        self.inprogress.add(request)
    def remove_request(self, request):
        self.inprogress.remove(request)
        self._maybe_fire_closing()
    def close(self):
        self.closing = defer.Deferred()
        self._maybe_fire_closing()
        return self.closing
    def _maybe_fire_closing(self):
        if self.closing and not self.inprogress:
            if self.nextcall:
                self.nextcall.cancel()
            self.closing.callback(None)

class ExecutionEngine(object):
    def __init__(self, crawler, spider_closed_callback):
        self.crawler = crawler
        self.settings = crawler.settings
        self.signals = crawler.signals
        self.logformatter = crawler.logformatter
        self.slot = None
        self.spider = None
        self.running = False
        self.paused = False
        self.scheduler_cls = load_object(self.settings['SCHEDULER'])
        downloader_cls = load_object(self.settings['DOWNLOADER'])
        self.downloader = downloader_cls(crawler)
        self.scraper = Scraper(crawler)
        self._spider_closed_callback = spider_closed_callback
    @defer.inlineCallbacks
    def start(self):
        """"""Start the execution engine""""""
        assert not self.running, ""Engine already running""
        self.start_time = time()
        yield self.signals.send_catch_log_deferred(signal=signals.engine_started)
        self.running = True
        self._closewait = defer.Deferred()
        yield self._closewait
    def stop(self):
        """"""Stop the execution engine gracefully""""""
        assert self.running, ""Engine not running""
        self.running = False
        dfd = self._close_all_spiders()
        return dfd.addBoth(lambda _: self._finish_stopping_engine())
    def pause(self):
        """"""Pause the execution engine""""""
        self.paused = True
    def unpause(self):
        """"""Resume the execution engine""""""
        self.paused = False
    def _next_request(self, spider):
        slot = self.slot
        if not slot:
            return
        if self.paused:
            slot.nextcall.schedule(5)
            return
        while not self._needs_backout(spider):
            if not self._next_request_from_scheduler(spider):
                break
        if slot.start_requests and not self._needs_backout(spider):
            try:
                request = next(slot.start_requests)
            except StopIteration:
                slot.start_requests = None
            except Exception:
                slot.start_requests = None
                logger.error('Error while obtaining start requests',
                             exc_info=True, extra={'spider': spider})
            else:
                self.crawl(request, spider)
        if self.spider_is_idle(spider) and slot.close_if_idle:
            self._spider_idle(spider)
    def _needs_backout(self, spider):
        slot = self.slot
        return not self.running \
            or slot.closing \
            or self.downloader.needs_backout() \",[18]
"            if enqueuer is not None:
                enqueuer.stop()
        if not isinstance(outs, list):
            return np.average(np.asarray(all_outs),
                              weights=batch_sizes)
        else:
            averages = []
            for i in range(len(outs)):
                averages.append(np.average([out[i] for out in all_outs],
                                           weights=batch_sizes))
            return averages
    @interfaces.legacy_generator_methods_support
    def predict_generator(self, generator, steps,
                          max_queue_size=10,
                          workers=1,
                          use_multiprocessing=False,
                          verbose=0):
        """"""Generates predictions for the input samples from a data generator.
        The generator should return the same kind of data as accepted by
        `predict_on_batch`.
        # Arguments
            generator: Generator yielding batches of input samples
                    or an instance of Sequence (keras.utils.Sequence)
                    object in order to avoid duplicate data
                    when using multiprocessing.
            steps: Total number of steps (batches of samples)
                to yield from `generator` before stopping.
                Not used if using Sequence.
            max_queue_size: Maximum size for the generator queue.
            workers: Maximum number of processes to spin up
                when using process based threading
            use_multiprocessing: If `True`, use process based threading.
                Note that because
                this implementation relies on multiprocessing,
                you should not pass
                non picklable arguments to the generator
                as they can't be passed
                easily to children processes.
            verbose: verbosity mode, 0 or 1.
        # Returns
            Numpy array(s) of predictions.
        # Raises
            ValueError: In case the generator yields
                data in an invalid format.
        """"""
        self._make_predict_function()
        steps_done = 0
        wait_time = 0.01
        all_outs = []
        is_sequence = isinstance(generator, Sequence)
        if not is_sequence and use_multiprocessing and workers > 1:
            warnings.warn(
                UserWarning('Using a generator with `use_multiprocessing=True`'
                            ' and multiple workers may duplicate your data.'
                            ' Please consider using the`keras.utils.Sequence'
                            ' class.'))
        if is_sequence:
            steps = len(generator)
        enqueuer = None
        try:
            if is_sequence:
                enqueuer = OrderedEnqueuer(generator,
                                           use_multiprocessing=use_multiprocessing)
            else:
                enqueuer = GeneratorEnqueuer(generator,
                                             use_multiprocessing=use_multiprocessing,
                                             wait_time=wait_time)
            enqueuer.start(workers=workers, max_queue_size=max_queue_size)
            output_generator = enqueuer.get()
            if verbose == 1:
                progbar = Progbar(target=steps)
            while steps_done < steps:
                generator_output = next(output_generator)
                if isinstance(generator_output, tuple):
                    # Compatibility with the generators
                    # used for training.
                    if len(generator_output) == 2:
                        x, _ = generator_output
                    elif len(generator_output) == 3:
                        x, _, _ = generator_output
                    else:
                        raise ValueError('Output of generator should be '
                                         'a tuple `(x, y, sample_weight)` '
                                         'or `(x, y)`. Found: ' +
                                         str(generator_output))
                else:
                    # Assumes a generator that only
                    # yields inputs (not targets and sample weights).
                    x = generator_output
                outs = self.predict_on_batch(x)
                if not isinstance(outs, list):
                    outs = [outs]
                if not all_outs:
                    for out in outs:
                        all_outs.append([])
                for i, out in enumerate(outs):
                    all_outs[i].append(out)
                steps_done += 1
                if verbose == 1:
                    progbar.update(steps_done)
        finally:
            if enqueuer is not None:
                enqueuer.stop()
        if len(all_outs) == 1:
            if steps_done == 1:
                return all_outs[0][0]
            else:
                return np.concatenate(all_outs[0])
        if steps_done == 1:
            return [out for out in all_outs]
        else:","[14, 31, 63, 64]"
"        as long as they are unique and understood by the subclass's
        overridden `get_content`.
        .. versionadded:: 3.1
        """"""
        abspath = os.path.abspath(os.path.join(root, path))
        return abspath
    def validate_absolute_path(self, root, absolute_path):
        """"""Validate and return the absolute path.
        ``root`` is the configured path for the `StaticFileHandler`,
        and ``path`` is the result of `get_absolute_path`
        This is an instance method called during request processing,
        so it may raise `HTTPError` or use methods like
        `RequestHandler.redirect` (return None after redirecting to
        halt further processing).  This is where 404 errors for missing files
        are generated.
        This method may modify the path before returning it, but note that
        any such modifications will not be understood by `make_static_url`.
        In instance methods, this method's result is available as
        ``self.absolute_path``.
        .. versionadded:: 3.1
        """"""
        root = os.path.abspath(root)
        # os.path.abspath strips a trailing /
        # it needs to be temporarily added back for requests to root/
        if not (absolute_path + os.path.sep).startswith(root):
            raise HTTPError(403, ""%s is not in root static directory"",
                            self.path)
        if (os.path.isdir(absolute_path) and
                self.default_filename is not None):
            # need to look at the request.path here for when path is empty
            # but there is some prefix to the path that was already
            # trimmed by the routing
            if not self.request.path.endswith(""/""):
                self.redirect(self.request.path + ""/"", permanent=True)
                return
            absolute_path = os.path.join(absolute_path, self.default_filename)
        if not os.path.exists(absolute_path):
            raise HTTPError(404)
        if not os.path.isfile(absolute_path):
            raise HTTPError(403, ""%s is not a file"", self.path)
        return absolute_path
    @classmethod
    def get_content(cls, abspath, start=None, end=None):
        """"""Retrieve the content of the requested resource which is located
        at the given absolute path.
        This class method may be overridden by subclasses.  Note that its
        signature is different from other overridable class methods
        (no ``settings`` argument); this is deliberate to ensure that
        ``abspath`` is able to stand on its own as a cache key.
        This method should either return a byte string or an iterator
        of byte strings.  The latter is preferred for large files
        as it helps reduce memory fragmentation.
        .. versionadded:: 3.1
        """"""
        with open(abspath, ""rb"") as file:
            if start is not None:
                file.seek(start)
            if end is not None:
                remaining = end - (start or 0)
            else:
                remaining = None
            while True:
                chunk_size = 64 * 1024
                if remaining is not None and remaining < chunk_size:
                    chunk_size = remaining
                chunk = file.read(chunk_size)
                if chunk:
                    if remaining is not None:
                        remaining -= len(chunk)
                    yield chunk
                else:
                    if remaining is not None:
                        assert remaining == 0
                    return
    @classmethod
    def get_content_version(cls, abspath):
        """"""Returns a version string for the resource at the given path.
        This class method may be overridden by subclasses.  The
        default implementation is a hash of the file's contents.
        .. versionadded:: 3.1
        """"""
        data = cls.get_content(abspath)
        hasher = hashlib.md5()
        if isinstance(data, bytes):
            hasher.update(data)
        else:
            for chunk in data:
                hasher.update(chunk)
        return hasher.hexdigest()
    def _stat(self):
        if not hasattr(self, '_stat_result'):
            self._stat_result = os.stat(self.absolute_path)
        return self._stat_result
    def get_content_size(self):
        """"""Retrieve the total size of the resource at the given path.
        This method may be overridden by subclasses.
        .. versionadded:: 3.1
        .. versionchanged:: 4.0
           This method is now always called, instead of only when
           partial results are requested.
        """"""
        stat_result = self._stat()
        return stat_result[stat.ST_SIZE]
    def get_modified_time(self):
        """"""Returns the time that ``self.absolute_path`` was last modified.
        May be overridden in subclasses.  Should return a `~datetime.datetime`",[28]
"            self._write_future = None
            future.set_result(None)
    def _can_keep_alive(self, start_line, headers):
        if self.params.no_keep_alive:
            return False
        connection_header = headers.get(""Connection"")
        if connection_header is not None:
            connection_header = connection_header.lower()
        if start_line.version == ""HTTP/1.1"":
            return connection_header != ""close""
        elif (""Content-Length"" in headers
              or headers.get(""Transfer-Encoding"", """").lower() == ""chunked""
              or start_line.method in (""HEAD"", ""GET"")):
            return connection_header == ""keep-alive""
        return False
    def _finish_request(self, future):
        self._clear_callbacks()
        if not self.is_client and self._disconnect_on_finish:
            self.close()
            return
        # Turn Nagle's algorithm back on, leaving the stream in its
        # default state for the next request.
        self.stream.set_nodelay(False)
        if not self._finish_future.done():
            self._finish_future.set_result(None)
    def _parse_headers(self, data):
        # The lstrip removes newlines that some implementations sometimes
        # insert between messages of a reused connection.  Per RFC 7230,
        # we SHOULD ignore at least one empty line before the request.
        # http://tools.ietf.org/html/rfc7230#section-3.5
        data = native_str(data.decode('latin1')).lstrip(""\r\n"")
        # RFC 7230 section allows for both CRLF and bare LF.
        eol = data.find(""\n"")
        start_line = data[:eol].rstrip(""\r"")
        try:
            headers = httputil.HTTPHeaders.parse(data[eol:])
        except ValueError:
            # probably form split() if there was no ':' in the line
            raise httputil.HTTPInputError(""Malformed HTTP headers: %r"" %
                                          data[eol:100])
        return start_line, headers
    def _read_body(self, code, headers, delegate):
        if ""Content-Length"" in headers:
            if ""Transfer-Encoding"" in headers:
                # Response cannot contain both Content-Length and
                # Transfer-Encoding headers.
                # http://tools.ietf.org/html/rfc7230#section-3.3.3
                raise httputil.HTTPInputError(
                    ""Response with both Transfer-Encoding and Content-Length"")
            if "","" in headers[""Content-Length""]:
                # Proxies sometimes cause Content-Length headers to get
                # duplicated.  If all the values are identical then we can
                # use them but if they differ it's an error.
                pieces = re.split(r',\s*', headers[""Content-Length""])
                if any(i != pieces[0] for i in pieces):
                    raise httputil.HTTPInputError(
                        ""Multiple unequal Content-Lengths: %r"" %
                        headers[""Content-Length""])
                headers[""Content-Length""] = pieces[0]
            content_length = int(headers[""Content-Length""])
            if content_length > self._max_body_size:
                raise httputil.HTTPInputError(""Content-Length too long"")
        else:
            content_length = None
        if code == 204:
            # This response code is not allowed to have a non-empty body,
            # and has an implicit length of zero instead of read-until-close.
            # http://www.w3.org/Protocols/rfc2616/rfc2616-sec4.html#sec4.3
            if (""Transfer-Encoding"" in headers or
                    content_length not in (None, 0)):
                raise httputil.HTTPInputError(
                    ""Response with code %d should not have body"" % code)
            content_length = 0
        if content_length is not None:
            return self._read_fixed_body(content_length, delegate)
        if headers.get(""Transfer-Encoding"") == ""chunked"":
            return self._read_chunked_body(delegate)
        if self.is_client:
            return self._read_body_until_close(delegate)
        return None
    @gen.coroutine
    def _read_fixed_body(self, content_length, delegate):
        while content_length > 0:
            body = yield self.stream.read_bytes(
                min(self.params.chunk_size, content_length), partial=True)
            content_length -= len(body)
            if not self._write_finished or self.is_client:
                with _ExceptionLoggingContext(app_log):
                    ret = delegate.data_received(body)
                    if ret is not None:
                        yield ret
    @gen.coroutine
    def _read_chunked_body(self, delegate):
        # TODO: ""chunk extensions"" http://tools.ietf.org/html/rfc2616#section-3.6.1
        total_size = 0
        while True:
            chunk_len = yield self.stream.read_until(b""\r\n"", max_bytes=64)
            chunk_len = int(chunk_len.strip(), 16)
            if chunk_len == 0:
                return
            total_size += chunk_len
            if total_size > self._max_body_size:
                raise httputil.HTTPInputError(""chunked body too large"")
            bytes_to_read = chunk_len
            while bytes_to_read:
                chunk = yield self.stream.read_bytes(
                    min(bytes_to_read, self.params.chunk_size), partial=True)
                bytes_to_read -= len(chunk)
                if not self._write_finished or self.is_client:
                    with _ExceptionLoggingContext(app_log):
                        ret = delegate.data_received(chunk)
                        if ret is not None:
                            yield ret
            # chunk ends with \r\n
            crlf = yield self.stream.read_bytes(2)
            assert crlf == b""\r\n""
    @gen.coroutine",[13]
"    Raises TypeError if the check fails
    :arg argument_spec: Argument spec dicitionary containing all parameters
        and their specification
    :arg module_paramaters: Dictionary of module parameters
    :returns: Empty list or raises TypeError if the check fails.
    """"""
    missing = []
    if argument_spec is None:
        return missing
    for (k, v) in argument_spec.items():
        required = v.get('required', False)
        if required and k not in module_parameters:
            missing.append(k)
    if missing:
        msg = ""missing required arguments: %s"" % "", "".join(missing)
        raise TypeError(to_native(msg))
    return missing

def check_required_if(requirements, module_parameters):
    """"""Check parameters that are conditionally required
    Raises TypeError if the check fails
    :arg requirements: List of lists specifying a parameter, value, parameters
        required when the given parameter is the specified value, and optionally
        a boolean indicating any or all parameters are required.
        Example:
            required_if=[
                ['state', 'present', ('path',), True],
                ['someint', 99, ('bool_param', 'string_param')],
            ]
    :arg module_paramaters: Dictionary of module parameters
    :returns: Empty list or raises TypeError if the check fails.
        The results attribute of the exception contains a list of dictionaries.
        Each dictionary is the result of evaluting each item in requirements.
        Each return dictionary contains the following keys:
            :key missing: List of parameters that are required but missing
            :key requires: 'any' or 'all'
            :key paramater: Parameter name that has the requirement
            :key value: Original value of the paramater
            :key requirements: Original required parameters
        Example:
            [
                {
                    'parameter': 'someint',
                    'value': 99
                    'requirements': ('bool_param', 'string_param'),
                    'missing': ['string_param'],
                    'requires': 'all',
                }
            ]
    """"""
    results = []
    if requirements is None:
        return results
    for req in requirements:
        missing = {}
        missing['missing'] = []
        max_missing_count = 0
        is_one_of = False
        if len(req) == 4:
            key, val, requirements, is_one_of = req
        else:
            key, val, requirements = req
        # is_one_of is True at least one requirement should be
        # present, else all requirements should be present.
        if is_one_of:
            max_missing_count = len(requirements)
            missing['requires'] = 'any'
        else:
            missing['requires'] = 'all'
        if key in module_parameters and module_parameters[key] == val:
            for check in requirements:
                count = count_terms(check, module_parameters)
                if count == 0:
                    missing['missing'].append(check)
        if len(missing['missing']) and len(missing['missing']) >= max_missing_count:
            missing['parameter'] = key
            missing['value'] = val
            missing['requirements'] = requirements
            results.append(missing)
    if results:
        for missing in results:
            msg = ""%s is %s but %s of the following are missing: %s"" % (
                missing['parameter'], missing['value'], missing['requires'], ', '.join(missing['missing']))
            raise TypeError(to_native(msg))
    return results

def check_missing_parameters(module_parameters, required_parameters=None):
    """"""This is for checking for required params when we can not check via
    argspec because we need more information than is simply given in the argspec.
    Raises TypeError if any required parameters are missing
    :arg module_paramaters: Dictionary of module parameters
    :arg required_parameters: List of parameters to look for in the given module
        parameters
    :returns: Empty list or raises TypeError if the check fails.
    """"""
    missing_params = []
    if required_parameters is None:
        return missing_params
    for param in required_parameters:
        if not module_parameters.get(param):
            missing_params.append(param)
",[19]
"            elif zf.mode == ""r"":
                zip_names = zf.namelist()
                if len(zip_names) == 1:
                    f = zf.open(zip_names.pop())
                elif len(zip_names) == 0:
                    raise ValueError(f""Zero files found in ZIP file {path_or_buf}"")
                else:
                    raise ValueError(
                        ""Multiple files found in ZIP file. ""
                        f""Only one file per ZIP: {zip_names}""
                    )
        # XZ Compression
        elif compression == ""xz"":
            f = _get_lzma_file(lzma)(path_or_buf, mode)
        # Unrecognized Compression
        else:
            msg = f""Unrecognized compression type: {compression}""
            raise ValueError(msg)
        handles.append(f)
    elif is_path:
        if encoding:
            # Encoding
            f = open(path_or_buf, mode, encoding=encoding, newline="""")
        elif is_text:
            # No explicit encoding
            f = open(path_or_buf, mode, errors=""replace"", newline="""")
        else:
            # Binary mode
            f = open(path_or_buf, mode)
        handles.append(f)
    # Convert BytesIO or file objects passed with an encoding
    if is_text and (compression or isinstance(f, need_text_wrapping)):
        from io import TextIOWrapper
        g = TextIOWrapper(f, encoding=encoding, newline="""")
        if not isinstance(f, BufferedIOBase):
            handles.append(g)
        f = g
    if memory_map and hasattr(f, ""fileno""):
        try:
            wrapped = _MMapWrapper(f)
            f.close()
            f = wrapped
        except Exception:
            # we catch any errors that may have occurred
            # because that is consistent with the lower-level
            # functionality of the C engine (pd.read_csv), so
            # leave the file handler as is then
            pass
    return f, handles

class _BytesZipFile(zipfile.ZipFile, BytesIO):  # type: ignore
    """"""
    Wrapper for standard library class ZipFile and allow the returned file-like
    handle to accept byte strings via `write` method.
    BytesIO provides attributes of file-like object and ZipFile.writestr writes
    bytes strings into a member of the archive.
    """"""
    # GH 17778
    def __init__(
        self,
        file: FilePathOrBuffer,
        mode: str,
        archive_name: Optional[str] = None,
        **kwargs,
    ):
        if mode in [""wb"", ""rb""]:
            mode = mode.replace(""b"", """")
        self.archive_name = archive_name
        super().__init__(file, mode, zipfile.ZIP_DEFLATED, **kwargs)
    def write(self, data):
        archive_name = self.filename
        if self.archive_name is not None:
            archive_name = self.archive_name
        super().writestr(archive_name, data)
    @property
    def closed(self):
        return self.fp is None

class _MMapWrapper(abc.Iterator):
    """"""
    Wrapper for the Python's mmap class so that it can be properly read in
    by Python's csv.reader class.
    Parameters
    ----------
    f : file object
        File object to be mapped onto memory. Must support the 'fileno'
        method or have an equivalent attribute
    """"""
    def __init__(self, f: IO):
        self.mmap = mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ)
    def __getattr__(self, name: str):
        return getattr(self.mmap, name)
    def __iter__(self) -> ""_MMapWrapper"":
        return self
    def __next__(self) -> str:
        newbytes = self.mmap.readline()
        # readline returns bytes, not str, but Python's CSV reader
        # expects str, so convert the output to str before continuing
        newline = newbytes.decode(""utf-8"")
        # mmap doesn't raise if reading past the allocated
        # data but instead returns an empty string, so raise
        # if that is returned
        if newline == """":
            raise StopIteration",[40]
"""""""Built-in weight initializers.
""""""
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function
import numpy as np
import six
from . import backend as K
from .utils.generic_utils import serialize_keras_object
from .utils.generic_utils import deserialize_keras_object

class Initializer(object):
    """"""Initializer base class: all initializers inherit from this class.
    """"""
    def __call__(self, shape, dtype=None):
        raise NotImplementedError
    def get_config(self):
        return {}
    @classmethod
    def from_config(cls, config):
        if 'dtype' in config:
            # Initializers saved from `tf.keras`
            # may contain an unused `dtype` argument.
            config.pop('dtype')
        return cls(**config)

class Zeros(Initializer):
    """"""Initializer that generates tensors initialized to 0.
    """"""
    def __call__(self, shape, dtype=None):
        return K.constant(0, shape=shape, dtype=dtype)

class Ones(Initializer):
    """"""Initializer that generates tensors initialized to 1.
    """"""
    def __call__(self, shape, dtype=None):
        return K.constant(1, shape=shape, dtype=dtype)

class Constant(Initializer):
    """"""Initializer that generates tensors initialized to a constant value.
    # Arguments
        value: float; the value of the generator tensors.
    """"""
    def __init__(self, value=0):
        self.value = value
    def __call__(self, shape, dtype=None):
        return K.constant(self.value, shape=shape, dtype=dtype)
    def get_config(self):
        return {'value': self.value}

class RandomNormal(Initializer):
    """"""Initializer that generates tensors with a normal distribution.
    # Arguments
        mean: a python scalar or a scalar tensor. Mean of the random values
          to generate.
        stddev: a python scalar or a scalar tensor. Standard deviation of the
          random values to generate.
        seed: A Python integer. Used to seed the random generator.
    """"""
    def __init__(self, mean=0., stddev=0.05, seed=None):
        self.mean = mean
        self.stddev = stddev
        self.seed = seed
    def __call__(self, shape, dtype=None):
        return K.random_normal(shape, self.mean, self.stddev,
                               dtype=dtype, seed=self.seed)
    def get_config(self):
        return {
            'mean': self.mean,
            'stddev': self.stddev,
            'seed': self.seed
        }

class RandomUniform(Initializer):
    """"""Initializer that generates tensors with a uniform distribution.
    # Arguments
        minval: A python scalar or a scalar tensor. Lower bound of the range
          of random values to generate.
        maxval: A python scalar or a scalar tensor. Upper bound of the range
          of random values to generate.  Defaults to 1 for float types.
        seed: A Python integer. Used to seed the random generator.
    """"""
    def __init__(self, minval=-0.05, maxval=0.05, seed=None):
        self.minval = minval
        self.maxval = maxval
        self.seed = seed
    def __call__(self, shape, dtype=None):
        return K.random_uniform(shape, self.minval, self.maxval,
                                dtype=dtype, seed=self.seed)
    def get_config(self):
        return {
            'minval': self.minval,
            'maxval': self.maxval,
            'seed': self.seed,
        }

class TruncatedNormal(Initializer):
    """"""Initializer that generates a truncated normal distribution.
    These values are similar to values from a `RandomNormal`
    except that values more than two standard deviations from the mean
    are discarded and redrawn. This is the recommended initializer for","[82, 83, 110, 111]"
"        return default

class Task(object):
    def __init__(self, task_id, status, deps, resources=None, priority=0, family='', module=None,
                 params=None, disable_failures=None, disable_window=None, disable_hard_timeout=None):
        self.id = task_id
        self.stakeholders = set()  # workers ids that are somehow related to this task (i.e. don't prune while any of these workers are still active)
        self.workers = set()  # workers ids that can perform task - task is 'BROKEN' if none of these workers are active
        if deps is None:
            self.deps = set()
        else:
            self.deps = set(deps)
        self.status = status  # PENDING, RUNNING, FAILED or DONE
        self.time = time.time()  # Timestamp when task was first added
        self.retry = None
        self.remove = None
        self.worker_running = None  # the worker id that is currently running the task or None
        self.time_running = None  # Timestamp when picked up by worker
        self.expl = None
        self.priority = priority
        self.resources = _get_default(resources, {})
        self.family = family
        self.module = module
        self.params = _get_default(params, {})
        self.disable_failures = disable_failures
        self.disable_hard_timeout = disable_hard_timeout
        self.failures = Failures(disable_window)
        self.scheduler_disable_time = None
    def __repr__(self):
        return ""Task(%r)"" % vars(self)
    def add_failure(self):
        self.failures.add_failure()
    def has_excessive_failures(self):
        excessive_failures = False
        if (self.failures.first_failure_time is not None and
                self.disable_hard_timeout):
            if (time.time() >= self.failures.first_failure_time +
                    self.disable_hard_timeout):
                excessive_failures = True
        if self.failures.num_failures() >= self.disable_failures:
            excessive_failures = True
        return excessive_failures
    def can_disable(self):
        return (self.disable_failures is not None or
                self.disable_hard_timeout is not None)

class Worker(object):
    """"""
    Structure for tracking worker activity and keeping their references.
    """"""
    def __init__(self, worker_id, last_active=None):
        self.id = worker_id
        self.reference = None  # reference to the worker in the real world. (Currently a dict containing just the host)
        self.last_active = last_active  # seconds since epoch
        self.started = time.time()  # seconds since epoch
        self.tasks = set()  # task objects
        self.info = {}
    def add_info(self, info):
        self.info.update(info)
    def update(self, worker_reference):
        if worker_reference:
            self.reference = worker_reference
        self.last_active = time.time()
    def prune(self, config):
        # Delete workers that haven't said anything for a while (probably killed)
        if self.last_active + config.worker_disconnect_delay < time.time():
            return True
    def get_pending_tasks(self, state):
        """"""
        Get PENDING (and RUNNING) tasks for this worker.
        You have to pass in the state for optimization reasons.
        """"""
        if len(self.tasks) < state.num_pending_tasks():
            return six.moves.filter(lambda task: task.status in [PENDING, RUNNING],
                                    self.tasks)
        else:
            return state.get_pending_tasks()
    def is_trivial_worker(self, state):
        """"""
        If it's not an assistant having only tasks that are without
        requirements.
        We have to pass the state parameter for optimization reasons.
        """"""
        if self.assistant:
            return False
        return all(not task.resources for task in self.get_pending_tasks(state))
    @property
    def assistant(self):
        return self.info.get('assistant', False)
    def __str__(self):
        return self.id

class SimpleTaskState(object):
    """"""
    Keep track of the current state and handle persistance.
    The point of this class is to enable other ways to keep state, eg. by using a database
    These will be implemented by creating an abstract base class that this and other classes
    inherit from.
    """"""
    def __init__(self, state_path):
        self._state_path = state_path
        self._tasks = {}  # map from id to a Task object
        self._status_tasks = collections.defaultdict(dict)",[65]
"    @property
    def value(self):
        """"""
        The value for this Parameter.
        This refers to any value defined by a default, a config option, or
        a global value.
        :raises MissingParameterException: if a value is not set.
        :return: the parsed value.
        """"""
        value = self._get_value()
        if value == _no_value:
            raise MissingParameterException(""No default specified"")
        else:
            return value
    def has_task_value(self, task_name, param_name):
        return self._get_value(task_name, param_name) != _no_value
    def task_value(self, task_name, param_name):
        value = self._get_value(task_name, param_name)
        if value == _no_value:
            raise MissingParameterException(""No default specified"")
        else:
            return value
    def set_global(self, value):
        """"""
        Set the global value of this Parameter.
        :param value: the new global value.
        """"""
        self.__global = value
    def reset_global(self):
        self.__global = _no_value
    def parse(self, x):
        """"""
        Parse an individual value from the input.
        The default implementation is an identify (it returns ``x``), but subclasses should override
        this method for specialized parsing. This method is called by :py:meth:`parse_from_input`
        if ``x`` exists. If this Parameter was specified with ``is_list=True``, then ``parse`` is
        called once for each item in the list.
        :param str x: the value to parse.
        :return: the parsed value.
        """"""
        return x  # default impl
    def serialize(self, x):  # opposite of parse
        """"""
        Opposite of :py:meth:`parse`.
        Converts the value ``x`` to a string.
        :param x: the value to serialize.
        """"""
        if self.is_list:
            return [str(v) for v in x]
        return str(x)
    def parse_from_input(self, param_name, x):
        """"""
        Parses the parameter value from input ``x``, handling defaults and is_list.
        :param param_name: the name of the parameter. This is used for the message in
                           ``MissingParameterException``.
        :param x: the input value to parse.
        :raises MissingParameterException: if x is false-y and no default is specified.
        """"""
        if not x:
            if self.has_value:
                return self.value
            elif self.is_bool:
                return False
            elif self.is_list:
                return []
            else:
                raise MissingParameterException(""No value for '%s' (%s) submitted and no default value has been assigned."" %
                                                (param_name, ""--"" + param_name.replace('_', '-')))
        elif self.is_list:
            return tuple(self.parse(p) for p in x)
        else:
            return self.parse(x)
    def serialize_to_input(self, x):
        if self.is_list:
            return tuple(self.serialize(p) for p in x)
        else:
            return self.serialize(x)
    def parser_dest(self, param_name, task_name, glob=False, is_without_section=False):
        if self.is_global or is_without_section:
            if glob:
                return param_name
            else:
                return None
        else:
            if glob:
                return task_name + '_' + param_name
            else:
                return param_name
    def add_to_cmdline_parser(self, parser, param_name, task_name, optparse=False, glob=False, is_without_section=False):
        dest = self.parser_dest(param_name, task_name, glob, is_without_section=is_without_section)
        if not dest:
            return
        flag = '--' + dest.replace('_', '-')
        description = []
        description.append('%s.%s' % (task_name, param_name))
        if glob:
            description.append('for all instances of class %s' % task_name)
        elif self.description:
            description.append(self.description)
        if self.has_value:
            description.append("" [default: %s]"" % (self.value,))
        if self.is_list:
            action = ""append""
        elif self.is_bool:
            action = ""store_true""
        else:
            action = ""store""","[64, 74, 75, 118, 119]"
"        except:
            return False
    else:
        return True

def _supports_unicode(fp):
    try:
        return _is_utf(fp.encoding)
    except AttributeError:
        return False

def _is_ascii(s):
    if isinstance(s, str):
        for c in s:
            if ord(c) > 255:
                return False
        return True
    return _supports_unicode(s)

def _environ_cols_wrapper():  # pragma: no cover
    """"""
    Return a function which gets width and height of console
    (linux,osx,windows,cygwin).
    """"""
    _environ_cols = None
    if IS_WIN:
        _environ_cols = _environ_cols_windows
        if _environ_cols is None:
            _environ_cols = _environ_cols_tput
    if IS_NIX:
        _environ_cols = _environ_cols_linux
    return _environ_cols

def _environ_cols_windows(fp):  # pragma: no cover
    try:
        from ctypes import windll, create_string_buffer
        import struct
        from sys import stdin, stdout
        io_handle = -12  # assume stderr
        if fp == stdin:
            io_handle = -10
        elif fp == stdout:
            io_handle = -11
        h = windll.kernel32.GetStdHandle(io_handle)
        csbi = create_string_buffer(22)
        res = windll.kernel32.GetConsoleScreenBufferInfo(h, csbi)
        if res:
            (_bufx, _bufy, _curx, _cury, _wattr, left, _top, right, _bottom,
             _maxx, _maxy) = struct.unpack(""hhhhHhhhhhh"", csbi.raw)
            # nlines = bottom - top + 1
            return right - left  # +1
    except:
        pass
    return None

def _environ_cols_tput(*_):  # pragma: no cover
    """"""cygwin xterm (windows)""""""
    try:
        import shlex
        cols = int(subprocess.check_call(shlex.split('tput cols')))
        # rows = int(subprocess.check_call(shlex.split('tput lines')))
        return cols
    except:
        pass
    return None

def _environ_cols_linux(fp):  # pragma: no cover
    try:
        from termios import TIOCGWINSZ
        from fcntl import ioctl
        from array import array
    except ImportError:
        return None
    else:
        try:
            return array('h', ioctl(fp, TIOCGWINSZ, '\0' * 8))[1]
        except:
            try:
                return int(os.environ[""COLUMNS""]) - 1
            except KeyError:
                return None

def _term_move_up():  # pragma: no cover
    return '' if (os.name == 'nt') and (colorama is None) else '\x1b[A'

try:
    # TODO consider using wcswidth third-party package for 0-width characters
    from unicodedata import east_asian_width
except ImportError:
    _text_width = len
else:
    def _text_width(s):
        return sum(
            2 if east_asian_width(ch) in 'FW' else 1 for ch in _unicode(s))

def disp_len(data):
    """"""
    Returns the real on-screen length of a string which may contain
    ANSI control codes and wide chars.
    """"""
    return _text_width(RE_ANSI.sub('', data))

def disp_trim(data, length):
    """"""
    Trim a string which may contain ANSI control characters.
    """"""
    if len(data) == disp_len(data):
        return data[:length]
    while disp_len(data) > length:  # carefully delete one char at a time
        data = data[:-1]
    if RE_ANSI.search(data):  # assume ANSI reset is required
        return data + ""\033[0m""","[124, 125]"
"""""""Base test suite for extension arrays.
These tests are intended for third-party libraries to subclass to validate
that their extension arrays and dtypes satisfy the interface. Moving or
renaming the tests should not be done lightly.
Libraries are expected to implement a few pytest fixtures to provide data
for the tests. The fixtures may be located in either
* The same module as your test class.
* A ``conftest.py`` in the same directory as your test class.
The full list of fixtures may be found in the ``conftest.py`` next to this
file.
.. code-block:: python
   import pytest
   from pandas.tests.extension.base import BaseDtypeTests

   @pytest.fixture
   def dtype():
       return MyDtype()

   class TestMyDtype(BaseDtypeTests):
       pass

Your class ``TestDtype`` will inherit all the tests defined on
``BaseDtypeTests``. pytest's fixture discover will supply your ``dtype``
wherever the test requires it. You're free to implement additional tests.
All the tests in these modules use ``self.assert_frame_equal`` or
``self.assert_series_equal`` for dataframe or series comparisons. By default,
they use the usual ``pandas.testing.assert_frame_equal`` and
``pandas.testing.assert_series_equal``. You can override the checks used
by defining the staticmethods ``assert_frame_equal`` and
``assert_series_equal`` on your base test class.
""""""
from .casting import BaseCastingTests  # noqa
from .constructors import BaseConstructorsTests  # noqa
from .dtype import BaseDtypeTests  # noqa
from .getitem import BaseGetitemTests  # noqa
from .groupby import BaseGroupbyTests  # noqa
from .interface import BaseInterfaceTests  # noqa
from .io import BaseParsingTests  # noqa
from .methods import BaseMethodsTests  # noqa
from .missing import BaseMissingTests  # noqa
from .ops import BaseArithmeticOpsTests, BaseComparisonOpsTests, BaseOpsUtil  # noqa
from .printing import BasePrintingTests  # noqa
from .reduce import (  # noqa
    BaseBooleanReduceTests,
    BaseNoReduceTests,
    BaseNumericReduceTests,
)
from .reshaping import BaseReshapingTests  # noqa
from .setitem import BaseSetitemTests  # noqa",[51]
"    has_none_blocks = False
    dtypes = [None] * len(join_units)
    for i, unit in enumerate(join_units):
        if unit.block is None:
            has_none_blocks = True
        else:
            dtypes[i] = unit.dtype
    upcast_classes = defaultdict(list)
    null_upcast_classes = defaultdict(list)
    for dtype, unit in zip(dtypes, join_units):
        if dtype is None:
            continue
        if is_categorical_dtype(dtype):
            upcast_cls = ""category""
        elif is_datetime64tz_dtype(dtype):
            upcast_cls = ""datetimetz""
        elif issubclass(dtype.type, np.bool_):
            upcast_cls = ""bool""
        elif issubclass(dtype.type, np.object_):
            upcast_cls = ""object""
        elif is_datetime64_dtype(dtype):
            upcast_cls = ""datetime""
        elif is_timedelta64_dtype(dtype):
            upcast_cls = ""timedelta""
        elif is_sparse(dtype):
            upcast_cls = dtype.subtype.name
        elif is_extension_array_dtype(dtype):
            upcast_cls = ""object""
        elif is_float_dtype(dtype) or is_numeric_dtype(dtype):
            upcast_cls = dtype.name
        else:
            upcast_cls = ""float""
        # Null blocks should not influence upcast class selection, unless there
        # are only null blocks, when same upcasting rules must be applied to
        # null upcast classes.
        if unit.is_na:
            null_upcast_classes[upcast_cls].append(dtype)
        else:
            upcast_classes[upcast_cls].append(dtype)
    if not upcast_classes:
        upcast_classes = null_upcast_classes
    # TODO: de-duplicate with maybe_promote?
    # create the result
    if ""object"" in upcast_classes:
        return np.dtype(np.object_), np.nan
    elif ""bool"" in upcast_classes:
        if has_none_blocks:
            return np.dtype(np.object_), np.nan
        else:
            return np.dtype(np.bool_), None
    elif ""category"" in upcast_classes:
        return np.dtype(np.object_), np.nan
    elif ""datetimetz"" in upcast_classes:
        # GH-25014. We use NaT instead of iNaT, since this eventually
        # ends up in DatetimeArray.take, which does not allow iNaT.
        dtype = upcast_classes[""datetimetz""]
        return dtype[0], tslibs.NaT
    elif ""datetime"" in upcast_classes:
        return np.dtype(""M8[ns]""), tslibs.iNaT
    elif ""timedelta"" in upcast_classes:
        return np.dtype(""m8[ns]""), np.timedelta64(""NaT"", ""ns"")
    else:  # pragma
        try:
            g = np.find_common_type(upcast_classes, [])
        except TypeError:
            # At least one is an ExtensionArray
            return np.dtype(np.object_), np.nan
        else:
            if is_float_dtype(g):
                return g, g.type(np.nan)
            elif is_numeric_dtype(g):
                if has_none_blocks:
                    return np.float64, np.nan
                else:
                    return g, None
    msg = ""invalid dtype determination in get_concat_dtype""
    raise AssertionError(msg)

def is_uniform_join_units(join_units):
    """"""
    Check if the join units consist of blocks of uniform type that can
    be concatenated using Block.concat_same_type instead of the generic
    concatenate_join_units (which uses `concat_compat`).
    """"""
    return (
        # all blocks need to have the same type
        all(type(ju.block) is type(join_units[0].block) for ju in join_units)
        and  # noqa
        # no blocks that would get missing values (can lead to type upcasts)
        # unless we're an extension dtype.
        all(not ju.is_na or ju.block.is_extension for ju in join_units)
        and
        # no blocks with indexers (as then the dimensions do not fit)
        all(not ju.indexers for ju in join_units)
        and
        # only use this path when there is something to concatenate
        len(join_units) > 1
    )

def _is_uniform_reindex(join_units) -> bool:
    return (
        # TODO: should this be ju.block._can_hold_na?
        all(ju.block and ju.block.is_extension for ju in join_units)
        and len({ju.block.dtype.name for ju in join_units}) == 1
    )

def _trim_join_unit(join_unit, length):
    """"""
    Reduce join_unit's shape along item axis to length.
    Extra items that didn't fit are returned as a separate block.
    """"""
    if 0 not in join_unit.indexers:
        extra_indexers = join_unit.indexers
",[64]
"    func : arithmetic or comparison operator
    str_rep : str or None, default None
    axis : {None, 0, 1, ""index"", ""columns""}
    Returns
    -------
    DataFrame
    """"""
    # Note: we use iloc to access columns for compat with cases
    #       with non-unique columns.
    import pandas.core.computation.expressions as expressions
    right = lib.item_from_zerodim(right)
    if lib.is_scalar(right) or np.ndim(right) == 0:
        def column_op(a, b):
            return {i: func(a.iloc[:, i], b) for i in range(len(a.columns))}
    elif isinstance(right, ABCDataFrame):
        assert right._indexed_same(left)
        def column_op(a, b):
            return {i: func(a.iloc[:, i], b.iloc[:, i]) for i in range(len(a.columns))}
    elif isinstance(right, ABCSeries) and axis == ""columns"":
        # We only get here if called via left._combine_match_columns,
        # in which case we specifically want to operate row-by-row
        assert right.index.equals(left.columns)
        def column_op(a, b):
            return {i: func(a.iloc[:, i], b.iloc[i]) for i in range(len(a.columns))}
    elif isinstance(right, ABCSeries):
        assert right.index.equals(left.index)  # Handle other cases later
        def column_op(a, b):
            return {i: func(a.iloc[:, i], b) for i in range(len(a.columns))}
    else:
        # Remaining cases have less-obvious dispatch rules
        raise NotImplementedError(right)
    new_data = expressions.evaluate(column_op, str_rep, left, right)
    return new_data

def dispatch_to_extension_op(
    op,
    left: Union[ABCExtensionArray, np.ndarray],
    right: Any,
    keep_null_freq: bool = False,
):
    """"""
    Assume that left or right is a Series backed by an ExtensionArray,
    apply the operator defined by op.
    Parameters
    ----------
    op : binary operator
    left : ExtensionArray or np.ndarray
    right : object
    keep_null_freq : bool, default False
        Whether to re-raise a NullFrequencyError unchanged, as opposed to
        catching and raising TypeError.
    Returns
    -------
    ExtensionArray or np.ndarray
        2-tuple of these if op is divmod or rdivmod
    """"""
    # NB: left and right should already be unboxed, so neither should be
    #  a Series or Index.
    if left.dtype.kind in ""mM"" and isinstance(left, np.ndarray):
        # We need to cast datetime64 and timedelta64 ndarrays to
        #  DatetimeArray/TimedeltaArray.  But we avoid wrapping others in
        #  PandasArray as that behaves poorly with e.g. IntegerArray.
        left = array(left)
    # The op calls will raise TypeError if the op is not defined
    # on the ExtensionArray
    try:
        res_values = op(left, right)
    except NullFrequencyError:
        # DatetimeIndex and TimedeltaIndex with freq == None raise ValueError
        # on add/sub of integers (or int-like).  We re-raise as a TypeError.
        if keep_null_freq:
            # TODO: remove keep_null_freq after Timestamp+int deprecation
            #  GH#22535 is enforced
            raise
        raise TypeError(
            ""incompatible type for a datetime/timedelta ""
            ""operation [{name}]"".format(name=op.__name__)
        )
    return res_values

# -----------------------------------------------------------------------------
# Series

def _align_method_SERIES(left, right, align_asobject=False):
    """""" align lhs and rhs Series """"""
    # ToDo: Different from _align_method_FRAME, list, tuple and ndarray
    # are not coerced here
    # because Series has inconsistencies described in #13637
    if isinstance(right, ABCSeries):
        # avoid repeated alignment
        if not left.index.equals(right.index):
            if align_asobject:
                # to keep original value's dtype for bool ops
                left = left.astype(object)
                right = right.astype(object)
            left, right = left.align(right, copy=False)
    return left, right

def _construct_result(left, result, index, name, dtype=None):
    """"""
    If the raw op result has a non-None name (e.g. it is an Index object) and
    the name argument is None, then passing name to the constructor will","[29, 30]"
"    # string dtypes must be come to this path for NumPy 1.7.1 compat
    if is_string_dtype(left) or is_string_dtype(right):
        if not strict_nan:
            # isna considers NaN and None to be equivalent.
            return lib.array_equivalent_object(
                ensure_object(left.ravel()), ensure_object(right.ravel())
            )
        for left_value, right_value in zip(left, right):
            if left_value is NaT and right_value is not NaT:
                return False
            elif isinstance(left_value, float) and np.isnan(left_value):
                if not isinstance(right_value, float) or not np.isnan(right_value):
                    return False
            else:
                if np.any(left_value != right_value):
                    return False
        return True
    # NaNs can occur in float and complex arrays.
    if is_float_dtype(left) or is_complex_dtype(left):
        # empty
        if not (np.prod(left.shape) and np.prod(right.shape)):
            return True
        return ((left == right) | (isna(left) & isna(right))).all()
    # numpy will will not allow this type of datetimelike vs integer comparison
    elif is_datetimelike_v_numeric(left, right):
        return False
    # M8/m8
    elif needs_i8_conversion(left) and needs_i8_conversion(right):
        if not is_dtype_equal(left.dtype, right.dtype):
            return False
        left = left.view(""i8"")
        right = right.view(""i8"")
    # if we have structured dtypes, compare first
    if left.dtype.type is np.void or right.dtype.type is np.void:
        if left.dtype != right.dtype:
            return False
    return np.array_equal(left, right)

def _infer_fill_value(val):
    """"""
    infer the fill value for the nan/NaT from the provided
    scalar/ndarray/list-like if we are a NaT, return the correct dtyped
    element to provide proper block construction
    """"""
    if not is_list_like(val):
        val = [val]
    val = np.array(val, copy=False)
    if is_datetimelike(val):
        return np.array(""NaT"", dtype=val.dtype)
    elif is_object_dtype(val.dtype):
        dtype = lib.infer_dtype(ensure_object(val), skipna=False)
        if dtype in [""datetime"", ""datetime64""]:
            return np.array(""NaT"", dtype=_NS_DTYPE)
        elif dtype in [""timedelta"", ""timedelta64""]:
            return np.array(""NaT"", dtype=_TD_DTYPE)
    return np.nan

def _maybe_fill(arr, fill_value=np.nan):
    """"""
    if we have a compatible fill_value and arr dtype, then fill
    """"""
    if _isna_compat(arr, fill_value):
        arr.fill(fill_value)
    return arr

def na_value_for_dtype(dtype, compat=True):
    """"""
    Return a dtype compat na value
    Parameters
    ----------
    dtype : string / dtype
    compat : boolean, default True
    Returns
    -------
    np.dtype or a pandas dtype
    Examples
    --------
    >>> na_value_for_dtype(np.dtype('int64'))
    0
    >>> na_value_for_dtype(np.dtype('int64'), compat=False)
    nan
    >>> na_value_for_dtype(np.dtype('float64'))
    nan
    >>> na_value_for_dtype(np.dtype('bool'))
    False
    >>> na_value_for_dtype(np.dtype('datetime64[ns]'))
    NaT
    """"""
    dtype = pandas_dtype(dtype)
    if is_extension_array_dtype(dtype):
        return dtype.na_value
    if (
        is_datetime64_dtype(dtype)
        or is_datetime64tz_dtype(dtype)
        or is_timedelta64_dtype(dtype)
        or is_period_dtype(dtype)
    ):
        return NaT
    elif is_float_dtype(dtype):
        return np.nan
    elif is_integer_dtype(dtype):
        if compat:
            return 0
        return np.nan
    elif is_bool_dtype(dtype):
        return False
    return np.nan
","[17, 18]"
"    n_frags = segment_run_entry[1]
    fragment_run_entry_table = boot_info['fragments'][0]['fragments']
    first_frag_number = fragment_run_entry_table[0]['first']
    for (i, frag_number) in zip(range(1, n_frags+1), itertools.count(first_frag_number)):
        res.append((1, frag_number))
    return res

def write_flv_header(stream, metadata):
    """"""Writes the FLV header and the metadata to stream""""""
    # FLV header
    stream.write(b'FLV\x01')
    stream.write(b'\x05')
    stream.write(b'\x00\x00\x00\x09')
    # FLV File body
    stream.write(b'\x00\x00\x00\x00')
    # FLVTAG
    # Script data
    stream.write(b'\x12')
    # Size of the metadata with 3 bytes
    stream.write(pack('!L', len(metadata))[1:])
    stream.write(b'\x00\x00\x00\x00\x00\x00\x00')
    stream.write(metadata)
    # Magic numbers extracted from the output files produced by AdobeHDS.php
    #(https://github.com/K-S-V/Scripts)
    stream.write(b'\x00\x00\x01\x73')

def _add_ns(prop):
    return '{http://ns.adobe.com/f4m/1.0}%s' % prop

class HttpQuietDownloader(HttpFD):
    def to_screen(self, *args, **kargs):
        pass

class F4mFD(FileDownloader):
    """"""
    A downloader for f4m manifests or AdobeHDS.
    """"""
    def real_download(self, filename, info_dict):
        man_url = info_dict['url']
        self.to_screen('[download] Downloading f4m manifest')
        manifest = self.ydl.urlopen(man_url).read()
        self.report_destination(filename)
        http_dl = HttpQuietDownloader(self.ydl, {'continuedl': True, 'quiet': True, 'noprogress': True})
        doc = etree.fromstring(manifest)
        formats = [(int(f.attrib.get('bitrate', -1)), f) for f in doc.findall(_add_ns('media'))]
        formats = sorted(formats, key=lambda f: f[0])
        rate, media = formats[-1]
        base_url = compat_urlparse.urljoin(man_url, media.attrib['url'])
        bootstrap = base64.b64decode(doc.find(_add_ns('bootstrapInfo')).text)
        metadata = base64.b64decode(media.find(_add_ns('metadata')).text)
        boot_info = read_bootstrap_info(bootstrap)
        fragments_list = build_fragments_list(boot_info)
        total_frags = len(fragments_list)
        tmpfilename = self.temp_name(filename)
        (dest_stream, tmpfilename) = sanitize_open(tmpfilename, 'wb')
        write_flv_header(dest_stream, metadata)
        # This dict stores the download progress, it's updated by the progress
        # hook
        state = {
            'downloaded_bytes': 0,
            'frag_counter': 0,
        }
        start = time.time()
        def frag_progress_hook(status):
            frag_total_bytes = status.get('total_bytes', 0)
            estimated_size = (state['downloaded_bytes'] +
                (total_frags - state['frag_counter']) * frag_total_bytes)
            if status['status'] == 'finished':
                state['downloaded_bytes'] += frag_total_bytes
                state['frag_counter'] += 1
                progress = self.calc_percent(state['frag_counter'], total_frags)
                byte_counter = state['downloaded_bytes']
            else:
                frag_downloaded_bytes = status['downloaded_bytes']
                byte_counter = state['downloaded_bytes'] + frag_downloaded_bytes
                frag_progress = self.calc_percent(frag_downloaded_bytes,
                    frag_total_bytes)
                progress = self.calc_percent(state['frag_counter'], total_frags)
                progress += frag_progress / float(total_frags)
            eta = self.calc_eta(start, time.time(), estimated_size, byte_counter)
            self.report_progress(progress, format_bytes(estimated_size),
                status.get('speed'), eta)
        http_dl.add_progress_hook(frag_progress_hook)
        frags_filenames = []
        for (seg_i, frag_i) in fragments_list:
            name = 'Seg%d-Frag%d' % (seg_i, frag_i)
            url = base_url + name
            frag_filename = '%s-%s' % (tmpfilename, name)
            success = http_dl.download(frag_filename, {'url': url})
            if not success:
                return False
            with open(frag_filename, 'rb') as down:
                down_data = down.read()
                reader = FlvReader(down_data)
                while True:
                    _, box_type, box_data = reader.read_box_info()
                    if box_type == b'mdat':
                        dest_stream.write(box_data)
                        break
            frags_filenames.append(frag_filename)
        self.report_finish(format_bytes(state['downloaded_bytes']), time.time() - start)
        self.try_rename(tmpfilename, filename)
        for frag_file in frags_filenames:
            os.remove(frag_file)
        fsize = os.path.getsize(encodeFilename(filename))
        self._hook_progress({
            'downloaded_bytes': fsize,
            'total_bytes': fsize,
            'filename': filename,
            'status': 'finished',
        })
",[20]
"""""""
This module provides some useful functions for working with
scrapy.http.Response objects
""""""
import os
import re
import weakref
import webbrowser
import tempfile
from twisted.web import http
from scrapy.utils.python import to_bytes, to_native_str
from w3lib import html
from scrapy.utils.decorators import deprecated

@deprecated
def body_or_str(*a, **kw):
    from scrapy.utils.iterators import _body_or_str
    return _body_or_str(*a, **kw)

_baseurl_cache = weakref.WeakKeyDictionary()
def get_base_url(response):
    """"""Return the base url of the given response, joined with the response url""""""
    if response not in _baseurl_cache:
        text = response.text[0:4096]
        _baseurl_cache[response] = html.get_base_url(text, response.url,
            response.encoding)
    return _baseurl_cache[response]

_noscript_re = re.compile(u'<noscript>.*?</noscript>', re.IGNORECASE | re.DOTALL)
_script_re = re.compile(u'<script.*?>.*?</script>', re.IGNORECASE | re.DOTALL)
_metaref_cache = weakref.WeakKeyDictionary()
def get_meta_refresh(response):
    """"""Parse the http-equiv refrsh parameter from the given response""""""
    if response not in _metaref_cache:
        text = response.text[0:4096]
        text = _noscript_re.sub(u'', text)
        text = _script_re.sub(u'', text)
        _metaref_cache[response] = html.get_meta_refresh(text, response.url,
            response.encoding)
    return _metaref_cache[response]

def response_status_message(status):
    """"""Return status code plus status text descriptive message
    >>> response_status_message(200)
    '200 OK'
    >>> response_status_message(404)
    '404 Not Found'
    """"""
    return '%s %s' % (status, to_native_str(http.RESPONSES.get(int(status))))

def response_httprepr(response):
    """"""Return raw HTTP representation (as bytes) of the given response. This
    is provided only for reference, since it's not the exact stream of bytes
    that was received (that's not exposed by Twisted).
    """"""
    s = b""HTTP/1.1 "" + to_bytes(str(response.status)) + b"" "" + \
        to_bytes(http.RESPONSES.get(response.status, b'')) + b""\r\n""
    if response.headers:
        s += response.headers.to_string() + b""\r\n""
    s += b""\r\n""
    s += response.body
    return s

def open_in_browser(response, _openfunc=webbrowser.open):
    """"""Open the given response in a local web browser, populating the <base>
    tag for external links to work
    """"""
    from scrapy.http import HtmlResponse, TextResponse
    # XXX: this implementation is a bit dirty and could be improved
    body = response.body
    if isinstance(response, HtmlResponse):
        if b'<base' not in body:
            repl = '<head><base href=""%s"">' % response.url
            body = body.replace(b'<head>', to_bytes(repl))
        ext = '.html'
    elif isinstance(response, TextResponse):
        ext = '.txt'
    else:
        raise TypeError(""Unsupported response type: %s"" %
                        response.__class__.__name__)
    fd, fname = tempfile.mkstemp(ext)
    os.write(fd, body)
    os.close(fd)
    return _openfunc(""file://%s"" % fname)","[49, 50, 51, 52, 53, 54, 56]"
"            (?P<strval>(?![0-9.])[a-z0-9A-Z]*)
        )
        \s*$
        ''' % '|'.join(map(re.escape, COMPARISON_OPERATORS.keys())))
    m = operator_rex.search(filter_part)
    if m:
        op = COMPARISON_OPERATORS[m.group('op')]
        actual_value = dct.get(m.group('key'))
        if (m.group('strval') is not None or
            # If the original field is a string and matching comparisonvalue is
            # a number we should respect the origin of the original field
            # and process comparison value as a string (see
            # https://github.com/rg3/youtube-dl/issues/11082).
            actual_value is not None and m.group('intval') is not None and
                isinstance(actual_value, compat_str)):
            if m.group('op') not in ('=', '!='):
                raise ValueError(
                    'Operator %s does not support string values!' % m.group('op'))
            comparison_value = m.group('strval') or m.group('intval')
        else:
            try:
                comparison_value = int(m.group('intval'))
            except ValueError:
                comparison_value = parse_filesize(m.group('intval'))
                if comparison_value is None:
                    comparison_value = parse_filesize(m.group('intval') + 'B')
                if comparison_value is None:
                    raise ValueError(
                        'Invalid integer value %r in filter part %r' % (
                            m.group('intval'), filter_part))
        if actual_value is None:
            return m.group('none_inclusive')
        return op(actual_value, comparison_value)
    UNARY_OPERATORS = {
        '': lambda v: v is not None,
        '!': lambda v: v is None,
    }
    operator_rex = re.compile(r'''(?x)\s*
        (?P<op>%s)\s*(?P<key>[a-z_]+)
        \s*$
        ''' % '|'.join(map(re.escape, UNARY_OPERATORS.keys())))
    m = operator_rex.search(filter_part)
    if m:
        op = UNARY_OPERATORS[m.group('op')]
        actual_value = dct.get(m.group('key'))
        return op(actual_value)
    raise ValueError('Invalid filter part %r' % filter_part)

def match_str(filter_str, dct):
    """""" Filter a dictionary with a simple string syntax. Returns True (=passes filter) or false """"""
    return all(
        _match_one(filter_part, dct) for filter_part in filter_str.split('&'))

def match_filter_func(filter_str):
    def _match_func(info_dict):
        if match_str(filter_str, info_dict):
            return None
        else:
            video_title = info_dict.get('title', info_dict.get('id', 'video'))
            return '%s does not pass filter %s, skipping ..' % (video_title, filter_str)
    return _match_func

def parse_dfxp_time_expr(time_expr):
    if not time_expr:
        return
    mobj = re.match(r'^(?P<time_offset>\d+(?:\.\d+)?)s?$', time_expr)
    if mobj:
        return float(mobj.group('time_offset'))
    mobj = re.match(r'^(\d+):(\d\d):(\d\d(?:(?:\.|:)\d+)?)$', time_expr)
    if mobj:
        return 3600 * int(mobj.group(1)) + 60 * int(mobj.group(2)) + float(mobj.group(3).replace(':', '.'))

def srt_subtitles_timecode(seconds):
    return '%02d:%02d:%02d,%03d' % (seconds / 3600, (seconds % 3600) / 60, seconds % 60, (seconds % 1) * 1000)

def dfxp2srt(dfxp_data):
    _x = functools.partial(xpath_with_ns, ns_map={
        'ttml': 'http://www.w3.org/ns/ttml',
        'ttaf1': 'http://www.w3.org/2006/10/ttaf1',
        'ttaf1_0604': 'http://www.w3.org/2006/04/ttaf1',
    })
    class TTMLPElementParser(object):
        out = ''
        def start(self, tag, attrib):
            if tag in (_x('ttml:br'), _x('ttaf1:br'), 'br'):
                self.out += '\n'
        def end(self, tag):
            pass
        def data(self, data):
            self.out += data
        def close(self):
            return self.out.strip()
    def parse_node(node):
        target = TTMLPElementParser()
        parser = xml.etree.ElementTree.XMLParser(target=target)
        parser.feed(xml.etree.ElementTree.tostring(node))
        return parser.close()
    dfxp = compat_etree_fromstring(dfxp_data.encode('utf-8'))
    out = []
    paras = dfxp.findall(_x('.//ttml:p')) or dfxp.findall(_x('.//ttaf1:p')) or dfxp.findall(_x('.//ttaf1_0604:p')) or dfxp.findall('.//p')
    if not paras:
        raise ValueError('Invalid dfxp/TTML subtitle')
    for para, index in zip(paras, itertools.count(1)):
        begin_time = parse_dfxp_time_expr(para.attrib.get('begin'))
        end_time = parse_dfxp_time_expr(para.attrib.get('end'))
        dur = parse_dfxp_time_expr(para.attrib.get('dur'))
        if begin_time is None:
            continue","[8, 18]"
"    else:
        return default

class Task(object):
    def __init__(self, task_id, status, deps, resources=None, priority=0, family='', module=None,
                 params=None, disable_failures=None, disable_window=None, disable_hard_timeout=None):
        self.id = task_id
        self.stakeholders = set()  # workers ids that are somehow related to this task (i.e. don't prune while any of these workers are still active)
        self.workers = set()  # workers ids that can perform task - task is 'BROKEN' if none of these workers are active
        if deps is None:
            self.deps = set()
        else:
            self.deps = set(deps)
        self.status = status  # PENDING, RUNNING, FAILED or DONE
        self.time = time.time()  # Timestamp when task was first added
        self.retry = None
        self.remove = None
        self.worker_running = None  # the worker id that is currently running the task or None
        self.time_running = None  # Timestamp when picked up by worker
        self.expl = None
        self.priority = priority
        self.resources = _get_default(resources, {})
        self.family = family
        self.module = module
        self.params = _get_default(params, {})
        self.disable_failures = disable_failures
        self.disable_hard_timeout = disable_hard_timeout
        self.failures = Failures(disable_window)
        self.scheduler_disable_time = None
    def __repr__(self):
        return ""Task(%r)"" % vars(self)
    def add_failure(self):
        self.failures.add_failure()
    def has_excessive_failures(self):
        excessive_failures = False
        if (self.failures.first_failure_time is not None and
                self.disable_hard_timeout):
            if (time.time() >= self.failures.first_failure_time +
                    self.disable_hard_timeout):
                excessive_failures = True
        if self.failures.num_failures() >= self.disable_failures:
            excessive_failures = True
        return excessive_failures
    def can_disable(self):
        return (self.disable_failures is not None or
                self.disable_hard_timeout is not None)

class Worker(object):
    """"""
    Structure for tracking worker activity and keeping their references.
    """"""
    def __init__(self, worker_id, last_active=None):
        self.id = worker_id
        self.reference = None  # reference to the worker in the real world. (Currently a dict containing just the host)
        self.last_active = last_active  # seconds since epoch
        self.started = time.time()  # seconds since epoch
        self.tasks = set()  # task objects
        self.info = {}
    def add_info(self, info):
        self.info.update(info)
    def update(self, worker_reference):
        if worker_reference:
            self.reference = worker_reference
        self.last_active = time.time()
    def prune(self, config):
        # Delete workers that haven't said anything for a while (probably killed)
        if self.last_active + config.worker_disconnect_delay < time.time():
            return True
    def get_pending_tasks(self):
        return six.moves.filter(lambda task: task.status in [PENDING, RUNNING],
                                self.tasks)
    def is_trivial_worker(self):
        """"""
        If it's not an assistant having only tasks that are without
        requirements
        """"""
        if self.assistant:
            return False
        return all(not task.resources for task in self.get_pending_tasks())
    @property
    def assistant(self):
        return self.info.get('assistant', False)
    def __str__(self):
        return self.id

class SimpleTaskState(object):
    """"""
    Keep track of the current state and handle persistance.
    The point of this class is to enable other ways to keep state, eg. by using a database
    These will be implemented by creating an abstract base class that this and other classes
    inherit from.
    """"""
    def __init__(self, state_path):
        self._state_path = state_path
        self._tasks = {}  # map from id to a Task object
        self._status_tasks = collections.defaultdict(dict)
        self._active_workers = {}  # map from id to a Worker object
    def dump(self):
        state = (self._tasks, self._active_workers)
        try:
            with open(self._state_path, 'wb') as fobj:
                pickle.dump(state, fobj)
        except IOError:
            logger.warning(""Failed saving scheduler state"", exc_info=1)",[63]
"
def unsmuggle_url(smug_url, default=None):
    if not '#__youtubedl_smuggle' in smug_url:
        return smug_url, default
    url, _, sdata = smug_url.rpartition(u'#')
    jsond = compat_parse_qs(sdata)[u'__youtubedl_smuggle'][0]
    data = json.loads(jsond)
    return url, data

def format_bytes(bytes):
    if bytes is None:
        return u'N/A'
    if type(bytes) is str:
        bytes = float(bytes)
    if bytes == 0.0:
        exponent = 0
    else:
        exponent = int(math.log(bytes, 1024.0))
    suffix = [u'B', u'KiB', u'MiB', u'GiB', u'TiB', u'PiB', u'EiB', u'ZiB', u'YiB'][exponent]
    converted = float(bytes) / float(1024 ** exponent)
    return u'%.2f%s' % (converted, suffix)

def str_to_int(int_str):
    int_str = re.sub(r'[,\.]', u'', int_str)
    return int(int_str)

def get_term_width():
    columns = os.environ.get('COLUMNS', None)
    if columns:
        return int(columns)
    try:
        sp = subprocess.Popen(
            ['stty', 'size'],
            stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        out, err = sp.communicate()
        return int(out.split()[1])
    except:
        pass
    return None

def month_by_name(name):
    """""" Return the number of a month by (locale-independently) English name """"""
    ENGLISH_NAMES = [
        u'January', u'February', u'March', u'April', u'May', u'June',
        u'July', u'August', u'September', u'October', u'November', u'December']
    try:
        return ENGLISH_NAMES.index(name) + 1
    except ValueError:
        return None

def fix_xml_all_ampersand(xml_str):
    """"""Replace all the '&' by '&amp;' in XML""""""
    return xml_str.replace(u'&', u'&amp;')

def setproctitle(title):
    assert isinstance(title, compat_str)
    try:
        libc = ctypes.cdll.LoadLibrary(""libc.so.6"")
    except OSError:
        return
    title = title
    buf = ctypes.create_string_buffer(len(title) + 1)
    buf.value = title.encode('utf-8')
    try:
        libc.prctl(15, ctypes.byref(buf), 0, 0, 0)
    except AttributeError:
        return  # Strange libc, just skip this

def remove_start(s, start):
    if s.startswith(start):
        return s[len(start):]
    return s

def url_basename(url):
    path = compat_urlparse.urlparse(url).path
    return path.strip(u'/').split(u'/')[-1]

class HEADRequest(compat_urllib_request.Request):
    def get_method(self):
        return ""HEAD""

def int_or_none(v):
    return v if v is None else int(v)

def parse_duration(s):
    if s is None:
        return None
    m = re.match(
        r'(?:(?:(?P<hours>[0-9]+):)?(?P<mins>[0-9]+):)?(?P<secs>[0-9]+)$', s)
    if not m:
        return None
    res = int(m.group('secs'))
    if m.group('mins'):
        res += int(m.group('mins')) * 60
        if m.group('hours'):
            res += int(m.group('hours')) * 60 * 60
    return res

def prepend_extension(filename, ext):
    name, real_ext = os.path.splitext(filename) 
    return u'{0}.{1}{2}'.format(name, ext, real_ext)

def check_executable(exe, args=[]):
    """""" Checks if the given binary is installed somewhere in PATH, and returns its name.
    args can be a list of arguments for a short output (like -version) """"""
    try:
        subprocess.Popen([exe] + args, stdout=subprocess.PIPE, stderr=subprocess.PIPE).communicate()
    except OSError:
        return False","[58, 60]"
""""""" routings for casting """"""
from datetime import datetime, timedelta
import numpy as np
from pandas._libs import lib, tslib, tslibs
from pandas._libs.tslibs import NaT, OutOfBoundsDatetime, Period, iNaT
from pandas.util._validators import validate_bool_kwarg
from .common import (
    _INT64_DTYPE,
    _NS_DTYPE,
    _POSSIBLY_CAST_DTYPES,
    _TD_DTYPE,
    ensure_int8,
    ensure_int16,
    ensure_int32,
    ensure_int64,
    ensure_object,
    ensure_str,
    is_bool,
    is_bool_dtype,
    is_complex,
    is_complex_dtype,
    is_datetime64_dtype,
    is_datetime64_ns_dtype,
    is_datetime64tz_dtype,
    is_datetime_or_timedelta_dtype,
    is_dtype_equal,
    is_extension_array_dtype,
    is_float,
    is_float_dtype,
    is_integer,
    is_integer_dtype,
    is_object_dtype,
    is_scalar,
    is_string_dtype,
    is_timedelta64_dtype,
    is_timedelta64_ns_dtype,
    is_unsigned_integer_dtype,
    pandas_dtype,
)
from .dtypes import DatetimeTZDtype, ExtensionDtype, PeriodDtype
from .generic import (
    ABCDataFrame,
    ABCDatetimeArray,
    ABCDatetimeIndex,
    ABCPeriodArray,
    ABCPeriodIndex,
    ABCSeries,
)
from .inference import is_list_like
from .missing import isna, notna
_int8_max = np.iinfo(np.int8).max
_int16_max = np.iinfo(np.int16).max
_int32_max = np.iinfo(np.int32).max
_int64_max = np.iinfo(np.int64).max

def maybe_convert_platform(values):
    """""" try to do platform conversion, allow ndarray or list here """"""
    if isinstance(values, (list, tuple, range)):
        values = construct_1d_object_array_from_listlike(values)
    if getattr(values, ""dtype"", None) == np.object_:
        if hasattr(values, ""_values""):
            values = values._values
        values = lib.maybe_convert_objects(values)
    return values

def is_nested_object(obj) -> bool:
    """"""
    return a boolean if we have a nested object, e.g. a Series with 1 or
    more Series elements
    This may not be necessarily be performant.
    """"""
    if isinstance(obj, ABCSeries) and is_object_dtype(obj):
        if any(isinstance(v, ABCSeries) for v in obj.values):
            return True
    return False

def maybe_downcast_to_dtype(result, dtype):
    """""" try to cast to the specified dtype (e.g. convert back to bool/int
    or could be an astype of float64->float32
    """"""
    do_round = False
    if is_scalar(result):
        return result
    elif isinstance(result, ABCDataFrame):
        # occurs in pivot_table doctest
        return result
    if isinstance(dtype, str):
        if dtype == ""infer"":
            inferred_type = lib.infer_dtype(ensure_object(result.ravel()), skipna=False)
            if inferred_type == ""boolean"":
                dtype = ""bool""
            elif inferred_type == ""integer"":
                dtype = ""int64""
            elif inferred_type == ""datetime64"":
                dtype = ""datetime64[ns]""
            elif inferred_type == ""timedelta64"":
                dtype = ""timedelta64[ns]""
            # try to upcast here
            elif inferred_type == ""floating"":
                dtype = ""int64""
                if issubclass(result.dtype.type, np.number):
                    do_round = True
            else:
                dtype = ""object""
        dtype = np.dtype(dtype)
    converted = maybe_downcast_numeric(result, dtype, do_round)",[43]
"    def add_api_route(
        self,
        path: str,
        endpoint: Callable,
        *,
        response_model: Type[BaseModel] = None,
        status_code: int = 200,
        tags: List[str] = None,
        summary: str = None,
        description: str = None,
        response_description: str = ""Successful Response"",
        responses: Dict[Union[int, str], Dict[str, Any]] = None,
        deprecated: bool = None,
        methods: List[str] = None,
        operation_id: str = None,
        include_in_schema: bool = True,
        content_type: Type[Response] = JSONResponse,
        name: str = None,
    ) -> None:
        route = APIRoute(
            path,
            endpoint=endpoint,
            response_model=response_model,
            status_code=status_code,
            tags=tags or [],
            summary=summary,
            description=description,
            response_description=response_description,
            responses=responses or {},
            deprecated=deprecated,
            methods=methods,
            operation_id=operation_id,
            include_in_schema=include_in_schema,
            content_type=content_type,
            name=name,
        )
        self.routes.append(route)
    def api_route(
        self,
        path: str,
        *,
        response_model: Type[BaseModel] = None,
        status_code: int = 200,
        tags: List[str] = None,
        summary: str = None,
        description: str = None,
        response_description: str = ""Successful Response"",
        responses: Dict[Union[int, str], Dict[str, Any]] = None,
        deprecated: bool = None,
        methods: List[str] = None,
        operation_id: str = None,
        include_in_schema: bool = True,
        content_type: Type[Response] = JSONResponse,
        name: str = None,
    ) -> Callable:
        def decorator(func: Callable) -> Callable:
            self.add_api_route(
                path,
                func,
                response_model=response_model,
                status_code=status_code,
                tags=tags or [],
                summary=summary,
                description=description,
                response_description=response_description,
                responses=responses or {},
                deprecated=deprecated,
                methods=methods,
                operation_id=operation_id,
                include_in_schema=include_in_schema,
                content_type=content_type,
                name=name,
            )
            return func
        return decorator
    def include_router(
        self,
        router: ""APIRouter"",
        *,
        prefix: str = """",
        tags: List[str] = None,
        responses: Dict[Union[int, str], Dict[str, Any]] = None,
    ) -> None:
        if prefix:
            assert prefix.startswith(""/""), ""A path prefix must start with '/'""
            assert not prefix.endswith(
                ""/""
            ), ""A path prefix must not end with '/', as the routes will start with '/'""
        for route in router.routes:
            if isinstance(route, APIRoute):
                if responses is None:
                    responses = {}
                responses = {**responses, **route.responses}
                self.add_api_route(
                    prefix + route.path,
                    route.endpoint,
                    response_model=route.response_model,
                    status_code=route.status_code,
                    tags=(route.tags or []) + (tags or []),
                    summary=route.summary,
                    description=route.description,
                    response_description=route.response_description,
                    responses=responses,
                    deprecated=route.deprecated,
                    methods=route.methods,
                    operation_id=route.operation_id,
                    include_in_schema=route.include_in_schema,
                    content_type=route.content_type,
                    name=route.name,
                )
            elif isinstance(route, routing.Route):
                self.add_route(
                    prefix + route.path,
                    route.endpoint,
                    methods=route.methods,
                    include_in_schema=route.include_in_schema,
                    name=route.name,
                )
            elif isinstance(route, routing.WebSocketRoute):
                self.add_websocket_route(
                    prefix + route.path, route.endpoint, name=route.name
                )
    def get(","[93, 94, 95, 105]"
"            that will be set to 0.
        noise_shape: shape for randomly generated keep/drop flags,
            must be broadcastable to the shape of `x`
        seed: random seed to ensure determinism.
    # Returns
        A tensor.
    """"""
    retain_prob = 1. - level
    if seed is None:
        seed = np.random.randint(10e6)
    # the dummy 1. works around a TF bug
    # (float32_ref vs. float32 incompatibility)
    return tf.nn.dropout(x * 1., retain_prob, noise_shape, seed=seed)

def l2_normalize(x, axis=None):
    """"""Normalizes a tensor wrt the L2 norm alongside the specified axis.
    # Arguments
        x: Tensor or variable.
        axis: axis along which to perform normalization.
    # Returns
        A tensor.
    """"""
    return tf.nn.l2_normalize(x, axis=axis)

def in_top_k(predictions, targets, k):
    """"""Returns whether the `targets` are in the top `k` `predictions`.
    # Arguments
        predictions: A tensor of shape `(batch_size, classes)` and type `float32`.
        targets: A 1D tensor of length `batch_size` and type `int32` or `int64`.
        k: An `int`, number of top elements to consider.
    # Returns
        A 1D tensor of length `batch_size` and type `bool`.
        `output[i]` is `True` if `predictions[i, targets[i]]` is within top-`k`
        values of `predictions[i]`.
    """"""
    return tf.nn.in_top_k(predictions, targets, k)

# CONVOLUTIONS

def _preprocess_conv1d_input(x, data_format):
    """"""Transpose and cast the input before the conv1d.
    # Arguments
        x: input tensor.
        data_format: string, `""channels_last""` or `""channels_first""`.
    # Returns
        A tensor.
    """"""
    # tensorflow doesn't support float64 for conv layer before 1.8.0
    if (dtype(x) == 'float64' and
            StrictVersion(tf.__version__.split('-')[0]) < StrictVersion('1.8.0')):
        x = tf.cast(x, 'float32')
    tf_data_format = 'NWC'  # to pass TF Conv2dNative operations
    if data_format == 'channels_first':
        if not _has_nchw_support():
            x = tf.transpose(x, (0, 2, 1))  # NCW -> NWC
        else:
            tf_data_format = 'NCW'
    return x, tf_data_format

def _preprocess_conv2d_input(x, data_format):
    """"""Transpose and cast the input before the conv2d.
    # Arguments
        x: input tensor.
        data_format: string, `""channels_last""` or `""channels_first""`.
    # Returns
        A tensor.
    """"""
    # tensorflow doesn't support float64 for conv layer before 1.8.0
    if (dtype(x) == 'float64' and
            StrictVersion(tf.__version__.split('-')[0]) < StrictVersion('1.8.0')):
        x = tf.cast(x, 'float32')
    tf_data_format = 'NHWC'
    if data_format == 'channels_first':
        if not _has_nchw_support():
            x = tf.transpose(x, (0, 2, 3, 1))  # NCHW -> NHWC
        else:
            tf_data_format = 'NCHW'
    return x, tf_data_format

def _preprocess_conv3d_input(x, data_format):
    """"""Transpose and cast the input before the conv3d.
    # Arguments
        x: input tensor.
        data_format: string, `""channels_last""` or `""channels_first""`.
    # Returns
        A tensor.
    """"""
    # tensorflow doesn't support float64 for conv layer before 1.8.0
    if (dtype(x) == 'float64' and
            StrictVersion(tf.__version__.split('-')[0]) < StrictVersion('1.8.0')):
        x = tf.cast(x, 'float32')
    tf_data_format = 'NDHWC'
    if data_format == 'channels_first':
        if not _has_nchw_support():
            x = tf.transpose(x, (0, 2, 3, 4, 1))
        else:
            tf_data_format = 'NCDHW'
    return x, tf_data_format

def _preprocess_padding(padding):
    """"""Convert keras' padding to tensorflow's padding.
    # Arguments
        padding: string, `""same""` or `""valid""`.
    # Returns
        a string, `""SAME""` or `""VALID""`.
    # Raises","[71, 87]"
"# -*- coding: utf-8 -*-
#
# Copyright 2012-2015 Spotify AB
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
""""""
This module contains the bindings for command line integration and dynamic loading of tasks
""""""
import argparse
import logging
import logging.config
import optparse
import os
import sys
import tempfile
from luigi import configuration
from luigi import lock
from luigi import parameter
from luigi import rpc
from luigi import scheduler
from luigi import task
from luigi import worker
from luigi.task_register import Register

def setup_interface_logging(conf_file=None):
    # use a variable in the function object to determine if it has run before
    if getattr(setup_interface_logging, ""has_run"", False):
        return
    if conf_file is None:
        logger = logging.getLogger('luigi-interface')
        logger.setLevel(logging.DEBUG)
        stream_handler = logging.StreamHandler()
        stream_handler.setLevel(logging.DEBUG)
        formatter = logging.Formatter('%(levelname)s: %(message)s')
        stream_handler.setFormatter(formatter)
        logger.addHandler(stream_handler)
    else:
        logging.config.fileConfig(conf_file, disable_existing_loggers=False)
    setup_interface_logging.has_run = True

class core(task.Config):
    ''' Keeps track of a bunch of environment params.
    Uses the internal luigi parameter mechanism.
    The nice thing is that we can instantiate this class
    and get an object with all the environment variables set.
    This is arguably a bit of a hack.
    '''
    use_cmdline_section = False
    local_scheduler = parameter.BoolParameter(
        default=False,
        description='Use local scheduling')
    scheduler_host = parameter.Parameter(
        default='localhost',
        description='Hostname of machine running remote scheduler',
        config_path=dict(section='core', name='default-scheduler-host'))
    scheduler_port = parameter.IntParameter(
        default=8082,
        description='Port of remote scheduler api process',
        config_path=dict(section='core', name='default-scheduler-port'))
    lock_size = parameter.IntParameter(
        default=1,
        description=""Maximum number of workers running the same command"")
    no_lock = parameter.BoolParameter(
        default=False,
        description='Ignore if similar process is already running')
    lock_pid_dir = parameter.Parameter(
        default=os.path.join(tempfile.gettempdir(), 'luigi'),
        description='Directory to store the pid file')
    workers = parameter.IntParameter(
        default=1,
        description='Maximum number of parallel tasks to run')
    logging_conf_file = parameter.Parameter(
        default=None,
        description='Configuration file for logging')
    module = parameter.Parameter(
        default=None,
        description='Used for dynamic loading of modules')  # see DynamicArgParseInterface
    parallel_scheduling = parameter.BoolParameter(
        default=False,
        description='Use multiprocessing to do scheduling in parallel.')
    assistant = parameter.BoolParameter(
        default=False,
        description='Run any task from the scheduler.')

class WorkerSchedulerFactory(object):
    def create_local_scheduler(self):
        return scheduler.CentralPlannerScheduler()
    def create_remote_scheduler(self, host, port):
        return rpc.RemoteScheduler(host=host, port=port)
    def create_worker(self, scheduler, worker_processes, assistant=False):
        return worker.Worker(
            scheduler=scheduler, worker_processes=worker_processes, assistant=assistant)

class Interface(object):
    def parse(self):
        raise NotImplementedError
    @staticmethod",[111]
"        """"""
        # For SparseDataFrame's benefit
        return self._join_compat(
            other, on=on, how=how, lsuffix=lsuffix, rsuffix=rsuffix, sort=sort
        )
    def _join_compat(
        self, other, on=None, how=""left"", lsuffix="""", rsuffix="""", sort=False
    ):
        from pandas.core.reshape.merge import merge
        from pandas.core.reshape.concat import concat
        if isinstance(other, Series):
            if other.name is None:
                raise ValueError(""Other Series must have a name"")
            other = DataFrame({other.name: other})
        if isinstance(other, DataFrame):
            return merge(
                self,
                other,
                left_on=on,
                how=how,
                left_index=on is None,
                right_index=True,
                suffixes=(lsuffix, rsuffix),
                sort=sort,
            )
        else:
            if on is not None:
                raise ValueError(
                    ""Joining multiple DataFrames only supported for joining on index""
                )
            frames = [self] + list(other)
            can_concat = all(df.index.is_unique for df in frames)
            # join indexes only using concat
            if can_concat:
                if how == ""left"":
                    res = concat(frames, axis=1, join=""outer"", verify_integrity=True)
                    return res.reindex(self.index, copy=False)
                else:
                    return concat(frames, axis=1, join=how, verify_integrity=True)
            joined = frames[0]
            for frame in frames[1:]:
                joined = merge(
                    joined, frame, how=how, left_index=True, right_index=True
                )
            return joined
    @Substitution("""")
    @Appender(_merge_doc, indents=2)
    def merge(
        self,
        right,
        how=""inner"",
        on=None,
        left_on=None,
        right_on=None,
        left_index=False,
        right_index=False,
        sort=False,
        suffixes=(""_x"", ""_y""),
        copy=True,
        indicator=False,
        validate=None,
    ):
        from pandas.core.reshape.merge import merge
        return merge(
            self,
            right,
            how=how,
            on=on,
            left_on=left_on,
            right_on=right_on,
            left_index=left_index,
            right_index=right_index,
            sort=sort,
            suffixes=suffixes,
            copy=copy,
            indicator=indicator,
            validate=validate,
        )
    def round(self, decimals=0, *args, **kwargs):
        """"""
        Round a DataFrame to a variable number of decimal places.
        Parameters
        ----------
        decimals : int, dict, Series
            Number of decimal places to round each column to. If an int is
            given, round each column to the same number of places.
            Otherwise dict and Series round to variable numbers of places.
            Column names should be in the keys if `decimals` is a
            dict-like, or in the index if `decimals` is a Series. Any
            columns not included in `decimals` will be left as is. Elements
            of `decimals` which are not columns of the input will be
            ignored.
        *args
            Additional keywords have no effect but might be accepted for
            compatibility with numpy.
        **kwargs
            Additional keywords have no effect but might be accepted for
            compatibility with numpy.
        Returns
        -------
        DataFrame
            A DataFrame with the affected columns rounded to the specified
            number of decimal places.
        See Also
        --------
        numpy.around : Round a numpy array to the given number of decimals.
        Series.round : Round a Series to the given number of decimals.
        Examples
        --------
        >>> df = pd.DataFrame([(.21, .32), (.01, .67), (.66, .03), (.21, .18)],
        ...                   columns=['dogs', 'cats'])","[41, 44]"
"    See Also
    --------
    read_pickle : Load pickled pandas object (or any object) from file.
    DataFrame.to_hdf : Write DataFrame to an HDF5 file.
    DataFrame.to_sql : Write DataFrame to a SQL database.
    DataFrame.to_parquet : Write a DataFrame to the binary parquet format.
    Examples
    --------
    >>> original_df = pd.DataFrame({""foo"": range(5), ""bar"": range(5, 10)})
    >>> original_df
       foo  bar
    0    0    5
    1    1    6
    2    2    7
    3    3    8
    4    4    9
    >>> pd.to_pickle(original_df, ""./dummy.pkl"")
    >>> unpickled_df = pd.read_pickle(""./dummy.pkl"")
    >>> unpickled_df
       foo  bar
    0    0    5
    1    1    6
    2    2    7
    3    3    8
    4    4    9
    >>> import os
    >>> os.remove(""./dummy.pkl"")
    """"""
    path = stringify_path(path)
    f, fh = get_handle(path, ""wb"", compression=compression, is_text=False)
    if protocol < 0:
        protocol = pickle.HIGHEST_PROTOCOL
    try:
        f.write(pickle.dumps(obj, protocol=protocol))
    finally:
        f.close()
        for _f in fh:
            _f.close()

def read_pickle(path, compression=""infer""):
    """"""
    Load pickled pandas object (or any object) from file.
    .. warning::
       Loading pickled data received from untrusted sources can be
       unsafe. See `here <https://docs.python.org/3/library/pickle.html>`__.
    Parameters
    ----------
    path : str
        File path where the pickled object will be loaded.
    compression : {'infer', 'gzip', 'bz2', 'zip', 'xz', None}, default 'infer'
        For on-the-fly decompression of on-disk data. If 'infer', then use
        gzip, bz2, xz or zip if path ends in '.gz', '.bz2', '.xz',
        or '.zip' respectively, and no decompression otherwise.
        Set to None for no decompression.
    Returns
    -------
    unpickled : same type as object stored in file
    See Also
    --------
    DataFrame.to_pickle : Pickle (serialize) DataFrame object to file.
    Series.to_pickle : Pickle (serialize) Series object to file.
    read_hdf : Read HDF5 file into a DataFrame.
    read_sql : Read SQL query or database table into a DataFrame.
    read_parquet : Load a parquet object, returning a DataFrame.
    Notes
    -----
    read_pickle is only guaranteed to be backwards compatible to pandas 0.20.3.
    Examples
    --------
    >>> original_df = pd.DataFrame({""foo"": range(5), ""bar"": range(5, 10)})
    >>> original_df
       foo  bar
    0    0    5
    1    1    6
    2    2    7
    3    3    8
    4    4    9
    >>> pd.to_pickle(original_df, ""./dummy.pkl"")
    >>> unpickled_df = pd.read_pickle(""./dummy.pkl"")
    >>> unpickled_df
       foo  bar
    0    0    5
    1    1    6
    2    2    7
    3    3    8
    4    4    9
    >>> import os
    >>> os.remove(""./dummy.pkl"")
    """"""
    path = stringify_path(path)
    f, fh = get_handle(path, ""rb"", compression=compression, is_text=False)
    # 1) try standard library Pickle
    # 2) try pickle_compat (older pandas version) to handle subclass changes
    excs_to_catch = (AttributeError, ImportError, ModuleNotFoundError)
    try:
        with warnings.catch_warnings(record=True):
            # We want to silence any warnings about, e.g. moved modules.
            warnings.simplefilter(""ignore"", Warning)
            return pickle.load(f)
    except excs_to_catch:
        # e.g.
        #  ""No module named 'pandas.core.sparse.series'""
        #  ""Can't get attribute '__nat_unpickle' on <module 'pandas._libs.tslib""
        return pc.load(f, encoding=None)
    except UnicodeDecodeError:
        # e.g. can occur for files written in py27; see GH#28645
        return pc.load(f, encoding=""latin-1"")
    finally:
        f.close()
        for _f in fh:","[31, 32, 43, 54, 55, 57, 58, 59, 60, 102, 103]"
"                            else:
                                left_drop.append(lk)
                    else:
                        right_keys.append(rk)
                    if lk is not None:
                        left_keys.append(left._get_label_or_level_values(lk))
                        join_names.append(lk)
                    else:
                        # work-around for merge_asof(left_index=True)
                        left_keys.append(left.index)
                        join_names.append(left.index.name)
        elif _any(self.left_on):
            for k in self.left_on:
                if is_lkey(k):
                    left_keys.append(k)
                    join_names.append(None)
                else:
                    left_keys.append(left._get_label_or_level_values(k))
                    join_names.append(k)
            if isinstance(self.right.index, MultiIndex):
                right_keys = [
                    lev._values.take(lev_codes)
                    for lev, lev_codes in zip(
                        self.right.index.levels, self.right.index.codes
                    )
                ]
            else:
                right_keys = [self.right.index._values]
        elif _any(self.right_on):
            for k in self.right_on:
                if is_rkey(k):
                    right_keys.append(k)
                    join_names.append(None)
                else:
                    right_keys.append(right._get_label_or_level_values(k))
                    join_names.append(k)
            if isinstance(self.left.index, MultiIndex):
                left_keys = [
                    lev._values.take(lev_codes)
                    for lev, lev_codes in zip(
                        self.left.index.levels, self.left.index.codes
                    )
                ]
            else:
                left_keys = [self.left.index.values]
        if left_drop:
            self.left = self.left._drop_labels_or_levels(left_drop)
        if right_drop:
            self.right = self.right._drop_labels_or_levels(right_drop)
        return left_keys, right_keys, join_names
    def _maybe_coerce_merge_keys(self):
        # we have valid mergees but we may have to further
        # coerce these if they are originally incompatible types
        #
        # for example if these are categorical, but are not dtype_equal
        # or if we have object and integer dtypes
        for lk, rk, name in zip(
            self.left_join_keys, self.right_join_keys, self.join_names
        ):
            if (len(lk) and not len(rk)) or (not len(lk) and len(rk)):
                continue
            lk_is_cat = is_categorical_dtype(lk)
            rk_is_cat = is_categorical_dtype(rk)
            lk_is_object = is_object_dtype(lk)
            rk_is_object = is_object_dtype(rk)
            # if either left or right is a categorical
            # then the must match exactly in categories & ordered
            if lk_is_cat and rk_is_cat:
                if lk.is_dtype_equal(rk):
                    continue
            elif lk_is_cat or rk_is_cat:
                pass
            elif is_dtype_equal(lk.dtype, rk.dtype):
                continue
            msg = (
                ""You are trying to merge on {lk_dtype} and ""
                ""{rk_dtype} columns. If you wish to proceed ""
                ""you should use pd.concat"".format(lk_dtype=lk.dtype, rk_dtype=rk.dtype)
            )
            # if we are numeric, then allow differing
            # kinds to proceed, eg. int64 and int8, int and float
            # further if we are object, but we infer to
            # the same, then proceed
            if is_numeric_dtype(lk) and is_numeric_dtype(rk):
                if lk.dtype.kind == rk.dtype.kind:
                    continue
                # check whether ints and floats
                elif is_integer_dtype(rk) and is_float_dtype(lk):
                    if not (lk == lk.astype(rk.dtype))[~np.isnan(lk)].all():
                        warnings.warn(
                            ""You are merging on int and float ""
                            ""columns where the float values ""
                            ""are not equal to their int ""
                            ""representation"",
                            UserWarning,
                        )
                    continue
                elif is_float_dtype(rk) and is_integer_dtype(lk):
                    if not (rk == rk.astype(lk.dtype))[~np.isnan(rk)].all():
                        warnings.warn(
                            ""You are merging on int and float ""
                            ""columns where the float values ""
                            ""are not equal to their int ""
                            ""representation"",
                            UserWarning,
                        )
                    continue
                # let's infer and see if we are ok
                elif lib.infer_dtype(lk, skipna=False) == lib.infer_dtype(
                    rk, skipna=False
                ):
                    continue
",[44]
"import textwrap
from typing import List, Set
from pandas._libs import NaT, lib
import pandas.core.common as com
from pandas.core.indexes.base import (
    Index,
    InvalidIndexError,
    _new_Index,
    ensure_index,
    ensure_index_from_sequences,
)
from pandas.core.indexes.category import CategoricalIndex
from pandas.core.indexes.datetimes import DatetimeIndex
from pandas.core.indexes.interval import IntervalIndex
from pandas.core.indexes.multi import MultiIndex
from pandas.core.indexes.numeric import (
    Float64Index,
    Int64Index,
    NumericIndex,
    UInt64Index,
)
from pandas.core.indexes.period import PeriodIndex
from pandas.core.indexes.range import RangeIndex
from pandas.core.indexes.timedeltas import TimedeltaIndex
_sort_msg = textwrap.dedent(
    """"""\
Sorting because non-concatenation axis is not aligned. A future version
of pandas will change to not sort by default.
To accept the future behavior, pass 'sort=False'.
To retain the current behavior and silence the warning, pass 'sort=True'.
""""""
)

__all__ = [
    ""Index"",
    ""MultiIndex"",
    ""NumericIndex"",
    ""Float64Index"",
    ""Int64Index"",
    ""CategoricalIndex"",
    ""IntervalIndex"",
    ""RangeIndex"",
    ""UInt64Index"",
    ""InvalidIndexError"",
    ""TimedeltaIndex"",
    ""PeriodIndex"",
    ""DatetimeIndex"",
    ""_new_Index"",
    ""NaT"",
    ""ensure_index"",
    ""ensure_index_from_sequences"",
    ""get_objs_combined_axis"",
    ""union_indexes"",
    ""get_consensus_names"",
    ""all_indexes_same"",
]

def get_objs_combined_axis(
    objs, intersect: bool = False, axis=0, sort: bool = True
) -> Index:
    """"""
    Extract combined index: return intersection or union (depending on the
    value of ""intersect"") of indexes on given axis, or None if all objects
    lack indexes (e.g. they are numpy arrays).
    Parameters
    ----------
    objs : list
        Series or DataFrame objects, may be mix of the two.
    intersect : bool, default False
        If True, calculate the intersection between indexes. Otherwise,
        calculate the union.
    axis : {0 or 'index', 1 or 'outer'}, default 0
        The axis to extract indexes from.
    sort : bool, default True
        Whether the result index should come out sorted or not.
    Returns
    -------
    Index
    """"""
    obs_idxes = [obj._get_axis(axis) for obj in objs]
    return _get_combined_index(obs_idxes, intersect=intersect, sort=sort)

def _get_distinct_objs(objs: List[Index]) -> List[Index]:
    """"""
    Return a list with distinct elements of ""objs"" (different ids).
    Preserves order.
    """"""
    ids: Set[int] = set()
    res = []
    for obj in objs:
        if id(obj) not in ids:
            ids.add(id(obj))
            res.append(obj)
    return res

def _get_combined_index(
    indexes: List[Index], intersect: bool = False, sort: bool = False
) -> Index:
    """"""
    Return the union or intersection of indexes.
    Parameters
    ----------
    indexes : list of Index or list objects
        When intersect=True, do not accept list of lists.
    intersect : bool, default False
        If True, calculate the intersection between indexes. Otherwise,
        calculate the union.
    sort : bool, default False
        Whether the result index should come out sorted or not.
    Returns
    -------
    Index
    """"""
    # TODO: handle index names!","[65, 89, 107]"
"    # Arguments
        maxlen: Int, maximum length of the output sequences.
        seq: List of lists, where each sublist is a sequence.
        label: List where each element is an integer.
    # Returns
        new_seq, new_label: shortened lists for `seq` and `label`.
    """"""
    new_seq, new_label = [], []
    for x, y in zip(seq, label):
        if len(x) < maxlen:
            new_seq.append(x)
            new_label.append(y)
    return new_seq, new_label

class TimeseriesGenerator(Sequence):
    """"""Utility class for generating batches of temporal data.
    This class takes in a sequence of data-points gathered at
    equal intervals, along with time series parameters such as
    stride, length of history, etc., to produce batches for
    training/validation.
    # Arguments
        data: Indexable generator (such as list or Numpy array)
            containing consecutive data points (timesteps).
            The data should be at 2D, and axis 0 is expected
            to be the time dimension.
        targets: Targets corresponding to timesteps in `data`.
            It should have same length as `data`.
        length: Length of the output sequences (in number of timesteps).
        sampling_rate: Period between successive individual timesteps
            within sequences. For rate `r`, timesteps
            `data[i]`, `data[i-r]`, ... `data[i - length]`
            are used for create a sample sequence.
        stride: Period between successive output sequences.
            For stride `s`, consecutive output samples would
            be centered around `data[i]`, `data[i+s]`, `data[i+2*s]`, etc.
        start_index, end_index: Data points earlier than `start_index`
            or later than `end_index` will not be used in the output sequences.
            This is useful to reserve part of the data for test or validation.
        shuffle: Whether to shuffle output samples,
            or instead draw them in chronological order.
        reverse: Boolean: if `true`, timesteps in each output sample will be
            in reverse chronological order.
        batch_size: Number of timeseries samples in each batch
            (except maybe the last one).
    # Returns
        A [Sequence](/utils/#sequence) instance.
    # Examples
    ```python
    from keras.preprocessing.sequence import TimeseriesGenerator
    import numpy as np
    data = np.array([[i] for i in range(50)])
    targets = np.array([[i] for i in range(50)])
    data_gen = TimeseriesGenerator(data, targets,
                                   length=10, sampling_rate=2,
                                   batch_size=2)
    assert len(data_gen) == 20
    batch_0 = data_gen[0]
    x, y = batch_0
    assert np.array_equal(x,
                          np.array([[[0], [2], [4], [6], [8]],
                                    [[1], [3], [5], [7], [9]]]))
    assert np.array_equal(y,
                          np.array([[10], [11]]))
    ```
    """"""
    def __init__(self, data, targets, length,
                 sampling_rate=1,
                 stride=1,
                 start_index=0,
                 end_index=None,
                 shuffle=False,
                 reverse=False,
                 batch_size=128):
        self.data = data
        self.targets = targets
        self.length = length
        self.sampling_rate = sampling_rate
        self.stride = stride
        self.start_index = start_index + length
        if end_index is None:
            end_index = len(data) - 1
        self.end_index = end_index
        self.shuffle = shuffle
        self.reverse = reverse
        self.batch_size = batch_size
    def __len__(self):
        return int(np.ceil(
            (self.end_index - self.start_index) /
            (self.batch_size * self.stride)))
    def _empty_batch(self, num_rows):
        samples_shape = [num_rows, self.length // self.sampling_rate]
        samples_shape.extend(self.data.shape[1:])
        targets_shape = [num_rows]
        targets_shape.extend(self.targets.shape[1:])
        return np.empty(samples_shape), np.empty(targets_shape)
    def __getitem__(self, index):
        if self.shuffle:
            rows = np.random.randint(
                self.start_index, self.end_index, size=self.batch_size)
        else:
            i = self.start_index + self.batch_size * self.stride * index
            rows = np.arange(i, min(i + self.batch_size *
                                    self.stride, self.end_index), self.stride)
        samples, targets = self._empty_batch(len(rows))
        for j, row in enumerate(rows):
            indices = range(rows[j] - self.length, rows[j], self.sampling_rate)
            samples[j] = self.data[indices]
            targets[j] = self.targets[rows[j]]
        if self.reverse:
            return samples[:, ::-1, ...], targets","[100, 113, 117]"
"        _callbacks += (callbacks or []) + [self.history]
        callbacks = cbks.CallbackList(_callbacks)
        # it's possible to callback a different model than self:
        if hasattr(self, 'callback_model') and self.callback_model:
            callback_model = self.callback_model
        else:
            callback_model = self
        callbacks.set_model(callback_model)
        callbacks.set_params({
            'epochs': epochs,
            'steps': steps_per_epoch,
            'verbose': verbose,
            'do_validation': do_validation,
            'metrics': callback_metrics,
        })
        callbacks.on_train_begin()
        enqueuer = None
        val_enqueuer = None
        try:
            if do_validation and not val_gen:
                # Prepare data for validation
                if len(validation_data) == 2:
                    val_x, val_y = validation_data
                    val_sample_weight = None
                elif len(validation_data) == 3:
                    val_x, val_y, val_sample_weight = validation_data
                else:
                    raise ValueError('`validation_data` should be a tuple '
                                     '`(val_x, val_y, val_sample_weight)` '
                                     'or `(val_x, val_y)`. Found: ' +
                                     str(validation_data))
                val_x, val_y, val_sample_weights = self._standardize_user_data(
                    val_x, val_y, val_sample_weight)
                val_data = val_x + val_y + val_sample_weights
                if self.uses_learning_phase and not isinstance(K.learning_phase(), int):
                    val_data += [0.]
                for cbk in callbacks:
                    cbk.validation_data = val_data
            if workers > 0:
                if is_sequence:
                    enqueuer = OrderedEnqueuer(generator,
                                               use_multiprocessing=use_multiprocessing,
                                               shuffle=shuffle)
                else:
                    enqueuer = GeneratorEnqueuer(generator,
                                                 use_multiprocessing=use_multiprocessing,
                                                 wait_time=wait_time)
                enqueuer.start(workers=workers, max_queue_size=max_queue_size)
                output_generator = enqueuer.get()
            else:
                if is_sequence:
                    output_generator = iter(generator)
                else:
                    output_generator = generator
            callback_model.stop_training = False
            # Construct epoch logs.
            epoch_logs = {}
            while epoch < epochs:
                for m in self.metrics:
                    if isinstance(m, Layer) and m.stateful:
                        m.reset_states()
                callbacks.on_epoch_begin(epoch)
                steps_done = 0
                batch_index = 0
                while steps_done < steps_per_epoch:
                    generator_output = next(output_generator)
                    if not hasattr(generator_output, '__len__'):
                        raise ValueError('Output of generator should be '
                                         'a tuple `(x, y, sample_weight)` '
                                         'or `(x, y)`. Found: ' +
                                         str(generator_output))
                    if len(generator_output) == 2:
                        x, y = generator_output
                        sample_weight = None
                    elif len(generator_output) == 3:
                        x, y, sample_weight = generator_output
                    else:
                        raise ValueError('Output of generator should be '
                                         'a tuple `(x, y, sample_weight)` '
                                         'or `(x, y)`. Found: ' +
                                         str(generator_output))
                    # build batch logs
                    batch_logs = {}
                    if isinstance(x, list):
                        batch_size = x[0].shape[0]
                    elif isinstance(x, dict):
                        batch_size = list(x.values())[0].shape[0]
                    else:
                        batch_size = x.shape[0]
                    batch_logs['batch'] = batch_index
                    batch_logs['size'] = batch_size
                    callbacks.on_batch_begin(batch_index, batch_logs)
                    outs = self.train_on_batch(x, y,
                                               sample_weight=sample_weight,
                                               class_weight=class_weight)
                    if not isinstance(outs, list):
                        outs = [outs]
                    for l, o in zip(out_labels, outs):
                        batch_logs[l] = o
                    callbacks.on_batch_end(batch_index, batch_logs)
                    batch_index += 1
                    steps_done += 1
                    # Epoch finished.
                    if steps_done >= steps_per_epoch and do_validation:
                        if val_gen:
                            val_outs = self.evaluate_generator(
                                validation_data,
                                validation_steps,
                                workers=workers,
                                use_multiprocessing=use_multiprocessing,
                                max_queue_size=max_queue_size)
                        else:
                            # No need for try/except because
                            # data has already been validated.
                            val_outs = self.evaluate(",[90]
"            if isinstance(cell, Layer):
                num_param = len(cell.weights)
                weights = weights[:num_param]
                for sw, w in zip(cell.weights, weights):
                    tuples.append((sw, w))
                weights = weights[num_param:]
        K.batch_set_value(tuples)
    @property
    def losses(self):
        losses = []
        for cell in self.cells:
            if isinstance(cell, Layer):
                cell_losses = cell.losses
                losses += cell_losses
        return losses
    def get_losses_for(self, inputs=None):
        losses = []
        for cell in self.cells:
            if isinstance(cell, Layer):
                cell_losses = cell.get_losses_for(inputs)
                losses += cell_losses
        return losses

class RNN(Layer):
    """"""Base class for recurrent layers.
    # Arguments
        cell: A RNN cell instance. A RNN cell is a class that has:
            - a `call(input_at_t, states_at_t)` method, returning
                `(output_at_t, states_at_t_plus_1)`. The call method of the
                cell can also take the optional argument `constants`, see
                section ""Note on passing external constants"" below.
            - a `state_size` attribute. This can be a single integer
                (single state) in which case it is
                the size of the recurrent state
                (which should be the same as the size of the cell output).
                This can also be a list/tuple of integers
                (one size per state). In this case, the first entry
                (`state_size[0]`) should be the same as
                the size of the cell output.
            It is also possible for `cell` to be a list of RNN cell instances,
            in which cases the cells get stacked on after the other in the RNN,
            implementing an efficient stacked RNN.
        return_sequences: Boolean. Whether to return the last output
            in the output sequence, or the full sequence.
        return_state: Boolean. Whether to return the last state
            in addition to the output.
        go_backwards: Boolean (default False).
            If True, process the input sequence backwards and return the
            reversed sequence.
        stateful: Boolean (default False). If True, the last state
            for each sample at index i in a batch will be used as initial
            state for the sample of index i in the following batch.
        unroll: Boolean (default False).
            If True, the network will be unrolled,
            else a symbolic loop will be used.
            Unrolling can speed-up a RNN,
            although it tends to be more memory-intensive.
            Unrolling is only suitable for short sequences.
        input_dim: dimensionality of the input (integer).
            This argument (or alternatively,
            the keyword argument `input_shape`)
            is required when using this layer as the first layer in a model.
        input_length: Length of input sequences, to be specified
            when it is constant.
            This argument is required if you are going to connect
            `Flatten` then `Dense` layers upstream
            (without it, the shape of the dense outputs cannot be computed).
            Note that if the recurrent layer is not the first layer
            in your model, you would need to specify the input length
            at the level of the first layer
            (e.g. via the `input_shape` argument)
    # Input shape
        3D tensor with shape `(batch_size, timesteps, input_dim)`.
    # Output shape
        - if `return_state`: a list of tensors. The first tensor is
            the output. The remaining tensors are the last states,
            each with shape `(batch_size, units)`.
        - if `return_sequences`: 3D tensor with shape
            `(batch_size, timesteps, units)`.
        - else, 2D tensor with shape `(batch_size, units)`.
    # Masking
        This layer supports masking for input data with a variable number
        of timesteps. To introduce masks to your data,
        use an [Embedding](embeddings.md) layer with the `mask_zero` parameter
        set to `True`.
    # Note on using statefulness in RNNs
        You can set RNN layers to be 'stateful', which means that the states
        computed for the samples in one batch will be reused as initial states
        for the samples in the next batch. This assumes a one-to-one mapping
        between samples in different successive batches.
        To enable statefulness:
            - specify `stateful=True` in the layer constructor.
            - specify a fixed batch size for your model, by passing
                if sequential model:
                  `batch_input_shape=(...)` to the first layer in your model.
                else for functional model with 1 or more Input layers:
                  `batch_shape=(...)` to all the first layers in your model.
                This is the expected shape of your inputs
                *including the batch size*.
                It should be a tuple of integers, e.g. `(32, 10, 100)`.
            - specify `shuffle=False` when calling fit().
        To reset the states of your model, call `.reset_states()` on either
        a specific layer, or on your entire model.
    # Note on specifying the initial state of RNNs
        You can specify the initial state of RNN layers symbolically by
        calling them with the keyword argument `initial_state`. The value of
        `initial_state` should be a tensor or list of tensors representing
        the initial state of the RNN layer.
        You can specify the initial state of RNN layers numerically by
        calling `reset_states` with the keyword argument `states`. The value of
        `states` should be a numpy array or list of numpy arrays representing
        the initial state of the RNN layer.
    # Note on passing external constants to RNNs
        You can pass ""external"" constants to the cell using the `constants`","[40, 41, 42]"
"        # we don't allow integer/float indexing for loc
        # we don't allow float indexing for ix/getitem
        if is_scalar(key):
            is_int = is_integer(key)
            is_flt = is_float(key)
            if kind in [""loc""] and (is_int or is_flt):
                self._invalid_indexer(""index"", key)
            elif kind in [""ix"", ""getitem""] and is_flt:
                self._invalid_indexer(""index"", key)
        return super()._convert_scalar_indexer(key, kind=kind)
    __add__ = make_wrapped_arith_op(""__add__"")
    __radd__ = make_wrapped_arith_op(""__radd__"")
    __sub__ = make_wrapped_arith_op(""__sub__"")
    __rsub__ = make_wrapped_arith_op(""__rsub__"")
    __pow__ = make_wrapped_arith_op(""__pow__"")
    __rpow__ = make_wrapped_arith_op(""__rpow__"")
    __mul__ = make_wrapped_arith_op(""__mul__"")
    __rmul__ = make_wrapped_arith_op(""__rmul__"")
    __floordiv__ = make_wrapped_arith_op(""__floordiv__"")
    __rfloordiv__ = make_wrapped_arith_op(""__rfloordiv__"")
    __mod__ = make_wrapped_arith_op(""__mod__"")
    __rmod__ = make_wrapped_arith_op(""__rmod__"")
    __divmod__ = make_wrapped_arith_op(""__divmod__"")
    __rdivmod__ = make_wrapped_arith_op(""__rdivmod__"")
    __truediv__ = make_wrapped_arith_op(""__truediv__"")
    __rtruediv__ = make_wrapped_arith_op(""__rtruediv__"")
    def isin(self, values, level=None):
        """"""
        Compute boolean array of whether each index value is found in the
        passed set of values.
        Parameters
        ----------
        values : set or sequence of values
        Returns
        -------
        is_contained : ndarray (boolean dtype)
        """"""
        if level is not None:
            self._validate_index_level(level)
        if not isinstance(values, type(self)):
            try:
                values = type(self)(values)
            except ValueError:
                return self.astype(object).isin(values)
        return algorithms.isin(self.asi8, values.asi8)
    @Appender(_index_shared_docs[""repeat""] % _index_doc_kwargs)
    def repeat(self, repeats, axis=None):
        nv.validate_repeat(tuple(), dict(axis=axis))
        result = type(self._data)(self.asi8.repeat(repeats), dtype=self.dtype)
        return self._shallow_copy(result)
    @Appender(_index_shared_docs[""where""] % _index_doc_kwargs)
    def where(self, cond, other=None):
        other = _ensure_datetimelike_to_i8(other, to_utc=True)
        values = _ensure_datetimelike_to_i8(self, to_utc=True)
        result = np.where(cond, values, other).astype(""i8"")
        result = self._ensure_localized(result, from_utc=True)
        return self._shallow_copy(result)
    def _summary(self, name=None):
        """"""
        Return a summarized representation.
        Parameters
        ----------
        name : str
            Name to use in the summary representation.
        Returns
        -------
        str
            Summarized representation of the index.
        """"""
        formatter = self._formatter_func
        if len(self) > 0:
            index_summary = f"", {formatter(self[0])} to {formatter(self[-1])}""
        else:
            index_summary = """"
        if name is None:
            name = type(self).__name__
        result = f""{name}: {len(self)} entries{index_summary}""
        if self.freq:
            result += f""\nFreq: {self.freqstr}""
        # display as values, not quoted
        result = result.replace(""'"", """")
        return result
    def _concat_same_dtype(self, to_concat, name):
        """"""
        Concatenate to_concat which has the same class.
        """"""
        attribs = self._get_attributes_dict()
        attribs[""name""] = name
        # do not pass tz to set because tzlocal cannot be hashed
        if len({str(x.dtype) for x in to_concat}) != 1:
            raise ValueError(""to_concat must have the same tz"")
        new_data = type(self._values)._concat_same_type(to_concat).asi8
        # GH 3232: If the concat result is evenly spaced, we can retain the
        # original frequency
        is_diff_evenly_spaced = len(unique_deltas(new_data)) == 1
        if not is_period_dtype(self) and not is_diff_evenly_spaced:
            # reset freq
            attribs[""freq""] = None
        return self._simple_new(new_data, **attribs)
    @Appender(_index_shared_docs[""astype""])
    def astype(self, dtype, copy=True):
        if is_dtype_equal(self.dtype, dtype) and copy is False:
            # Ensure that self.astype(self.dtype) is self
            return self
        new_values = self._data.astype(dtype, copy=copy)","[62, 63, 64, 66]"
"                this implementation relies on multiprocessing,
                you should not pass
                non picklable arguments to the generator
                as they can't be passed
                easily to children processes.
            shuffle: Whether to shuffle the order of the batches at
                the beginning of each epoch. Only used with instances
                of `Sequence` (keras.utils.Sequence).
            initial_epoch: Epoch at which to start training
                (useful for resuming a previous training run)
        # Returns
            A `History` object.
        # Example
        ```python
            def generate_arrays_from_file(path):
                while 1:
                    f = open(path)
                    for line in f:
                        # create numpy arrays of input data
                        # and labels, from each line in the file
                        x1, x2, y = process_line(line)
                        yield ({'input_1': x1, 'input_2': x2}, {'output': y})
                    f.close()
            model.fit_generator(generate_arrays_from_file('/my_file.txt'),
                                steps_per_epoch=10000, epochs=10)
        ```
        # Raises
            ValueError: In case the generator yields
                data in an invalid format.
        """"""
        wait_time = 0.01  # in seconds
        epoch = initial_epoch
        do_validation = bool(validation_data)
        self._make_train_function()
        if do_validation:
            self._make_test_function()
        # python 2 has 'next', 3 has '__next__'
        # avoid any explicit version checks
        val_gen = (hasattr(validation_data, 'next') or
                   hasattr(validation_data, '__next__') or
                   isinstance(validation_data, Sequence))
        if val_gen and not validation_steps:
            raise ValueError('When using a generator for validation data, '
                             'you must specify a value for '
                             '`validation_steps`.')
        # Prepare display labels.
        out_labels = self._get_deduped_metrics_names()
        callback_metrics = out_labels + ['val_' + n for n in out_labels]
        # prepare callbacks
        self.history = cbks.History()
        callbacks = [cbks.BaseLogger()] + (callbacks or []) + [self.history]
        if verbose:
            callbacks += [cbks.ProgbarLogger(count_mode='steps')]
        callbacks = cbks.CallbackList(callbacks)
        # it's possible to callback a different model than self:
        if hasattr(self, 'callback_model') and self.callback_model:
            callback_model = self.callback_model
        else:
            callback_model = self
        callbacks.set_model(callback_model)
        callbacks.set_params({
            'epochs': epochs,
            'steps': steps_per_epoch,
            'verbose': verbose,
            'do_validation': do_validation,
            'metrics': callback_metrics,
        })
        callbacks.on_train_begin()
        if do_validation and not val_gen:
            if len(validation_data) == 2:
                val_x, val_y = validation_data
                val_sample_weight = None
            elif len(validation_data) == 3:
                val_x, val_y, val_sample_weight = validation_data
            else:
                raise ValueError('`validation_data` should be a tuple '
                                 '`(val_x, val_y, val_sample_weight)` '
                                 'or `(val_x, val_y)`. Found: ' +
                                 str(validation_data))
            val_x, val_y, val_sample_weights = self._standardize_user_data(
                val_x, val_y, val_sample_weight)
            val_data = val_x + val_y + val_sample_weights
            if self.uses_learning_phase and not isinstance(K.learning_phase(), int):
                val_data += [0.]
            for cbk in callbacks:
                cbk.validation_data = val_data
        is_sequence = isinstance(generator, Sequence)
        if not is_sequence and use_multiprocessing and workers > 1:
            warnings.warn(
                UserWarning('Using a generator with `use_multiprocessing=True`'
                            ' and multiple workers may duplicate your data.'
                            ' Please consider using the`keras.utils.Sequence'
                            ' class.'))
        if is_sequence:
            steps_per_epoch = len(generator)
        enqueuer = None
        try:
            if is_sequence:
                enqueuer = OrderedEnqueuer(generator,
                                           use_multiprocessing=use_multiprocessing,
                                           shuffle=shuffle)
            else:
                enqueuer = GeneratorEnqueuer(generator,
                                             use_multiprocessing=use_multiprocessing,
                                             wait_time=wait_time)
            enqueuer.start(workers=workers, max_queue_size=max_queue_size)
            output_generator = enqueuer.get()
            callback_model.stop_training = False
            while epoch < epochs:
                callbacks.on_epoch_begin(epoch)
                steps_done = 0
                batch_index = 0
                while steps_done < steps_per_epoch:
                    generator_output = next(output_generator)","[48, 49, 50, 51, 97, 98, 99, 100, 101, 102, 103, 104, 105]"
"""""""Part of the training engine related to Python generators of array data.
""""""
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function
import warnings
import numpy as np
from .training_utils import iter_sequence_infinite
from .. import backend as K
from ..utils.data_utils import Sequence
from ..utils.data_utils import GeneratorEnqueuer
from ..utils.data_utils import OrderedEnqueuer
from ..utils.generic_utils import Progbar
from ..utils.generic_utils import to_list
from ..utils.generic_utils import unpack_singleton
from .. import callbacks as cbks

def fit_generator(model,
                  generator,
                  steps_per_epoch=None,
                  epochs=1,
                  verbose=1,
                  callbacks=None,
                  validation_data=None,
                  validation_steps=None,
                  class_weight=None,
                  max_queue_size=10,
                  workers=1,
                  use_multiprocessing=False,
                  shuffle=True,
                  initial_epoch=0):
    """"""See docstring for `Model.fit_generator`.""""""
    wait_time = 0.01  # in seconds
    epoch = initial_epoch
    do_validation = bool(validation_data)
    model._make_train_function()
    if do_validation:
        model._make_test_function()
    is_sequence = isinstance(generator, Sequence)
    if not is_sequence and use_multiprocessing and workers > 1:
        warnings.warn(
            UserWarning('Using a generator with `use_multiprocessing=True`'
                        ' and multiple workers may duplicate your data.'
                        ' Please consider using the`keras.utils.Sequence'
                        ' class.'))
    if steps_per_epoch is None:
        if is_sequence:
            steps_per_epoch = len(generator)
        else:
            raise ValueError('`steps_per_epoch=None` is only valid for a'
                             ' generator based on the '
                             '`keras.utils.Sequence`'
                             ' class. Please specify `steps_per_epoch` '
                             'or use the `keras.utils.Sequence` class.')
    # python 2 has 'next', 3 has '__next__'
    # avoid any explicit version checks
    val_gen = (hasattr(validation_data, 'next') or
               hasattr(validation_data, '__next__') or
               isinstance(validation_data, Sequence))
    if (val_gen and not isinstance(validation_data, Sequence) and
            not validation_steps):
        raise ValueError('`validation_steps=None` is only valid for a'
                         ' generator based on the `keras.utils.Sequence`'
                         ' class. Please specify `validation_steps` or use'
                         ' the `keras.utils.Sequence` class.')
    # Prepare display labels.
    out_labels = model.metrics_names
    callback_metrics = out_labels + ['val_' + n for n in out_labels]
    # prepare callbacks
    model.history = cbks.History()
    _callbacks = [cbks.BaseLogger(
        stateful_metrics=model.stateful_metric_names)]
    if verbose:
        _callbacks.append(
            cbks.ProgbarLogger(
                count_mode='steps',
                stateful_metrics=model.stateful_metric_names))
    _callbacks += (callbacks or []) + [model.history]
    callbacks = cbks.CallbackList(_callbacks)
    # it's possible to callback a different model than self:
    if hasattr(model, 'callback_model') and model.callback_model:
        callback_model = model.callback_model
    else:
        callback_model = model
    callbacks.set_model(callback_model)
    callbacks.set_params({
        'epochs': epochs,
        'steps': steps_per_epoch,
        'verbose': verbose,
        'do_validation': do_validation,
        'metrics': callback_metrics,
    })
    callbacks.on_train_begin()
    enqueuer = None
    val_enqueuer = None
    try:
        if do_validation:
            if val_gen and workers > 0:
                # Create an Enqueuer that can be reused
                val_data = validation_data
                if isinstance(val_data, Sequence):
                    val_enqueuer = OrderedEnqueuer(
                        val_data,
                        use_multiprocessing=use_multiprocessing)
                    validation_steps = validation_steps or len(val_data)
                else:
                    val_enqueuer = GeneratorEnqueuer(
                        val_data,
                        use_multiprocessing=use_multiprocessing)
                val_enqueuer.start(workers=workers,
                                   max_queue_size=max_queue_size)
                val_enqueuer_gen = val_enqueuer.get()
            elif val_gen:
                val_data = validation_data
                if isinstance(val_data, Sequence):
                    val_enqueuer_gen = iter_sequence_infinite(generator)",[126]
"import asyncio
import inspect
from typing import Any, Callable, Dict, List, Optional, Sequence, Set, Type, Union
from fastapi import params
from fastapi.dependencies.models import Dependant
from fastapi.dependencies.utils import (
    get_body_field,
    get_dependant,
    get_parameterless_sub_dependant,
    solve_dependencies,
)
from fastapi.encoders import DictIntStrAny, SetIntStr, jsonable_encoder
from fastapi.exceptions import RequestValidationError, WebSocketRequestValidationError
from fastapi.logger import logger
from fastapi.openapi.constants import STATUS_CODES_WITH_NO_BODY
from fastapi.utils import (
    PYDANTIC_1,
    create_cloned_field,
    create_response_field,
    generate_operation_id_for_path,
    get_field_info,
    warning_response_model_skip_defaults_deprecated,
)
from pydantic import BaseModel
from pydantic.error_wrappers import ErrorWrapper, ValidationError
from starlette import routing
from starlette.concurrency import run_in_threadpool
from starlette.exceptions import HTTPException
from starlette.requests import Request
from starlette.responses import JSONResponse, Response
from starlette.routing import Mount  # noqa
from starlette.routing import (
    compile_path,
    get_name,
    request_response,
    websocket_session,
)
from starlette.status import WS_1008_POLICY_VIOLATION
from starlette.types import ASGIApp
from starlette.websockets import WebSocket
try:
    from pydantic.fields import FieldInfo, ModelField
except ImportError:  # pragma: nocover
    # TODO: remove when removing support for Pydantic < 1.0.0
    from pydantic import Schema as FieldInfo  # type: ignore
    from pydantic.fields import Field as ModelField  # type: ignore

def _prepare_response_content(
    res: Any, *, by_alias: bool = True, exclude_unset: bool
) -> Any:
    if isinstance(res, BaseModel):
        if PYDANTIC_1:
            return res.dict(by_alias=by_alias, exclude_unset=exclude_unset)
        else:
            return res.dict(
                by_alias=by_alias, skip_defaults=exclude_unset
            )  # pragma: nocover
    elif isinstance(res, list):
        return [
            _prepare_response_content(item, exclude_unset=exclude_unset) for item in res
        ]
    elif isinstance(res, dict):
        return {
            k: _prepare_response_content(v, exclude_unset=exclude_unset)
            for k, v in res.items()
        }
    return res

async def serialize_response(
    *,
    field: ModelField = None,
    response_content: Any,
    include: Union[SetIntStr, DictIntStrAny] = None,
    exclude: Union[SetIntStr, DictIntStrAny] = set(),
    by_alias: bool = True,
    exclude_unset: bool = False,
    is_coroutine: bool = True,
) -> Any:
    if field:
        errors = []
        response_content = _prepare_response_content(
            response_content, by_alias=by_alias, exclude_unset=exclude_unset
        )
        if is_coroutine:
            value, errors_ = field.validate(response_content, {}, loc=(""response"",))
        else:
            value, errors_ = await run_in_threadpool(
                field.validate, response_content, {}, loc=(""response"",)
            )
        if isinstance(errors_, ErrorWrapper):
            errors.append(errors_)
        elif isinstance(errors_, list):
            errors.extend(errors_)
        if errors:
            raise ValidationError(errors, field.type_)
        return jsonable_encoder(
            value,
            include=include,
            exclude=exclude,
            by_alias=by_alias,
            exclude_unset=exclude_unset,
        )
    else:
        return jsonable_encoder(response_content)

async def run_endpoint_function(
    *, dependant: Dependant, values: Dict[str, Any], is_coroutine: bool
) -> Any:
    # Only called by get_request_handler. Has been split into its own function to
    # facilitate profiling endpoints, since inner functions are harder to profile.
    assert dependant.call is not None, ""dependant.call must be a function""
    if is_coroutine:
        return await dependant.call(**values)
    else:
        return await run_in_threadpool(dependant.call, **values)

def get_request_handler(
    dependant: Dependant,
    body_field: ModelField = None,
    status_code: int = 200,","[51, 55, 58, 62, 66, 85]"
"    On Python 3, `IOLoop` is a wrapper around the `asyncio` event
    loop. On Python 2, it uses ``epoll`` (Linux) or ``kqueue`` (BSD
    and Mac OS X) if they are available, or else we fall back on
    select(). If you are implementing a system that needs to handle
    thousands of simultaneous connections, you should use a system
    that supports either ``epoll`` or ``kqueue``.
    Example usage for a simple TCP server:
    .. testcode::
        import errno
        import functools
        import socket
        import tornado.ioloop
        from tornado import gen
        from tornado.iostream import IOStream
        @gen.coroutine
        def handle_connection(connection, address):
            stream = IOStream(connection)
            message = yield stream.read_until_close()
            print(""message from client:"", message.decode().strip())
        def connection_ready(sock, fd, events):
            while True:
                try:
                    connection, address = sock.accept()
                except socket.error as e:
                    if e.args[0] not in (errno.EWOULDBLOCK, errno.EAGAIN):
                        raise
                    return
                connection.setblocking(0)
                handle_connection(connection, address)
        if __name__ == '__main__':
            sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM, 0)
            sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
            sock.setblocking(0)
            sock.bind(("""", 8888))
            sock.listen(128)
            io_loop = tornado.ioloop.IOLoop.current()
            callback = functools.partial(connection_ready, sock)
            io_loop.add_handler(sock.fileno(), callback, io_loop.READ)
            io_loop.start()
    .. testoutput::
       :hide:
    By default, a newly-constructed `IOLoop` becomes the thread's current
    `IOLoop`, unless there already is a current `IOLoop`. This behavior
    can be controlled with the ``make_current`` argument to the `IOLoop`
    constructor: if ``make_current=True``, the new `IOLoop` will always
    try to become current and it raises an error if there is already a
    current instance. If ``make_current=False``, the new `IOLoop` will
    not try to become current.
    In general, an `IOLoop` cannot survive a fork or be shared across
    processes in any way. When multiple processes are being used, each
    process should create its own `IOLoop`, which also implies that
    any objects which depend on the `IOLoop` (such as
    `.AsyncHTTPClient`) must also be created in the child processes.
    As a guideline, anything that starts processes (including the
    `tornado.process` and `multiprocessing` modules) should do so as
    early as possible, ideally the first thing the application does
    after loading its configuration in ``main()``.
    .. versionchanged:: 4.2
       Added the ``make_current`` keyword argument to the `IOLoop`
       constructor.
    .. versionchanged:: 5.0
       Uses the `asyncio` event loop by default. The
       ``IOLoop.configure`` method cannot be used on Python 3 except
       to redundantly specify the `asyncio` event loop.
    """"""
    # Constants from the epoll module
    _EPOLLIN = 0x001
    _EPOLLPRI = 0x002
    _EPOLLOUT = 0x004
    _EPOLLERR = 0x008
    _EPOLLHUP = 0x010
    _EPOLLRDHUP = 0x2000
    _EPOLLONESHOT = (1 << 30)
    _EPOLLET = (1 << 31)
    # Our events map exactly to the epoll events
    NONE = 0
    READ = _EPOLLIN
    WRITE = _EPOLLOUT
    ERROR = _EPOLLERR | _EPOLLHUP
    # In Python 2, _current.instance points to the current IOLoop.
    _current = threading.local()
    # In Python 3, _ioloop_for_asyncio maps from asyncio loops to IOLoops.
    _ioloop_for_asyncio = weakref.WeakKeyDictionary()
    @classmethod
    def configure(cls, impl, **kwargs):
        if asyncio is not None:
            from tornado.platform.asyncio import BaseAsyncIOLoop
            if isinstance(impl, (str, unicode_type)):
                impl = import_object(impl)
            if not issubclass(impl, BaseAsyncIOLoop):
                raise RuntimeError(
                    ""only AsyncIOLoop is allowed when asyncio is available"")
        super(IOLoop, cls).configure(impl, **kwargs)
    @staticmethod
    def instance():
        """"""Deprecated alias for `IOLoop.current()`.
        .. versionchanged:: 5.0
           Previously, this method returned a global singleton
           `IOLoop`, in contrast with the per-thread `IOLoop` returned
           by `current()`. In nearly all cases the two were the same
           (when they differed, it was generally used from non-Tornado
           threads to communicate back to the main thread's `IOLoop`).
           This distinction is not present in `asyncio`, so in order
           to facilitate integration with that package `instance()`",[100]
"    """"""Extract a list of unicode strings from the given text/encoding using the following policies:
    * if the regex contains a named group called ""extract"" that will be returned
    * if the regex contains multiple numbered groups, all those will be returned (flattened)
    * if the regex doesn't contain any group the entire regex matching is returned
    """"""
    if isinstance(regex, str):
        regex = re.compile(regex, re.UNICODE)
    try:
        strings = [regex.search(text).group('extract')]   # named group
    except Exception:
        strings = regex.findall(text)    # full regex or numbered groups
    strings = flatten(strings)
    if isinstance(text, str):
        return [replace_entities(s, keep=['lt', 'amp']) for s in strings]
    else:
        return [replace_entities(to_unicode(s, encoding), keep=['lt', 'amp'])
                for s in strings]

def md5sum(file):
    """"""Calculate the md5 checksum of a file-like object without reading its
    whole content in memory.
    >>> from io import BytesIO
    >>> md5sum(BytesIO(b'file content to hash'))
    '784406af91dd5a54fbb9c84c2236595a'
    """"""
    m = hashlib.md5()
    while True:
        d = file.read(8096)
        if not d:
            break
        m.update(d)
    return m.hexdigest()

def rel_has_nofollow(rel):
    """"""Return True if link rel attribute has nofollow type""""""
    return rel is not None and 'nofollow' in rel.split()

def create_instance(objcls, settings, crawler, *args, **kwargs):
    """"""Construct a class instance using its ``from_crawler`` or
    ``from_settings`` constructors, if available.
    At least one of ``settings`` and ``crawler`` needs to be different from
    ``None``. If ``settings `` is ``None``, ``crawler.settings`` will be used.
    If ``crawler`` is ``None``, only the ``from_settings`` constructor will be
    tried.
    ``*args`` and ``**kwargs`` are forwarded to the constructors.
    Raises ``ValueError`` if both ``settings`` and ``crawler`` are ``None``.
    """"""
    if settings is None:
        if crawler is None:
            raise ValueError(""Specify at least one of settings and crawler."")
        settings = crawler.settings
    if crawler and hasattr(objcls, 'from_crawler'):
        return objcls.from_crawler(crawler, *args, **kwargs)
    elif hasattr(objcls, 'from_settings'):
        return objcls.from_settings(settings, *args, **kwargs)
    else:
        return objcls(*args, **kwargs)

@contextmanager
def set_environ(**kwargs):
    """"""Temporarily set environment variables inside the context manager and
    fully restore previous environment afterwards
    """"""
    original_env = {k: os.environ.get(k) for k in kwargs}
    os.environ.update(kwargs)
    try:
        yield
    finally:
        for k, v in original_env.items():
            if v is None:
                del os.environ[k]
            else:
                os.environ[k] = v

_generator_callbacks_cache = LocalWeakReferencedCache(limit=128)

def is_generator_with_return_value(callable):
    """"""
    Returns True if a callable is a generator function which includes a
    'return' statement with a value different than None, False otherwise
    """"""
    if callable in _generator_callbacks_cache:
        return _generator_callbacks_cache[callable]
    def returns_none(return_node):
        value = return_node.value
        return value is None or isinstance(value, ast.NameConstant) and value.value is None
    if inspect.isgeneratorfunction(callable):
        tree = ast.parse(dedent(inspect.getsource(callable)))
        for node in ast.walk(tree):
            if isinstance(node, ast.Return) and not returns_none(node):
                _generator_callbacks_cache[callable] = True
                return _generator_callbacks_cache[callable]
    _generator_callbacks_cache[callable] = False
    return _generator_callbacks_cache[callable]

def warn_on_generator_with_return_value(spider, callable):
    """"""
    Logs a warning if a callable is a generator function and includes
    a 'return' statement with a value different than None
    """"""
    if is_generator_with_return_value(callable):
        warnings.warn(
            'The ""{}.{}"" method is a generator and includes a ""return"" statement with a '
            'value different than None. This could lead to unexpected behaviour. Please see '
            'https://docs.python.org/3/reference/simple_stmts.html#the-return-statement '
            'for details about the semantics of the ""return"" statement within generators'
            .format(spider.__class__.__name__, callable.__name__), stacklevel=2,","[63, 65, 67]"
"            multi_join_idx = multi_join_idx.remove_unused_levels()
            return multi_join_idx, lidx, ridx
        jl = list(overlap)[0]
        # Case where only one index is multi
        # make the indices into mi's that match
        flip_order = False
        if self_is_mi:
            self, other = other, self
            flip_order = True
            # flip if join method is right or left
            how = {""right"": ""left"", ""left"": ""right""}.get(how, how)
        level = other.names.index(jl)
        result = self._join_level(
            other, level, how=how, return_indexers=return_indexers
        )
        if flip_order:
            if isinstance(result, tuple):
                return result[0], result[2], result[1]
        return result
    def _join_non_unique(self, other, how=""left"", return_indexers=False):
        from pandas.core.reshape.merge import _get_join_indexers
        # We only get here if dtypes match
        assert self.dtype == other.dtype
        lvalues = self._get_engine_target()
        rvalues = other._get_engine_target()
        left_idx, right_idx = _get_join_indexers(
            [lvalues], [rvalues], how=how, sort=True
        )
        left_idx = ensure_platform_int(left_idx)
        right_idx = ensure_platform_int(right_idx)
        join_index = np.asarray(lvalues.take(left_idx))
        mask = left_idx == -1
        np.putmask(join_index, mask, rvalues.take(right_idx))
        join_index = self._wrap_joined_index(join_index, other)
        if return_indexers:
            return join_index, left_idx, right_idx
        else:
            return join_index
    def _join_level(
        self, other, level, how=""left"", return_indexers=False, keep_order=True
    ):
        """"""
        The join method *only* affects the level of the resulting
        MultiIndex. Otherwise it just exactly aligns the Index data to the
        labels of the level in the MultiIndex.
        If ```keep_order == True```, the order of the data indexed by the
        MultiIndex will not be changed; otherwise, it will tie out
        with `other`.
        """"""
        from pandas.core.indexes.multi import MultiIndex
        def _get_leaf_sorter(labels):
            """"""
            Returns sorter for the inner most level while preserving the
            order of higher levels.
            """"""
            if labels[0].size == 0:
                return np.empty(0, dtype=""int64"")
            if len(labels) == 1:
                lab = ensure_int64(labels[0])
                sorter, _ = libalgos.groupsort_indexer(lab, 1 + lab.max())
                return sorter
            # find indexers of beginning of each set of
            # same-key labels w.r.t all but last level
            tic = labels[0][:-1] != labels[0][1:]
            for lab in labels[1:-1]:
                tic |= lab[:-1] != lab[1:]
            starts = np.hstack(([True], tic, [True])).nonzero()[0]
            lab = ensure_int64(labels[-1])
            return lib.get_level_sorter(lab, ensure_int64(starts))
        if isinstance(self, MultiIndex) and isinstance(other, MultiIndex):
            raise TypeError(""Join on level between two MultiIndex objects is ambiguous"")
        left, right = self, other
        flip_order = not isinstance(self, MultiIndex)
        if flip_order:
            left, right = right, left
            how = {""right"": ""left"", ""left"": ""right""}.get(how, how)
        level = left._get_level_number(level)
        old_level = left.levels[level]
        if not right.is_unique:
            raise NotImplementedError(
                ""Index._join_level on non-unique index is not implemented""
            )
        new_level, left_lev_indexer, right_lev_indexer = old_level.join(
            right, how=how, return_indexers=True
        )
        if left_lev_indexer is None:
            if keep_order or len(left) == 0:
                left_indexer = None
                join_index = left
            else:  # sort the leaves
                left_indexer = _get_leaf_sorter(left.codes[: level + 1])
                join_index = left[left_indexer]
        else:
            left_lev_indexer = ensure_int64(left_lev_indexer)
            rev_indexer = lib.get_reverse_indexer(left_lev_indexer, len(old_level))
            new_lev_codes = algos.take_nd(
                rev_indexer, left.codes[level], allow_fill=False
            )",[3]
"    if not fast:
        assert_equivalent(src_contents, dst_contents)
        assert_stable(src_contents, dst_contents, mode=mode)
    return dst_contents

def format_str(src_contents: str, *, mode: FileMode) -> FileContent:
    """"""Reformat a string and return new contents.
    `line_length` determines how many characters per line are allowed.
    """"""
    src_node = lib2to3_parse(src_contents.lstrip(), mode.target_versions)
    dst_contents = """"
    future_imports = get_future_imports(src_node)
    if mode.target_versions:
        versions = mode.target_versions
    else:
        versions = detect_target_versions(src_node)
    normalize_fmt_off(src_node)
    lines = LineGenerator(
        remove_u_prefix=""unicode_literals"" in future_imports
        or supports_feature(versions, Feature.UNICODE_LITERALS),
        is_pyi=mode.is_pyi,
        normalize_strings=mode.string_normalization,
    )
    elt = EmptyLineTracker(is_pyi=mode.is_pyi)
    empty_line = Line()
    after = 0
    for current_line in lines.visit(src_node):
        for _ in range(after):
            dst_contents += str(empty_line)
        before, after = elt.maybe_empty_lines(current_line)
        for _ in range(before):
            dst_contents += str(empty_line)
        for line in split_line(
            current_line,
            line_length=mode.line_length,
            supports_trailing_commas=supports_feature(versions, Feature.TRAILING_COMMA),
        ):
            dst_contents += str(line)
    return dst_contents

def decode_bytes(src: bytes) -> Tuple[FileContent, Encoding, NewLine]:
    """"""Return a tuple of (decoded_contents, encoding, newline).
    `newline` is either CRLF or LF but `decoded_contents` is decoded with
    universal newlines (i.e. only contains LF).
    """"""
    srcbuf = io.BytesIO(src)
    encoding, lines = tokenize.detect_encoding(srcbuf.readline)
    if not lines:
        return """", encoding, ""\n""
    newline = ""\r\n"" if b""\r\n"" == lines[0][-2:] else ""\n""
    srcbuf.seek(0)
    with io.TextIOWrapper(srcbuf, encoding) as tiow:
        return tiow.read(), encoding, newline

GRAMMARS = [
    pygram.python_grammar_no_print_statement_no_exec_statement,
    pygram.python_grammar_no_print_statement,
    pygram.python_grammar,
]

def get_grammars(target_versions: Set[TargetVersion]) -> List[Grammar]:
    if not target_versions:
        return GRAMMARS
    elif all(not version.is_python2() for version in target_versions):
        # Python 2-compatible code, so don't try Python 3 grammar.
        return [
            pygram.python_grammar_no_print_statement_no_exec_statement,
            pygram.python_grammar_no_print_statement,
        ]
    else:
        return [pygram.python_grammar]

def lib2to3_parse(src_txt: str, target_versions: Iterable[TargetVersion] = ()) -> Node:
    """"""Given a string with source, return the lib2to3 Node.""""""
    if src_txt[-1:] != ""\n"":
        src_txt += ""\n""
    for grammar in get_grammars(set(target_versions)):
        drv = driver.Driver(grammar, pytree.convert)
        try:
            result = drv.parse_string(src_txt, True)
            break
        except ParseError as pe:
            lineno, column = pe.context[1]
            lines = src_txt.splitlines()
            try:
                faulty_line = lines[lineno - 1]
            except IndexError:
                faulty_line = ""<line number missing in source>""
            exc = InvalidInput(f""Cannot parse: {lineno}:{column}: {faulty_line}"")
    else:
        raise exc from None
    if isinstance(result, Leaf):
        result = Node(syms.file_input, [result])
    return result

def lib2to3_unparse(node: Node) -> str:
    """"""Given a lib2to3 node, return its string representation.""""""
    code = str(node)
    return code

T = TypeVar(""T"")

class Visitor(Generic[T]):
    """"""Basic lib2to3 visitor that yields things of type `T` on `visit()`.""""""
    def visit(self, node: LN) -> Iterator[T]:
        """"""Main method to visit `node` and its children.
        It tries to find a `visit_*()` method for the given `node.type`, like
        `visit_simple_stmt` for Node objects or `visit_INDENT` for Leaf objects.
        If no dedicated `visit_*()` method is found, chooses `visit_default()`
        instead.",[78]
"    This class, when installed with an OpenerDirector, automatically adds
    the standard headers to every HTTP request and handles gzipped and
    deflated responses from web servers. If compression is to be avoided in
    a particular request, the original request in the program code only has
    to include the HTTP header ""Youtubedl-No-Compression"", which will be
    removed before making the real request.
    Part of this code was copied from:
    http://techknack.net/python-urllib2-handlers/
    Andrew Rowls, the author of that code, agreed to release it to the
    public domain.
    """"""
    @staticmethod
    def deflate(data):
        try:
            return zlib.decompress(data, -zlib.MAX_WBITS)
        except zlib.error:
            return zlib.decompress(data)
    @staticmethod
    def addinfourl_wrapper(stream, headers, url, code):
        if hasattr(compat_urllib_request.addinfourl, 'getcode'):
            return compat_urllib_request.addinfourl(stream, headers, url, code)
        ret = compat_urllib_request.addinfourl(stream, headers, url)
        ret.code = code
        return ret
    def http_request(self, req):
        for h, v in std_headers.items():
            if h not in req.headers:
                req.add_header(h, v)
        if 'Youtubedl-no-compression' in req.headers:
            if 'Accept-encoding' in req.headers:
                del req.headers['Accept-encoding']
            del req.headers['Youtubedl-no-compression']
        if 'Youtubedl-user-agent' in req.headers:
            if 'User-agent' in req.headers:
                del req.headers['User-agent']
            req.headers['User-agent'] = req.headers['Youtubedl-user-agent']
            del req.headers['Youtubedl-user-agent']
        if sys.version_info < (2, 7) and '#' in req.get_full_url():
            # Python 2.6 is brain-dead when it comes to fragments
            req._Request__original = req._Request__original.partition('#')[0]
            req._Request__r_type = req._Request__r_type.partition('#')[0]
        return req
    def http_response(self, req, resp):
        old_resp = resp
        # gzip
        if resp.headers.get('Content-encoding', '') == 'gzip':
            content = resp.read()
            gz = gzip.GzipFile(fileobj=io.BytesIO(content), mode='rb')
            try:
                uncompressed = io.BytesIO(gz.read())
            except IOError as original_ioerror:
                # There may be junk add the end of the file
                # See http://stackoverflow.com/q/4928560/35070 for details
                for i in range(1, 1024):
                    try:
                        gz = gzip.GzipFile(fileobj=io.BytesIO(content[:-i]), mode='rb')
                        uncompressed = io.BytesIO(gz.read())
                    except IOError:
                        continue
                    break
                else:
                    raise original_ioerror
            resp = self.addinfourl_wrapper(uncompressed, old_resp.headers, old_resp.url, old_resp.code)
            resp.msg = old_resp.msg
        # deflate
        if resp.headers.get('Content-encoding', '') == 'deflate':
            gz = io.BytesIO(self.deflate(resp.read()))
            resp = self.addinfourl_wrapper(gz, old_resp.headers, old_resp.url, old_resp.code)
            resp.msg = old_resp.msg
        return resp
    https_request = http_request
    https_response = http_response

def parse_iso8601(date_str, delimiter='T'):
    """""" Return a UNIX timestamp from the given date """"""
    if date_str is None:
        return None
    m = re.search(
        r'Z$| ?(?P<sign>\+|-)(?P<hours>[0-9]{2}):?(?P<minutes>[0-9]{2})$',
        date_str)
    if not m:
        timezone = datetime.timedelta()
    else:
        date_str = date_str[:-len(m.group(0))]
        if not m.group('sign'):
            timezone = datetime.timedelta()
        else:
            sign = 1 if m.group('sign') == '+' else -1
            timezone = datetime.timedelta(
                hours=sign * int(m.group('hours')),
                minutes=sign * int(m.group('minutes')))
    date_format =  '%Y-%m-%d{0}%H:%M:%S'.format(delimiter)
    dt = datetime.datetime.strptime(date_str, date_format) - timezone
    return calendar.timegm(dt.timetuple())

def unified_strdate(date_str):
    """"""Return a string with the date in the format YYYYMMDD""""""
    if date_str is None:
        return None
    upload_date = None
    #Replace commas
    date_str = date_str.replace(',', ' ')
    # %z (UTC offset) is only supported in python>=3.2
    date_str = re.sub(r' ?(\+|-)[0-9]{2}:?[0-9]{2}$', '', date_str)
    format_expressions = [
        '%d %B %Y',
        '%d %b %Y',
        '%B %d %Y',
        '%b %d %Y',
        '%b %dst %Y %I:%M%p',
        '%b %dnd %Y %I:%M%p',","[91, 104]"
"        # Raises
            ValueError: In case the generator yields
                data in an invalid format.
        """"""
        self._make_test_function()
        stateful_metric_indices = []
        if hasattr(self, 'metrics'):
            for i, m in enumerate(self.metrics):
                if isinstance(m, Layer) and m.stateful:
                    m.reset_states()
            stateful_metric_indices = [
                i for i, name in enumerate(self.metrics_names)
                if str(name) in self.stateful_metric_names]
        else:
            stateful_metric_indices = []
        steps_done = 0
        wait_time = 0.01
        outs_per_batch = []
        batch_sizes = []
        is_sequence = isinstance(generator, Sequence)
        if not is_sequence and use_multiprocessing and workers > 1:
            warnings.warn(
                UserWarning('Using a generator with `use_multiprocessing=True`'
                            ' and multiple workers may duplicate your data.'
                            ' Please consider using the`keras.utils.Sequence'
                            ' class.'))
        if steps is None:
            if is_sequence:
                steps = len(generator)
            else:
                raise ValueError('`steps=None` is only valid for a generator'
                                 ' based on the `keras.utils.Sequence` class.'
                                 ' Please specify `steps` or use the'
                                 ' `keras.utils.Sequence` class.')
        enqueuer = None
        try:
            if workers > 0:
                if is_sequence:
                    enqueuer = OrderedEnqueuer(generator,
                                               use_multiprocessing=use_multiprocessing)
                else:
                    enqueuer = GeneratorEnqueuer(generator,
                                                 use_multiprocessing=use_multiprocessing,
                                                 wait_time=wait_time)
                enqueuer.start(workers=workers, max_queue_size=max_queue_size)
                output_generator = enqueuer.get()
            else:
                if is_sequence:
                    output_generator = iter(generator)
                else:
                    output_generator = generator
            if verbose == 1:
                progbar = Progbar(target=steps)
            while steps_done < steps:
                generator_output = next(output_generator)
                if not hasattr(generator_output, '__len__'):
                    raise ValueError('Output of generator should be a tuple '
                                     '(x, y, sample_weight) '
                                     'or (x, y). Found: ' +
                                     str(generator_output))
                if len(generator_output) == 2:
                    x, y = generator_output
                    sample_weight = None
                elif len(generator_output) == 3:
                    x, y, sample_weight = generator_output
                else:
                    raise ValueError('Output of generator should be a tuple '
                                     '(x, y, sample_weight) '
                                     'or (x, y). Found: ' +
                                     str(generator_output))
                outs = self.test_on_batch(x, y, sample_weight=sample_weight)
                if not isinstance(outs, list):
                    outs = [outs]
                outs_per_batch.append(outs)
                if isinstance(x, list):
                    batch_size = x[0].shape[0]
                elif isinstance(x, dict):
                    batch_size = list(x.values())[0].shape[0]
                else:
                    batch_size = x.shape[0]
                if batch_size == 0:
                    raise ValueError('Received an empty batch. '
                                     'Batches should at least contain one item.')
                steps_done += 1
                batch_sizes.append(batch_size)
                if verbose == 1:
                    progbar.update(steps_done)
        finally:
            if enqueuer is not None:
                enqueuer.stop()
        averages = []
        for i in range(len(outs)):
            if i not in stateful_metric_indices:
                averages.append(np.average([out[i] for out in outs_per_batch],
                                           weights=batch_sizes))
            else:
                averages.append(float(outs_per_batch[-1][i]))
        if len(averages) == 1:
            return averages[0]
        return averages
    @interfaces.legacy_generator_methods_support
    def predict_generator(self, generator, steps=None,
                          max_queue_size=10,
                          workers=1,
                          use_multiprocessing=False,
                          verbose=0):
        """"""Generates predictions for the input samples from a data generator.
        The generator should return the same kind of data as accepted by
        `predict_on_batch`.
        # Arguments
            generator: Generator yielding batches of input samples
                or an instance of Sequence (keras.utils.Sequence)
                object in order to avoid duplicate data
                when using multiprocessing.
            steps: Total number of steps (batches of samples)",[80]
"        if isinstance(context, tuple) and len(context) > 0:
            func = context[0]
            if func is np.add:
                pass
            elif func is np.subtract:
                name = self.name
                left = context[1][0]
                right = context[1][1]
                if isinstance(left, PeriodIndex) and isinstance(right, PeriodIndex):
                    name = left.name if left.name == right.name else None
                    return Index(result, name=name)
                elif isinstance(left, Period) or isinstance(right, Period):
                    return Index(result, name=name)
            elif isinstance(func, np.ufunc):
                if ""M->M"" not in func.types:
                    msg = f""ufunc '{func.__name__}' not supported for the PeriodIndex""
                    # This should be TypeError, but TypeError cannot be raised
                    # from here because numpy catches.
                    raise ValueError(msg)
        if is_bool_dtype(result):
            return result
        # the result is object dtype array of Period
        # cannot pass _simple_new as it is
        return type(self)(result, freq=self.freq, name=self.name)
    def asof_locs(self, where, mask):
        """"""
        where : array of timestamps
        mask : array of booleans where data is not NA
        """"""
        where_idx = where
        if isinstance(where_idx, DatetimeIndex):
            where_idx = PeriodIndex(where_idx.values, freq=self.freq)
        locs = self._ndarray_values[mask].searchsorted(
            where_idx._ndarray_values, side=""right""
        )
        locs = np.where(locs > 0, locs - 1, 0)
        result = np.arange(len(self))[mask].take(locs)
        first = mask.argmax()
        result[
            (locs == 0) & (where_idx._ndarray_values < self._ndarray_values[first])
        ] = -1
        return result
    @Appender(_index_shared_docs[""astype""])
    def astype(self, dtype, copy=True, how=""start""):
        dtype = pandas_dtype(dtype)
        if is_datetime64_any_dtype(dtype):
            # 'how' is index-specific, isn't part of the EA interface.
            tz = getattr(dtype, ""tz"", None)
            return self.to_timestamp(how=how).tz_localize(tz)
        # TODO: should probably raise on `how` here, so we don't ignore it.
        return super().astype(dtype, copy=copy)
    @Substitution(klass=""PeriodIndex"")
    @Appender(_shared_docs[""searchsorted""])
    def searchsorted(self, value, side=""left"", sorter=None):
        if isinstance(value, Period):
            if value.freq != self.freq:
                raise raise_on_incompatible(self, value)
            value = value.ordinal
        elif isinstance(value, str):
            try:
                value = Period(value, freq=self.freq).ordinal
            except DateParseError:
                raise KeyError(f""Cannot interpret '{value}' as period"")
        return self._ndarray_values.searchsorted(value, side=side, sorter=sorter)
    @property
    def is_full(self) -> bool:
        """"""
        Returns True if this PeriodIndex is range-like in that all Periods
        between start and end are present, in order.
        """"""
        if len(self) == 0:
            return True
        if not self.is_monotonic:
            raise ValueError(""Index is not monotonic"")
        values = self.asi8
        return ((values[1:] - values[:-1]) < 2).all()
    @property
    def inferred_type(self) -> str:
        # b/c data is represented as ints make sure we can't have ambiguous
        # indexing
        return ""period""
    def get_value(self, series, key):
        """"""
        Fast lookup of value from 1-dimensional ndarray. Only use this if you
        know what you're doing
        """"""
        s = com.values_from_object(series)
        try:
            return com.maybe_box(self, super().get_value(s, key), series, key)
        except (KeyError, IndexError):
            if isinstance(key, str):
                asdt, parsed, reso = parse_time_string(key, self.freq)
                grp = resolution.Resolution.get_freq_group(reso)
                freqn = resolution.get_freq_group(self.freq)
                vals = self._ndarray_values
                # if our data is higher resolution than requested key, slice
                if grp < freqn:
                    iv = Period(asdt, freq=(grp, 1))
                    ord1 = iv.asfreq(self.freq, how=""S"").ordinal
                    ord2 = iv.asfreq(self.freq, how=""E"").ordinal
                    if ord2 < vals[0] or ord1 > vals[-1]:
                        raise KeyError(key)
                    pos = np.searchsorted(self._ndarray_values, [ord1, ord2])
                    key = slice(pos[0], pos[1] + 1)
                    return series[key]
                elif grp == freqn:
                    key = Period(asdt, freq=self.freq).ordinal
                    return com.maybe_box(","[65, 66, 67, 68, 71, 75]"
"                (self.task.task_id, status, error_message, [], []))
AbstractTaskProcess.register(ExternalTaskProcess)

class TaskProcess(AbstractTaskProcess):
    """""" Wrap all task execution in this class.
    Mainly for convenience since this is run in a separate process. """"""
    def _run_get_new_deps(self):
        task_gen = self.task.run()
        if not isinstance(task_gen, types.GeneratorType):
            return None
        next_send = None
        while True:
            try:
                if next_send is None:
                    requires = six.next(task_gen)
                else:
                    requires = task_gen.send(next_send)
            except StopIteration:
                return None
            new_req = flatten(requires)
            new_deps = [(t.task_module, t.task_family, t.to_str_params())
                        for t in new_req]
            if all(t.complete() for t in new_req):
                next_send = getpaths(requires)
            else:
                return new_deps
    def run(self):
        logger.info('[pid %s] Worker %s running   %s', os.getpid(), self.worker_id, self.task.task_id)
        if self.random_seed:
            # Need to have different random seeds if running in separate processes
            random.seed((os.getpid(), time.time()))
        status = FAILED
        error_message = ''
        missing = []
        new_deps = []
        try:
            # Verify that all the tasks are fulfilled!
            missing = [dep.task_id for dep in self.task.deps() if not dep.complete()]
            if missing:
                deps = 'dependency' if len(missing) == 1 else 'dependencies'
                raise RuntimeError('Unfulfilled %s at run time: %s' % (deps, ', '.join(missing)))
            self.task.trigger_event(Event.START, self.task)
            t0 = time.time()
            status = None
            try:
                new_deps = self._run_get_new_deps()
                if new_deps is None:
                    status = RUNNING
                else:
                    status = SUSPENDED
                    logger.info(
                        '[pid %s] Worker %s new requirements      %s',
                        os.getpid(), self.worker_id, self.task.task_id)
                    return
            finally:
                if status != SUSPENDED:
                    self.task.trigger_event(
                        Event.PROCESSING_TIME, self.task, time.time() - t0)
                    error_message = json.dumps(self.task.on_success())
                    logger.info('[pid %s] Worker %s done      %s', os.getpid(),
                                self.worker_id, self.task.task_id)
                    self.task.trigger_event(Event.SUCCESS, self.task)
                    status = DONE
        except KeyboardInterrupt:
            raise
        except BaseException as ex:
            status = FAILED
            logger.exception(""[pid %s] Worker %s failed    %s"", os.getpid(), self.worker_id, self.task)
            error_message = notifications.wrap_traceback(self.task.on_failure(ex))
            self.task.trigger_event(Event.FAILURE, self.task, ex)
            subject = ""Luigi: %s FAILED"" % self.task
            notifications.send_error_email(subject, error_message)
        finally:
            self.result_queue.put(
                (self.task.task_id, status, error_message, missing, new_deps))
AbstractTaskProcess.register(TaskProcess)

class SingleProcessPool(object):
    """"""
    Dummy process pool for using a single processor.
    Imitates the api of multiprocessing.Pool using single-processor equivalents.
    """"""
    def apply_async(self, function, args):
        return function(*args)

class DequeQueue(collections.deque):
    """"""
    deque wrapper implementing the Queue interface.
    """"""
    put = collections.deque.append
    get = collections.deque.pop

class AsyncCompletionException(Exception):
    """"""
    Exception indicating that something went wrong with checking complete.
    """"""
    def __init__(self, trace):
        self.trace = trace

class TracebackWrapper(object):
    """"""
    Class to wrap tracebacks so we can know they're not just strings.
    """"""
    def __init__(self, trace):
        self.trace = trace
","[54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72]"
"    data = {}
    data.update(zip(rownames, index))
    data.update(zip(colnames, columns))
    if values is None and aggfunc is not None:
        raise ValueError(""aggfunc cannot be used without values."")
    if values is not None and aggfunc is None:
        raise ValueError(""values cannot be used without an aggfunc."")
    from pandas import DataFrame
    df = DataFrame(data, index=common_idx)
    if values is None:
        df[""__dummy__""] = 0
        kwargs = {""aggfunc"": len, ""fill_value"": 0}
    else:
        df[""__dummy__""] = values
        kwargs = {""aggfunc"": aggfunc}
    table = df.pivot_table(
        ""__dummy__"",
        index=rownames,
        columns=colnames,
        margins=margins,
        margins_name=margins_name,
        dropna=dropna,
        **kwargs
    )
    # Post-process
    if normalize is not False:
        table = _normalize(
            table, normalize=normalize, margins=margins, margins_name=margins_name
        )
    return table

def _normalize(table, normalize, margins, margins_name=""All""):
    if not isinstance(normalize, (bool, str)):
        axis_subs = {0: ""index"", 1: ""columns""}
        try:
            normalize = axis_subs[normalize]
        except KeyError:
            raise ValueError(""Not a valid normalize argument"")
    if margins is False:
        # Actual Normalizations
        normalizers = {
            ""all"": lambda x: x / x.sum(axis=1).sum(axis=0),
            ""columns"": lambda x: x / x.sum(),
            ""index"": lambda x: x.div(x.sum(axis=1), axis=0),
        }
        normalizers[True] = normalizers[""all""]
        try:
            f = normalizers[normalize]
        except KeyError:
            raise ValueError(""Not a valid normalize argument"")
        table = f(table)
        table = table.fillna(0)
    elif margins is True:
        column_margin = table.loc[:, margins_name].drop(margins_name)
        index_margin = table.loc[margins_name, :].drop(margins_name)
        table = table.drop(margins_name, axis=1).drop(margins_name)
        # to keep index and columns names
        table_index_names = table.index.names
        table_columns_names = table.columns.names
        # Normalize core
        table = _normalize(table, normalize=normalize, margins=False)
        # Fix Margins
        if normalize == ""columns"":
            column_margin = column_margin / column_margin.sum()
            table = concat([table, column_margin], axis=1)
            table = table.fillna(0)
        elif normalize == ""index"":
            index_margin = index_margin / index_margin.sum()
            table = table.append(index_margin)
            table = table.fillna(0)
        elif normalize == ""all"" or normalize is True:
            column_margin = column_margin / column_margin.sum()
            index_margin = index_margin / index_margin.sum()
            index_margin.loc[margins_name] = 1
            table = concat([table, column_margin], axis=1)
            table = table.append(index_margin)
            table = table.fillna(0)
        else:
            raise ValueError(""Not a valid normalize argument"")
        table.index.names = table_index_names
        table.columns.names = table_columns_names
    else:
        raise ValueError(""Not a valid margins argument"")
    return table

def _get_names(arrs, names, prefix=""row""):
    if names is None:
        names = []
        for i, arr in enumerate(arrs):
            if isinstance(arr, ABCSeries) and arr.name is not None:
                names.append(arr.name)
            else:
                names.append(""{prefix}_{i}"".format(prefix=prefix, i=i))
    else:
        if len(names) != len(arrs):
            raise AssertionError(""arrays and names must have the same length"")
        if not isinstance(names, list):
            names = list(names)
","[69, 70, 71, 72, 74, 75, 103, 104, 105]"
"    Do nothing otherwise.
    A left- or right-hand split is based on a pair of brackets. Content before
    (and including) the opening bracket is left on one line, content inside the
    brackets is put on a separate line, and finally content starting with and
    following the closing bracket is put on a separate line.
    Those are called `head`, `body`, and `tail`, respectively. If the split
    produced the same line (all content in `head`) or ended up with an empty `body`
    and the `tail` is just the closing bracket, then it's considered failed.
    """"""
    tail_len = len(str(tail).strip())
    if not body:
        if tail_len == 0:
            raise CannotSplit(""Splitting brackets produced the same line"")
        elif tail_len < 3:
            raise CannotSplit(
                f""Splitting brackets on an empty body to save ""
                f""{tail_len} characters is not worth it""
            )

def bracket_split_build_line(
    leaves: List[Leaf], original: Line, opening_bracket: Leaf, *, is_body: bool = False
) -> Line:
    """"""Return a new line with given `leaves` and respective comments from `original`.
    If `is_body` is True, the result line is one-indented inside brackets and as such
    has its first leaf's prefix normalized and a trailing comma added when expected.
    """"""
    result = Line(depth=original.depth)
    if is_body:
        result.inside_brackets = True
        result.depth += 1
        if leaves:
            # Since body is a new indent level, remove spurious leading whitespace.
            normalize_prefix(leaves[0], inside_brackets=True)
            # Ensure a trailing comma when expected.
            if original.is_import:
                if leaves[-1].type != token.COMMA:
                    leaves.append(Leaf(token.COMMA, "",""))
    # Populate the line
    for leaf in leaves:
        result.append(leaf, preformatted=True)
        for comment_after in original.comments_after(leaf):
            result.append(comment_after, preformatted=True)
    if is_body:
        result.should_explode = should_explode(result, opening_bracket)
    return result

def dont_increase_indentation(split_func: SplitFunc) -> SplitFunc:
    """"""Normalize prefix of the first leaf in every line returned by `split_func`.
    This is a decorator over relevant split functions.
    """"""
    @wraps(split_func)
    def split_wrapper(line: Line, features: Collection[Feature] = ()) -> Iterator[Line]:
        for l in split_func(line, features):
            normalize_prefix(l.leaves[0], inside_brackets=True)
            yield l
    return split_wrapper

@dont_increase_indentation
def delimiter_split(line: Line, features: Collection[Feature] = ()) -> Iterator[Line]:
    """"""Split according to delimiters of the highest priority.
    If the appropriate Features are given, the split will add trailing commas
    also in function signatures and calls that contain `*` and `**`.
    """"""
    try:
        last_leaf = line.leaves[-1]
    except IndexError:
        raise CannotSplit(""Line empty"")
    bt = line.bracket_tracker
    try:
        delimiter_priority = bt.max_delimiter_priority(exclude={id(last_leaf)})
    except ValueError:
        raise CannotSplit(""No delimiters found"")
    if delimiter_priority == DOT_PRIORITY:
        if bt.delimiter_count_with_priority(delimiter_priority) == 1:
            raise CannotSplit(""Splitting a single attribute from its owner looks wrong"")
    current_line = Line(depth=line.depth, inside_brackets=line.inside_brackets)
    lowest_depth = sys.maxsize
    trailing_comma_safe = True
    def append_to_line(leaf: Leaf) -> Iterator[Line]:
        """"""Append `leaf` to current line or to new line if appending impossible.""""""
        nonlocal current_line
        try:
            current_line.append_safe(leaf, preformatted=True)
        except ValueError:
            yield current_line
            current_line = Line(depth=line.depth, inside_brackets=line.inside_brackets)
            current_line.append(leaf)
    for leaf in line.leaves:
        yield from append_to_line(leaf)
        for comment_after in line.comments_after(leaf):
            yield from append_to_line(comment_after)
        lowest_depth = min(lowest_depth, leaf.bracket_depth)
        if leaf.bracket_depth == lowest_depth:
            if is_vararg(leaf, within={syms.typedargslist}):
                trailing_comma_safe = (
                    trailing_comma_safe and Feature.TRAILING_COMMA_IN_DEF in features
                )
            elif is_vararg(leaf, within={syms.arglist, syms.argument}):
                trailing_comma_safe = (
                    trailing_comma_safe and Feature.TRAILING_COMMA_IN_CALL in features
                )
        leaf_priority = bt.delimiters.get(id(leaf))
        if leaf_priority == delimiter_priority:
            yield current_line
            current_line = Line(depth=line.depth, inside_brackets=line.inside_brackets)
    if current_line:","[40, 41]"
"                if mask.any():
                    codes = codes.copy()
                    if isna(value):
                        codes[mask] = -1
                    else:
                        codes[mask] = self.categories.get_loc(value)
            else:
                raise TypeError(
                    f""'value' parameter must be a scalar, dict ""
                    f""or Series, but you passed a {type(value).__name__}""
                )
        return self._constructor(codes, dtype=self.dtype, fastpath=True)
    def take(self: _T, indexer, allow_fill: bool = False, fill_value=None) -> _T:
        """"""
        Take elements from the Categorical.
        Parameters
        ----------
        indexer : sequence of int
            The indices in `self` to take. The meaning of negative values in
            `indexer` depends on the value of `allow_fill`.
        allow_fill : bool, default False
            How to handle negative values in `indexer`.
            * False: negative values in `indices` indicate positional indices
              from the right. This is similar to
              :func:`numpy.take`.
            * True: negative values in `indices` indicate missing values
              (the default). These values are set to `fill_value`. Any other
              other negative values raise a ``ValueError``.
            .. versionchanged:: 1.0.0
               Default value changed from ``True`` to ``False``.
        fill_value : object
            The value to use for `indices` that are missing (-1), when
            ``allow_fill=True``. This should be the category, i.e. a value
            in ``self.categories``, not a code.
        Returns
        -------
        Categorical
            This Categorical will have the same categories and ordered as
            `self`.
        See Also
        --------
        Series.take : Similar method for Series.
        numpy.ndarray.take : Similar method for NumPy arrays.
        Examples
        --------
        >>> cat = pd.Categorical(['a', 'a', 'b'])
        >>> cat
        [a, a, b]
        Categories (2, object): [a, b]
        Specify ``allow_fill==False`` to have negative indices mean indexing
        from the right.
        >>> cat.take([0, -1, -2], allow_fill=False)
        [a, b, a]
        Categories (2, object): [a, b]
        With ``allow_fill=True``, indices equal to ``-1`` mean ""missing""
        values that should be filled with the `fill_value`, which is
        ``np.nan`` by default.
        >>> cat.take([0, -1, -1], allow_fill=True)
        [a, NaN, NaN]
        Categories (2, object): [a, b]
        The fill value can be specified.
        >>> cat.take([0, -1, -1], allow_fill=True, fill_value='a')
        [a, a, a]
        Categories (2, object): [a, b]
        Specifying a fill value that's not in ``self.categories``
        will raise a ``ValueError``.
        """"""
        return NDArrayBackedExtensionArray.take(
            self, indexer, allow_fill=allow_fill, fill_value=fill_value
        )
    # ------------------------------------------------------------------
    # NDArrayBackedExtensionArray compat
    @property
    def _ndarray(self) -> np.ndarray:
        return self._codes
    def _from_backing_data(self, arr: np.ndarray) -> ""Categorical"":
        return self._constructor(arr, dtype=self.dtype, fastpath=True)
    # ------------------------------------------------------------------
    def take_nd(self, indexer, allow_fill: bool = False, fill_value=None):
        # GH#27745 deprecate alias that other EAs dont have
        warn(
            ""Categorical.take_nd is deprecated, use Categorical.take instead"",
            FutureWarning,
            stacklevel=2,
        )
        return self.take(indexer, allow_fill=allow_fill, fill_value=fill_value)
    def __iter__(self):
        """"""
        Returns an Iterator over the values of this Categorical.
        """"""
        return iter(self._internal_get_values().tolist())
    def __contains__(self, key) -> bool:
        """"""
        Returns True if `key` is in this Categorical.
        """"""
        # if key is a NaN, check if any NaN is in self.
        if is_scalar(key) and isna(key):
            return self.isna().any()
        return contains(self, key, container=self._codes)
",[122]
"        return self.validate()[0]
    def validate(self):
        # Validate the rule type
        if self.rule_type not in VALID_TYPES:
            return False, ""Rule type, "" + self.rule_type + "", is not valid in rule "" + self.line
        # Validate the rule control
        if isinstance(self._control, str) and self.rule_control not in PamdRule.valid_simple_controls:
            return False, ""Rule control, "" + self.rule_control + "", is not valid in rule "" + self.line
        elif isinstance(self._control, list):
            for control in self._control:
                value, action = control.split(""="")
                if value not in PamdRule.valid_control_values:
                    return False, ""Rule control value, "" + value + "", is not valid in rule "" + self.line
                if action not in PamdRule.valid_control_actions and not PamdRule.is_action_unsigned_int(action):
                    return False, ""Rule control action, "" + action + "", is not valid in rule "" + self.line
        # TODO: Validate path
        return True, ""Rule is valid "" + self.line

# PamdService encapsulates an entire service and contains one or more rules.  It seems the best way is to do this
# as a doubly linked list.
class PamdService(object):
    def __init__(self, content):
        self._head = None
        self._tail = None
        for line in content.splitlines():
            if line.lstrip().startswith('#'):
                pamd_line = PamdComment(line)
            elif line.lstrip().startswith('@include'):
                pamd_line = PamdInclude(line)
            elif line == '':
                pamd_line = PamdLine(line)
            else:
                pamd_line = PamdRule.rule_from_string(line)
            self.append(pamd_line)
    def append(self, pamd_line):
        if self._head is None:
            self._head = self._tail = pamd_line
        else:
            pamd_line.prev = self._tail
            pamd_line.next = None
            self._tail.next = pamd_line
            self._tail = pamd_line
    def remove(self, rule_type, rule_control, rule_path):
        current_line = self._head
        changed = 0
        while current_line is not None:
            if current_line.matches(rule_type, rule_control, rule_path):
                if current_line.prev is not None:
                    current_line.prev.next = current_line.next
                    current_line.next.prev = current_line.prev
                else:
                    self._head = current_line.next
                    current_line.next.prev = None
                changed += 1
            current_line = current_line.next
        return changed
    def get(self, rule_type, rule_control, rule_path):
        lines = []
        current_line = self._head
        while current_line is not None:
            if isinstance(current_line, PamdRule) and current_line.matches(rule_type, rule_control, rule_path):
                lines.append(current_line)
            current_line = current_line.next
        return lines
    def has_rule(self, rule_type, rule_control, rule_path):
        if self.get(rule_type, rule_control, rule_path):
            return True
        return False
    def update_rule(self, rule_type, rule_control, rule_path,
                    new_type=None, new_control=None, new_path=None, new_args=None):
        # Get a list of rules we want to change
        rules_to_find = self.get(rule_type, rule_control, rule_path)
        new_args = parse_module_arguments(new_args)
        changes = 0
        for current_rule in rules_to_find:
            rule_changed = False
            if new_type:
                if(current_rule.rule_type != new_type):
                    rule_changed = True
                    current_rule.rule_type = new_type
            if new_control:
                if(current_rule.rule_control != new_control):
                    rule_changed = True
                    current_rule.rule_control = new_control
            if new_path:
                if(current_rule.rule_path != new_path):
                    rule_changed = True
                    current_rule.rule_path = new_path
            if new_args:
                if(current_rule.rule_args != new_args):
                    rule_changed = True
                    current_rule.rule_args = new_args
            if rule_changed:
                changes += 1
        return changes
    def insert_before(self, rule_type, rule_control, rule_path,
                      new_type=None, new_control=None, new_path=None, new_args=None):
        # Get a list of rules we want to change
        rules_to_find = self.get(rule_type, rule_control, rule_path)
        changes = 0
        # There are two cases to consider.
        # 1. The new rule doesn't exist before the existing rule
        # 2. The new rule exists
        for current_rule in rules_to_find:
            # Create a new rule",[58]
"        self.signals = crawler.signals
        self.logformatter = crawler.logformatter
        self.slot = None
        self.spider = None
        self.running = False
        self.paused = False
        self.scheduler_cls = load_object(self.settings['SCHEDULER'])
        downloader_cls = load_object(self.settings['DOWNLOADER'])
        self.downloader = downloader_cls(crawler)
        self.scraper = Scraper(crawler)
        self._spider_closed_callback = spider_closed_callback
    @defer.inlineCallbacks
    def start(self):
        """"""Start the execution engine""""""
        assert not self.running, ""Engine already running""
        self.start_time = time()
        yield self.signals.send_catch_log_deferred(signal=signals.engine_started)
        self.running = True
        self._closewait = defer.Deferred()
        yield self._closewait
    def stop(self):
        """"""Stop the execution engine gracefully""""""
        assert self.running, ""Engine not running""
        self.running = False
        dfd = self._close_all_spiders()
        return dfd.addBoth(lambda _: self._finish_stopping_engine())
    def pause(self):
        """"""Pause the execution engine""""""
        self.paused = True
    def unpause(self):
        """"""Resume the execution engine""""""
        self.paused = False
    def _next_request(self, spider):
        slot = self.slot
        if not slot:
            return
        if self.paused:
            slot.nextcall.schedule(5)
            return
        while not self._needs_backout(spider):
            if not self._next_request_from_scheduler(spider):
                break
        if slot.start_requests and not self._needs_backout(spider):
            try:
                request = next(slot.start_requests)
            except StopIteration:
                slot.start_requests = None
            except Exception:
                slot.start_requests = None
                logger.error('Error while obtaining start requests',
                             exc_info=True, extra={'spider': spider})
            else:
                self.crawl(request, spider)
        if self.spider_is_idle(spider) and slot.close_if_idle:
            self._spider_idle(spider)
    def _needs_backout(self, spider):
        slot = self.slot
        return not self.running \
            or slot.closing \
            or self.downloader.needs_backout() \
            or self.scraper.slot.needs_backout()
    def _next_request_from_scheduler(self, spider):
        slot = self.slot
        request = slot.scheduler.next_request()
        if not request:
            return
        d = self._download(request, spider)
        d.addBoth(self._handle_downloader_output, request, spider)
        d.addErrback(lambda f: logger.info('Error while handling downloader output',
                                           extra={'spider': spider, 'failure': f}))
        d.addBoth(lambda _: slot.remove_request(request))
        d.addErrback(lambda f: logger.info('Error while removing request from slot',
                                           extra={'spider': spider, 'failure': f}))
        d.addBoth(lambda _: slot.nextcall.schedule())
        d.addErrback(lambda f: logger.info('Error while scheduling new request',
                                           extra={'spider': spider, 'failure': f}))
        return d
    def _handle_downloader_output(self, response, request, spider):
        assert isinstance(response, (Request, Response, Failure)), response
        # downloader middleware can return requests (for example, redirects)
        if isinstance(response, Request):
            self.crawl(response, spider)
            return
        # response is a Response or Failure
        d = self.scraper.enqueue_scrape(response, request, spider)
        d.addErrback(lambda f: logger.error('Error while enqueuing downloader output',
                                            extra={'spider': spider, 'failure': f}))
        return d
    def spider_is_idle(self, spider):
        scraper_idle = self.scraper.slot.is_idle()
        pending = self.slot.scheduler.has_pending_requests()
        downloading = bool(self.downloader.active)
        pending_start_requests = self.slot.start_requests is not None
        idle = scraper_idle and not (pending or downloading or pending_start_requests)
        return idle
    @property
    def open_spiders(self):
        return [self.spider] if self.spider else []
    def has_capacity(self):
        """"""Does the engine have capacity to handle more spiders""""""
        return not bool(self.slot)
    def crawl(self, request, spider):
        assert spider in self.open_spiders, \
            ""Spider %r not opened when crawling: %s"" % (spider.name, request)
        self.schedule(request, spider)
        self.slot.nextcall.schedule()
    def schedule(self, request, spider):
        self.signals.send_catch_log(signal=signals.request_scheduled,
                request=request, spider=spider)
        if not self.slot.scheduler.enqueue_request(request):","[80, 83, 86, 98]"
"                # try to python agg
                if alt is None:
                    # we cannot perform the operation
                    # in an alternate way, exclude the block
                    assert how == ""ohlc""
                    deleted_items.append(locs)
                    continue
                # call our grouper again with only this block
                obj = self.obj[data.items[locs]]
                if obj.shape[1] == 1:
                    # Avoid call to self.values that can occur in DataFrame
                    #  reductions; see GH#28949
                    obj = obj.iloc[:, 0]
                s = get_groupby(obj, self.grouper)
                try:
                    result = s.aggregate(lambda x: alt(x, axis=self.axis))
                except TypeError:
                    # we may have an exception in trying to aggregate
                    # continue and exclude the block
                    deleted_items.append(locs)
                    continue
                else:
                    result = cast(DataFrame, result)
                    # unwrap DataFrame to get array
                    if len(result._data.blocks) != 1:
                        # We've split an object block! Everything we've assumed
                        # about a single block input returning a single block output
                        # is a lie. To keep the code-path for the typical non-split case
                        # clean, we choose to clean up this mess later on.
                        split_items.append(locs)
                        split_frames.append(result)
                        continue
                    assert len(result._data.blocks) == 1
                    result = result._data.blocks[0].values
                    if isinstance(result, np.ndarray) and result.ndim == 1:
                        result = result.reshape(1, -1)
            assert not isinstance(result, DataFrame)
            if result is not no_result:
                # see if we can cast the block back to the original dtype
                result = maybe_downcast_numeric(result, block.dtype)
                if block.is_extension and isinstance(result, np.ndarray):
                    # e.g. block.values was an IntegerArray
                    # (1, N) case can occur if block.values was Categorical
                    #  and result is ndarray[object]
                    assert result.ndim == 1 or result.shape[0] == 1
                    try:
                        # Cast back if feasible
                        result = type(block.values)._from_sequence(
                            result.ravel(), dtype=block.values.dtype
                        )
                    except ValueError:
                        # reshape to be valid for non-Extension Block
                        result = result.reshape(1, -1)
                agg_block: Block = block.make_block(result)
            new_items.append(locs)
            agg_blocks.append(agg_block)
        if not (agg_blocks or split_frames):
            raise DataError(""No numeric types to aggregate"")
        if split_items:
            # Clean up the mess left over from split blocks.
            for locs, result in zip(split_items, split_frames):
                assert len(locs) == result.shape[1]
                for i, loc in enumerate(locs):
                    new_items.append(np.array([loc], dtype=locs.dtype))
                    agg_blocks.append(result.iloc[:, [i]]._data.blocks[0])
        # reset the locs in the blocks to correspond to our
        # current ordering
        indexer = np.concatenate(new_items)
        agg_items = data.items.take(np.sort(indexer))
        if deleted_items:
            # we need to adjust the indexer to account for the
            # items we have removed
            # really should be done in internals :<
            deleted = np.concatenate(deleted_items)
            ai = np.arange(len(data))
            mask = np.zeros(len(data))
            mask[deleted] = 1
            indexer = (ai - mask.cumsum())[indexer]
        offset = 0
        for blk in agg_blocks:
            loc = len(blk.mgr_locs)
            blk.mgr_locs = indexer[offset : (offset + loc)]
            offset += loc
        return agg_blocks, agg_items
    def _aggregate_frame(self, func, *args, **kwargs) -> DataFrame:
        if self.grouper.nkeys != 1:
            raise AssertionError(""Number of keys must be 1"")
        axis = self.axis
        obj = self._obj_with_exclusions
        result: Dict[Union[int, str], Union[NDFrame, np.ndarray]] = {}
        if axis != obj._info_axis_number:
            for name, data in self:
                fres = func(data, *args, **kwargs)
                result[name] = fres
        else:
            for name in self.indices:
                data = self.get_group(name, obj=obj)
                fres = func(data, *args, **kwargs)
                result[name] = fres
        return self._wrap_frame_output(result, obj)
    def _aggregate_item_by_item(self, func, *args, **kwargs) -> DataFrame:
        # only for axis==0
        obj = self._obj_with_exclusions
        result: Dict[Union[int, str], NDFrame] = {}",[57]
"            matching will have precedence in case of a size matching with *x*
            and *y*.
            If you wish to specify a single color for all points
            prefer the *color* keyword argument.
            Defaults to `None`. In that case the marker color is determined
            by the value of *color*, *facecolor* or *facecolors*. In case
            those are not specified or `None`, the marker color is determined
            by the next color of the ``Axes``' current ""shape and fill"" color
            cycle. This cycle defaults to :rc:`axes.prop_cycle`.
        marker : `~.markers.MarkerStyle`, default: :rc:`scatter.marker`
            The marker style. *marker* can be either an instance of the class
            or the text shorthand for a particular marker.
            See :mod:`matplotlib.markers` for more information about marker
            styles.
        cmap : str or `~matplotlib.colors.Colormap`, default: :rc:`image.cmap`
            A `.Colormap` instance or registered colormap name. *cmap* is only
            used if *c* is an array of floats.
        norm : `~matplotlib.colors.Normalize`, default: None
            If *c* is an array of floats, *norm* is used to scale the color
            data, *c*, in the range 0 to 1, in order to map into the colormap
            *cmap*.
            If *None*, use the default `.colors.Normalize`.
        vmin, vmax : float, default: None
            *vmin* and *vmax* are used in conjunction with the default norm to
            map the color array *c* to the colormap *cmap*. If None, the
            respective min and max of the color array is used.
            It is deprecated to use *vmin*/*vmax* when *norm* is given.
        alpha : float, default: None
            The alpha blending value, between 0 (transparent) and 1 (opaque).
        linewidths : float or array-like, default: :rc:`lines.linewidth`
            The linewidth of the marker edges. Note: The default *edgecolors*
            is 'face'. You may want to change this as well.
        edgecolors : {'face', 'none', *None*} or color or sequence of color, \
default: :rc:`scatter.edgecolors`
            The edge color of the marker. Possible values:
            - 'face': The edge color will always be the same as the face color.
            - 'none': No patch boundary will be drawn.
            - A color or sequence of colors.
            For non-filled markers, the *edgecolors* kwarg is ignored and
            forced to 'face' internally.
        plotnonfinite : bool, default: False
            Set to plot points with nonfinite *c*, in conjunction with
            `~matplotlib.colors.Colormap.set_bad`.
        Returns
        -------
        `~matplotlib.collections.PathCollection`
        Other Parameters
        ----------------
        **kwargs : `~matplotlib.collections.Collection` properties
        See Also
        --------
        plot : To plot scatter plots when markers are identical in size and
            color.
        Notes
        -----
        * The `.plot` function will be faster for scatterplots where markers
          don't vary in size or color.
        * Any or all of *x*, *y*, *s*, and *c* may be masked arrays, in which
          case all masks will be combined and only unmasked points will be
          plotted.
        * Fundamentally, scatter works with 1-D arrays; *x*, *y*, *s*, and *c*
          may be input as N-D arrays, but within scatter they will be
          flattened. The exception is *c*, which will be flattened only if its
          size matches the size of *x* and *y*.
        """"""
        # Process **kwargs to handle aliases, conflicts with explicit kwargs:
        self._process_unit_info(xdata=x, ydata=y, kwargs=kwargs)
        x = self.convert_xunits(x)
        y = self.convert_yunits(y)
        # np.ma.ravel yields an ndarray, not a masked array,
        # unless its argument is a masked array.
        x = np.ma.ravel(x)
        y = np.ma.ravel(y)
        if x.size != y.size:
            raise ValueError(""x and y must be the same size"")
        if s is None:
            s = (20 if rcParams['_internal.classic_mode'] else
                 rcParams['lines.markersize'] ** 2.0)
        s = np.ma.ravel(s)
        if len(s) not in (1, x.size):
            raise ValueError(""s must be a scalar, or the same size as x and y"")
        c, colors, edgecolors = \
            self._parse_scatter_color_args(
                c, edgecolors, kwargs, x.size,
                get_next_color_func=self._get_patches_for_fill.get_next_color)
        if plotnonfinite and colors is None:
            c = np.ma.masked_invalid(c)
            x, y, s, edgecolors, linewidths = \
                cbook._combine_masks(x, y, s, edgecolors, linewidths)
        else:
            x, y, s, c, colors, edgecolors, linewidths = \
                cbook._combine_masks(
                    x, y, s, c, colors, edgecolors, linewidths)
        scales = s   # Renamed for readability below.
        # load default marker from rcParams
        if marker is None:
            marker = rcParams['scatter.marker']
        if isinstance(marker, mmarkers.MarkerStyle):
            marker_obj = marker
        else:","[49, 50]"
"                self._engine.get_pad_indexer
                if method == ""pad""
                else self._engine.get_backfill_indexer
            )
            indexer = method(target._ndarray_values, limit)
        else:
            indexer = self._get_fill_indexer_searchsorted(target, method, limit)
        if tolerance is not None:
            indexer = self._filter_indexer_tolerance(
                target._ndarray_values, indexer, tolerance
            )
        return indexer
    def _get_fill_indexer_searchsorted(self, target, method, limit=None):
        """"""
        Fallback pad/backfill get_indexer that works for monotonic decreasing
        indexes and non-monotonic targets.
        """"""
        if limit is not None:
            raise ValueError(
                f""limit argument for {repr(method)} method only well-defined ""
                ""if index and target are monotonic""
            )
        side = ""left"" if method == ""pad"" else ""right""
        # find exact matches first (this simplifies the algorithm)
        indexer = self.get_indexer(target)
        nonexact = indexer == -1
        indexer[nonexact] = self._searchsorted_monotonic(target[nonexact], side)
        if side == ""left"":
            # searchsorted returns ""indices into a sorted array such that,
            # if the corresponding elements in v were inserted before the
            # indices, the order of a would be preserved"".
            # Thus, we need to subtract 1 to find values to the left.
            indexer[nonexact] -= 1
            # This also mapped not found values (values of 0 from
            # np.searchsorted) to -1, which conveniently is also our
            # sentinel for missing values
        else:
            # Mark indices to the right of the largest value as not found
            indexer[indexer == len(self)] = -1
        return indexer
    def _get_nearest_indexer(self, target, limit, tolerance):
        """"""
        Get the indexer for the nearest index labels; requires an index with
        values that can be subtracted from each other (e.g., not strings or
        tuples).
        """"""
        left_indexer = self.get_indexer(target, ""pad"", limit=limit)
        right_indexer = self.get_indexer(target, ""backfill"", limit=limit)
        target = np.asarray(target)
        left_distances = abs(self.values[left_indexer] - target)
        right_distances = abs(self.values[right_indexer] - target)
        op = operator.lt if self.is_monotonic_increasing else operator.le
        indexer = np.where(
            op(left_distances, right_distances) | (right_indexer == -1),
            left_indexer,
            right_indexer,
        )
        if tolerance is not None:
            indexer = self._filter_indexer_tolerance(target, indexer, tolerance)
        return indexer
    def _filter_indexer_tolerance(self, target, indexer, tolerance):
        distance = abs(self.values[indexer] - target)
        indexer = np.where(distance <= tolerance, indexer, -1)
        return indexer
    # --------------------------------------------------------------------
    # Indexer Conversion Methods
    _index_shared_docs[
        ""_convert_scalar_indexer""
    ] = """"""
        Convert a scalar indexer.
        Parameters
        ----------
        key : label of the slice bound
        kind : {'ix', 'loc', 'getitem', 'iloc'} or None
    """"""
    @Appender(_index_shared_docs[""_convert_scalar_indexer""])
    def _convert_scalar_indexer(self, key, kind=None):
        assert kind in [""ix"", ""loc"", ""getitem"", ""iloc"", None]
        if kind == ""iloc"":
            return self._validate_indexer(""positional"", key, kind)
        if len(self) and not isinstance(self, ABCMultiIndex):
            # we can raise here if we are definitive that this
            # is positional indexing (eg. .ix on with a float)
            # or label indexing if we are using a type able
            # to be represented in the index
            if kind in [""getitem"", ""ix""] and is_float(key):
                if not self.is_floating():
                    return self._invalid_indexer(""label"", key)
            elif kind in [""loc""] and is_float(key):
                # we want to raise KeyError on string/mixed here
                # technically we *could* raise a TypeError
                # on anything but mixed though
                if self.inferred_type not in [
                    ""floating"",
                    ""mixed-integer-float"",
                    ""integer-na"",
                    ""string"",
                    ""unicode"",
                    ""mixed"",
                ]:
                    return self._invalid_indexer(""label"", key)
            elif kind in [""loc""] and is_integer(key):
                if not self.holds_integer():
                    return self._invalid_indexer(""label"", key)
        return key
    _index_shared_docs[
        ""_convert_slice_indexer""","[117, 121]"
"        ...                    'alpha-2': [""IT"", ""FR"", ""MT"", ""MV"", ""BN"",
        ...                                ""IS"", ""NR"", ""TV"", ""AI""]},
        ...                   index=[""Italy"", ""France"", ""Malta"",
        ...                          ""Maldives"", ""Brunei"", ""Iceland"",
        ...                          ""Nauru"", ""Tuvalu"", ""Anguilla""])
        >>> df
                  population      GDP alpha-2
        Italy       59000000  1937894      IT
        France      65000000  2583560      FR
        Malta         434000    12011      MT
        Maldives      434000     4520      MV
        Brunei        434000    12128      BN
        Iceland       337000    17036      IS
        Nauru          11300      182      NR
        Tuvalu         11300       38      TV
        Anguilla       11300      311      AI
        In the following example, we will use ``nsmallest`` to select the
        three rows having the smallest values in column ""a"".
        >>> df.nsmallest(3, 'population')
                  population  GDP alpha-2
        Nauru          11300  182      NR
        Tuvalu         11300   38      TV
        Anguilla       11300  311      AI
        When using ``keep='last'``, ties are resolved in reverse order:
        >>> df.nsmallest(3, 'population', keep='last')
                  population  GDP alpha-2
        Anguilla       11300  311      AI
        Tuvalu         11300   38      TV
        Nauru          11300  182      NR
        When using ``keep='all'``, all duplicate items are maintained:
        >>> df.nsmallest(3, 'population', keep='all')
                  population  GDP alpha-2
        Nauru          11300  182      NR
        Tuvalu         11300   38      TV
        Anguilla       11300  311      AI
        To order by the largest values in column ""a"" and then ""c"", we can
        specify multiple columns like in the next example.
        >>> df.nsmallest(3, ['population', 'GDP'])
                  population  GDP alpha-2
        Tuvalu         11300   38      TV
        Nauru          11300  182      NR
        Anguilla       11300  311      AI
        """"""
        return algorithms.SelectNFrame(
            self, n=n, keep=keep, columns=columns
        ).nsmallest()
    def swaplevel(self, i=-2, j=-1, axis=0):
        """"""
        Swap levels i and j in a MultiIndex on a particular axis.
        Parameters
        ----------
        i, j : int, string (can be mixed)
            Level of index to be swapped. Can pass level name as string.
        Returns
        -------
        DataFrame
        """"""
        result = self.copy()
        axis = self._get_axis_number(axis)
        if axis == 0:
            result.index = result.index.swaplevel(i, j)
        else:
            result.columns = result.columns.swaplevel(i, j)
        return result
    def reorder_levels(self, order, axis=0):
        """"""
        Rearrange index levels using input order. May not drop or
        duplicate levels.
        Parameters
        ----------
        order : list of int or list of str
            List representing new level order. Reference level by number
            (position) or by key (label).
        axis : int
            Where to reorder levels.
        Returns
        -------
        type of caller (new object)
        """"""
        axis = self._get_axis_number(axis)
        if not isinstance(self._get_axis(axis), MultiIndex):  # pragma: no cover
            raise TypeError(""Can only reorder levels on a hierarchical axis."")
        result = self.copy()
        if axis == 0:
            result.index = result.index.reorder_levels(order)
        else:
            result.columns = result.columns.reorder_levels(order)
        return result
    # ----------------------------------------------------------------------
    # Arithmetic / combination related
    def _combine_frame(self, other, func, fill_value=None, level=None):
        this, other = self.align(other, join=""outer"", level=level, copy=False)
        new_index, new_columns = this.index, this.columns
        def _arith_op(left, right):
            # for the mixed_type case where we iterate over columns,
            # _arith_op(left, right) is equivalent to
            # left._binop(right, func, fill_value=fill_value)
            left, right = ops.fill_binop(left, right, fill_value)
            return func(left, right)
        if ops.should_series_dispatch(this, other, func):
            # iterate over columns
            return ops.dispatch_to_series(this, other, _arith_op)
        else:
            result = _arith_op(this.values, other.values)
            return self._constructor(
                result, index=new_index, columns=new_columns, copy=False",[124]
"        dst_contents = format_file_contents(
            src_contents, line_length=line_length, fast=fast, mode=mode
        )
    except NothingChanged:
        return False
    if write_back == write_back.YES:
        with open(src, ""w"", encoding=src_buffer.encoding) as f:
            f.write(dst_contents)
    elif write_back == write_back.DIFF:
        src_name = f""{src}  (original)""
        dst_name = f""{src}  (formatted)""
        diff_contents = diff(src_contents, dst_contents, src_name, dst_name)
        if lock:
            lock.acquire()
        try:
            sys.stdout.write(diff_contents)
        finally:
            if lock:
                lock.release()
    return True

def format_stdin_to_stdout(
    line_length: int,
    fast: bool,
    write_back: WriteBack = WriteBack.NO,
    mode: FileMode = FileMode.AUTO_DETECT,
) -> bool:
    """"""Format file on stdin. Return True if changed.
    If `write_back` is True, write reformatted code back to stdout.
    `line_length`, `fast`, `is_pyi`, and `force_py36` arguments are passed to
    :func:`format_file_contents`.
    """"""
    src = sys.stdin.read()
    dst = src
    try:
        dst = format_file_contents(src, line_length=line_length, fast=fast, mode=mode)
        return True
    except NothingChanged:
        return False
    finally:
        if write_back == WriteBack.YES:
            sys.stdout.write(dst)
        elif write_back == WriteBack.DIFF:
            src_name = ""<stdin>  (original)""
            dst_name = ""<stdin>  (formatted)""
            sys.stdout.write(diff(src, dst, src_name, dst_name))

def format_file_contents(
    src_contents: str,
    *,
    line_length: int,
    fast: bool,
    mode: FileMode = FileMode.AUTO_DETECT,
) -> FileContent:
    """"""Reformat contents a file and return new contents.
    If `fast` is False, additionally confirm that the reformatted code is
    valid by calling :func:`assert_equivalent` and :func:`assert_stable` on it.
    `line_length` is passed to :func:`format_str`.
    """"""
    if src_contents.strip() == """":
        raise NothingChanged
    dst_contents = format_str(src_contents, line_length=line_length, mode=mode)
    if src_contents == dst_contents:
        raise NothingChanged
    if not fast:
        assert_equivalent(src_contents, dst_contents)
        assert_stable(src_contents, dst_contents, line_length=line_length, mode=mode)
    return dst_contents

def format_str(
    src_contents: str, line_length: int, *, mode: FileMode = FileMode.AUTO_DETECT
) -> FileContent:
    """"""Reformat a string and return new contents.
    `line_length` determines how many characters per line are allowed.
    """"""
    src_node = lib2to3_parse(src_contents)
    dst_contents = """"
    future_imports = get_future_imports(src_node)
    is_pyi = bool(mode & FileMode.PYI)
    py36 = bool(mode & FileMode.PYTHON36) or is_python36(src_node)
    normalize_strings = not bool(mode & FileMode.NO_STRING_NORMALIZATION)
    lines = LineGenerator(
        remove_u_prefix=py36 or ""unicode_literals"" in future_imports,
        is_pyi=is_pyi,
        normalize_strings=normalize_strings,
    )
    elt = EmptyLineTracker(is_pyi=is_pyi)
    empty_line = Line()
    after = 0
    for current_line in lines.visit(src_node):
        for _ in range(after):
            dst_contents += str(empty_line)
        before, after = elt.maybe_empty_lines(current_line)
        for _ in range(before):
            dst_contents += str(empty_line)
        for line in split_line(current_line, line_length=line_length, py36=py36):
            dst_contents += str(line)
    return dst_contents

GRAMMARS = [
    pygram.python_grammar_no_print_statement_no_exec_statement,
    pygram.python_grammar_no_print_statement,
    pygram.python_grammar,
]

def lib2to3_parse(src_txt: str) -> Node:
    """"""Given a string with source, return the lib2to3 Node.""""""
    grammar = pygram.python_grammar_no_print_statement
    if src_txt[-1] != ""\n"":
        nl = ""\r\n"" if ""\r\n"" in src_txt[:1024] else ""\n""
        src_txt += nl
    for grammar in GRAMMARS:
        drv = driver.Driver(grammar, pytree.convert)
        try:","[7, 16, 35, 46, 50, 122, 123]"
"        Parameters
        ----------
        freq : str or Offset, optional
            One of pandas' :ref:`offset strings <timeseries.offset_aliases>`
            or an Offset object. Will be inferred by default.
        Returns
        -------
        PeriodArray/Index
        Raises
        ------
        ValueError
            When converting a DatetimeArray/Index with non-regular values,
            so that a frequency cannot be inferred.
        See Also
        --------
        PeriodIndex: Immutable ndarray holding ordinal values.
        DatetimeIndex.to_pydatetime: Return DatetimeIndex as object.
        Examples
        --------
        >>> df = pd.DataFrame({""y"": [1, 2, 3]},
        ...                   index=pd.to_datetime([""2000-03-31 00:00:00"",
        ...                                         ""2000-05-31 00:00:00"",
        ...                                         ""2000-08-31 00:00:00""]))
        >>> df.index.to_period(""M"")
        PeriodIndex(['2000-03', '2000-05', '2000-08'],
                    dtype='period[M]', freq='M')
        Infer the daily frequency
        >>> idx = pd.date_range(""2017-01-01"", periods=2)
        >>> idx.to_period()
        PeriodIndex(['2017-01-01', '2017-01-02'],
                    dtype='period[D]', freq='D')
        """"""
        from pandas.core.arrays import PeriodArray
        if self.tz is not None:
            warnings.warn(
                ""Converting to PeriodArray/Index representation ""
                ""will drop timezone information."",
                UserWarning,
            )
        if freq is None:
            freq = self.freqstr or self.inferred_freq
            if freq is None:
                raise ValueError(
                    ""You must pass a freq argument as current index has none.""
                )
            freq = get_period_alias(freq)
        return PeriodArray._from_datetime64(self._data, freq, tz=self.tz)
    def to_perioddelta(self, freq):
        """"""
        Calculate TimedeltaArray of difference between index
        values and index converted to PeriodArray at specified
        freq. Used for vectorized offsets.
        Parameters
        ----------
        freq : Period frequency
        Returns
        -------
        TimedeltaArray/Index
        """"""
        # TODO: consider privatizing (discussion in GH#23113)
        from pandas.core.arrays.timedeltas import TimedeltaArray
        i8delta = self.asi8 - self.to_period(freq).to_timestamp().asi8
        m8delta = i8delta.view(""m8[ns]"")
        return TimedeltaArray(m8delta)
    # -----------------------------------------------------------------
    # Properties - Vectorized Timestamp Properties/Methods
    def month_name(self, locale=None):
        """"""
        Return the month names of the DateTimeIndex with specified locale.
        .. versionadded:: 0.23.0
        Parameters
        ----------
        locale : str, optional
            Locale determining the language in which to return the month name.
            Default is English locale.
        Returns
        -------
        Index
            Index of month names.
        Examples
        --------
        >>> idx = pd.date_range(start='2018-01', freq='M', periods=3)
        >>> idx
        DatetimeIndex(['2018-01-31', '2018-02-28', '2018-03-31'],
                      dtype='datetime64[ns]', freq='M')
        >>> idx.month_name()
        Index(['January', 'February', 'March'], dtype='object')
        """"""
        if self.tz is not None and not timezones.is_utc(self.tz):
            values = self._local_timestamps()
        else:
            values = self.asi8
        result = fields.get_date_name_field(values, ""month_name"", locale=locale)
        result = self._maybe_mask_results(result, fill_value=None)
        return result
    def day_name(self, locale=None):
        """"""
        Return the day names of the DateTimeIndex with specified locale.
        .. versionadded:: 0.23.0
        Parameters
        ----------
        locale : str, optional",[55]
"        else:
            row_margin[k] = grand_margin[k[0]]
    from pandas import DataFrame
    margin_dummy = DataFrame(row_margin, columns=[key]).T
    row_names = result.index.names
    try:
        for dtype in set(result.dtypes):
            cols = result.select_dtypes([dtype]).columns
            margin_dummy[cols] = margin_dummy[cols].astype(dtype)
        result = result.append(margin_dummy)
    except TypeError:
        # we cannot reshape, so coerce the axis
        result.index = result.index._to_safe_for_reshape()
        result = result.append(margin_dummy)
    result.index.names = row_names
    return result

def _compute_grand_margin(data, values, aggfunc, margins_name: str = ""All""):
    if values:
        grand_margin = {}
        for k, v in data[values].items():
            try:
                if isinstance(aggfunc, str):
                    grand_margin[k] = getattr(v, aggfunc)()
                elif isinstance(aggfunc, dict):
                    if isinstance(aggfunc[k], str):
                        grand_margin[k] = getattr(v, aggfunc[k])()
                    else:
                        grand_margin[k] = aggfunc[k](v)
                else:
                    grand_margin[k] = aggfunc(v)
            except TypeError:
                pass
        return grand_margin
    else:
        return {margins_name: aggfunc(data.index)}

def _generate_marginal_results(
    table,
    data,
    values,
    rows,
    cols,
    aggfunc,
    observed,
    grand_margin,
    margins_name: str = ""All"",
):
    if len(cols) > 0:
        # need to ""interleave"" the margins
        table_pieces = []
        margin_keys = []
        def _all_key(key):
            return (key, margins_name) + ("""",) * (len(cols) - 1)
        if len(rows) > 0:
            margin = data[rows + values].groupby(rows, observed=observed).agg(aggfunc)
            cat_axis = 1
            for key, piece in table.groupby(level=0, axis=cat_axis, observed=observed):
                all_key = _all_key(key)
                # we are going to mutate this, so need to copy!
                piece = piece.copy()
                try:
                    piece[all_key] = margin[key]
                except TypeError:
                    # we cannot reshape, so coerce the axis
                    piece.set_axis(
                        piece._get_axis(cat_axis)._to_safe_for_reshape(),
                        axis=cat_axis,
                        inplace=True,
                    )
                    piece[all_key] = margin[key]
                table_pieces.append(piece)
                margin_keys.append(all_key)
        else:
            margin = grand_margin
            cat_axis = 0
            for key, piece in table.groupby(level=0, axis=cat_axis, observed=observed):
                all_key = _all_key(key)
                table_pieces.append(piece)
                table_pieces.append(Series(margin[key], index=[all_key]))
                margin_keys.append(all_key)
        result = concat(table_pieces, axis=cat_axis)
        if len(rows) == 0:
            return result
    else:
        result = table
        margin_keys = table.columns
    if len(cols) > 0:
        row_margin = data[cols + values].groupby(cols, observed=observed).agg(aggfunc)
        row_margin = row_margin.stack()
        # slight hack
        new_order = [len(cols)] + list(range(len(cols)))
        row_margin.index = row_margin.index.reorder_levels(new_order)
    else:
        row_margin = Series(np.nan, index=result.columns)
    return result, margin_keys, row_margin

def _generate_marginal_results_without_values(
    table: ""DataFrame"", data, rows, cols, aggfunc, observed, margins_name: str = ""All""
):
    if len(cols) > 0:
        # need to ""interleave"" the margins
        margin_keys = []
        def _all_key():
            if len(cols) == 1:
                return margins_name",[11]
"        left_distances = np.abs(self[left_indexer] - target)
        right_distances = np.abs(self[right_indexer] - target)
        op = operator.lt if self.is_monotonic_increasing else operator.le
        indexer = np.where(
            op(left_distances, right_distances) | (right_indexer == -1),
            left_indexer,
            right_indexer,
        )
        if tolerance is not None:
            indexer = self._filter_indexer_tolerance(target, indexer, tolerance)
        return indexer
    def _filter_indexer_tolerance(
        self, target: ""Index"", indexer: np.ndarray, tolerance
    ) -> np.ndarray:
        distance = abs(self.values[indexer] - target)
        indexer = np.where(distance <= tolerance, indexer, -1)
        return indexer
    # --------------------------------------------------------------------
    # Indexer Conversion Methods
    def _get_partial_string_timestamp_match_key(self, key):
        """"""
        Translate any partial string timestamp matches in key, returning the
        new key.
        Only relevant for MultiIndex.
        """"""
        # GH#10331
        return key
    def _convert_scalar_indexer(self, key, kind: str_t):
        """"""
        Convert a scalar indexer.
        Parameters
        ----------
        key : label of the slice bound
        kind : {'loc', 'getitem'}
        """"""
        assert kind in [""loc"", ""getitem""]
        if len(self) and not isinstance(self, ABCMultiIndex):
            # we can raise here if we are definitive that this
            # is positional indexing (eg. .loc on with a float)
            # or label indexing if we are using a type able
            # to be represented in the index
            if kind == ""getitem"" and is_float(key):
                if not self.is_floating():
                    self._invalid_indexer(""label"", key)
            elif kind == ""loc"" and is_float(key):
                # we want to raise KeyError on string/mixed here
                # technically we *could* raise a TypeError
                # on anything but mixed though
                if self.inferred_type not in [
                    ""floating"",
                    ""mixed-integer-float"",
                    ""integer-na"",
                    ""string"",
                    ""mixed"",
                ]:
                    self._invalid_indexer(""label"", key)
            elif kind == ""loc"" and is_integer(key):
                if not self.holds_integer():
                    self._invalid_indexer(""label"", key)
        return key
    def _validate_positional_slice(self, key: slice):
        """"""
        For positional indexing, a slice must have either int or None
        for each of start, stop, and step.
        """"""
        self._validate_indexer(""positional"", key.start, ""iloc"")
        self._validate_indexer(""positional"", key.stop, ""iloc"")
        self._validate_indexer(""positional"", key.step, ""iloc"")
    def _convert_slice_indexer(self, key: slice, kind: str_t):
        """"""
        Convert a slice indexer.
        By definition, these are labels unless 'iloc' is passed in.
        Floats are not allowed as the start, step, or stop of the slice.
        Parameters
        ----------
        key : label of the slice bound
        kind : {'loc', 'getitem'}
        """"""
        assert kind in [""loc"", ""getitem""], kind
        # potentially cast the bounds to integers
        start, stop, step = key.start, key.stop, key.step
        # figure out if this is a positional indexer
        def is_int(v):
            return v is None or is_integer(v)
        is_null_slicer = start is None and stop is None
        is_index_slice = is_int(start) and is_int(stop)
        is_positional = is_index_slice and not (
            self.is_integer() or self.is_categorical()
        )
        if kind == ""getitem"":
            """"""
            called from the getitem slicers, validate that we are in fact
            integers
            """"""
            if self.is_integer() or is_index_slice:
                self._validate_indexer(""slice"", key.start, ""getitem"")
                self._validate_indexer(""slice"", key.stop, ""getitem"")
                self._validate_indexer(""slice"", key.step, ""getitem"")
                return key
        # convert the slice to an indexer here
        # if we are mixed and have integers
        if is_positional and self.is_mixed():
            try:",[70]
"            self._request_autoscale_view(
                scalex=self._autoscaleXon, scaley=self._autoscaleYon)
        return dict(whiskers=whiskers, caps=caps, boxes=boxes,
                    medians=medians, fliers=fliers, means=means)
    @staticmethod
    def _parse_scatter_color_args(c, edgecolors, kwargs, xsize,
                                  get_next_color_func):
        """"""
        Helper function to process color related arguments of `.Axes.scatter`.
        Argument precedence for facecolors:
        - c (if not None)
        - kwargs['facecolors']
        - kwargs['facecolor']
        - kwargs['color'] (==kwcolor)
        - 'b' if in classic mode else the result of ``get_next_color_func()``
        Argument precedence for edgecolors:
        - edgecolors (is an explicit kw argument in scatter())
        - kwargs['edgecolor']
        - kwargs['color'] (==kwcolor)
        - 'face' if not in classic mode else None
        Parameters
        ----------
        c : color or sequence or sequence of color or None
            See argument description of `.Axes.scatter`.
        edgecolors : color or sequence of color or {'face', 'none'} or None
            See argument description of `.Axes.scatter`.
        kwargs : dict
            Additional kwargs. If these keys exist, we pop and process them:
            'facecolors', 'facecolor', 'edgecolor', 'color'
            Note: The dict is modified by this function.
        xsize : int
            The size of the x and y arrays passed to `.Axes.scatter`.
        get_next_color_func : callable
            A callable that returns a color. This color is used as facecolor
            if no other color is provided.
            Note, that this is a function rather than a fixed color value to
            support conditional evaluation of the next color.  As of the
            current implementation obtaining the next color from the
            property cycle advances the cycle. This must only happen if we
            actually use the color, which will only be decided within this
            method.
        Returns
        -------
        c
            The input *c* if it was not *None*, else a color derived from the
            other inputs or defaults.
        colors : array(N, 4) or None
            The facecolors as RGBA values, or *None* if a colormap is used.
        edgecolors
            The edgecolor.
        """"""
        facecolors = kwargs.pop('facecolors', None)
        facecolors = kwargs.pop('facecolor', facecolors)
        edgecolors = kwargs.pop('edgecolor', edgecolors)
        kwcolor = kwargs.pop('color', None)
        if kwcolor is not None and c is not None:
            raise ValueError(""Supply a 'c' argument or a 'color'""
                             "" kwarg but not both; they differ but""
                             "" their functionalities overlap."")
        if kwcolor is not None:
            try:
                mcolors.to_rgba_array(kwcolor)
            except ValueError as err:
                raise ValueError(
                    ""'color' kwarg must be an color or sequence of color ""
                    ""specs.  For a sequence of values to be color-mapped, use ""
                    ""the 'c' argument instead."") from err
            if edgecolors is None:
                edgecolors = kwcolor
            if facecolors is None:
                facecolors = kwcolor
        if edgecolors is None and not rcParams['_internal.classic_mode']:
            edgecolors = rcParams['scatter.edgecolors']
        c_was_none = c is None
        if c is None:
            c = (facecolors if facecolors is not None
                 else ""b"" if rcParams['_internal.classic_mode']
                 else get_next_color_func())
        c_is_string_or_strings = (
            isinstance(c, str)
            or (np.iterable(c) and len(c) > 0
                and isinstance(cbook.safe_first_element(c), str)))
        def invalid_shape_exception(csize, xsize):
            return ValueError(
                f""'c' argument has {csize} elements, which is inconsistent ""
                f""with 'x' and 'y' with size {xsize}."")
        c_is_mapped = False  # Unless proven otherwise below.
        valid_shape = True  # Unless proven otherwise below.
        if not c_was_none and kwcolor is None and not c_is_string_or_strings:
            try:  # First, does 'c' look suitable for value-mapping?
                c = np.asanyarray(c, dtype=float)
            except ValueError:
                pass  # Failed to convert to float array; must be color specs.
            else:
                # If c can be either mapped values or a RGB(A) color, prefer
                # the former if shapes match, the latter otherwise.
                if c.size == xsize:
                    c = c.ravel()
                    c_is_mapped = True
                else:  # Wrong size; it must not be intended for mapping.
                    if c.shape in ((3,), (4,)):
                        _log.warning(
                            ""'c' argument looks like a single numeric RGB or ""
                            ""RGBA sequence, which should be avoided as value-""
                            ""mapping will have precedence in case its length ""
                            ""matches with 'x' & 'y'.  Please use a 2-D array ""
                            ""with a single row if you really want to specify ""
                            ""the same RGB or RGBA value for all points."")
                    valid_shape = False",[114]
"                    computed_data.append(tensor_map[x])
            if len(computed_data) == len(reference_input_tensors):
                # Call layer.
                if node.arguments:
                    kwargs = node.arguments
                else:
                    kwargs = {}
                if len(computed_data) == 1:
                    computed_tensor, computed_mask = computed_data[0]
                    if has_arg(layer.call, 'mask'):
                        if 'mask' not in kwargs:
                            kwargs['mask'] = computed_mask
                    output_tensors = to_list(
                        layer(computed_tensor, **kwargs))
                    output_masks = to_list(
                        layer.compute_mask(computed_tensor,
                                           computed_mask))
                    computed_tensors = [computed_tensor]
                    computed_masks = [computed_mask]
                else:
                    computed_tensors = [x[0] for x in computed_data]
                    computed_masks = [x[1] for x in computed_data]
                    if has_arg(layer.call, 'mask'):
                        if 'mask' not in kwargs:
                            kwargs['mask'] = computed_masks
                    output_tensors = to_list(
                        layer(computed_tensors, **kwargs))
                    output_masks = to_list(
                        layer.compute_mask(computed_tensors,
                                           computed_masks))
                # Update tensor_map.
                for x, y, mask in zip(reference_output_tensors,
                                      output_tensors,
                                      output_masks):
                    tensor_map[x] = (y, mask)
    # Check that we did compute the model outputs,
    # then instantiate a new model from inputs and outputs.
    output_tensors = []
    for x in model.outputs:
        assert x in tensor_map, 'Could not compute output ' + str(x)
        tensor, _ = tensor_map[x]
        output_tensors.append(tensor)
    return Model(input_tensors, output_tensors, name=model.name)

def _clone_sequential_model(model, input_tensors=None):
    """"""Clone a `Sequential` model instance.
    Model cloning is similar to calling a model on new inputs,
    except that it creates new layers (and thus new weights) instead
    of sharing the weights of the existing layers.
    # Arguments
        model: Instance of `Sequential`.
        input_tensors: optional list of input tensors
            to build the model upon. If not provided,
            placeholders will be created.
    # Returns
        An instance of `Sequential` reproducing the behavior
        of the original model, on top of new inputs tensors,
        using newly instantiated weights.
    # Raises
        ValueError: in case of invalid `model` argument value.
    """"""
    if not isinstance(model, Sequential):
        raise ValueError('Expected `model` argument '
                         'to be a `Sequential` model instance, '
                         'but got:', model)
    def clone(layer):
        return layer.__class__.from_config(layer.get_config())
    layers = [clone(layer) for layer in model.layers]
    if input_tensors is None:
        return Sequential(layers=layers, name=model.name)
    else:
        if len(to_list(input_tensors)) != 1:
            raise ValueError('To clone a `Sequential` model, we expect '
                             ' at most one tensor '
                             'as part of `input_tensors`.')
        x = to_list(input_tensors)[0]
        if K.is_keras_tensor(x):
            origin_layer = x._keras_history[0]
            if isinstance(origin_layer, InputLayer):
                return Sequential(layers=[origin_layer] + layers,
                                  name=model.name)
            else:
                raise ValueError('Cannot clone a `Sequential` model on top '
                                 'of a tensor that comes from a Keras layer '
                                 'other than an `InputLayer`. '
                                 'Use the functional API instead.')
        input_tensor = Input(tensor=x,
                             name='input_wrapper_for_' + str(x.name))
        input_layer = input_tensor._keras_history[0]
        return Sequential(layers=[input_layer] + layers, name=model.name)

def clone_model(model, input_tensors=None):
    """"""Clone any `Model` instance.
    Model cloning is similar to calling a model on new inputs,
    except that it creates new layers (and thus new weights) instead
    of sharing the weights of the existing layers.
    # Arguments
        model: Instance of `Model`
            (could be a functional model or a Sequential model).
        input_tensors: optional list of input tensors
            to build the model upon. If not provided,
            placeholders will be created.
    # Returns
        An instance of `Model` reproducing the behavior
        of the original model, on top of new inputs tensors,
        using newly instantiated weights.
    # Raises
        ValueError: in case of invalid `model` argument value.
    """"""
    if isinstance(model, Sequential):
        return _clone_sequential_model(model, input_tensors=input_tensors)
    else:","[15, 16, 17, 28, 29, 30]"
"    unit
    tz
    Methods
    -------
    None
    Raises
    ------
    pytz.UnknownTimeZoneError
        When the requested timezone cannot be found.
    Examples
    --------
    >>> pd.DatetimeTZDtype(tz='UTC')
    datetime64[ns, UTC]
    >>> pd.DatetimeTZDtype(tz='dateutil/US/Central')
    datetime64[ns, tzfile('/usr/share/zoneinfo/US/Central')]
    """"""
    type = Timestamp  # type: Type[Timestamp]
    kind = ""M""  # type: str_type
    str = ""|M8[ns]""
    num = 101
    base = np.dtype(""M8[ns]"")
    na_value = NaT
    _metadata = (""unit"", ""tz"")
    _match = re.compile(r""(datetime64|M8)\[(?P<unit>.+), (?P<tz>.+)\]"")
    _cache = {}  # type: Dict[str_type, PandasExtensionDtype]
    def __init__(self, unit=""ns"", tz=None):
        if isinstance(unit, DatetimeTZDtype):
            unit, tz = unit.unit, unit.tz
        if unit != ""ns"":
            if isinstance(unit, str) and tz is None:
                # maybe a string like datetime64[ns, tz], which we support for
                # now.
                result = type(self).construct_from_string(unit)
                unit = result.unit
                tz = result.tz
                msg = (
                    ""Passing a dtype alias like 'datetime64[ns, {tz}]' ""
                    ""to DatetimeTZDtype is deprecated. Use ""
                    ""'DatetimeTZDtype.construct_from_string()' instead.""
                )
                warnings.warn(msg.format(tz=tz), FutureWarning, stacklevel=2)
            else:
                raise ValueError(""DatetimeTZDtype only supports ns units"")
        if tz:
            tz = timezones.maybe_get_tz(tz)
            tz = timezones.tz_standardize(tz)
        elif tz is not None:
            raise pytz.UnknownTimeZoneError(tz)
        elif tz is None:
            raise TypeError(""A 'tz' is required."")
        self._unit = unit
        self._tz = tz
    @property
    def unit(self):
        """"""
        The precision of the datetime data.
        """"""
        return self._unit
    @property
    def tz(self):
        """"""
        The timezone.
        """"""
        return self._tz
    @classmethod
    def construct_array_type(cls):
        """"""
        Return the array type associated with this dtype
        Returns
        -------
        type
        """"""
        from pandas.core.arrays import DatetimeArray
        return DatetimeArray
    @classmethod
    def construct_from_string(cls, string):
        """"""
        Construct a DatetimeTZDtype from a string.
        Parameters
        ----------
        string : str
            The string alias for this DatetimeTZDtype.
            Should be formatted like ``datetime64[ns, <tz>]``,
            where ``<tz>`` is the timezone name.
        Examples
        --------
        >>> DatetimeTZDtype.construct_from_string('datetime64[ns, UTC]')
        datetime64[ns, UTC]
        """"""
        if isinstance(string, str):
            msg = ""Could not construct DatetimeTZDtype from '{}'""
            try:
                match = cls._match.match(string)
                if match:
                    d = match.groupdict()
                    return cls(unit=d[""unit""], tz=d[""tz""])
            except Exception:
                # TODO(py3): Change this pass to `raise TypeError(msg) from e`
                pass
            raise TypeError(msg.format(string))
        raise TypeError(""Could not construct DatetimeTZDtype"")
    def __str__(self):
        return ""datetime64[{unit}, {tz}]"".format(unit=self.unit, tz=self.tz)
    @property
    def name(self):
        """"""A string representation of the dtype.""""""
        return str(self)","[56, 108, 109, 110, 111, 113, 115]"
"        DatetimeIndex(['2011-01-11', '2011-02-11', '2011-03-11', '2011-04-11',
                       '2011-05-11'],
                      dtype='datetime64[ns]', freq=None)
        The default value of `freq` is the `freq` attribute of the index,
        which is 'MS' (month start) in this example.
        >>> month_starts.shift(10)
        DatetimeIndex(['2011-11-01', '2011-12-01', '2012-01-01', '2012-02-01',
                       '2012-03-01'],
                      dtype='datetime64[ns]', freq='MS')
        """"""
        raise NotImplementedError(f""Not supported for type {type(self).__name__}"")
    def argsort(self, *args, **kwargs):
        """"""
        Return the integer indices that would sort the index.
        Parameters
        ----------
        *args
            Passed to `numpy.ndarray.argsort`.
        **kwargs
            Passed to `numpy.ndarray.argsort`.
        Returns
        -------
        numpy.ndarray
            Integer indices that would sort the index if used as
            an indexer.
        See Also
        --------
        numpy.argsort : Similar method for NumPy arrays.
        Index.sort_values : Return sorted copy of Index.
        Examples
        --------
        >>> idx = pd.Index(['b', 'a', 'd', 'c'])
        >>> idx
        Index(['b', 'a', 'd', 'c'], dtype='object')
        >>> order = idx.argsort()
        >>> order
        array([1, 0, 3, 2])
        >>> idx[order]
        Index(['a', 'b', 'c', 'd'], dtype='object')
        """"""
        result = self.asi8
        if result is None:
            result = np.array(self)
        return result.argsort(*args, **kwargs)
    _index_shared_docs[
        ""get_value""
    ] = """"""
        Fast lookup of value from 1-dimensional ndarray. Only use this if you
        know what you're doing.
        Returns
        -------
        scalar
            A value in the Series with the index of the key value in self.
        """"""
    @Appender(_index_shared_docs[""get_value""] % _index_doc_kwargs)
    def get_value(self, series, key):
        # if we have something that is Index-like, then
        # use this, e.g. DatetimeIndex
        # Things like `Series._get_value` (via .at) pass the EA directly here.
        s = getattr(series, ""_values"", series)
        if isinstance(s, (ExtensionArray, Index)) and is_scalar(key):
            # GH 20882, 21257
            # Unify Index and ExtensionArray treatment
            # First try to convert the key to a location
            # If that fails, raise a KeyError if an integer
            # index, otherwise, see if key is an integer, and
            # try that
            try:
                iloc = self.get_loc(key)
                return s[iloc]
            except KeyError:
                if len(self) > 0 and (self.holds_integer() or self.is_boolean()):
                    raise
                elif is_integer(key):
                    return s[key]
        s = com.values_from_object(series)
        k = com.values_from_object(key)
        k = self._convert_scalar_indexer(k, kind=""getitem"")
        try:
            return self._engine.get_value(s, k, tz=getattr(series.dtype, ""tz"", None))
        except KeyError as e1:
            if len(self) > 0 and (self.holds_integer() or self.is_boolean()):
                raise
            try:
                return libindex.get_value_at(s, key)
            except IndexError:
                raise
            except TypeError:
                # generator/iterator-like
                if is_iterator(key):
                    raise InvalidIndexError(key)
                else:
                    raise e1
            except Exception:
                raise e1
        except TypeError:
            # e.g. ""[False] is an invalid key""
            if is_scalar(key):
                raise IndexError(key)
            raise InvalidIndexError(key)
    def set_value(self, arr, key, value):
        """"""
        Fast lookup of value from 1-dimensional ndarray.
        .. deprecated:: 1.0
        Notes
        -----
        Only use this if you know what you're doing.
        """"""","[72, 73, 80, 81, 82, 83, 84, 85, 86, 87]"
"    def content_type(self):
        return self.headers.get(""Content-Type"", DEFAULT_HTTP_CONTENT_TYPE)
    @property
    def match_info(self):
        """"""return matched info after resolving route""""""
        return self.app.router.get(self)[2]
    @property
    def path(self):
        return self._parsed_url.path.decode(""utf-8"")
    @property
    def query_string(self):
        if self._parsed_url.query:
            return self._parsed_url.query.decode(""utf-8"")
        else:
            return """"
    @property
    def url(self):
        return urlunparse(
            (self.scheme, self.host, self.path, None, self.query_string, None)
        )
    def url_for(self, view_name, **kwargs):
        """"""
        Same as :func:`sanic.Sanic.url_for`, but automatically determine
        `scheme` and `netloc` base on the request. Since this method is aiming
        to generate correct schema & netloc, `_external` is implied.
        :param kwargs: takes same parameters as in :func:`sanic.Sanic.url_for`
        :return: an absolute url to the given view
        :rtype: str
        """"""
        # Full URL SERVER_NAME can only be handled in app.url_for
        if ""//"" in self.app.config.SERVER_NAME:
            return self.app.url_for(view_name, _external=True, **kwargs)
        scheme = self.scheme
        host = self.server_name
        port = self.server_port
        if (scheme.lower() in (""http"", ""ws"") and port == 80) or (
            scheme.lower() in (""https"", ""wss"") and port == 443
        ):
            netloc = host
        else:
            netloc = ""{}:{}"".format(host, port)
        return self.app.url_for(
            view_name, _external=True, _scheme=scheme, _server=netloc, **kwargs
        )

File = namedtuple(""File"", [""type"", ""body"", ""name""])

def parse_multipart_form(body, boundary):
    """"""Parse a request body and returns fields and files
    :param body: bytes request body
    :param boundary: bytes multipart boundary
    :return: fields (RequestParameters), files (RequestParameters)
    """"""
    files = RequestParameters()
    fields = RequestParameters()
    form_parts = body.split(boundary)
    for form_part in form_parts[1:-1]:
        file_name = None
        content_type = ""text/plain""
        content_charset = ""utf-8""
        field_name = None
        line_index = 2
        line_end_index = 0
        while not line_end_index == -1:
            line_end_index = form_part.find(b""\r\n"", line_index)
            form_line = form_part[line_index:line_end_index].decode(""utf-8"")
            line_index = line_end_index + 2
            if not form_line:
                break
            colon_index = form_line.index("":"")
            form_header_field = form_line[0:colon_index].lower()
            form_header_value, form_parameters = parse_content_header(
                form_line[colon_index + 2 :]
            )
            if form_header_field == ""content-disposition"":
                field_name = form_parameters.get(""name"")
                file_name = form_parameters.get(""filename"")
                # non-ASCII filenames in RFC2231, ""filename*"" format
                if file_name is None and form_parameters.get(""filename*""):
                    encoding, _, value = email.utils.decode_rfc2231(
                        form_parameters[""filename*""]
                    )
                    file_name = unquote(value, encoding=encoding)
            elif form_header_field == ""content-type"":
                content_type = form_header_value
                content_charset = form_parameters.get(""charset"", ""utf-8"")
        if field_name:
            post_data = form_part[line_index:-4]
            if file_name is None:
                value = post_data.decode(content_charset)
                if field_name in fields:
                    fields[field_name].append(value)
                else:
                    fields[field_name] = [value]
            else:
                form_file = File(
                    type=content_type, name=file_name, body=post_data
                )
                if field_name in files:
                    files[field_name].append(form_file)
                else:
                    files[field_name] = [form_file]
        else:
            logger.debug(
                ""Form-data field does not have a 'name' parameter ""
                ""in the Content-Disposition header""
            )
","[36, 37]"
"    banner: login
    text: |
      this is my login banner
      that contains a multiline
      string
    state: present
- name: remove the motd banner
  ios_banner:
    banner: motd
    state: absent
- name: Configure banner from file
  ios_banner:
    banner:  motd
    text: ""{{ lookup('file', './config_partial/raw_banner.cfg') }}""
    state: present
""""""
RETURN = """"""
commands:
  description: The list of configuration mode commands to send to the device
  returned: always
  type: list
  sample:
    - banner login
    - this is my login banner
    - that contains a multiline
    - string
""""""
from ansible.module_utils.basic import AnsibleModule
from ansible.module_utils.connection import exec_command
from ansible.module_utils.network.ios.ios import load_config
from ansible.module_utils.network.ios.ios import ios_argument_spec
import re

def map_obj_to_commands(updates, module):
    commands = list()
    want, have = updates
    state = module.params['state']
    if state == 'absent' and 'text' in have.keys() and have['text']:
        commands.append('no banner %s' % module.params['banner'])
    elif state == 'present':
        if want['text'] and (want['text'] != have.get('text')):
            banner_cmd = 'banner %s' % module.params['banner']
            banner_cmd += ' @\n'
            banner_cmd += want['text'].strip()
            banner_cmd += '\n@'
            commands.append(banner_cmd)
    return commands

def map_config_to_obj(module):
    rc, out, err = exec_command(module, 'show banner %s' % module.params['banner'])
    if rc == 0:
        output = out
    else:
        rc, out, err = exec_command(module,
                                    'show running-config | begin banner %s'
                                    % module.params['banner'])
        if out:
            output = re.search(r'\^C(.*?)\^C', out, re.S).group(1).strip()
        else:
            output = None
    obj = {'banner': module.params['banner'], 'state': 'absent'}
    if output:
        obj['text'] = output
        obj['state'] = 'present'
    return obj

def map_params_to_obj(module):
    text = module.params['text']
    if text:
        text = str(text).strip()
    return {
        'banner': module.params['banner'],
        'text': text,
        'state': module.params['state']
    }

def main():
    """""" main entry point for module execution
    """"""
    argument_spec = dict(
        banner=dict(required=True, choices=['login', 'motd', 'exec', 'incoming', 'slip-ppp']),
        text=dict(),
        state=dict(default='present', choices=['present', 'absent'])
    )
    argument_spec.update(ios_argument_spec)
    required_if = [('state', 'present', ('text',))]
    module = AnsibleModule(argument_spec=argument_spec,
                           required_if=required_if,
                           supports_check_mode=True)
    warnings = list()
    result = {'changed': False}
    if warnings:
        result['warnings'] = warnings
    want = map_params_to_obj(module)
    have = map_config_to_obj(module)
    commands = map_obj_to_commands((want, have), module)
    result['commands'] = commands
    if commands:
        if not module.check_mode:
            load_config(module, commands)
        result['changed'] = True
    module.exit_json(**result)

if __name__ == '__main__':","[32, 33, 35, 50, 58, 59, 60, 61, 62, 63, 64, 65, 66, 78, 79, 80]"
"    def move_to_final_destination(self):
        os.rename(self.tmp_path, self.path)
    def generate_tmp_path(self, path):
        return path + '-luigi-tmp-%09d' % random.randrange(0, 1e10)

class LocalFileSystem(FileSystem):
    """"""
    Wrapper for access to file system operations.
    Work in progress - add things as needed.
    """"""
    def exists(self, path):
        return os.path.exists(path)
    def mkdir(self, path, parents=True, raise_if_exists=False):
        if self.exists(path):
            if raise_if_exists:
                raise FileAlreadyExists()
            elif not self.isdir(path):
                raise NotADirectory()
            else:
                return
        if parents:
            os.makedirs(path)
        else:
            if not os.path.exists(os.path.dirname(path)):
                raise MissingParentDirectory()
            os.mkdir(path)
    def isdir(self, path):
        return os.path.isdir(path)
    def listdir(self, path):
        for dir_, _, files in os.walk(path):
            assert dir_.startswith(path)
            for name in files:
                yield os.path.join(dir_, name)
    def remove(self, path, recursive=True):
        if recursive and self.isdir(path):
            shutil.rmtree(path)
        else:
            os.remove(path)
    def move(self, old_path, new_path, raise_if_exists=False):
        if raise_if_exists and os.path.exists(new_path):
            raise RuntimeError('Destination exists: %s' % new_path)
        d = os.path.dirname(new_path)
        if d and not os.path.exists(d):
            self.fs.mkdir(d)
        os.rename(old_path, new_path)

class LocalTarget(FileSystemTarget):
    fs = LocalFileSystem()
    def __init__(self, path=None, format=None, is_tmp=False):
        if format is None:
            format = get_default_format()
        if not path:
            if not is_tmp:
                raise Exception('path or is_tmp must be set')
            path = os.path.join(tempfile.gettempdir(), 'luigi-tmp-%09d' % random.randint(0, 999999999))
        super(LocalTarget, self).__init__(path)
        self.format = format
        self.is_tmp = is_tmp
    def makedirs(self):
        """"""
        Create all parent folders if they do not exist.
        """"""
        normpath = os.path.normpath(self.path)
        parentfolder = os.path.dirname(normpath)
        if parentfolder:
            try:
                os.makedirs(parentfolder)
            except OSError:
                pass
    def open(self, mode='r'):
        rwmode = mode.replace('b', '').replace('t', '')
        if rwmode == 'w':
            self.makedirs()
            return self.format.pipe_writer(atomic_file(self.path))
        elif rwmode == 'r':
            fileobj = FileWrapper(io.BufferedReader(io.FileIO(self.path, mode)))
            return self.format.pipe_reader(fileobj)
        else:
            raise Exception('mode must be r/w (got:%s)' % mode)
    def move(self, new_path, raise_if_exists=False):
        self.fs.move(self.path, new_path, raise_if_exists=raise_if_exists)
    def move_dir(self, new_path):
        self.move(new_path)
    def remove(self):
        self.fs.remove(self.path)
    def copy(self, new_path, raise_if_exists=False):
        if raise_if_exists and os.path.exists(new_path):
            raise RuntimeError('Destination exists: %s' % new_path)
        tmp = LocalTarget(new_path + '-luigi-tmp-%09d' % random.randrange(0, 1e10), is_tmp=True)
        tmp.makedirs()
        shutil.copy(self.path, tmp.fn)
        tmp.move(new_path)
    @property
    def fn(self):
        return self.path
    def __del__(self):
        if self.is_tmp and self.exists():
            self.remove()

class File(LocalTarget):
    def __init__(self, *args, **kwargs):
        warnings.warn(""File has been renamed LocalTarget"", DeprecationWarning, stacklevel=2)",[53]
"        prefix  : str, optional
            Prefix message (included in total width) [default: ''].
        ascii  : bool, optional
            If not set, use unicode (smooth blocks) to fill the meter
            [default: False]. The fallback is to use ASCII characters
            (1-9 #).
        unit  : str, optional
            The iteration unit [default: 'it'].
        unit_scale  : bool, optional
            If set, the number of iterations will printed with an
            appropriate SI metric prefix (K = 10^3, M = 10^6, etc.)
            [default: False].
        rate  : float, optional
            Manual override for iteration rate.
            If [default: None], uses n/elapsed.
        bar_format  : str, optional
            Specify a custom bar string formatting. May impact performance.
            [default: '{l_bar}{bar}{r_bar}'], where l_bar is
            '{desc}{percentage:3.0f}%|' and r_bar is
            '| {n_fmt}/{total_fmt} [{elapsed_str}<{remaining_str}, {rate_fmt}]'
            Possible vars: bar, n, n_fmt, total, total_fmt, percentage,
            rate, rate_fmt, elapsed, remaining, l_bar, r_bar, desc.
        Returns
        -------
        out  : Formatted meter and stats, ready to display.
        """"""
        # sanity check: total
        if total and n > total:
            total = None
        format_interval = tqdm.format_interval
        elapsed_str = format_interval(elapsed)
        # if unspecified, attempt to use rate = average speed
        # (we allow manual override since predicting time is an arcane art)
        if rate is None and elapsed:
            rate = n / elapsed
        inv_rate = 1 / rate if (rate and (rate < 1)) else None
        format_sizeof = tqdm.format_sizeof
        rate_fmt = ((format_sizeof(inv_rate if inv_rate else rate)
                    if unit_scale else
                    '{0:5.2f}'.format(inv_rate if inv_rate else rate))
                    if rate else '?') \
            + ('s' if inv_rate else unit) + '/' + (unit if inv_rate else 's')
        if unit_scale:
            n_fmt = format_sizeof(n)
            total_fmt = format_sizeof(total) if total else None
        else:
            n_fmt = str(n)
            total_fmt = str(total)
        # total is known: we can predict some stats
        if total:
            # fractional and percentage progress
            frac = n / total
            percentage = frac * 100
            remaining_str = format_interval((total - n) / rate) \
                if rate else '?'
            # format the stats displayed to the left and right sides of the bar
            l_bar = (prefix if prefix else '') + \
                '{0:3.0f}%|'.format(percentage)
            r_bar = '| {0}/{1} [{2}<{3}, {4}]'.format(
                    n_fmt, total_fmt, elapsed_str, remaining_str, rate_fmt)
            if ncols == 0:
                return l_bar[:-1] + r_bar[1:]
            if bar_format:
                # Custom bar formatting
                # Populate a dict with all available progress indicators
                bar_args = {'n': n,
                            'n_fmt': n_fmt,
                            'total': total,
                            'total_fmt': total_fmt,
                            'percentage': percentage,
                            'rate': rate if inv_rate is None else inv_rate,
                            'rate_noinv': rate,
                            'rate_noinv_fmt': ((format_sizeof(rate)
                                                    if unit_scale else
                                                    '{0:5.2f}'.format(rate))
                                                    if rate else '?') + 'it/s',
                            'rate_fmt': rate_fmt,
                            'elapsed': elapsed_str,
                            'remaining': remaining_str,
                            'l_bar': l_bar,
                            'r_bar': r_bar,
                            'desc': prefix if prefix else '',
                            # 'bar': full_bar  # replaced by procedure below
                            }
                # Interpolate supplied bar format with the dict
                if '{bar}' in bar_format:
                    # Format left/right sides of the bar, and format the bar
                    # later in the remaining space (avoid breaking display)
                    l_bar_user, r_bar_user = bar_format.split('{bar}')
                    l_bar, r_bar = l_bar.format(**bar_args), r_bar.format(**bar_args)
                else:
                    # Else no progress bar, we can just format and return
                    return bar_format.format(**bar_args)
            # Formatting progress bar
            # space available for bar's display
            N_BARS = max(1, ncols - len(l_bar) - len(r_bar)) if ncols \
                else 10
            # format bar depending on availability of unicode/ascii chars
            if ascii:
                bar_length, frac_bar_length = divmod(
                    int(frac * N_BARS * 10), 10)
                bar = '#' * bar_length
                frac_bar = chr(48 + frac_bar_length) if frac_bar_length \
                    else ' '
            else:
                bar_length, frac_bar_length = divmod(int(frac * N_BARS * 8), 8)
                bar = _unich(0x2588) * bar_length
                frac_bar = _unich(0x2590 - frac_bar_length) \
                    if frac_bar_length else ' '
            # whitespace padding",[100]
"        if (self._expected_content_remaining is not None and
                self._expected_content_remaining != 0 and
                not self.stream.closed()):
            self.stream.close()
            raise httputil.HTTPOutputError(
                ""Tried to write %d bytes less than Content-Length"" %
                self._expected_content_remaining)
        if self._chunking_output:
            if not self.stream.closed():
                self._pending_write = self.stream.write(b""0\r\n\r\n"")
                self._pending_write.add_done_callback(self._on_write_complete)
        self._write_finished = True
        # If the app finished the request while we're still reading,
        # divert any remaining data away from the delegate and
        # close the connection when we're done sending our response.
        # Closing the connection is the only way to avoid reading the
        # whole input body.
        if not self._read_finished:
            self._disconnect_on_finish = True
        # No more data is coming, so instruct TCP to send any remaining
        # data immediately instead of waiting for a full packet or ack.
        self.stream.set_nodelay(True)
        if self._pending_write is None:
            self._finish_request(None)
        else:
            self._pending_write.add_done_callback(self._finish_request)
    def _on_write_complete(self, future):
        exc = future.exception()
        if exc is not None and not isinstance(exc, iostream.StreamClosedError):
            future.result()
        if self._write_callback is not None:
            callback = self._write_callback
            self._write_callback = None
            self.stream.io_loop.add_callback(callback)
        if self._write_future is not None:
            future = self._write_future
            self._write_future = None
            future.set_result(None)
    def _can_keep_alive(self, start_line, headers):
        if self.params.no_keep_alive:
            return False
        connection_header = headers.get(""Connection"")
        if connection_header is not None:
            connection_header = connection_header.lower()
        if start_line.version == ""HTTP/1.1"":
            return connection_header != ""close""
        elif (""Content-Length"" in headers or
              headers.get(""Transfer-Encoding"", """").lower() == ""chunked"" or
              getattr(start_line, 'method', None) in (""HEAD"", ""GET"")):
            # start_line may be a request or reponse start line; only
            # the former has a method attribute.
            return connection_header == ""keep-alive""
        return False
    def _finish_request(self, future):
        self._clear_callbacks()
        if not self.is_client and self._disconnect_on_finish:
            self.close()
            return
        # Turn Nagle's algorithm back on, leaving the stream in its
        # default state for the next request.
        self.stream.set_nodelay(False)
        if not self._finish_future.done():
            self._finish_future.set_result(None)
    def _parse_headers(self, data):
        # The lstrip removes newlines that some implementations sometimes
        # insert between messages of a reused connection.  Per RFC 7230,
        # we SHOULD ignore at least one empty line before the request.
        # http://tools.ietf.org/html/rfc7230#section-3.5
        data = native_str(data.decode('latin1')).lstrip(""\r\n"")
        # RFC 7230 section allows for both CRLF and bare LF.
        eol = data.find(""\n"")
        start_line = data[:eol].rstrip(""\r"")
        try:
            headers = httputil.HTTPHeaders.parse(data[eol:])
        except ValueError:
            # probably form split() if there was no ':' in the line
            raise httputil.HTTPInputError(""Malformed HTTP headers: %r"" %
                                          data[eol:100])
        return start_line, headers
    def _read_body(self, code, headers, delegate):
        if ""Content-Length"" in headers:
            if ""Transfer-Encoding"" in headers:
                # Response cannot contain both Content-Length and
                # Transfer-Encoding headers.
                # http://tools.ietf.org/html/rfc7230#section-3.3.3
                raise httputil.HTTPInputError(
                    ""Response with both Transfer-Encoding and Content-Length"")
            if "","" in headers[""Content-Length""]:
                # Proxies sometimes cause Content-Length headers to get
                # duplicated.  If all the values are identical then we can
                # use them but if they differ it's an error.
                pieces = re.split(r',\s*', headers[""Content-Length""])
                if any(i != pieces[0] for i in pieces):
                    raise httputil.HTTPInputError(
                        ""Multiple unequal Content-Lengths: %r"" %
                        headers[""Content-Length""])
                headers[""Content-Length""] = pieces[0]
            try:
                content_length = int(headers[""Content-Length""])
            except ValueError:
                # Handles non-integer Content-Length value.
                raise httputil.HTTPInputError(
                    ""Only integer Content-Length is allowed: %s"" % headers[""Content-Length""])
            if content_length > self._max_body_size:
                raise httputil.HTTPInputError(""Content-Length too long"")
        else:
            content_length = None
        if code == 204:
            # This response code is not allowed to have a non-empty body,
            # and has an implicit length of zero instead of read-until-close.
            # http://www.w3.org/Protocols/rfc2616/rfc2616-sec4.html#sec4.3
            if (""Transfer-Encoding"" in headers or
                    content_length not in (None, 0)):
                raise httputil.HTTPInputError(
                    ""Response with code %d should not have body"" % code)
            content_length = 0
        if content_length is not None:
            return self._read_fixed_body(content_length, delegate)",[127]
"        return self._get_with(key)
    def _get_with(self, key):
        # other: fancy integer or otherwise
        if isinstance(key, slice):
            # _convert_slice_indexer to determin if this slice is positional
            #  or label based, and if the latter, convert to positional
            slobj = self.index._convert_slice_indexer(key, kind=""getitem"")
            return self._slice(slobj)
        elif isinstance(key, ABCDataFrame):
            raise TypeError(
                ""Indexing a Series with DataFrame is not ""
                ""supported, use the appropriate DataFrame column""
            )
        elif isinstance(key, tuple):
            return self._get_values_tuple(key)
        elif not is_list_like(key):
            # e.g. scalars that aren't recognized by lib.is_scalar, GH#32684
            return self.loc[key]
        if not isinstance(key, (list, np.ndarray, ExtensionArray, Series, Index)):
            key = list(key)
        if isinstance(key, Index):
            key_type = key.inferred_type
        else:
            key_type = lib.infer_dtype(key, skipna=False)
        # Note: The key_type == ""boolean"" case should be caught by the
        #  com.is_bool_indexer check in __getitem__
        if key_type == ""integer"":
            # We need to decide whether to treat this as a positional indexer
            #  (i.e. self.iloc) or label-based (i.e. self.loc)
            if not self.index._should_fallback_to_positional():
                return self.loc[key]
            else:
                return self.iloc[key]
        if isinstance(key, list):
            # handle the dup indexing case GH#4246
            return self.loc[key]
        return self.reindex(key)
    def _get_values_tuple(self, key):
        # mpl hackaround
        if com.any_none(*key):
            # suppress warning from slicing the index with a 2d indexer.
            # eventually we'll want Series itself to warn.
            with warnings.catch_warnings():
                warnings.filterwarnings(
                    ""ignore"", ""Support for multi-dim"", DeprecationWarning
                )
                return self._get_values(key)
        if not isinstance(self.index, MultiIndex):
            raise ValueError(""Can only tuple-index with a MultiIndex"")
        # If key is contained, would have returned by now
        indexer, new_index = self.index.get_loc_level(key)
        return self._constructor(self._values[indexer], index=new_index).__finalize__(
            self,
        )
    def _get_values(self, indexer):
        try:
            return self._constructor(self._mgr.get_slice(indexer)).__finalize__(self,)
        except ValueError:
            # mpl compat if we look up e.g. ser[:, np.newaxis];
            #  see tests.series.timeseries.test_mpl_compat_hack
            return self._values[indexer]
    def _get_value(self, label, takeable: bool = False):
        """"""
        Quickly retrieve single value at passed index label.
        Parameters
        ----------
        label : object
        takeable : interpret the index as indexers, default False
        Returns
        -------
        scalar value
        """"""
        if takeable:
            return self._values[label]
        # Similar to Index.get_value, but we do not fall back to positional
        loc = self.index.get_loc(label)
        return self.index._get_values_for_loc(self, loc, label)
    def __setitem__(self, key, value):
        key = com.apply_if_callable(key, self)
        cacher_needs_updating = self._check_is_chained_assignment_possible()
        if key is Ellipsis:
            key = slice(None)
        try:
            self._set_with_engine(key, value)
        except (KeyError, ValueError):
            values = self._values
            if is_integer(key) and not self.index.inferred_type == ""integer"":
                # positional setter
                values[key] = value
            else:
                # GH#12862 adding an new key to the Series
                self.loc[key] = value
        except TypeError as e:
            if isinstance(key, tuple) and not isinstance(self.index, MultiIndex):
                raise ValueError(""Can only tuple-index with a MultiIndex"") from e
            if com.is_bool_indexer(key):
                key = check_bool_indexer(self.index, key)
                key = np.asarray(key, dtype=bool)
                try:
                    self._where(~key, value, inplace=True)
                except InvalidIndexError:
                    self.iloc[key] = value
                return
            else:
                self._set_with(key, value)
","[39, 41, 42, 43]"
"from subprocess import Popen, PIPE
from time import time
import os
import sys
import six
from .. import logs
from ..conf import settings
from ..utils import DEVNULL, cache
from .generic import Generic

@cache('~/.config/fish/config.fish', '~/.config/fish/functions')
def _get_functions(overridden):
    proc = Popen(['fish', '-ic', 'functions'], stdout=PIPE, stderr=DEVNULL)
    functions = proc.stdout.read().decode('utf-8').strip().split('\n')
    return {func: func for func in functions if func not in overridden}

@cache('~/.config/fish/config.fish')
def _get_aliases(overridden):
    aliases = {}
    proc = Popen(['fish', '-ic', 'alias'], stdout=PIPE, stderr=DEVNULL)
    alias_out = proc.stdout.read().decode('utf-8').strip().split('\n')
    for alias in alias_out:
        name, value = alias.replace('alias ', '', 1).split(' ', 1)
        if name not in overridden:
            aliases[name] = value
    return aliases

class Fish(Generic):
    def _get_overridden_aliases(self):
        overridden = os.environ.get('THEFUCK_OVERRIDDEN_ALIASES',
                                    os.environ.get('TF_OVERRIDDEN_ALIASES', ''))
        default = {'cd', 'grep', 'ls', 'man', 'open'}
        for alias in overridden.split(','):
            default.add(alias.strip())
        return default
    def app_alias(self, alias_name):
        if settings.alter_history:
            alter_history = ('    builtin history delete --exact'
                             ' --case-sensitive -- $fucked_up_command\n'
                             '    builtin history merge ^ /dev/null\n')
        else:
            alter_history = ''
        # It is VERY important to have the variables declared WITHIN the alias
        return ('function {0} -d ""Correct your previous console command""\n'
                '  set -l fucked_up_command $history[1]\n'
                '  env TF_SHELL=fish TF_ALIAS={0} PYTHONIOENCODING=utf-8'
                ' thefuck $fucked_up_command | read -l unfucked_command\n'
                '  if [ ""$unfucked_command"" != """" ]\n'
                '    eval $unfucked_command\n{1}'
                '  end\n'
                'end').format(alias_name, alter_history)
    def get_aliases(self):
        overridden = self._get_overridden_aliases()
        functions = _get_functions(overridden)
        raw_aliases = _get_aliases(overridden)
        functions.update(raw_aliases)
        return functions
    def _expand_aliases(self, command_script):
        aliases = self.get_aliases()
        binary = command_script.split(' ')[0]
        if binary in aliases and aliases[binary] != binary:
            return command_script.replace(binary, aliases[binary], 1)
        elif binary in aliases:
            return u'fish -ic ""{}""'.format(command_script.replace('""', r'\""'))
        else:
            return command_script
    def _get_history_file_name(self):
        return os.path.expanduser('~/.config/fish/fish_history')
    def _get_history_line(self, command_script):
        return u'- cmd: {}\n   when: {}\n'.format(command_script, int(time()))
    def _script_from_history(self, line):
        if '- cmd: ' in line:
            return line.split('- cmd: ', 1)[1]
        else:
            return ''
    def and_(self, *commands):
        return u'; and '.join(commands)
    def or_(self, *commands):
        return u'; or '.join(commands)
    def how_to_configure(self):
        return self._create_shell_configuration(
            content=u""thefuck --alias | source"",
            path='~/.config/fish/config.fish',
            reload='fish')
    def put_to_history(self, command):
        try:
            return self._put_to_history(command)
        except IOError:
            logs.exception(""Can't update history"", sys.exc_info())
    def _put_to_history(self, command_script):
        """"""Puts command script to shell history.""""""
        history_file_name = self._get_history_file_name()
        if os.path.isfile(history_file_name):
            with open(history_file_name, 'a') as history:
                entry = self._get_history_line(command_script)
                if six.PY2:
                    history.write(entry.encode('utf-8'))
                else:
                    history.write(entry)","[22, 23, 24]"
"#!/usr/bin/env python3
import asyncio
import sys
from third_party import X, Y, Z
from library import some_connection, \
                    some_decorator
f'trigger 3.6 mode'
def func_no_args():
  a; b; c
  if True: raise RuntimeError
  if False: ...
  for i in range(10):
    print(i)
    continue
  return None
async def coroutine(arg):
 ""Single-line docstring. Multiline is harder to reformat.""
 async with some_connection() as conn:
     await conn.do_what_i_mean('SELECT bobby, tables FROM xkcd', timeout=2)
 await asyncio.sleep(1)
@asyncio.coroutine
@some_decorator(
with_args=True,
many_args=[1,2,3]
)
def function_signature_stress_test(number:int,no_annotation=None,text:str=""default"",* ,debug:bool=False,**kwargs) -> str:
 return text[number:-1]
def spaces(a=1, b=(), c=[], d={}, e=True, f=-1, g=1 if False else 2, h="""", i=r''):
 offset = attr.ib(default=attr.Factory( lambda: _r.uniform(10000, 200000)))
 assert task._cancel_stack[:len(old_stack)] == old_stack
def spaces2(result= _core.Value(None)):
 ...
def example(session):
    result = session.query(models.Customer.id).filter(
        models.Customer.account_id == account_id,
        models.Customer.email == email_address,
    ).order_by(
        models.Customer.id.asc()
    ).all()
def long_lines():
    if True:
        typedargslist.extend(
            gen_annotated_params(ast_args.kwonlyargs, ast_args.kw_defaults, parameters, implicit_default=True)
        )
    _type_comment_re = re.compile(
        r""""""
        ^
        [\t ]*
        \#[ ]type:[ ]*
        (?P<type>
            [^#\t\n]+?
        )
        (?<!ignore)     # note: this will force the non-greedy + in <type> to match
                        # a trailing space which is why we need the silliness below
        (?<!ignore[ ]{1})(?<!ignore[ ]{2})(?<!ignore[ ]{3})(?<!ignore[ ]{4})
        (?<!ignore[ ]{5})(?<!ignore[ ]{6})(?<!ignore[ ]{7})(?<!ignore[ ]{8})
        (?<!ignore[ ]{9})(?<!ignore[ ]{10})
        [\t ]*
        (?P<nl>
            (?:\#[^\n]*)?
            \n?
        )
        $
        """""", re.MULTILINE | re.VERBOSE
    )
# output

#!/usr/bin/env python3
import asyncio
import sys
from third_party import X, Y, Z
from library import some_connection, some_decorator
f'trigger 3.6 mode'

def func_no_args():
    a
    b
    c
    if True:
        raise RuntimeError
    if False:
        ...
    for i in range(10):
        print(i)
        continue
    return None

async def coroutine(arg):
    ""Single-line docstring. Multiline is harder to reformat.""
    async with some_connection() as conn:
        await conn.do_what_i_mean('SELECT bobby, tables FROM xkcd', timeout=2)
    await asyncio.sleep(1)

@asyncio.coroutine
@some_decorator(with_args=True, many_args=[1, 2, 3])
def function_signature_stress_test(
    number: int,
    no_annotation=None,
    text: str = ""default"",
    *,
    debug: bool = False,
    **kwargs,
) -> str:
    return text[number:-1]

def spaces(a=1, b=(), c=[], d={}, e=True, f=-1, g=1 if False else 2, h="""", i=r''):
    offset = attr.ib(default=attr.Factory(lambda: _r.uniform(10000, 200000)))
    assert task._cancel_stack[:len(old_stack)] == old_stack

def spaces2(result=_core.Value(None)):
    ...
","[17, 98]"
"from thefuck.utils import for_app

@for_app('man', at_least=1)
def match(command):
    return True

def get_new_command(command):
    if '3' in command.script:
        return command.script.replace(""3"", ""2"")
    if '2' in command.script:
        return command.script.replace(""2"", ""3"")
    split_cmd2 = command.script_parts
    split_cmd3 = split_cmd2[:]
    split_cmd2.insert(1, ' 2 ')
    split_cmd3.insert(1, ' 3 ')
    last_arg = command.script_parts[-1]
    return [
        last_arg + ' --help',
        """".join(split_cmd3),
        """".join(split_cmd2),
    ]","[20, 21, 23]"
"        rgb0 = cmap(norm(data))
        rgb1 = self.shade_rgb(rgb0, elevation=data, blend_mode=blend_mode,
                              vert_exag=vert_exag, dx=dx, dy=dy,
                              fraction=fraction, **kwargs)
        # Don't overwrite the alpha channel, if present.
        rgb0[..., :3] = rgb1[..., :3]
        return rgb0
    def shade_rgb(self, rgb, elevation, fraction=1., blend_mode='hsv',
                  vert_exag=1, dx=1, dy=1, **kwargs):
        """"""
        Use this light source to adjust the colors of the *rgb* input array to
        give the impression of a shaded relief map with the given *elevation*.
        Parameters
        ----------
        rgb : array-like
            An (M, N, 3) RGB array, assumed to be in the range of 0 to 1.
        elevation : array-like
            An (M, N) array of the height values used to generate a shaded map.
        fraction : number
            Increases or decreases the contrast of the hillshade.  Values
            greater than one will cause intermediate values to move closer to
            full illumination or shadow (and clipping any values that move
            beyond 0 or 1). Note that this is not visually or mathematically
            the same as vertical exaggeration.
        blend_mode : {'hsv', 'overlay', 'soft'} or callable, optional
            The type of blending used to combine the colormapped data values
            with the illumination intensity.  For backwards compatibility, this
            defaults to ""hsv"". Note that for most topographic surfaces,
            ""overlay"" or ""soft"" appear more visually realistic. If a
            user-defined function is supplied, it is expected to combine an
            MxNx3 RGB array of floats (ranging 0 to 1) with an MxNx1 hillshade
            array (also 0 to 1).  (Call signature
            ``func(rgb, illum, **kwargs)``)
            Additional kwargs supplied to this function will be passed on to
            the *blend_mode* function.
        vert_exag : number, optional
            The amount to exaggerate the elevation values by when calculating
            illumination. This can be used either to correct for differences in
            units between the x-y coordinate system and the elevation
            coordinate system (e.g. decimal degrees vs. meters) or to
            exaggerate or de-emphasize topography.
        dx : number, optional
            The x-spacing (columns) of the input *elevation* grid.
        dy : number, optional
            The y-spacing (rows) of the input *elevation* grid.
        Additional kwargs are passed on to the *blend_mode* function.
        Returns
        -------
        ndarray
            An (m, n, 3) array of floats ranging between 0-1.
        """"""
        # Calculate the ""hillshade"" intensity.
        intensity = self.hillshade(elevation, vert_exag, dx, dy, fraction)
        intensity = intensity[..., np.newaxis]
        # Blend the hillshade and rgb data using the specified mode
        lookup = {
                'hsv': self.blend_hsv,
                'soft': self.blend_soft_light,
                'overlay': self.blend_overlay,
                }
        if blend_mode in lookup:
            blend = lookup[blend_mode](rgb, intensity, **kwargs)
        else:
            try:
                blend = blend_mode(rgb, intensity, **kwargs)
            except TypeError as err:
                raise ValueError('""blend_mode"" must be callable or one of {}'
                                 .format(lookup.keys)) from err
        # Only apply result where hillshade intensity isn't masked
        if hasattr(intensity, 'mask'):
            mask = intensity.mask[..., 0]
            for i in range(3):
                blend[..., i][mask] = rgb[..., i][mask]
        return blend
    def blend_hsv(self, rgb, intensity, hsv_max_sat=None, hsv_max_val=None,
                  hsv_min_val=None, hsv_min_sat=None):
        """"""
        Take the input data array, convert to HSV values in the given colormap,
        then adjust those color values to give the impression of a shaded
        relief map with a specified light source.  RGBA values are returned,
        which can then be used to plot the shaded image with imshow.
        The color of the resulting image will be darkened by moving the (s, v)
        values (in hsv colorspace) toward (hsv_min_sat, hsv_min_val) in the
        shaded regions, or lightened by sliding (s, v) toward (hsv_max_sat,
        hsv_max_val) in regions that are illuminated.  The default extremes are
        chose so that completely shaded points are nearly black (s = 1, v = 0)
        and completely illuminated points are nearly white (s = 0, v = 1).
        Parameters
        ----------
        rgb : ndarray
            An MxNx3 RGB array of floats ranging from 0 to 1 (color image).
        intensity : ndarray
            An MxNx1 array of floats ranging from 0 to 1 (grayscale image).
        hsv_max_sat : number, default: 1
            The maximum saturation value that the *intensity* map can shift the
            output image to.
        hsv_min_sat : number, optional
            The minimum saturation value that the *intensity* map can shift the
            output image to. Defaults to 0.
        hsv_max_val : number, optional
            The maximum value (""v"" in ""hsv"") that the *intensity* map can shift
            the output image to. Defaults to 1.
        hsv_min_val : number, optional
            The minimum value (""v"" in ""hsv"") that the *intensity* map can shift
            the output image to. Defaults to 0.
        Returns
        -------
        ndarray
            An MxNx3 RGB array representing the combined images.
        """"""
        # Backward compatibility...
        if hsv_max_sat is None:
            hsv_max_sat = self.hsv_max_sat
        if hsv_max_val is None:
            hsv_max_val = self.hsv_max_val
        if hsv_min_sat is None:",[75]
"        Compute the pairwise covariance among the series of a DataFrame.
        The returned data frame is the `covariance matrix
        <https://en.wikipedia.org/wiki/Covariance_matrix>`__ of the columns
        of the DataFrame.
        Both NA and null values are automatically excluded from the
        calculation. (See the note below about bias from missing values.)
        A threshold can be set for the minimum number of
        observations for each value created. Comparisons with observations
        below this threshold will be returned as ``NaN``.
        This method is generally used for the analysis of time series data to
        understand the relationship between different measures
        across time.
        Parameters
        ----------
        min_periods : int, optional
            Minimum number of observations required per pair of columns
            to have a valid result.
        Returns
        -------
        DataFrame
            The covariance matrix of the series of the DataFrame.
        See Also
        --------
        Series.cov : Compute covariance with another Series.
        core.window.EWM.cov: Exponential weighted sample covariance.
        core.window.Expanding.cov : Expanding sample covariance.
        core.window.Rolling.cov : Rolling sample covariance.
        Notes
        -----
        Returns the covariance matrix of the DataFrame's time series.
        The covariance is normalized by N-1.
        For DataFrames that have Series that are missing data (assuming that
        data is `missing at random
        <https://en.wikipedia.org/wiki/Missing_data#Missing_at_random>`__)
        the returned covariance matrix will be an unbiased estimate
        of the variance and covariance between the member Series.
        However, for many applications this estimate may not be acceptable
        because the estimate covariance matrix is not guaranteed to be positive
        semi-definite. This could lead to estimate correlations having
        absolute values which are greater than one, and/or a non-invertible
        covariance matrix. See `Estimation of covariance matrices
        <https://en.wikipedia.org/w/index.php?title=Estimation_of_covariance_
        matrices>`__ for more details.
        Examples
        --------
        >>> df = pd.DataFrame([(1, 2), (0, 3), (2, 0), (1, 1)],
        ...                   columns=['dogs', 'cats'])
        >>> df.cov()
                  dogs      cats
        dogs  0.666667 -1.000000
        cats -1.000000  1.666667
        >>> np.random.seed(42)
        >>> df = pd.DataFrame(np.random.randn(1000, 5),
        ...                   columns=['a', 'b', 'c', 'd', 'e'])
        >>> df.cov()
                  a         b         c         d         e
        a  0.998438 -0.020161  0.059277 -0.008943  0.014144
        b -0.020161  1.059352 -0.008543 -0.024738  0.009826
        c  0.059277 -0.008543  1.010670 -0.001486 -0.000271
        d -0.008943 -0.024738 -0.001486  0.921297 -0.013692
        e  0.014144  0.009826 -0.000271 -0.013692  0.977795
        **Minimum number of periods**
        This method also supports an optional ``min_periods`` keyword
        that specifies the required minimum number of non-NA observations for
        each column pair in order to have a valid result:
        >>> np.random.seed(42)
        >>> df = pd.DataFrame(np.random.randn(20, 3),
        ...                   columns=['a', 'b', 'c'])
        >>> df.loc[df.index[:5], 'a'] = np.nan
        >>> df.loc[df.index[5:10], 'b'] = np.nan
        >>> df.cov(min_periods=12)
                  a         b         c
        a  0.316741       NaN -0.150812
        b       NaN  1.248003  0.191417
        c -0.150812  0.191417  0.895202
        """"""
        numeric_df = self._get_numeric_data()
        cols = numeric_df.columns
        idx = cols.copy()
        mat = numeric_df.values
        if notna(mat).all():
            if min_periods is not None and min_periods > len(mat):
                baseCov = np.empty((mat.shape[1], mat.shape[1]))
                baseCov.fill(np.nan)
            else:
                baseCov = np.cov(mat.T)
            baseCov = baseCov.reshape((len(cols), len(cols)))
        else:
            baseCov = libalgos.nancorr(ensure_float64(mat), cov=True, minp=min_periods)
        return self._constructor(baseCov, index=idx, columns=cols)
    def corrwith(self, other, axis=0, drop=False, method=""pearson"") -> Series:
        """"""
        Compute pairwise correlation.
        Pairwise correlation is computed between rows or columns of
        DataFrame with rows or columns of Series or DataFrame. DataFrames
        are first aligned along both axes before computing the
        correlations.
        Parameters
        ----------
        other : DataFrame, Series
            Object with which to compute correlations.
        axis : {0 or 'index', 1 or 'columns'}, default 0
            The axis to use. 0 or 'index' to compute column-wise, 1 or 'columns' for
            row-wise.
        drop : bool, default False
            Drop missing indices from result.
        method : {'pearson', 'kendall', 'spearman'} or callable
            Method of correlation:","[93, 97, 98, 100, 101, 103, 105]"
"import re
import os
from thefuck.utils import memoize
from thefuck import shells

patterns = (
        # js, node:
        '^    at {file}:{line}:{col}',
        # cargo:
        '^   {file}:{line}:{col}',
        # python, thefuck:
        '^  File ""{file}"", line {line}',
        # awk:
        '^awk: {file}:{line}:',
        # git
        '^fatal: bad config file line {line} in {file}',
        # llc:
        '^llc: {file}:{line}:{col}:',
        # lua:
        '^lua: {file}:{line}:',
        # fish:
        '^{file} \(line {line}\):',
        # bash, sh, ssh:
        '^{file}: line {line}: ',
        # ghc, make, ruby, zsh:
        '^{file}:{line}:',
        # cargo, clang, gcc, go, pep8, rustc:
        '^{file}:{line}:{col}',
        # perl:
        'at {file} line {line}',
    )

# for the sake of readability do not use named groups above
def _make_pattern(pattern):
    pattern = pattern.replace('{file}', '(?P<file>[^:\n]+)')
    pattern = pattern.replace('{line}', '(?P<line>[0-9]+)')
    pattern = pattern.replace('{col}',  '(?P<col>[0-9]+)')
    return re.compile(pattern, re.MULTILINE)
patterns = [_make_pattern(p) for p in patterns]

@memoize
def _search(stderr):
    for pattern in patterns:
        m = re.search(pattern, stderr)
        if m and os.path.isfile(m.group('file')):
            return m

def match(command, settings):
    if 'EDITOR' not in os.environ:
        return False
    return _search(command.stderr) or _search(command.stdout)

def get_new_command(command, settings):
    m = _search(command.stderr) or _search(command.stdout)
    # Note: there does not seem to be a standard for columns, so they are just
    # ignored for now
    editor_call = '{} {} +{}'.format(os.environ['EDITOR'],
                                     m.group('file'),
                                     m.group('line'))
    return shells.and_(editor_call, command.script)","[2, 22, 26, 63, 64, 65]"
"            if col != 0:
                raise IndexError(f""{self} only contains one item"")
            return self.values
    def should_store(self, value: ArrayLike) -> bool:
        """"""
        Can we set the given array-like value inplace?
        """"""
        return isinstance(value, self._holder)
    def set(self, locs, values):
        assert locs.tolist() == [0]
        self.values[:] = values
    def putmask(
        self, mask, new, inplace: bool = False, axis: int = 0, transpose: bool = False,
    ) -> List[""Block""]:
        """"""
        See Block.putmask.__doc__
        """"""
        inplace = validate_bool_kwarg(inplace, ""inplace"")
        mask = _extract_bool_array(mask)
        new_values = self.values if inplace else self.values.copy()
        if isinstance(new, np.ndarray) and len(new) == len(mask):
            new = new[mask]
        mask = _safe_reshape(mask, new_values.shape)
        new_values[mask] = new
        return [self.make_block(values=new_values)]
    def _maybe_coerce_values(self, values):
        """"""
        Unbox to an extension array.
        This will unbox an ExtensionArray stored in an Index or Series.
        ExtensionArrays pass through. No dtype coercion is done.
        Parameters
        ----------
        values : Index, Series, ExtensionArray
        Returns
        -------
        ExtensionArray
        """"""
        return extract_array(values)
    @property
    def _holder(self):
        # For extension blocks, the holder is values-dependent.
        return type(self.values)
    @property
    def fill_value(self):
        # Used in reindex_indexer
        return self.values.dtype.na_value
    @property
    def _can_hold_na(self):
        # The default ExtensionArray._can_hold_na is True
        return self._holder._can_hold_na
    @property
    def is_view(self) -> bool:
        """"""Extension arrays are never treated as views.""""""
        return False
    @property
    def is_numeric(self):
        return self.values.dtype._is_numeric
    def setitem(self, indexer, value):
        """"""
        Attempt self.values[indexer] = value, possibly creating a new array.
        This differs from Block.setitem by not allowing setitem to change
        the dtype of the Block.
        Parameters
        ----------
        indexer : tuple, list-like, array-like, slice
            The subset of self.values to set
        value : object
            The value being set
        Returns
        -------
        Block
        Notes
        -----
        `indexer` is a direct slice/positional indexer. `value` must
        be a compatible shape.
        """"""
        if isinstance(indexer, tuple):
            # TODO(EA2D): not needed with 2D EAs
            # we are always 1-D
            indexer = indexer[0]
        check_setitem_lengths(indexer, value, self.values)
        self.values[indexer] = value
        return self
    def get_values(self, dtype=None):
        # ExtensionArrays must be iterable, so this works.
        # TODO(EA2D): reshape not needed with 2D EAs
        return np.asarray(self.values).reshape(self.shape)
    def array_values(self) -> ExtensionArray:
        return self.values
    def to_native_types(self, na_rep=""nan"", quoting=None, **kwargs):
        """"""override to use ExtensionArray astype for the conversion""""""
        values = self.values
        mask = isna(values)
        values = np.asarray(values.astype(object))
        values[mask] = na_rep
        # TODO(EA2D): reshape not needed with 2D EAs
        # we are expected to return a 2-d ndarray
        return values.reshape(1, len(values))
",[26]
"class CustomBusinessHour(_CustomMixin, BusinessHourMixin, SingleConstructorOffset):
    """"""
    DateOffset subclass representing possibly n custom business days.
    """"""
    _prefix = ""CBH""
    _anchor = 0
    _attributes = frozenset(
        [""n"", ""normalize"", ""weekmask"", ""holidays"", ""calendar"", ""start"", ""end"", ""offset""]
    )
    def __init__(
        self,
        n=1,
        normalize=False,
        weekmask=""Mon Tue Wed Thu Fri"",
        holidays=None,
        calendar=None,
        start=""09:00"",
        end=""17:00"",
        offset=timedelta(0),
    ):
        BaseOffset.__init__(self, n, normalize)
        object.__setattr__(self, ""_offset"", offset)
        _CustomMixin.__init__(self, weekmask, holidays, calendar)
        BusinessHourMixin.__init__(self, start=start, end=end, offset=offset)

# ---------------------------------------------------------------------
# Month-Based Offset Classes

class MonthOffset(SingleConstructorOffset):
    _adjust_dst = True
    _attributes = frozenset([""n"", ""normalize""])
    __init__ = BaseOffset.__init__
    @property
    def name(self) -> str:
        if self.is_anchored:
            return self.rule_code
        else:
            month = ccalendar.MONTH_ALIASES[self.n]
            return f""{self.code_rule}-{month}""
    def is_on_offset(self, dt: datetime) -> bool:
        if self.normalize and not _is_normalized(dt):
            return False
        return dt.day == self._get_offset_day(dt)
    @apply_wraps
    def apply(self, other):
        compare_day = self._get_offset_day(other)
        n = liboffsets.roll_convention(other.day, self.n, compare_day)
        return shift_month(other, n, self._day_opt)
    @apply_index_wraps
    def apply_index(self, i):
        shifted = liboffsets.shift_months(i.asi8, self.n, self._day_opt)
        # TODO: going through __new__ raises on call to _validate_frequency;
        #  are we passing incorrect freq?
        return type(i)._simple_new(shifted, freq=i.freq, dtype=i.dtype)

class MonthEnd(MonthOffset):
    """"""
    DateOffset of one month end.
    """"""
    _prefix = ""M""
    _day_opt = ""end""

class MonthBegin(MonthOffset):
    """"""
    DateOffset of one month at beginning.
    """"""
    _prefix = ""MS""
    _day_opt = ""start""

class BusinessMonthEnd(MonthOffset):
    """"""
    DateOffset increments between business EOM dates.
    """"""
    _prefix = ""BM""
    _day_opt = ""business_end""

class BusinessMonthBegin(MonthOffset):
    """"""
    DateOffset of one business month at beginning.
    """"""
    _prefix = ""BMS""
    _day_opt = ""business_start""

class _CustomBusinessMonth(_CustomMixin, BusinessMixin, MonthOffset):
    """"""
    DateOffset subclass representing custom business month(s).
    Increments between %(bound)s of month dates.
    Parameters
    ----------
    n : int, default 1
        The number of months represented.
    normalize : bool, default False
        Normalize start/end dates to midnight before generating date range.
    weekmask : str, Default 'Mon Tue Wed Thu Fri'
        Weekmask of valid business days, passed to ``numpy.busdaycalendar``.
    holidays : list
        List/array of dates to exclude from the set of valid business days,
        passed to ``numpy.busdaycalendar``.
    calendar : pd.HolidayCalendar or np.busdaycalendar
        Calendar to integrate.
    offset : timedelta, default timedelta(0)
        Time offset to apply.
    """"""
    _attributes = frozenset(",[64]
"import operator
from shutil import get_terminal_size
import textwrap
from typing import Type, Union, cast
from warnings import warn
import numpy as np
from pandas._config import get_option
from pandas._libs import algos as libalgos, hashtable as htable, lib
from pandas.compat.numpy import function as nv
from pandas.util._decorators import (
    Appender,
    Substitution,
    cache_readonly,
    deprecate_kwarg,
)
from pandas.util._validators import validate_bool_kwarg, validate_fillna_kwargs
from pandas.core.dtypes.cast import coerce_indexer_dtype, maybe_infer_to_datetimelike
from pandas.core.dtypes.common import (
    ensure_int64,
    ensure_object,
    ensure_platform_int,
    is_categorical_dtype,
    is_datetime64_dtype,
    is_datetimelike,
    is_dict_like,
    is_dtype_equal,
    is_extension_array_dtype,
    is_float_dtype,
    is_integer_dtype,
    is_iterator,
    is_list_like,
    is_object_dtype,
    is_scalar,
    is_sequence,
    is_timedelta64_dtype,
)
from pandas.core.dtypes.dtypes import CategoricalDtype
from pandas.core.dtypes.generic import ABCDataFrame, ABCIndexClass, ABCSeries
from pandas.core.dtypes.inference import is_hashable
from pandas.core.dtypes.missing import isna, notna
from pandas._typing import ArrayLike, Dtype, Ordered
from pandas.core import ops
from pandas.core.accessor import PandasDelegate, delegate_names
import pandas.core.algorithms as algorithms
from pandas.core.algorithms import (
    _get_data_algo,
    _hashtables,
    factorize,
    take,
    take_1d,
    unique1d,
)
from pandas.core.base import NoNewAttributesMixin, PandasObject, _shared_docs
import pandas.core.common as com
from pandas.core.construction import extract_array, sanitize_array
from pandas.core.missing import interpolate_2d
from pandas.core.sorting import nargsort
from pandas.io.formats import console
from .base import ExtensionArray, _extension_array_shared_docs
_take_msg = textwrap.dedent(
    """"""\
    Interpreting negative values in 'indexer' as missing values.
    In the future, this will change to meaning positional indices
    from the right.
    Use 'allow_fill=True' to retain the previous behavior and silence this
    warning.
    Use 'allow_fill=False' to accept the new behavior.""""""
)

def _cat_compare_op(op):
    opname = ""__{op}__"".format(op=op.__name__)
    def f(self, other):
        # On python2, you can usually compare any type to any type, and
        # Categoricals can be seen as a custom type, but having different
        # results depending whether categories are the same or not is kind of
        # insane, so be a bit stricter here and use the python3 idea of
        # comparing only things of equal type.
        if isinstance(other, (ABCDataFrame, ABCSeries, ABCIndexClass)):
            return NotImplemented
        other = lib.item_from_zerodim(other)
        if is_list_like(other) and len(other) != len(self):
            # TODO: Could this fail if the categories are listlike objects?
            raise ValueError(""Lengths must match."")
        if not self.ordered:
            if opname in [""__lt__"", ""__gt__"", ""__le__"", ""__ge__""]:
                raise TypeError(
                    ""Unordered Categoricals can only compare equality or not""
                )
        if isinstance(other, Categorical):
            # Two Categoricals can only be be compared if the categories are
            # the same (maybe up to ordering, depending on ordered)
            msg = ""Categoricals can only be compared if 'categories' are the same.""
            if len(self.categories) != len(other.categories):
                raise TypeError(msg + "" Categories are different lengths"")
            elif self.ordered and not (self.categories == other.categories).all():
                raise TypeError(msg)
            elif not set(self.categories) == set(other.categories):
                raise TypeError(msg)
            if not (self.ordered == other.ordered):
                raise TypeError(
                    ""Categoricals can only be compared if 'ordered' is the same""
                )
            if not self.ordered and not self.categories.equals(other.categories):
                # both unordered and different order
                other_codes = _get_codes_for_values(other, self.categories)
            else:
                other_codes = other._codes
            mask = (self._codes == -1) | (other_codes == -1)
            f = getattr(self._codes, opname)
            ret = f(other_codes)",[59]
"        DataFrame.diff : Compute the difference of two elements in a DataFrame.
        Series.shift : Shift the index by some number of periods.
        DataFrame.shift : Shift the index by some number of periods.
        Examples
        --------
        **Series**
        >>> s = pd.Series([90, 91, 85])
        >>> s
        0    90
        1    91
        2    85
        dtype: int64
        >>> s.pct_change()
        0         NaN
        1    0.011111
        2   -0.065934
        dtype: float64
        >>> s.pct_change(periods=2)
        0         NaN
        1         NaN
        2   -0.055556
        dtype: float64
        See the percentage change in a Series where filling NAs with last
        valid observation forward to next valid.
        >>> s = pd.Series([90, 91, None, 85])
        >>> s
        0    90.0
        1    91.0
        2     NaN
        3    85.0
        dtype: float64
        >>> s.pct_change(fill_method='ffill')
        0         NaN
        1    0.011111
        2    0.000000
        3   -0.065934
        dtype: float64
        **DataFrame**
        Percentage change in French franc, Deutsche Mark, and Italian lira from
        1980-01-01 to 1980-03-01.
        >>> df = pd.DataFrame({
        ...     'FR': [4.0405, 4.0963, 4.3149],
        ...     'GR': [1.7246, 1.7482, 1.8519],
        ...     'IT': [804.74, 810.01, 860.13]},
        ...     index=['1980-01-01', '1980-02-01', '1980-03-01'])
        >>> df
                        FR      GR      IT
        1980-01-01  4.0405  1.7246  804.74
        1980-02-01  4.0963  1.7482  810.01
        1980-03-01  4.3149  1.8519  860.13
        >>> df.pct_change()
                          FR        GR        IT
        1980-01-01       NaN       NaN       NaN
        1980-02-01  0.013810  0.013684  0.006549
        1980-03-01  0.053365  0.059318  0.061876
        Percentage of change in GOOG and APPL stock volume. Shows computing
        the percentage change between columns.
        >>> df = pd.DataFrame({
        ...     '2016': [1769950, 30586265],
        ...     '2015': [1500923, 40912316],
        ...     '2014': [1371819, 41403351]},
        ...     index=['GOOG', 'APPL'])
        >>> df
                  2016      2015      2014
        GOOG   1769950   1500923   1371819
        APPL  30586265  40912316  41403351
        >>> df.pct_change(axis='columns')
              2016      2015      2014
        GOOG   NaN -0.151997 -0.086016
        APPL   NaN  0.337604  0.012002
        """"""
    @Appender(_shared_docs[""pct_change""] % _shared_doc_kwargs)
    def pct_change(self, periods=1, fill_method=""pad"", limit=None, freq=None, **kwargs):
        # TODO: Not sure if above is correct - need someone to confirm.
        axis = self._get_axis_number(kwargs.pop(""axis"", self._stat_axis_name))
        if fill_method is None:
            data = self
        else:
            data = self.fillna(method=fill_method, limit=limit, axis=axis)
        rs = data.div(data.shift(periods=periods, freq=freq, axis=axis, **kwargs)) - 1
        rs = rs.loc[~rs.index.duplicated()]
        rs = rs.reindex_like(data)
        if freq is None:
            mask = isna(com.values_from_object(data))
            np.putmask(rs.values, mask, np.nan)
        return rs
    def _agg_by_level(self, name, axis=0, level=0, skipna=True, **kwargs):
        if axis is None:
            raise ValueError(""Must specify 'axis' when aggregating by level."")
        grouped = self.groupby(level=level, axis=axis, sort=False)
        if hasattr(grouped, name) and skipna:
            return getattr(grouped, name)(**kwargs)
        axis = self._get_axis_number(axis)
        method = getattr(type(self), name)
        applyf = lambda x: method(x, axis=axis, skipna=skipna, **kwargs)
        return grouped.aggregate(applyf)
    @classmethod
    def _add_numeric_operations(cls):
        """"""
        Add the operations to the cls; evaluate the doc strings again
        """"""
        axis_descr, name, name2 = _doc_parms(cls)
        cls.any = _make_logical_function(
            cls,
            ""any"",
            name,
            name2,","[96, 97, 98, 99, 100]"
"import logging
import sys

LOGGING_CONFIG_DEFAULTS = dict(
    version=1,
    disable_existing_loggers=False,
    loggers={
        ""root"": {""level"": ""INFO"", ""handlers"": [""console""]},
        ""sanic.error"": {
            ""level"": ""INFO"",
            ""handlers"": [""error_console""],
            ""propagate"": True,
            ""qualname"": ""sanic.error"",
        },
        ""sanic.access"": {
            ""level"": ""INFO"",
            ""handlers"": [""access_console""],
            ""propagate"": True,
            ""qualname"": ""sanic.access"",
        },
    },
    handlers={
        ""console"": {
            ""class"": ""logging.StreamHandler"",
            ""formatter"": ""generic"",
            ""stream"": sys.stdout,
        },
        ""error_console"": {
            ""class"": ""logging.StreamHandler"",
            ""formatter"": ""generic"",
            ""stream"": sys.stderr,
        },
        ""access_console"": {
            ""class"": ""logging.StreamHandler"",
            ""formatter"": ""access"",
            ""stream"": sys.stdout,
        },
    },
    formatters={
        ""generic"": {
            ""format"": ""%(asctime)s [%(process)d] [%(levelname)s] %(message)s"",
            ""datefmt"": ""[%Y-%m-%d %H:%M:%S %z]"",
            ""class"": ""logging.Formatter"",
        },
        ""access"": {
            ""format"": ""%(asctime)s - (%(name)s)[%(levelname)s][%(host)s]: ""
            + ""%(request)s %(message)s %(status)d %(byte)d"",
            ""datefmt"": ""[%Y-%m-%d %H:%M:%S %z]"",
            ""class"": ""logging.Formatter"",
        },
    },
)

logger = logging.getLogger(""sanic.root"")
error_logger = logging.getLogger(""sanic.error"")
access_logger = logging.getLogger(""sanic.access"")",[8]
"    A Luigi Task describes a unit or work.
    The key methods of a Task, which must be implemented in a subclass are:
    * :py:meth:`run` - the computation done by this task.
    * :py:meth:`requires` - the list of Tasks that this Task depends on.
    * :py:meth:`output` - the output :py:class:`Target` that this Task creates.
    Parameters to the Task should be declared as members of the class, e.g.::
    .. code-block:: python
        class MyTask(luigi.Task):
            count = luigi.IntParameter()
    Each Task exposes a constructor accepting all :py:class:`Parameter` (and
    values) as kwargs. e.g. ``MyTask(count=10)`` would instantiate `MyTask`.
    In addition to any declared properties and methods, there are a few
    non-declared properties, which are created by the :py:class:`Register`
    metaclass:
    ``Task.task_namespace``
      optional string which is prepended to the task name for the sake of
      scheduling. If it isn't overridden in a Task, whatever was last declared
      using `luigi.namespace` will be used.
    ``Task._parameters``
      list of ``(parameter_name, parameter)`` tuples for this task class
    """"""
    _event_callbacks = {}
    # Priority of the task: the scheduler should favor available
    # tasks with higher priority values first.
    priority = 0
    disabled = False
    # Resources used by the task. Should be formatted like {""scp"": 1} to indicate that the
    # task requires 1 unit of the scp resource.
    resources = {}
    # Number of seconds after which to time out the run function. No timeout if set to 0. Defaults
    # to 0 or value in client.cfg
    worker_timeout = None
    @classmethod
    def event_handler(cls, event):
        """"""
        Decorator for adding event handlers.
        """"""
        def wrapped(callback):
            cls._event_callbacks.setdefault(cls, {}).setdefault(event, set()).add(callback)
            return callback
        return wrapped
    def trigger_event(self, event, *args, **kwargs):
        """"""
        Trigger that calls all of the specified events associated with this class.
        """"""
        for event_class, event_callbacks in six.iteritems(self._event_callbacks):
            if not isinstance(self, event_class):
                continue
            for callback in event_callbacks.get(event, []):
                try:
                    # callbacks are protected
                    callback(*args, **kwargs)
                except KeyboardInterrupt:
                    return
                except BaseException:
                    logger.exception(""Error in event callback for %r"", event)
    @property
    def task_module(self):
        # Returns what Python module to import to get access to this class
        # TODO(erikbern): we should think about a language-agnostic mechanism
        return self.__class__.__module__
    @property
    def task_family(self):
        """"""
        Convenience method since a property on the metaclass isn't directly accessible through the class instances.
        """"""
        return self.__class__.task_family
    @classmethod
    def get_params(cls):
        """"""
        Returns all of the Parameters for this Task.
        """"""
        # We want to do this here and not at class instantiation, or else there is no room to extend classes dynamically
        params = []
        for param_name in dir(cls):
            param_obj = getattr(cls, param_name)
            if not isinstance(param_obj, Parameter):
                continue
            params.append((param_name, param_obj))
        # The order the parameters are created matters. See Parameter class
        params.sort(key=lambda t: t[1].counter)
        return params
    @classmethod
    def get_param_values(cls, params, args, kwargs):
        """"""
        Get the values of the parameters from the args and kwargs.
        :param params: list of (param_name, Parameter).
        :param args: positional arguments
        :param kwargs: keyword arguments.
        :returns: list of `(name, value)` tuples, one for each parameter.
        """"""
        result = {}
        params_dict = dict(params)
        task_name = cls.task_family
        # In case any exceptions are thrown, create a helpful description of how the Task was invoked
        # TODO: should we detect non-reprable arguments? These will lead to mysterious errors
        exc_desc = '%s[args=%s, kwargs=%s]' % (task_name, args, kwargs)
        # Fill in the positional arguments
        positional_params = [(n, p) for n, p in params if p.significant]
        for i, arg in enumerate(args):",[125]
"# Autogenerated by boilerplate.py.  Do not edit as changes will be lost.
@_copy_docstring_and_deprecators(Axes.grid)
def grid(b=None, which='major', axis='both', **kwargs):
    return gca().grid(b=b, which=which, axis=axis, **kwargs)

# Autogenerated by boilerplate.py.  Do not edit as changes will be lost.
@_copy_docstring_and_deprecators(Axes.hexbin)
def hexbin(
        x, y, C=None, gridsize=100, bins=None, xscale='linear',
        yscale='linear', extent=None, cmap=None, norm=None, vmin=None,
        vmax=None, alpha=None, linewidths=None, edgecolors='face',
        reduce_C_function=np.mean, mincnt=None, marginals=False, *,
        data=None, **kwargs):
    __ret = gca().hexbin(
        x, y, C=C, gridsize=gridsize, bins=bins, xscale=xscale,
        yscale=yscale, extent=extent, cmap=cmap, norm=norm, vmin=vmin,
        vmax=vmax, alpha=alpha, linewidths=linewidths,
        edgecolors=edgecolors, reduce_C_function=reduce_C_function,
        mincnt=mincnt, marginals=marginals,
        **({""data"": data} if data is not None else {}), **kwargs)
    sci(__ret)
    return __ret

# Autogenerated by boilerplate.py.  Do not edit as changes will be lost.
@_copy_docstring_and_deprecators(Axes.hist)
def hist(
        x, bins=None, range=None, density=False, weights=None,
        cumulative=False, bottom=None, histtype='bar', align='mid',
        orientation='vertical', rwidth=None, log=False, color=None,
        label=None, stacked=False, *, data=None, **kwargs):
    return gca().hist(
        x, bins=bins, range=range, density=density, weights=weights,
        cumulative=cumulative, bottom=bottom, histtype=histtype,
        align=align, orientation=orientation, rwidth=rwidth, log=log,
        color=color, label=label, stacked=stacked,
        **({""data"": data} if data is not None else {}), **kwargs)

# Autogenerated by boilerplate.py.  Do not edit as changes will be lost.
@_copy_docstring_and_deprecators(Axes.hist2d)
def hist2d(
        x, y, bins=10, range=None, density=False, weights=None,
        cmin=None, cmax=None, *, data=None, **kwargs):
    __ret = gca().hist2d(
        x, y, bins=bins, range=range, density=density,
        weights=weights, cmin=cmin, cmax=cmax,
        **({""data"": data} if data is not None else {}), **kwargs)
    sci(__ret[-1])
    return __ret

# Autogenerated by boilerplate.py.  Do not edit as changes will be lost.
@_copy_docstring_and_deprecators(Axes.hlines)
def hlines(
        y, xmin, xmax, colors='k', linestyles='solid', label='', *,
        data=None, **kwargs):
    return gca().hlines(
        y, xmin, xmax, colors=colors, linestyles=linestyles,
        label=label, **({""data"": data} if data is not None else {}),
        **kwargs)

# Autogenerated by boilerplate.py.  Do not edit as changes will be lost.
@_copy_docstring_and_deprecators(Axes.imshow)
def imshow(
        X, cmap=None, norm=None, aspect=None, interpolation=None,
        alpha=None, vmin=None, vmax=None, origin=None, extent=None, *,
        filternorm=True, filterrad=4.0, resample=None, url=None,
        data=None, **kwargs):
    __ret = gca().imshow(
        X, cmap=cmap, norm=norm, aspect=aspect,
        interpolation=interpolation, alpha=alpha, vmin=vmin,
        vmax=vmax, origin=origin, extent=extent,
        filternorm=filternorm, filterrad=filterrad, resample=resample,
        url=url, **({""data"": data} if data is not None else {}),
        **kwargs)
    sci(__ret)
    return __ret

# Autogenerated by boilerplate.py.  Do not edit as changes will be lost.
@_copy_docstring_and_deprecators(Axes.legend)
def legend(*args, **kwargs):
    return gca().legend(*args, **kwargs)

# Autogenerated by boilerplate.py.  Do not edit as changes will be lost.
@_copy_docstring_and_deprecators(Axes.locator_params)
def locator_params(axis='both', tight=None, **kwargs):
    return gca().locator_params(axis=axis, tight=tight, **kwargs)

# Autogenerated by boilerplate.py.  Do not edit as changes will be lost.
@_copy_docstring_and_deprecators(Axes.loglog)
def loglog(*args, **kwargs):
    return gca().loglog(*args, **kwargs)

# Autogenerated by boilerplate.py.  Do not edit as changes will be lost.
@_copy_docstring_and_deprecators(Axes.magnitude_spectrum)
def magnitude_spectrum(
        x, Fs=None, Fc=None, window=None, pad_to=None, sides=None,
        scale=None, *, data=None, **kwargs):
    return gca().magnitude_spectrum(
        x, Fs=Fs, Fc=Fc, window=window, pad_to=pad_to, sides=sides,
        scale=scale, **({""data"": data} if data is not None else {}),
        **kwargs)

# Autogenerated by boilerplate.py.  Do not edit as changes will be lost.
@_copy_docstring_and_deprecators(Axes.margins)
def margins(*margins, x=None, y=None, tight=True):
    return gca().margins(*margins, x=x, y=y, tight=tight)

# Autogenerated by boilerplate.py.  Do not edit as changes will be lost.
@_copy_docstring_and_deprecators(Axes.minorticks_off)
def minorticks_off():
    return gca().minorticks_off()

# Autogenerated by boilerplate.py.  Do not edit as changes will be lost.
@_copy_docstring_and_deprecators(Axes.minorticks_on)
def minorticks_on():
    return gca().minorticks_on()",[56]
"        index_array: array of indices to be shuffled.
        batch_size: integer.
    # Returns
        The `index_array` array, shuffled in a batch-wise fashion.
    """"""
    batch_count = int(len(index_array) / batch_size)
    # to reshape we need to be cleanly divisible by batch size
    # we stash extra items and reappend them after shuffling
    last_batch = index_array[batch_count * batch_size:]
    index_array = index_array[:batch_count * batch_size]
    index_array = index_array.reshape((batch_count, batch_size))
    np.random.shuffle(index_array)
    index_array = index_array.flatten()
    return np.append(index_array, last_batch)

def make_batches(size, batch_size):
    """"""Returns a list of batch indices (tuples of indices).
    # Arguments
        size: Integer, total size of the data to slice into batches.
        batch_size: Integer, batch size.
    # Returns
        A list of tuples of array indices.
    """"""
    num_batches = (size + batch_size - 1) // batch_size  # round up
    return [(i * batch_size, min(size, (i + 1) * batch_size))
            for i in range(num_batches)]

def weighted_masked_objective(fn):
    """"""Adds support for masking and sample-weighting to an objective function.
    It transforms an objective function `fn(y_true, y_pred)`
    into a sample-weighted, cost-masked objective function
    `fn(y_true, y_pred, weights, mask)`.
    # Arguments
        fn: The objective function to wrap,
            with signature `fn(y_true, y_pred)`.
    # Returns
        A function with signature `fn(y_true, y_pred, weights, mask)`.
    """"""
    if fn is None:
        return None
    def weighted(y_true, y_pred, weights, mask=None):
        """"""Wrapper function.
        # Arguments
            y_true: `y_true` argument of `fn`.
            y_pred: `y_pred` argument of `fn`.
            weights: Weights tensor.
            mask: Mask tensor.
        # Returns
            Scalar tensor.
        """"""
        # score_array has ndim >= 2
        score_array = fn(y_true, y_pred)
        if mask is not None:
            # Cast the mask to floatX to avoid float64 upcasting in Theano
            mask = K.cast(mask, K.floatx())
            # mask should have the same shape as score_array
            score_array *= mask
            #  the loss per batch should be proportional
            #  to the number of unmasked samples.
            score_array /= K.mean(mask) + K.epsilon()
        # apply sample weighting
        if weights is not None:
            # reduce score_array to same ndim as weight array
            ndim = K.ndim(score_array)
            weight_ndim = K.ndim(weights)
            score_array = K.mean(score_array,
                                 axis=list(range(weight_ndim, ndim)))
            score_array *= weights
            score_array /= K.mean(K.cast(K.not_equal(weights, 0), K.floatx()))
        return K.mean(score_array)
    return weighted

def standardize_weights(y,
                        sample_weight=None,
                        class_weight=None,
                        sample_weight_mode=None):
    """"""Performs sample weight validation and standardization.
    Everything gets normalized to a single sample-wise (or timestep-wise)
    weight array.
    # Arguments
        y: Numpy array of model targets to be weighted.
        sample_weight: User-provided `sample_weight` argument.
        class_weight: User-provided `class_weight` argument.
        sample_weight_mode: One of `None` or `""temporal""`.
            `""temporal""` indicated that we expect 2D weight data
            that will be applied to the last 2 dimensions of
            the targets (i.e. we are weighting timesteps, not samples).
    # Returns
        A Numpy array of target weights, one entry per sample to weight.
    # Raises
        ValueError: In case of invalid user-provided arguments.
    """"""
    if sample_weight_mode is not None:
        if sample_weight_mode != 'temporal':
            raise ValueError('""sample_weight_mode '
                             'should be None or ""temporal"". '
                             'Found: ' + str(sample_weight_mode))
        if len(y.shape) < 3:
            raise ValueError('Found a sample_weight array for '
                             'an input with shape ' +
                             str(y.shape) + '. '
                             'Timestep-wise sample weighting (use of '
                             'sample_weight_mode=""temporal"") is restricted to '
                             'outputs that are at least 3D, i.e. that have '
                             'a time dimension.')
        if sample_weight is not None and len(sample_weight.shape) != 2:
            raise ValueError('Found a sample_weight array with shape ' +
                             str(sample_weight.shape) + '. '
                             'In order to use timestep-wise sample weighting, '
                             'you should pass a 2D sample_weight array.')",[92]
"            sample_weight: Optional array of the same length as x, containing
                weights to apply to the model's loss for each sample.
                In the case of temporal data, you can pass a 2D array
                with shape (samples, sequence_length),
                to apply a different weight to every timestep of every sample.
                In this case you should make sure to specify
                sample_weight_mode=""temporal"" in compile().
            class_weight: Optional dictionary mapping
                class indices (integers) to
                a weight (float) to apply to the model's loss for the samples
                from this class during training.
                This can be useful to tell the model to ""pay more attention"" to
                samples from an under-represented class.
        # Returns
            Scalar training loss
            (if the model has a single output and no metrics)
            or list of scalars (if the model has multiple outputs
            and/or metrics). The attribute `model.metrics_names` will give you
            the display labels for the scalar outputs.
        """"""
        x, y, sample_weights = self._standardize_user_data(
            x, y,
            sample_weight=sample_weight,
            class_weight=class_weight,
            check_batch_axis=True)
        if self.uses_learning_phase and not isinstance(K.learning_phase(), int):
            ins = x + y + sample_weights + [1.]
        else:
            ins = x + y + sample_weights
        self._make_train_function()
        outputs = self.train_function(ins)
        if len(outputs) == 1:
            return outputs[0]
        return outputs
    def test_on_batch(self, x, y, sample_weight=None):
        """"""Test the model on a single batch of samples.
        # Arguments
            x: Numpy array of test data,
                or list of Numpy arrays if the model has multiple inputs.
                If all inputs in the model are named,
                you can also pass a dictionary
                mapping input names to Numpy arrays.
            y: Numpy array of target data,
                or list of Numpy arrays if the model has multiple outputs.
                If all outputs in the model are named,
                you can also pass a dictionary
                mapping output names to Numpy arrays.
            sample_weight: Optional array of the same length as x, containing
                weights to apply to the model's loss for each sample.
                In the case of temporal data, you can pass a 2D array
                with shape (samples, sequence_length),
                to apply a different weight to every timestep of every sample.
                In this case you should make sure to specify
                sample_weight_mode=""temporal"" in compile().
        # Returns
            Scalar test loss (if the model has a single output and no metrics)
            or list of scalars (if the model has multiple outputs
            and/or metrics). The attribute `model.metrics_names` will give you
            the display labels for the scalar outputs.
        """"""
        x, y, sample_weights = self._standardize_user_data(
            x, y,
            sample_weight=sample_weight,
            check_batch_axis=True)
        if self.uses_learning_phase and not isinstance(K.learning_phase(), int):
            ins = x + y + sample_weights + [0.]
        else:
            ins = x + y + sample_weights
        self._make_test_function()
        outputs = self.test_function(ins)
        if len(outputs) == 1:
            return outputs[0]
        return outputs
    def predict_on_batch(self, x):
        """"""Returns predictions for a single batch of samples.
        # Arguments
            x: Input samples, as a Numpy array.
        # Returns
            Numpy array(s) of predictions.
        """"""
        x = _standardize_input_data(x, self._feed_input_names,
                                    self._feed_input_shapes)
        if self.uses_learning_phase and not isinstance(K.learning_phase(), int):
            ins = x + [0.]
        else:
            ins = x
        self._make_predict_function()
        outputs = self.predict_function(ins)
        if len(outputs) == 1:
            return outputs[0]
        return outputs
    @interfaces.legacy_generator_methods_support
    def fit_generator(self,
                      generator,
                      steps_per_epoch,
                      epochs=1,
                      verbose=1,
                      callbacks=None,
                      validation_data=None,
                      validation_steps=None,
                      class_weight=None,
                      max_queue_size=10,
                      workers=1,
                      use_multiprocessing=False,
                      shuffle=True,
                      initial_epoch=0):
        """"""Fits the model on data yielded batch-by-batch by a Python generator.
        The generator is run in parallel to the model, for efficiency.
        For instance, this allows you to do real-time data augmentation
        on images on CPU in parallel to training your model on GPU.
        The use of `keras.utils.Sequence` guarantees the ordering
        and guarantees the single use of every input per epoch when
        using `use_multiprocessing=True`.
        # Arguments
            generator: A generator or an instance of `Sequence` (`keras.utils.Sequence`)
                    object in order to avoid duplicate data",[102]
"""""""Download handlers for http and https schemes""""""
import re
import logging
from io import BytesIO
from time import time
from six.moves.urllib.parse import urldefrag
from zope.interface import implementer
from twisted.internet import defer, reactor, protocol
from twisted.web.http_headers import Headers as TxHeaders
from twisted.web.iweb import IBodyProducer, UNKNOWN_LENGTH
from twisted.internet.error import TimeoutError
from twisted.web.http import PotentialDataLoss
from scrapy.xlib.tx import Agent, ProxyAgent, ResponseDone, \
    HTTPConnectionPool, TCP4ClientEndpoint
from scrapy.http import Headers
from scrapy.responsetypes import responsetypes
from scrapy.core.downloader.webclient import _parse
from scrapy.utils.misc import load_object
from scrapy.utils.python import to_bytes, to_unicode
from scrapy import twisted_version
logger = logging.getLogger(__name__)

class HTTP11DownloadHandler(object):
    def __init__(self, settings):
        self._pool = HTTPConnectionPool(reactor, persistent=True)
        self._pool.maxPersistentPerHost = settings.getint('CONCURRENT_REQUESTS_PER_DOMAIN')
        self._pool._factory.noisy = False
        self._contextFactoryClass = load_object(settings['DOWNLOADER_CLIENTCONTEXTFACTORY'])
        self._contextFactory = self._contextFactoryClass()
        self._default_maxsize = settings.getint('DOWNLOAD_MAXSIZE')
        self._default_warnsize = settings.getint('DOWNLOAD_WARNSIZE')
        self._disconnect_timeout = 1
    def download_request(self, request, spider):
        """"""Return a deferred for the HTTP download""""""
        agent = ScrapyAgent(contextFactory=self._contextFactory, pool=self._pool,
            maxsize=getattr(spider, 'download_maxsize', self._default_maxsize),
            warnsize=getattr(spider, 'download_warnsize', self._default_warnsize))
        return agent.download_request(request)
    def close(self):
        d = self._pool.closeCachedConnections()
        # closeCachedConnections will hang on network or server issues, so
        # we'll manually timeout the deferred.
        #
        # Twisted issue addressing this problem can be found here:
        # https://twistedmatrix.com/trac/ticket/7738.
        #
        # closeCachedConnections doesn't handle external errbacks, so we'll
        # issue a callback after `_disconnect_timeout` seconds.
        delayed_call = reactor.callLater(self._disconnect_timeout, d.callback, [])
        def cancel_delayed_call(result):
            if delayed_call.active():
                delayed_call.cancel()
            return result
        d.addBoth(cancel_delayed_call)
        return d

class TunnelError(Exception):
    """"""An HTTP CONNECT tunnel could not be established by the proxy.""""""

class TunnelingTCP4ClientEndpoint(TCP4ClientEndpoint):
    """"""An endpoint that tunnels through proxies to allow HTTPS downloads. To
    accomplish that, this endpoint sends an HTTP CONNECT to the proxy.
    The HTTP CONNECT is always sent when using this endpoint, I think this could
    be improved as the CONNECT will be redundant if the connection associated
    with this endpoint comes from the pool and a CONNECT has already been issued
    for it.
    """"""
    _responseMatcher = re.compile('HTTP/1\.. 200')
    def __init__(self, reactor, host, port, proxyConf, contextFactory,
                 timeout=30, bindAddress=None):
        proxyHost, proxyPort, self._proxyAuthHeader = proxyConf
        super(TunnelingTCP4ClientEndpoint, self).__init__(reactor, proxyHost,
            proxyPort, timeout, bindAddress)
        self._tunnelReadyDeferred = defer.Deferred()
        self._tunneledHost = host
        self._tunneledPort = port
        self._contextFactory = contextFactory
    def requestTunnel(self, protocol):
        """"""Asks the proxy to open a tunnel.""""""
        tunnelReq = 'CONNECT %s:%s HTTP/1.1\r\n' % (self._tunneledHost,
                                                  self._tunneledPort)
        if self._proxyAuthHeader:
            tunnelReq += 'Proxy-Authorization: %s\r\n' % self._proxyAuthHeader
        tunnelReq += '\r\n'
        protocol.transport.write(tunnelReq)
        self._protocolDataReceived = protocol.dataReceived
        protocol.dataReceived = self.processProxyResponse
        self._protocol = protocol
        return protocol
    def processProxyResponse(self, bytes):
        """"""Processes the response from the proxy. If the tunnel is successfully
        created, notifies the client that we are ready to send requests. If not
        raises a TunnelError.
        """"""
        self._protocol.dataReceived = self._protocolDataReceived
        if  TunnelingTCP4ClientEndpoint._responseMatcher.match(bytes):
            self._protocol.transport.startTLS(self._contextFactory,
                                              self._protocolFactory)
            self._tunnelReadyDeferred.callback(self._protocol)
        else:
            self._tunnelReadyDeferred.errback(
                TunnelError('Could not open CONNECT tunnel.'))
    def connectFailed(self, reason):
        """"""Propagates the errback to the appropriate deferred.""""""
        self._tunnelReadyDeferred.errback(reason)
    def connect(self, protocolFactory):
        self._protocolFactory = protocolFactory
        connectDeferred = super(TunnelingTCP4ClientEndpoint,
                                self).connect(protocolFactory)","[80, 94, 95, 97, 98]"
"import logging
from enum import Enum
from typing import Any, Dict, List, Optional, Union
from pydantic import BaseModel, Schema as PSchema
from pydantic.types import UrlStr
try:
    import email_validator
    assert email_validator  # make autoflake ignore the unused import
    from pydantic.types import EmailStr  # type: ignore
except ImportError:  # pragma: no cover
    logging.warning(
        ""email-validator not installed, email fields will be treated as str.\n""
        + ""To install, run: pip install email-validator""
    )
    class EmailStr(str):  # type: ignore
        pass

class Contact(BaseModel):
    name: Optional[str] = None
    url: Optional[UrlStr] = None
    email: Optional[EmailStr] = None

class License(BaseModel):
    name: str
    url: Optional[UrlStr] = None

class Info(BaseModel):
    title: str
    description: Optional[str] = None
    termsOfService: Optional[str] = None
    contact: Optional[Contact] = None
    license: Optional[License] = None
    version: str

class ServerVariable(BaseModel):
    enum: Optional[List[str]] = None
    default: str
    description: Optional[str] = None

class Server(BaseModel):
    url: UrlStr
    description: Optional[str] = None
    variables: Optional[Dict[str, ServerVariable]] = None

class Reference(BaseModel):
    ref: str = PSchema(..., alias=""$ref"")  # type: ignore

class Discriminator(BaseModel):
    propertyName: str
    mapping: Optional[Dict[str, str]] = None

class XML(BaseModel):
    name: Optional[str] = None
    namespace: Optional[str] = None
    prefix: Optional[str] = None
    attribute: Optional[bool] = None
    wrapped: Optional[bool] = None

class ExternalDocumentation(BaseModel):
    description: Optional[str] = None
    url: UrlStr

class SchemaBase(BaseModel):
    ref: Optional[str] = PSchema(None, alias=""$ref"")  # type: ignore
    title: Optional[str] = None
    multipleOf: Optional[float] = None
    maximum: Optional[float] = None
    exclusiveMaximum: Optional[float] = None
    minimum: Optional[float] = None
    exclusiveMinimum: Optional[float] = None
    maxLength: Optional[int] = PSchema(None, gte=0)  # type: ignore
    minLength: Optional[int] = PSchema(None, gte=0)  # type: ignore
    pattern: Optional[str] = None
    maxItems: Optional[int] = PSchema(None, gte=0)  # type: ignore
    minItems: Optional[int] = PSchema(None, gte=0)  # type: ignore
    uniqueItems: Optional[bool] = None
    maxProperties: Optional[int] = PSchema(None, gte=0)  # type: ignore
    minProperties: Optional[int] = PSchema(None, gte=0)  # type: ignore
    required: Optional[List[str]] = None
    enum: Optional[List[str]] = None
    type: Optional[str] = None
    allOf: Optional[List[Any]] = None
    oneOf: Optional[List[Any]] = None
    anyOf: Optional[List[Any]] = None
    not_: Optional[List[Any]] = PSchema(None, alias=""not"")  # type: ignore
    items: Optional[Any] = None
    properties: Optional[Dict[str, Any]] = None
    additionalProperties: Optional[Union[bool, Any]] = None
    description: Optional[str] = None
    format: Optional[str] = None
    default: Optional[Any] = None
    nullable: Optional[bool] = None
    discriminator: Optional[Discriminator] = None
    readOnly: Optional[bool] = None
    writeOnly: Optional[bool] = None
    xml: Optional[XML] = None
    externalDocs: Optional[ExternalDocumentation] = None
    example: Optional[Any] = None
    deprecated: Optional[bool] = None

class Schema(SchemaBase):
    allOf: Optional[List[SchemaBase]] = None
    oneOf: Optional[List[SchemaBase]] = None
    anyOf: Optional[List[SchemaBase]] = None
    not_: Optional[List[SchemaBase]] = PSchema(None, alias=""not"")  # type: ignore
    items: Optional[SchemaBase] = None
    properties: Optional[Dict[str, SchemaBase]] = None
    additionalProperties: Optional[Union[bool, SchemaBase]] = None

class Example(BaseModel):
    summary: Optional[str] = None","[101, 122]"
"        self._scrape_next(spider, slot)
        return dfd
    def _scrape_next(self, spider, slot):
        while slot.queue:
            response, request, deferred = slot.next_response_request_deferred()
            self._scrape(response, request, spider).chainDeferred(deferred)
    def _scrape(self, response, request, spider):
        """"""Handle the downloaded response or failure trough the spider
        callback/errback""""""
        assert isinstance(response, (Response, Failure))
        dfd = self._scrape2(response, request, spider) # returns spiders processed output
        dfd.addErrback(self.handle_spider_error, request, response, spider)
        dfd.addCallback(self.handle_spider_output, request, response, spider)
        return dfd
    def _scrape2(self, request_result, request, spider):
        """"""Handle the different cases of request's result been a Response or a
        Failure""""""
        if not isinstance(request_result, Failure):
            return self.spidermw.scrape_response(
                self.call_spider, request_result, request, spider)
        else:
            # FIXME: don't ignore errors in spider middleware
            dfd = self.call_spider(request_result, request, spider)
            return dfd.addErrback(
                self._log_download_errors, request_result, request, spider)
    def call_spider(self, result, request, spider):
        result.request = request
        dfd = defer_result(result)
        dfd.addCallbacks(request.callback or spider.parse, request.errback)
        return dfd.addCallback(iterate_spider_output)
    def handle_spider_error(self, _failure, request, response, spider):
        exc = _failure.value
        if isinstance(exc, CloseSpider):
            self.crawler.engine.close_spider(spider, exc.reason or 'cancelled')
            return
        referer = request.headers.get('Referer')
        logger.error(
            ""Spider error processing %(request)s (referer: %(referer)s)"",
            {'request': request, 'referer': referer},
            extra={'spider': spider, 'failure': _failure}
        )
        self.signals.send_catch_log(
            signal=signals.spider_error,
            failure=_failure, response=response,
            spider=spider
        )
        self.crawler.stats.inc_value(
            ""spider_exceptions/%s"" % _failure.value.__class__.__name__,
            spider=spider
        )
    def handle_spider_output(self, result, request, response, spider):
        if not result:
            return defer_succeed(None)
        it = iter_errback(result, self.handle_spider_error, request, response, spider)
        dfd = parallel(it, self.concurrent_items,
            self._process_spidermw_output, request, response, spider)
        return dfd
    def _process_spidermw_output(self, output, request, response, spider):
        """"""Process each Request/Item (given in the output parameter) returned
        from the given spider
        """"""
        if isinstance(output, Request):
            self.crawler.engine.crawl(request=output, spider=spider)
        elif isinstance(output, (BaseItem, dict)):
            self.slot.itemproc_size += 1
            dfd = self.itemproc.process_item(output, spider)
            dfd.addBoth(self._itemproc_finished, output, response, spider)
            return dfd
        elif output is None:
            pass
        else:
            typename = type(output).__name__
            logger.error('Spider must return Request, BaseItem, dict or None, '
                         'got %(typename)r in %(request)s',
                         {'request': request, 'typename': typename},
                         extra={'spider': spider})
    def _log_download_errors(self, spider_failure, download_failure, request, spider):
        """"""Log and silence errors that come from the engine (typically download
        errors that got propagated thru here)
        """"""
        if (isinstance(download_failure, Failure) and
                not download_failure.check(IgnoreRequest)):
            if download_failure.frames:
                logger.error('Error downloading %(request)s',
                             {'request': request},
                             extra={'spider': spider, 'failure': download_failure})
            else:
                errmsg = download_failure.getErrorMessage()
                if errmsg:
                    logger.error('Error downloading %(request)s: %(errmsg)s',
                                 {'request': request, 'errmsg': errmsg},
                                 extra={'spider': spider})
        if spider_failure is not download_failure:
            return spider_failure
    def _itemproc_finished(self, output, item, response, spider):
        """"""ItemProcessor finished for the given ``item`` and returned ``output``
        """"""
        self.slot.itemproc_size -= 1
        if isinstance(output, Failure):
            ex = output.value
            if isinstance(ex, DropItem):
                logkws = self.logformatter.dropped(item, ex, response, spider)
                logger.log(*logformatter_adapter(logkws), extra={'spider': spider})
                return self.signals.send_catch_log_deferred(
                    signal=signals.item_dropped, item=item, response=response,
                    spider=spider, exception=output.value)
            else:
                logger.error('Error processing %(item)s', {'item': item},
                             extra={'spider': spider, 'failure': output})
        else:
            logkws = self.logformatter.scraped(output, response, spider)
            logger.log(*logformatter_adapter(logkws), extra={'spider': spider})
            return self.signals.send_catch_log_deferred(
                signal=signals.item_scraped, item=output, response=response,
                spider=spider)","[45, 94, 119]"
"    description: Optional[str] = None
    format: Optional[str] = None
    default: Optional[Any] = None
    nullable: Optional[bool] = None
    discriminator: Optional[Discriminator] = None
    readOnly: Optional[bool] = None
    writeOnly: Optional[bool] = None
    xml: Optional[XML] = None
    externalDocs: Optional[ExternalDocumentation] = None
    example: Optional[Any] = None
    deprecated: Optional[bool] = None

class Schema(SchemaBase):
    allOf: Optional[List[SchemaBase]] = None
    oneOf: Optional[List[SchemaBase]] = None
    anyOf: Optional[List[SchemaBase]] = None
    not_: Optional[List[SchemaBase]] = PSchema(None, alias=""not"")  # type: ignore
    items: Optional[SchemaBase] = None
    properties: Optional[Dict[str, SchemaBase]] = None
    additionalProperties: Optional[Union[bool, SchemaBase]] = None

class Example(BaseModel):
    summary: Optional[str] = None
    description: Optional[str] = None
    value: Optional[Any] = None
    externalValue: Optional[UrlStr] = None

class ParameterInType(Enum):
    query = ""query""
    header = ""header""
    path = ""path""
    cookie = ""cookie""

class Encoding(BaseModel):
    contentType: Optional[str] = None
    # Workaround OpenAPI recursive reference, using Any
    headers: Optional[Dict[str, Union[Any, Reference]]] = None
    style: Optional[str] = None
    explode: Optional[bool] = None
    allowReserved: Optional[bool] = None

class MediaType(BaseModel):
    schema_: Optional[Union[Schema, Reference]] = PSchema(
        None, alias=""schema""
    )  # type: ignore
    example: Optional[Any] = None
    examples: Optional[Dict[str, Union[Example, Reference]]] = None
    encoding: Optional[Dict[str, Encoding]] = None

class ParameterBase(BaseModel):
    description: Optional[str] = None
    required: Optional[bool] = None
    deprecated: Optional[bool] = None
    # Serialization rules for simple scenarios
    style: Optional[str] = None
    explode: Optional[bool] = None
    allowReserved: Optional[bool] = None
    schema_: Optional[Union[Schema, Reference]] = PSchema(
        None, alias=""schema""
    )  # type: ignore
    example: Optional[Any] = None
    examples: Optional[Dict[str, Union[Example, Reference]]] = None
    # Serialization rules for more complex scenarios
    content: Optional[Dict[str, MediaType]] = None

class Parameter(ParameterBase):
    name: str
    in_: ParameterInType = PSchema(..., alias=""in"")  # type: ignore

class Header(ParameterBase):
    pass

# Workaround OpenAPI recursive reference
class EncodingWithHeaders(Encoding):
    headers: Optional[Dict[str, Union[Header, Reference]]] = None

class RequestBody(BaseModel):
    description: Optional[str] = None
    content: Dict[str, MediaType]
    required: Optional[bool] = None

class Link(BaseModel):
    operationRef: Optional[str] = None
    operationId: Optional[str] = None
    parameters: Optional[Dict[str, Union[Any, str]]] = None
    requestBody: Optional[Union[Any, str]] = None
    description: Optional[str] = None
    server: Optional[Server] = None

class Response(BaseModel):
    description: str
    headers: Optional[Dict[str, Union[Header, Reference]]] = None
    content: Optional[Dict[str, MediaType]] = None
    links: Optional[Dict[str, Union[Link, Reference]]] = None

class Responses(BaseModel):
    default: Response

class Operation(BaseModel):
    tags: Optional[List[str]] = None
    summary: Optional[str] = None
    description: Optional[str] = None
    externalDocs: Optional[ExternalDocumentation] = None
    operationId: Optional[str] = None
    parameters: Optional[List[Union[Parameter, Reference]]] = None
    requestBody: Optional[Union[RequestBody, Reference]] = None
    responses: Union[Responses, Dict[Union[str], Response]]
    # Workaround OpenAPI recursive reference
    callbacks: Optional[Dict[str, Union[Dict[str, Any], Reference]]] = None
    deprecated: Optional[bool] = None
    security: Optional[List[Dict[str, List[str]]]] = None
    servers: Optional[List[Server]] = None
","[20, 120]"
"        raise ValueError(""No UTC format matches {}"".format(utcTime))

class RecentRunHandler(BaseTaskHistoryHandler):
    def get(self):
        tasks = self._scheduler.task_history.find_latest_runs()
        self.render(""recent.html"", tasks=tasks)

class ByNameHandler(BaseTaskHistoryHandler):
    def get(self, name):
        tasks = self._scheduler.task_history.find_all_by_name(name)
        self.render(""recent.html"", tasks=tasks)

class ByIdHandler(BaseTaskHistoryHandler):
    def get(self, id):
        task = self._scheduler.task_history.find_task_by_id(id)
        self.render(""show.html"", task=task)

class ByParamsHandler(BaseTaskHistoryHandler):
    def get(self, name):
        payload = self.get_argument('data', default=""{}"")
        arguments = json.loads(payload)
        tasks = self._scheduler.task_history.find_all_by_parameters(name, session=None, **arguments)
        self.render(""recent.html"", tasks=tasks)

class RootPathHandler(BaseTaskHistoryHandler):
    def get(self):
        self.redirect(""/static/visualiser/index.html"")

class MetricsHandler(tornado.web.RequestHandler):
    def initialize(self, scheduler):
        self._scheduler = scheduler
    def get(self):
        metrics = self._scheduler._state._metrics_collector.generate_latest()
        if metrics:
            metrics.configure_http_handler(self)
            self.write(metrics)

def app(scheduler):
    settings = {""static_path"": os.path.join(os.path.dirname(__file__), ""static""),
                ""unescape"": tornado.escape.xhtml_unescape,
                ""compress_response"": True,
                }
    handlers = [
        (r'/api/(.*)', RPCHandler, {""scheduler"": scheduler}),
        (r'/', RootPathHandler, {'scheduler': scheduler}),
        (r'/tasklist', AllRunHandler, {'scheduler': scheduler}),
        (r'/tasklist/(.*?)', SelectedRunHandler, {'scheduler': scheduler}),
        (r'/history', RecentRunHandler, {'scheduler': scheduler}),
        (r'/history/by_name/(.*?)', ByNameHandler, {'scheduler': scheduler}),
        (r'/history/by_id/(.*?)', ByIdHandler, {'scheduler': scheduler}),
        (r'/history/by_params/(.*?)', ByParamsHandler, {'scheduler': scheduler}),
        (r'/metrics', MetricsHandler, {'scheduler': scheduler})
    ]
    api_app = tornado.web.Application(handlers, **settings)
    return api_app

def _init_api(scheduler, api_port=None, address=None, unix_socket=None):
    api_app = app(scheduler)
    if unix_socket is not None:
        api_sockets = [tornado.netutil.bind_unix_socket(unix_socket)]
    else:
        api_sockets = tornado.netutil.bind_sockets(api_port, address=address)
    server = tornado.httpserver.HTTPServer(api_app)
    server.add_sockets(api_sockets)
    # Return the bound socket names.  Useful for connecting client in test scenarios.
    return [s.getsockname() for s in api_sockets]

def run(api_port=8082, address=None, unix_socket=None, scheduler=None):
    """"""
    Runs one instance of the API server.
    """"""
    if scheduler is None:
        scheduler = Scheduler()
    # load scheduler state
    scheduler.load()
    _init_api(
        scheduler=scheduler,
        api_port=api_port,
        address=address,
        unix_socket=unix_socket,
    )
    # prune work DAG every 60 seconds
    pruner = tornado.ioloop.PeriodicCallback(scheduler.prune, 60000)
    pruner.start()
    def shutdown_handler(signum, frame):
        exit_handler()
        sys.exit(0)
    @atexit.register
    def exit_handler():
        logger.info(""Scheduler instance shutting down"")
        scheduler.dump()
        stop()
    signal.signal(signal.SIGINT, shutdown_handler)
    signal.signal(signal.SIGTERM, shutdown_handler)
    if os.name == 'nt':
        signal.signal(signal.SIGBREAK, shutdown_handler)
    else:
        signal.signal(signal.SIGQUIT, shutdown_handler)
    logger.info(""Scheduler starting up"")
    tornado.ioloop.IOLoop.instance().start()

def stop():
    tornado.ioloop.IOLoop.instance().stop()

if __name__ == ""__main__"":","[39, 41]"
"            self._for_loop_variable += 1
            return True
        return False
    def maybe_decrement_after_for_loop_variable(self, leaf: Leaf) -> bool:
        """"""See `maybe_increment_for_loop_variable` above for explanation.""""""
        if self._for_loop_variable and leaf.type == token.NAME and leaf.value == ""in"":
            self.depth -= 1
            self._for_loop_variable -= 1
            return True
        return False
    def maybe_increment_lambda_arguments(self, leaf: Leaf) -> bool:
        """"""In a lambda expression, there might be more than one argument.
        To avoid splitting on the comma in this situation, increase the depth of
        tokens between `lambda` and `:`.
        """"""
        if leaf.type == token.NAME and leaf.value == ""lambda"":
            self.depth += 1
            self._lambda_arguments += 1
            return True
        return False
    def maybe_decrement_after_lambda_arguments(self, leaf: Leaf) -> bool:
        """"""See `maybe_increment_lambda_arguments` above for explanation.""""""
        if self._lambda_arguments and leaf.type == token.COLON:
            self.depth -= 1
            self._lambda_arguments -= 1
            return True
        return False
    def get_open_lsqb(self) -> Optional[Leaf]:
        """"""Return the most recent opening square bracket (if any).""""""
        return self.bracket_match.get((self.depth - 1, token.RSQB))

@dataclass
class Line:
    """"""Holds leaves and comments. Can be printed with `str(line)`.""""""
    depth: int = 0
    leaves: List[Leaf] = Factory(list)
    comments: List[Tuple[Index, Leaf]] = Factory(list)
    bracket_tracker: BracketTracker = Factory(BracketTracker)
    inside_brackets: bool = False
    should_explode: bool = False
    def append(self, leaf: Leaf, preformatted: bool = False) -> None:
        """"""Add a new `leaf` to the end of the line.
        Unless `preformatted` is True, the `leaf` will receive a new consistent
        whitespace prefix and metadata applied by :class:`BracketTracker`.
        Trailing commas are maybe removed, unpacked for loop variables are
        demoted from being delimiters.
        Inline comments are put aside.
        """"""
        has_value = leaf.type in BRACKETS or bool(leaf.value.strip())
        if not has_value:
            return
        if token.COLON == leaf.type and self.is_class_paren_empty:
            del self.leaves[-2:]
        if self.leaves and not preformatted:
            # Note: at this point leaf.prefix should be empty except for
            # imports, for which we only preserve newlines.
            leaf.prefix += whitespace(
                leaf, complex_subscript=self.is_complex_subscript(leaf)
            )
        if self.inside_brackets or not preformatted:
            self.bracket_tracker.mark(leaf)
            self.maybe_remove_trailing_comma(leaf)
        if not self.append_comment(leaf):
            self.leaves.append(leaf)
    def append_safe(self, leaf: Leaf, preformatted: bool = False) -> None:
        """"""Like :func:`append()` but disallow invalid standalone comment structure.
        Raises ValueError when any `leaf` is appended after a standalone comment
        or when a standalone comment is not the first leaf on the line.
        """"""
        if self.bracket_tracker.depth == 0:
            if self.is_comment:
                raise ValueError(""cannot append to standalone comments"")
            if self.leaves and leaf.type == STANDALONE_COMMENT:
                raise ValueError(
                    ""cannot append standalone comments to a populated line""
                )
        self.append(leaf, preformatted=preformatted)
    @property
    def is_comment(self) -> bool:
        """"""Is this line a standalone comment?""""""
        return len(self.leaves) == 1 and self.leaves[0].type == STANDALONE_COMMENT
    @property
    def is_decorator(self) -> bool:
        """"""Is this line a decorator?""""""
        return bool(self) and self.leaves[0].type == token.AT
    @property
    def is_import(self) -> bool:
        """"""Is this an import line?""""""
        return bool(self) and is_import(self.leaves[0])
    @property
    def is_class(self) -> bool:
        """"""Is this line a class definition?""""""
        return (
            bool(self)
            and self.leaves[0].type == token.NAME
            and self.leaves[0].value == ""class""
        )
    @property
    def is_stub_class(self) -> bool:
        """"""Is this line a class definition with a body consisting only of ""...""?""""""
        return self.is_class and self.leaves[-3:] == [
            Leaf(token.DOT, ""."") for _ in range(3)
        ]","[7, 9, 22, 29, 31]"
"""""""This module implements the Scraper component which parses responses and
extracts information from them""""""
import logging
from collections import deque
from twisted.python.failure import Failure
from twisted.internet import defer
from scrapy.utils.defer import defer_result, defer_succeed, parallel, iter_errback
from scrapy.utils.spider import iterate_spider_output
from scrapy.utils.misc import load_object
from scrapy.utils.log import logformatter_adapter
from scrapy.exceptions import CloseSpider, DropItem, IgnoreRequest
from scrapy import signals
from scrapy.http import Request, Response
from scrapy.item import BaseItem
from scrapy.core.spidermw import SpiderMiddlewareManager
logger = logging.getLogger(__name__)

class Slot(object):
    """"""Scraper slot (one per running spider)""""""
    MIN_RESPONSE_SIZE = 1024
    def __init__(self, max_active_size=5000000):
        self.max_active_size = max_active_size
        self.queue = deque()
        self.active = set()
        self.active_size = 0
        self.itemproc_size = 0
        self.closing = None
    def add_response_request(self, response, request):
        deferred = defer.Deferred()
        self.queue.append((response, request, deferred))
        if isinstance(response, Response):
            self.active_size += max(len(response.body), self.MIN_RESPONSE_SIZE)
        else:
            self.active_size += self.MIN_RESPONSE_SIZE
        return deferred
    def next_response_request_deferred(self):
        response, request, deferred = self.queue.popleft()
        self.active.add(request)
        return response, request, deferred
    def finish_response(self, response, request):
        self.active.remove(request)
        if isinstance(response, Response):
            self.active_size -= max(len(response.body), self.MIN_RESPONSE_SIZE)
        else:
            self.active_size -= self.MIN_RESPONSE_SIZE
    def is_idle(self):
        return not (self.queue or self.active)
    def needs_backout(self):
        return self.active_size > self.max_active_size

class Scraper(object):
    def __init__(self, crawler):
        self.slot = None
        self.spidermw = SpiderMiddlewareManager.from_crawler(crawler)
        itemproc_cls = load_object(crawler.settings['ITEM_PROCESSOR'])
        self.itemproc = itemproc_cls.from_crawler(crawler)
        self.concurrent_items = crawler.settings.getint('CONCURRENT_ITEMS')
        self.crawler = crawler
        self.signals = crawler.signals
        self.logformatter = crawler.logformatter
    @defer.inlineCallbacks
    def open_spider(self, spider):
        """"""Open the given spider for scraping and allocate resources for it""""""
        self.slot = Slot()
        yield self.itemproc.open_spider(spider)
    def close_spider(self, spider):
        """"""Close a spider being scraped and release its resources""""""
        slot = self.slot
        slot.closing = defer.Deferred()
        slot.closing.addCallback(self.itemproc.close_spider)
        self._check_if_closing(spider, slot)
        return slot.closing
    def is_idle(self):
        """"""Return True if there isn't any more spiders to process""""""
        return not self.slot
    def _check_if_closing(self, spider, slot):
        if slot.closing and slot.is_idle():
            slot.closing.callback(spider)
    def enqueue_scrape(self, response, request, spider):
        slot = self.slot
        dfd = slot.add_response_request(response, request)
        def finish_scraping(_):
            slot.finish_response(response, request)
            self._check_if_closing(spider, slot)
            self._scrape_next(spider, slot)
            return _
        dfd.addBoth(finish_scraping)
        dfd.addErrback(
            lambda f: logger.error('Scraper bug processing %(request)s',
                                   {'request': request},
                                   extra={'spider': spider, 'failure': f}))
        self._scrape_next(spider, slot)
        return dfd
    def _scrape_next(self, spider, slot):
        while slot.queue:
            response, request, deferred = slot.next_response_request_deferred()
            self._scrape(response, request, spider).chainDeferred(deferred)
    def _scrape(self, response, request, spider):
        """"""Handle the downloaded response or failure trough the spider
        callback/errback""""""
        assert isinstance(response, (Response, Failure))
        dfd = self._scrape2(response, request, spider) # returns spiders processed output
        dfd.addErrback(self.handle_spider_error, request, response, spider)
        dfd.addCallback(self.handle_spider_output, request, response, spider)
        return dfd","[12, 109]"
"        """"""
        Return a new Index of the values set with the mask.
        Returns
        -------
        Index
        See Also
        --------
        numpy.ndarray.putmask
        """"""
        values = self.values.copy()
        try:
            np.putmask(values, mask, self._convert_for_op(value))
            return self._shallow_copy(values)
        except (ValueError, TypeError) as err:
            if is_object_dtype(self):
                raise err
            # coerces to object
            return self.astype(object).putmask(mask, value)
    def equals(self, other):
        """"""
        Determine if two Index objects contain the same elements.
        Returns
        -------
        bool
            True if ""other"" is an Index and it has the same elements as calling
            index; False otherwise.
        """"""
        if self.is_(other):
            return True
        if not isinstance(other, Index):
            return False
        if is_object_dtype(self) and not is_object_dtype(other):
            # if other is not object, use other's logic for coercion
            return other.equals(self)
        try:
            return array_equivalent(
                com.values_from_object(self), com.values_from_object(other)
            )
        except Exception:
            return False
    def identical(self, other):
        """"""
        Similar to equals, but check that other comparable attributes are
        also equal.
        Returns
        -------
        bool
            If two Index objects have equal elements and same type True,
            otherwise False.
        """"""
        return (
            self.equals(other)
            and all(
                (
                    getattr(self, c, None) == getattr(other, c, None)
                    for c in self._comparables
                )
            )
            and type(self) == type(other)
        )
    def asof(self, label):
        """"""
        Return the label from the index, or, if not present, the previous one.
        Assuming that the index is sorted, return the passed index label if it
        is in the index, or return the previous index label if the passed one
        is not in the index.
        Parameters
        ----------
        label : object
            The label up to which the method returns the latest index label.
        Returns
        -------
        object
            The passed label if it is in the index. The previous label if the
            passed label is not in the sorted index or `NaN` if there is no
            such label.
        See Also
        --------
        Series.asof : Return the latest value in a Series up to the
            passed index.
        merge_asof : Perform an asof merge (similar to left join but it
            matches on nearest key rather than equal key).
        Index.get_loc : An `asof` is a thin wrapper around `get_loc`
            with method='pad'.
        Examples
        --------
        `Index.asof` returns the latest index label up to the passed label.
        >>> idx = pd.Index(['2013-12-31', '2014-01-02', '2014-01-03'])
        >>> idx.asof('2014-01-01')
        '2013-12-31'
        If the label is in the index, the method returns the passed label.
        >>> idx.asof('2014-01-02')
        '2014-01-02'
        If all of the labels in the index are later than the passed label,
        NaN is returned.
        >>> idx.asof('1999-01-02')
        nan
        If the index is not sorted, an error is raised.
        >>> idx_not_sorted = pd.Index(['2013-12-31', '2015-01-02',
        ...                            '2014-01-03'])
        >>> idx_not_sorted.asof('2013-12-31')
        Traceback (most recent call last):
        ValueError: index must be monotonic increasing or decreasing
        """"""","[42, 43, 44, 45, 46, 47]"
"        renderer : `.RendererBase` subclass
            renderer that will be used to draw the figures (i.e.
            ``fig.canvas.get_renderer()``)
        bbox_extra_artists : list of `.Artist` or ``None``
            List of artists to include in the tight bounding box.  If
            ``None`` (default), then all artist children of each axes are
            included in the tight bounding box.
        Returns
        -------
        `.BboxBase`
            containing the bounding box (in figure inches).
        """"""
        bb = []
        if bbox_extra_artists is None:
            artists = self.get_default_bbox_extra_artists()
        else:
            artists = bbox_extra_artists
        for a in artists:
            bbox = a.get_tightbbox(renderer)
            if bbox is not None and (bbox.width != 0 or bbox.height != 0):
                bb.append(bbox)
        for ax in self.axes:
            if ax.get_visible():
                # some axes don't take the bbox_extra_artists kwarg so we
                # need this conditional....
                try:
                    bbox = ax.get_tightbbox(
                        renderer, bbox_extra_artists=bbox_extra_artists)
                except TypeError:
                    bbox = ax.get_tightbbox(renderer)
                bb.append(bbox)
        bb = [b for b in bb
              if (np.isfinite(b.width) and np.isfinite(b.height)
                  and (b.width != 0 or b.height != 0))]
        if len(bb) == 0:
            return self.bbox_inches
        _bbox = Bbox.union(bb)
        bbox_inches = TransformedBbox(_bbox, Affine2D().scale(1 / self.dpi))
        return bbox_inches
    def init_layoutbox(self):
        """"""Initialize the layoutbox for use in constrained_layout.""""""
        if self._layoutbox is None:
            self._layoutbox = layoutbox.LayoutBox(
                parent=None, name='figlb', artist=self)
            self._layoutbox.constrain_geometry(0., 0., 1., 1.)
    def execute_constrained_layout(self, renderer=None):
        """"""
        Use ``layoutbox`` to determine pos positions within axes.
        See also `.set_constrained_layout_pads`.
        """"""
        from matplotlib._constrained_layout import do_constrained_layout
        _log.debug('Executing constrainedlayout')
        if self._layoutbox is None:
            cbook._warn_external(""Calling figure.constrained_layout, but ""
                                 ""figure not setup to do constrained layout. ""
                                 "" You either called GridSpec without the ""
                                 ""fig keyword, you are using plt.subplot, ""
                                 ""or you need to call figure or subplots ""
                                 ""with the constrained_layout=True kwarg."")
            return
        w_pad, h_pad, wspace, hspace = self.get_constrained_layout_pads()
        # convert to unit-relative lengths
        fig = self
        width, height = fig.get_size_inches()
        w_pad = w_pad / width
        h_pad = h_pad / height
        if renderer is None:
            renderer = layoutbox.get_renderer(fig)
        do_constrained_layout(fig, renderer, h_pad, w_pad, hspace, wspace)
    @cbook._delete_parameter(""3.2"", ""renderer"")
    def tight_layout(self, renderer=None, pad=1.08, h_pad=None, w_pad=None,
                     rect=None):
        """"""
        Adjust the padding between and around subplots.
        To exclude an artist on the axes from the bounding box calculation
        that determines the subplot parameters (i.e. legend, or annotation),
        set ``a.set_in_layout(False)`` for that artist.
        Parameters
        ----------
        renderer : subclass of `~.backend_bases.RendererBase`, optional
            Defaults to the renderer for the figure.  Deprecated.
        pad : float, default: 1.08
            Padding between the figure edge and the edges of subplots,
            as a fraction of the font size.
        h_pad, w_pad : float, default: *pad*
            Padding (height/width) between edges of adjacent subplots,
            as a fraction of the font size.
        rect : tuple (left, bottom, right, top), default: (0, 0, 1, 1)
            A rectangle in normalized figure coordinates into which the whole
            subplots area (including labels) will fit.
        See Also
        --------
        .Figure.set_tight_layout
        .pyplot.tight_layout
        """"""
        from .tight_layout import (
            get_renderer, get_subplotspec_list, get_tight_layout_figure)
        subplotspec_list = get_subplotspec_list(self.axes)
        if None in subplotspec_list:
            cbook._warn_external(""This figure includes Axes that are not ""
                                 ""compatible with tight_layout, so results ""
                                 ""might be incorrect."")
        if renderer is None:
            renderer = get_renderer(self)
        kwargs = get_tight_layout_figure(","[126, 127]"
"        indexer, missing = self._engine.get_indexer_non_unique(codes)
        return ensure_platform_int(indexer), missing
    @Appender(_index_shared_docs[""_convert_scalar_indexer""])
    def _convert_scalar_indexer(self, key, kind=None):
        if self.categories._defer_to_indexing:
            return self.categories._convert_scalar_indexer(key, kind=kind)
        return super()._convert_scalar_indexer(key, kind=kind)
    @Appender(_index_shared_docs[""_convert_list_indexer""])
    def _convert_list_indexer(self, keyarr, kind=None):
        # Return our indexer or raise if all of the values are not included in
        # the categories
        if self.categories._defer_to_indexing:
            indexer = self.categories._convert_list_indexer(keyarr, kind=kind)
            return Index(self.codes).get_indexer_for(indexer)
        indexer = self.categories.get_indexer(np.asarray(keyarr))
        if (indexer == -1).any():
            raise KeyError(
                ""a list-indexer must only ""
                ""include values that are ""
                ""in the categories""
            )
        return self.get_indexer(keyarr)
    @Appender(_index_shared_docs[""_convert_arr_indexer""])
    def _convert_arr_indexer(self, keyarr):
        keyarr = com.asarray_tuplesafe(keyarr)
        if self.categories._defer_to_indexing:
            return keyarr
        return self._shallow_copy(keyarr)
    @Appender(_index_shared_docs[""_convert_index_indexer""])
    def _convert_index_indexer(self, keyarr):
        return self._shallow_copy(keyarr)
    @Appender(_index_shared_docs[""take""] % _index_doc_kwargs)
    def take(self, indices, axis=0, allow_fill=True, fill_value=None, **kwargs):
        nv.validate_take(tuple(), kwargs)
        indices = ensure_platform_int(indices)
        taken = self._assert_take_fillable(
            self.codes,
            indices,
            allow_fill=allow_fill,
            fill_value=fill_value,
            na_value=-1,
        )
        return self._create_from_codes(taken)
    def is_dtype_equal(self, other):
        return self._data.is_dtype_equal(other)
    take_nd = take
    def map(self, mapper):
        """"""
        Map values using input correspondence (a dict, Series, or function).
        Maps the values (their categories, not the codes) of the index to new
        categories. If the mapping correspondence is one-to-one the result is a
        :class:`~pandas.CategoricalIndex` which has the same order property as
        the original, otherwise an :class:`~pandas.Index` is returned.
        If a `dict` or :class:`~pandas.Series` is used any unmapped category is
        mapped to `NaN`. Note that if this happens an :class:`~pandas.Index`
        will be returned.
        Parameters
        ----------
        mapper : function, dict, or Series
            Mapping correspondence.
        Returns
        -------
        pandas.CategoricalIndex or pandas.Index
            Mapped index.
        See Also
        --------
        Index.map : Apply a mapping correspondence on an
            :class:`~pandas.Index`.
        Series.map : Apply a mapping correspondence on a
            :class:`~pandas.Series`.
        Series.apply : Apply more complex functions on a
            :class:`~pandas.Series`.
        Examples
        --------
        >>> idx = pd.CategoricalIndex(['a', 'b', 'c'])
        >>> idx
        CategoricalIndex(['a', 'b', 'c'], categories=['a', 'b', 'c'],
                         ordered=False, dtype='category')
        >>> idx.map(lambda x: x.upper())
        CategoricalIndex(['A', 'B', 'C'], categories=['A', 'B', 'C'],
                         ordered=False, dtype='category')
        >>> idx.map({'a': 'first', 'b': 'second', 'c': 'third'})
        CategoricalIndex(['first', 'second', 'third'], categories=['first',
                         'second', 'third'], ordered=False, dtype='category')
        If the mapping is one-to-one the ordering of the categories is
        preserved:
        >>> idx = pd.CategoricalIndex(['a', 'b', 'c'], ordered=True)
        >>> idx
        CategoricalIndex(['a', 'b', 'c'], categories=['a', 'b', 'c'],
                         ordered=True, dtype='category')
        >>> idx.map({'a': 3, 'b': 2, 'c': 1})
        CategoricalIndex([3, 2, 1], categories=[3, 2, 1], ordered=True,
                         dtype='category')
        If the mapping is not one-to-one an :class:`~pandas.Index` is returned:
        >>> idx.map({'a': 'first', 'b': 'second', 'c': 'first'})
        Index(['first', 'second', 'first'], dtype='object')
        If a `dict` is used, all unmapped categories are mapped to `NaN` and
        the result is an :class:`~pandas.Index`:
        >>> idx.map({'a': 'first', 'b': 'second'})
        Index(['first', 'second', nan], dtype='object')
        """"""","[5, 6, 7]"
"# (c) 2012, Jan-Piet Mens <jpmens(at)gmail.com>
# (c) 2017 Ansible Project
# GNU General Public License v3.0+ (see COPYING or https://www.gnu.org/licenses/gpl-3.0.txt)
from __future__ import (absolute_import, division, print_function)
__metaclass__ = type
DOCUMENTATION = """"""
    lookup: env
    author: Jan-Piet Mens (@jpmens) <jpmens(at)gmail.com>
    version_added: ""0.9""
    short_description: read the value of environment variables
    description:
        - Allows you to query the environment variables available on the controller when you invoked Ansible.
    options:
      _terms:
        description: Environment variable or list of them to lookup the values for
        required: True
""""""
EXAMPLES = """"""
- debug: msg=""{{ lookup('env','HOME') }} is an environment variable""
""""""
RETURN = """"""
  _list:
    description:
      - values from the environment variables.
    type: list
""""""
import os
from ansible.plugins.lookup import LookupBase

class LookupModule(LookupBase):
    def run(self, terms, variables, **kwargs):
        ret = []
        for term in terms:
            var = term.split()[0]
            ret.append(os.getenv(var, ''))
        return ret","[29, 30, 41]"
"from thefuck import utils
from thefuck.utils import replace_argument
from thefuck.specific.git import git_support

@git_support
def match(command):
    return (command.script.split()[1] == 'stash'
            and 'usage:' in command.stderr)
# git's output here is too complicated to be parsed (see the test file)
stash_commands = (
    'apply',
    'branch',
    'clear',
    'drop',
    'list',
    'pop',
    'save',
    'show')

@git_support
def get_new_command(command):
    stash_cmd = command.script.split()[2]
    fixed = utils.get_closest(stash_cmd, stash_commands, fallback_to_first=False)
    if fixed is not None:
        return replace_argument(command.script, stash_cmd, fixed)
    else:
        cmd = command.script.split()
        cmd.insert(2, 'save')
        return ' '.join(cmd)","[7, 8]"
"from datetime import datetime, timedelta
from typing import Any
import weakref
import numpy as np
from pandas._libs import index as libindex
from pandas._libs.lib import no_default
from pandas._libs.tslibs import frequencies as libfrequencies, resolution
from pandas._libs.tslibs.parsing import parse_time_string
from pandas._libs.tslibs.period import Period
from pandas._typing import Label
from pandas.util._decorators import Appender, cache_readonly
from pandas.core.dtypes.common import (
    ensure_platform_int,
    is_bool_dtype,
    is_datetime64_any_dtype,
    is_dtype_equal,
    is_float,
    is_integer,
    is_object_dtype,
    is_scalar,
    pandas_dtype,
)
from pandas.core.arrays.period import (
    PeriodArray,
    period_array,
    raise_on_incompatible,
    validate_dtype_freq,
)
import pandas.core.common as com
import pandas.core.indexes.base as ibase
from pandas.core.indexes.base import (
    InvalidIndexError,
    _index_shared_docs,
    ensure_index,
    maybe_extract_name,
)
from pandas.core.indexes.datetimelike import DatetimeIndexOpsMixin
from pandas.core.indexes.datetimes import DatetimeIndex, Index
from pandas.core.indexes.extension import inherit_names
from pandas.core.indexes.numeric import Int64Index
from pandas.core.ops import get_op_result_name
from pandas.core.tools.datetimes import DateParseError
from pandas.tseries import frequencies
from pandas.tseries.offsets import DateOffset, Tick
_index_doc_kwargs = dict(ibase._index_doc_kwargs)
_index_doc_kwargs.update(dict(target_klass=""PeriodIndex or list of Periods""))
# --- Period index sketch

def _new_PeriodIndex(cls, **d):
    # GH13277 for unpickling
    values = d.pop(""data"")
    if values.dtype == ""int64"":
        freq = d.pop(""freq"", None)
        values = PeriodArray(values, freq=freq)
        return cls._simple_new(values, **d)
    else:
        return cls(values, **d)

@inherit_names(
    [""strftime"", ""to_timestamp"", ""asfreq"", ""start_time"", ""end_time""]
    + PeriodArray._field_ops,
    PeriodArray,
    wrap=True,
)
@inherit_names([""is_leap_year"", ""freq"", ""_format_native_types""], PeriodArray)
class PeriodIndex(DatetimeIndexOpsMixin, Int64Index):
    """"""
    Immutable ndarray holding ordinal values indicating regular periods in time.
    Index keys are boxed to Period objects which carries the metadata (eg,
    frequency information).
    Parameters
    ----------
    data : array-like (1d int np.ndarray or PeriodArray), optional
        Optional period-like data to construct index with.
    copy : bool
        Make a copy of input ndarray.
    freq : str or period object, optional
        One of pandas period strings or corresponding objects.
    year : int, array, or Series, default None
    month : int, array, or Series, default None
    quarter : int, array, or Series, default None
    day : int, array, or Series, default None
    hour : int, array, or Series, default None
    minute : int, array, or Series, default None
    second : int, array, or Series, default None
    tz : object, default None
        Timezone for converting datetime64 data to Periods.
    dtype : str or PeriodDtype, default None
    Attributes
    ----------
    day
    dayofweek
    dayofyear
    days_in_month
    daysinmonth
    end_time
    freq
    freqstr
    hour
    is_leap_year
    minute
    month
    quarter
    qyear
    second
    start_time
    week
    weekday
    weekofyear
    year
    Methods
    -------
    asfreq
    strftime",[11]
"        widths : array-like, default: None
          Either a scalar or a vector and sets the width of each
          box. The default is ``0.15*(distance between extreme
          positions)``, clipped to no less than 0.15 and no more than
          0.5.
        vert : bool, default: True
          If `True` (default), makes the boxes vertical.  If `False`,
          makes horizontal boxes.
        patch_artist : bool, default: False
          If `False` produces boxes with the `.Line2D` artist.
          If `True` produces boxes with the `~matplotlib.patches.Patch` artist.
        shownotches : bool, default: False
          If `False` (default), produces a rectangular box plot.
          If `True`, will produce a notched box plot
        showmeans : bool, default: False
          If `True`, will toggle on the rendering of the means
        showcaps  : bool, default: True
          If `True`, will toggle on the rendering of the caps
        showbox  : bool, default: True
          If `True`, will toggle on the rendering of the box
        showfliers : bool, default: True
          If `True`, will toggle on the rendering of the fliers
        boxprops : dict or None (default)
          If provided, will set the plotting style of the boxes
        whiskerprops : dict or None (default)
          If provided, will set the plotting style of the whiskers
        capprops : dict or None (default)
          If provided, will set the plotting style of the caps
        flierprops : dict or None (default)
          If provided will set the plotting style of the fliers
        medianprops : dict or None (default)
          If provided, will set the plotting style of the medians
        meanprops : dict or None (default)
          If provided, will set the plotting style of the means
        meanline : bool, default: False
          If `True` (and *showmeans* is `True`), will try to render the mean
          as a line spanning the full width of the box according to
          *meanprops*. Not recommended if *shownotches* is also True.
          Otherwise, means will be shown as points.
        manage_ticks : bool, default: True
          If True, the tick locations and labels will be adjusted to match the
          boxplot positions.
        zorder : scalar, default: None
          The zorder of the resulting boxplot.
        Returns
        -------
        result : dict
          A dictionary mapping each component of the boxplot to a list
          of the `.Line2D` instances created. That dictionary has the
          following keys (assuming vertical boxplots):
          - ``boxes``: the main body of the boxplot showing the
            quartiles and the median's confidence intervals if
            enabled.
          - ``medians``: horizontal lines at the median of each box.
          - ``whiskers``: the vertical lines extending to the most
            extreme, non-outlier data points.
          - ``caps``: the horizontal lines at the ends of the
            whiskers.
          - ``fliers``: points representing data that extend beyond
            the whiskers (fliers).
          - ``means``: points or lines representing the means.
        Examples
        --------
        .. plot:: gallery/statistics/bxp.py
        """"""
        # lists of artists to be output
        whiskers = []
        caps = []
        boxes = []
        medians = []
        means = []
        fliers = []
        # empty list of xticklabels
        datalabels = []
        # Use default zorder if none specified
        if zorder is None:
            zorder = mlines.Line2D.zorder
        zdelta = 0.1
        def line_props_with_rcdefaults(subkey, explicit, zdelta=0):
            d = {k.split('.')[-1]: v for k, v in rcParams.items()
                 if k.startswith(f'boxplot.{subkey}')}
            d['zorder'] = zorder + zdelta
            if explicit is not None:
                d.update(
                    cbook.normalize_kwargs(explicit, mlines.Line2D._alias_map))
            return d
        # box properties
        if patch_artist:
            final_boxprops = dict(
                linestyle=rcParams['boxplot.boxprops.linestyle'],
                linewidth=rcParams['boxplot.boxprops.linewidth'],
                edgecolor=rcParams['boxplot.boxprops.color'],
                facecolor=('white' if rcParams['_internal.classic_mode'] else
                           rcParams['patch.facecolor']),
                zorder=zorder,
            )",[108]
"import logging
from six.moves.urllib.parse import urljoin
from scrapy.http import HtmlResponse
from scrapy.utils.response import get_meta_refresh
from scrapy.exceptions import IgnoreRequest, NotConfigured
logger = logging.getLogger(__name__)

class BaseRedirectMiddleware(object):
    enabled_setting = 'REDIRECT_ENABLED'
    def __init__(self, settings):
        if not settings.getbool(self.enabled_setting):
            raise NotConfigured
        self.max_redirect_times = settings.getint('REDIRECT_MAX_TIMES')
        self.priority_adjust = settings.getint('REDIRECT_PRIORITY_ADJUST')
    @classmethod
    def from_crawler(cls, crawler):
        return cls(crawler.settings)
    def _redirect(self, redirected, request, spider, reason):
        ttl = request.meta.setdefault('redirect_ttl', self.max_redirect_times)
        redirects = request.meta.get('redirect_times', 0) + 1
        if ttl and redirects <= self.max_redirect_times:
            redirected.meta['redirect_times'] = redirects
            redirected.meta['redirect_ttl'] = ttl - 1
            redirected.meta['redirect_urls'] = request.meta.get('redirect_urls', []) + \
                [request.url]
            redirected.dont_filter = request.dont_filter
            redirected.priority = request.priority + self.priority_adjust
            logger.debug(""Redirecting (%(reason)s) to %(redirected)s from %(request)s"",
                         {'reason': reason, 'redirected': redirected, 'request': request},
                         extra={'spider': spider})
            return redirected
        else:
            logger.debug(""Discarding %(request)s: max redirections reached"",
                         {'request': request}, extra={'spider': spider})
            raise IgnoreRequest(""max redirections reached"")
    def _redirect_request_using_get(self, request, redirect_url):
        redirected = request.replace(url=redirect_url, method='GET', body='')
        redirected.headers.pop('Content-Type', None)
        redirected.headers.pop('Content-Length', None)
        return redirected

class RedirectMiddleware(BaseRedirectMiddleware):
    """"""Handle redirection of requests based on response status and meta-refresh html tag""""""
    def process_response(self, request, response, spider):
        if (request.meta.get('dont_redirect', False) or
               response.status in getattr(spider, 'handle_httpstatus_list', [])):
            return response
        if request.method == 'HEAD':
            if response.status in [301, 302, 303, 307] and 'Location' in response.headers:
                redirected_url = urljoin(request.url, response.headers['location'])
                redirected = request.replace(url=redirected_url)
                return self._redirect(redirected, request, spider, response.status)
            else:
                return response
        if response.status in [302, 303] and 'Location' in response.headers:
            redirected_url = urljoin(request.url, response.headers['location'])
            redirected = self._redirect_request_using_get(request, redirected_url)
            return self._redirect(redirected, request, spider, response.status)
        if response.status in [301, 307] and 'Location' in response.headers:
            redirected_url = urljoin(request.url, response.headers['location'])
            redirected = request.replace(url=redirected_url)
            return self._redirect(redirected, request, spider, response.status)
        return response

class MetaRefreshMiddleware(BaseRedirectMiddleware):
    enabled_setting = 'METAREFRESH_ENABLED'
    def __init__(self, settings):
        super(MetaRefreshMiddleware, self).__init__(settings)
        self._maxdelay = settings.getint('REDIRECT_MAX_METAREFRESH_DELAY',
                                         settings.getint('METAREFRESH_MAXDELAY'))
    def process_response(self, request, response, spider):
        if request.meta.get('dont_redirect', False) or request.method == 'HEAD' or \
                not isinstance(response, HtmlResponse):
            return response
        if isinstance(response, HtmlResponse):
            interval, url = get_meta_refresh(response)
            if url and interval < self._maxdelay:
                redirected = self._redirect_request_using_get(request, url)
                return self._redirect(redirected, request, spider, 'meta refresh')
        return response",[57]
"#!/usr/bin/env python
# -*- coding: utf-8 -*-
""""""
cookiecutter.generate
---------------------
Functions for generating a project from a project template.
""""""
from __future__ import unicode_literals
from collections import OrderedDict
import fnmatch
import io
import json
import logging
import os
import shutil
from jinja2 import FileSystemLoader, Template
from jinja2.environment import Environment
from jinja2.exceptions import TemplateSyntaxError
from binaryornot.check import is_binary
from .exceptions import (
    NonTemplatedInputDirException,
    ContextDecodingException,
    OutputDirExistsException
)
from .find import find_template
from .utils import make_sure_path_exists, work_in
from .hooks import run_hook, EXIT_SUCCESS

def copy_without_render(path, context):
    """"""
    Returns True if `path` matches some pattern in the
    `_copy_without_render` context setting.
    :param path: A file-system path referring to a file or dir that
        should be rendered or just copied.
    :param context: cookiecutter context.
    """"""
    try:
        for dont_render in context['cookiecutter']['_copy_without_render']:
            if fnmatch.fnmatch(path, dont_render):
                return True
    except KeyError:
        return False
    return False

def apply_overwrites_to_context(context, overwrite_context):
    """"""Modify the given context in place based on the overwrite_context.""""""
    for variable, overwrite in overwrite_context.items():
        if variable not in context:
            # Do not include variables which are not used in the template
            continue
        context_value = context[variable]
        if isinstance(context_value, list):
            # We are dealing with a choice variable
            if overwrite in context_value:
                # This overwrite is actually valid for the given context
                # Let's set it as default (by definition first item in list)
                # see ``cookiecutter.prompt.prompt_choice_for_config``
                context_value.remove(overwrite)
                context_value.insert(0, overwrite)
        else:
            # Simply overwrite the value for this variable
            context[variable] = overwrite

def generate_context(context_file='cookiecutter.json', default_context=None,
                     extra_context=None):
    """"""
    Generates the context for a Cookiecutter project template.
    Loads the JSON file as a Python object, with key being the JSON filename.
    :param context_file: JSON file containing key/value pairs for populating
        the cookiecutter's variables.
    :param default_context: Dictionary containing config to take into account.
    :param extra_context: Dictionary containing configuration overrides
    """"""
    context = {}
    file_handle = open(context_file)
    try:
        obj = json.load(file_handle, object_pairs_hook=OrderedDict)
    except ValueError as e:
        # JSON decoding error.  Let's throw a new exception that is more
        # friendly for the developer or user.
        full_fpath = os.path.abspath(context_file)
        json_exc_message = str(e)
        our_exc_message = (
            'JSON decoding error while loading ""{0}"".  Decoding'
            ' error details: ""{1}""'.format(full_fpath, json_exc_message))
        raise ContextDecodingException(our_exc_message)
    # Add the Python object to the context dictionary
    file_name = os.path.split(context_file)[1]
    file_stem = file_name.split('.')[0]
    context[file_stem] = obj
    # Overwrite context variable defaults with the default context from the
    # user's global config, if available
    if default_context:
        apply_overwrites_to_context(obj, default_context)
    if extra_context:
        apply_overwrites_to_context(obj, extra_context)
    logging.debug('Context generated is {0}'.format(context))
    return context

def generate_file(project_dir, infile, context, env):
    """"""
    1. Render the filename of infile as the name of outfile.
    2. Deal with infile appropriately:
        a. If infile is a binary file, copy it over without rendering.
        b. If infile is a text file, render its contents and write the
           rendered infile to outfile.
    Precondition:",[30]
"                #  will always make a copy
                result = DatetimeIndex(
                    data, copy=False, name=name, **kwargs
                )  # type: ""Index""
                return result.astype(object)
            else:
                return DatetimeIndex(data, copy=copy, name=name, dtype=dtype, **kwargs)
        elif is_timedelta64_dtype(data) or is_timedelta64_dtype(dtype):
            if is_dtype_equal(_o_dtype, dtype):
                # Note we can pass copy=False because the .astype below
                #  will always make a copy
                result = TimedeltaIndex(data, copy=False, name=name, **kwargs)
                return result.astype(object)
            else:
                return TimedeltaIndex(data, copy=copy, name=name, dtype=dtype, **kwargs)
        elif is_period_dtype(data) and not is_object_dtype(dtype):
            return PeriodIndex(data, copy=copy, name=name, **kwargs)
        # extension dtype
        elif is_extension_array_dtype(data) or is_extension_array_dtype(dtype):
            data = np.asarray(data)
            if not (dtype is None or is_object_dtype(dtype)):
                # coerce to the provided dtype
                ea_cls = dtype.construct_array_type()
                data = ea_cls._from_sequence(data, dtype=dtype, copy=False)
            # coerce to the object dtype
            data = data.astype(object)
            return Index(data, dtype=object, copy=copy, name=name, **kwargs)
        # index-like
        elif isinstance(data, (np.ndarray, Index, ABCSeries)):
            if dtype is not None:
                # we need to avoid having numpy coerce
                # things that look like ints/floats to ints unless
                # they are actually ints, e.g. '0' and 0.0
                # should not be coerced
                # GH 11836
                if is_integer_dtype(dtype):
                    inferred = lib.infer_dtype(data, skipna=False)
                    if inferred == ""integer"":
                        data = maybe_cast_to_integer_array(data, dtype, copy=copy)
                    elif inferred in [""floating"", ""mixed-integer-float""]:
                        if isna(data).any():
                            raise ValueError(""cannot convert float NaN to integer"")
                        if inferred == ""mixed-integer-float"":
                            data = maybe_cast_to_integer_array(data, dtype)
                        # If we are actually all equal to integers,
                        # then coerce to integer.
                        try:
                            return cls._try_convert_to_int_index(
                                data, copy, name, dtype
                            )
                        except ValueError:
                            pass
                        # Return an actual float index.
                        return Float64Index(data, copy=copy, dtype=dtype, name=name)
                    elif inferred == ""string"":
                        pass
                    else:
                        data = data.astype(dtype)
                elif is_float_dtype(dtype):
                    inferred = lib.infer_dtype(data, skipna=False)
                    if inferred == ""string"":
                        pass
                    else:
                        data = data.astype(dtype)
                else:
                    data = np.array(data, dtype=dtype, copy=copy)
            # maybe coerce to a sub-class
            if is_signed_integer_dtype(data.dtype):
                return Int64Index(data, copy=copy, dtype=dtype, name=name)
            elif is_unsigned_integer_dtype(data.dtype):
                return UInt64Index(data, copy=copy, dtype=dtype, name=name)
            elif is_float_dtype(data.dtype):
                return Float64Index(data, copy=copy, dtype=dtype, name=name)
            elif issubclass(data.dtype.type, np.bool) or is_bool_dtype(data):
                subarr = data.astype(""object"")
            else:
                subarr = com.asarray_tuplesafe(data, dtype=object)
            # asarray_tuplesafe does not always copy underlying data,
            # so need to make sure that this happens
            if copy:
                subarr = subarr.copy()
            if dtype is None:
                inferred = lib.infer_dtype(subarr, skipna=False)
                if inferred == ""integer"":
                    try:
                        return cls._try_convert_to_int_index(subarr, copy, name, dtype)
                    except ValueError:
                        pass
                    return Index(subarr, copy=copy, dtype=object, name=name)
                elif inferred in [""floating"", ""mixed-integer-float"", ""integer-na""]:
                    # TODO: Returns IntegerArray for integer-na case in the future
                    return Float64Index(subarr, copy=copy, name=name)
                elif inferred == ""interval"":
                    try:
                        return IntervalIndex(subarr, name=name, copy=copy)
                    except ValueError:
                        # GH27172: mixed closed Intervals --> object dtype
                        pass
                elif inferred == ""boolean"":
                    # don't support boolean explicitly ATM
                    pass
                elif inferred != ""string"":
                    if inferred.startswith(""datetime""):
                        try:
                            return DatetimeIndex(subarr, copy=copy, name=name, **kwargs)
                        except (ValueError, OutOfBoundsDatetime):
                            # GH 27011
                            # If we have mixed timezones, just send it
                            # down the base constructor
                            pass
                    elif inferred.startswith(""timedelta""):
                        return TimedeltaIndex(subarr, copy=copy, name=name, **kwargs)
                    elif inferred == ""period"":",[61]
"            - 'none': No patch boundary will be drawn.
            - A color or sequence of colors.
            For non-filled markers, the *edgecolors* kwarg is ignored and
            forced to 'face' internally.
        plotnonfinite : bool, default: False
            Set to plot points with nonfinite *c*, in conjunction with
            `~matplotlib.colors.Colormap.set_bad`.
        Returns
        -------
        `~matplotlib.collections.PathCollection`
        Other Parameters
        ----------------
        **kwargs : `~matplotlib.collections.Collection` properties
        See Also
        --------
        plot : To plot scatter plots when markers are identical in size and
            color.
        Notes
        -----
        * The `.plot` function will be faster for scatterplots where markers
          don't vary in size or color.
        * Any or all of *x*, *y*, *s*, and *c* may be masked arrays, in which
          case all masks will be combined and only unmasked points will be
          plotted.
        * Fundamentally, scatter works with 1-D arrays; *x*, *y*, *s*, and *c*
          may be input as N-D arrays, but within scatter they will be
          flattened. The exception is *c*, which will be flattened only if its
          size matches the size of *x* and *y*.
        """"""
        # Process **kwargs to handle aliases, conflicts with explicit kwargs:
        self._process_unit_info(xdata=x, ydata=y, kwargs=kwargs)
        x = self.convert_xunits(x)
        y = self.convert_yunits(y)
        # np.ma.ravel yields an ndarray, not a masked array,
        # unless its argument is a masked array.
        x = np.ma.ravel(x)
        y = np.ma.ravel(y)
        if x.size != y.size:
            raise ValueError(""x and y must be the same size"")
        if s is None:
            s = (20 if rcParams['_internal.classic_mode'] else
                 rcParams['lines.markersize'] ** 2.0)
        s = np.ma.ravel(s)
        if len(s) not in (1, x.size):
            raise ValueError(""s must be a scalar, or the same size as x and y"")
        c, colors, edgecolors = \
            self._parse_scatter_color_args(
                c, edgecolors, kwargs, x.size,
                get_next_color_func=self._get_patches_for_fill.get_next_color)
        if plotnonfinite and colors is None:
            c = np.ma.masked_invalid(c)
            x, y, s, edgecolors, linewidths = \
                cbook._combine_masks(x, y, s, edgecolors, linewidths)
        else:
            x, y, s, c, colors, edgecolors, linewidths = \
                cbook._combine_masks(
                    x, y, s, c, colors, edgecolors, linewidths)
        scales = s   # Renamed for readability below.
        # load default marker from rcParams
        if marker is None:
            marker = rcParams['scatter.marker']
        if isinstance(marker, mmarkers.MarkerStyle):
            marker_obj = marker
        else:
            marker_obj = mmarkers.MarkerStyle(marker)
        path = marker_obj.get_path().transformed(
            marker_obj.get_transform())
        if not marker_obj.is_filled():
            edgecolors = 'face'
            if linewidths is None:
                linewidths = rcParams['lines.linewidth']
            elif np.iterable(linewidths):
                linewidths = [
                    lw if lw is not None else rcParams['lines.linewidth']
                    for lw in linewidths]
        offsets = np.ma.column_stack([x, y])
        collection = mcoll.PathCollection(
                (path,), scales,
                facecolors=colors,
                edgecolors=edgecolors,
                linewidths=linewidths,
                offsets=offsets,
                transOffset=kwargs.pop('transform', self.transData),
                alpha=alpha
                )
        collection.set_transform(mtransforms.IdentityTransform())
        collection.update(kwargs)
        if colors is None:
            collection.set_array(c)
            collection.set_cmap(cmap)
            collection.set_norm(norm)
            collection._scale_norm(norm, vmin, vmax)
        # Classic mode only:
        # ensure there are margins to allow for the
        # finite size of the symbols.  In v2.x, margins
        # are present by default, so we disable this
        # scatter-specific override.
        if rcParams['_internal.classic_mode']:
            if self._xmargin < 0.05 and x.size > 0:
                self.set_xmargin(0.05)
            if self._ymargin < 0.05 and x.size > 0:
                self.set_ymargin(0.05)
        self.add_collection(collection)
        self._request_autoscale_view()","[3, 4, 86, 98, 99]"
"            .. versionadded:: 0.24.0
        Returns
        -------
        Same type as self
            Array/Index converted to the specified time zone.
        Raises
        ------
        TypeError
            If the Datetime Array/Index is tz-aware and tz is not None.
        See Also
        --------
        DatetimeIndex.tz_convert : Convert tz-aware DatetimeIndex from
            one time zone to another.
        Examples
        --------
        >>> tz_naive = pd.date_range('2018-03-01 09:00', periods=3)
        >>> tz_naive
        DatetimeIndex(['2018-03-01 09:00:00', '2018-03-02 09:00:00',
                       '2018-03-03 09:00:00'],
                      dtype='datetime64[ns]', freq='D')
        Localize DatetimeIndex in US/Eastern time zone:
        >>> tz_aware = tz_naive.tz_localize(tz='US/Eastern')
        >>> tz_aware
        DatetimeIndex(['2018-03-01 09:00:00-05:00',
                       '2018-03-02 09:00:00-05:00',
                       '2018-03-03 09:00:00-05:00'],
                      dtype='datetime64[ns, US/Eastern]', freq='D')
        With the ``tz=None``, we can remove the time zone information
        while keeping the local time (not converted to UTC):
        >>> tz_aware.tz_localize(None)
        DatetimeIndex(['2018-03-01 09:00:00', '2018-03-02 09:00:00',
                       '2018-03-03 09:00:00'],
                      dtype='datetime64[ns]', freq='D')
        Be careful with DST changes. When there is sequential data, pandas can
        infer the DST time:
        >>> s = pd.to_datetime(pd.Series(['2018-10-28 01:30:00',
        ...                               '2018-10-28 02:00:00',
        ...                               '2018-10-28 02:30:00',
        ...                               '2018-10-28 02:00:00',
        ...                               '2018-10-28 02:30:00',
        ...                               '2018-10-28 03:00:00',
        ...                               '2018-10-28 03:30:00']))
        >>> s.dt.tz_localize('CET', ambiguous='infer')
        0   2018-10-28 01:30:00+02:00
        1   2018-10-28 02:00:00+02:00
        2   2018-10-28 02:30:00+02:00
        3   2018-10-28 02:00:00+01:00
        4   2018-10-28 02:30:00+01:00
        5   2018-10-28 03:00:00+01:00
        6   2018-10-28 03:30:00+01:00
        dtype: datetime64[ns, CET]
        In some cases, inferring the DST is impossible. In such cases, you can
        pass an ndarray to the ambiguous parameter to set the DST explicitly
        >>> s = pd.to_datetime(pd.Series(['2018-10-28 01:20:00',
        ...                               '2018-10-28 02:36:00',
        ...                               '2018-10-28 03:46:00']))
        >>> s.dt.tz_localize('CET', ambiguous=np.array([True, True, False]))
        0   2018-10-28 01:20:00+02:00
        1   2018-10-28 02:36:00+02:00
        2   2018-10-28 03:46:00+01:00
        dtype: datetime64[ns, CET]
        If the DST transition causes nonexistent times, you can shift these
        dates forward or backwards with a timedelta object or `'shift_forward'`
        or `'shift_backwards'`.
        >>> s = pd.to_datetime(pd.Series(['2015-03-29 02:30:00',
        ...                               '2015-03-29 03:30:00']))
        >>> s.dt.tz_localize('Europe/Warsaw', nonexistent='shift_forward')
        0   2015-03-29 03:00:00+02:00
        1   2015-03-29 03:30:00+02:00
        dtype: datetime64[ns, Europe/Warsaw]
        >>> s.dt.tz_localize('Europe/Warsaw', nonexistent='shift_backward')
        0   2015-03-29 01:59:59.999999999+01:00
        1   2015-03-29 03:30:00+02:00
        dtype: datetime64[ns, Europe/Warsaw]
        >>> s.dt.tz_localize('Europe/Warsaw', nonexistent=pd.Timedelta('1H'))
        0   2015-03-29 03:30:00+02:00
        1   2015-03-29 03:30:00+02:00
        dtype: datetime64[ns, Europe/Warsaw]
        """"""
        nonexistent_options = (""raise"", ""NaT"", ""shift_forward"", ""shift_backward"")
        if nonexistent not in nonexistent_options and not isinstance(
            nonexistent, timedelta
        ):
            raise ValueError(
                ""The nonexistent argument must be one of 'raise', ""
                ""'NaT', 'shift_forward', 'shift_backward' or ""
                ""a timedelta object""
            )
        if self.tz is not None:
            if tz is None:
                new_dates = tzconversion.tz_convert(self.asi8, timezones.UTC, self.tz)
            else:
                raise TypeError(""Already tz-aware, use tz_convert to convert."")
        else:
            tz = timezones.maybe_get_tz(tz)
            # Convert to UTC
            new_dates = conversion.tz_localize_to_utc(
                self.asi8, tz, ambiguous=ambiguous, nonexistent=nonexistent
            )
        new_dates = new_dates.view(DT64NS_DTYPE)
        dtype = tz_to_dtype(tz)
        return self._simple_new(new_dates, dtype=dtype, freq=self.freq)
    # ----------------------------------------------------------------
    # Conversion Methods - Vectorized analogues of Timestamp methods
    def to_pydatetime(self) -> np.ndarray:
        """"""","[33, 41, 120]"
"    ]
    for bom, enc in BOMS:
        if first_bytes.startswith(bom):
            s = first_bytes[len(bom):].decode(enc, 'replace')
            break
    else:
        s = first_bytes.decode('utf-8', 'replace')
    return re.match(r'^\s*<', s)

def determine_protocol(info_dict):
    protocol = info_dict.get('protocol')
    if protocol is not None:
        return protocol
    url = info_dict['url']
    if url.startswith('rtmp'):
        return 'rtmp'
    elif url.startswith('mms'):
        return 'mms'
    elif url.startswith('rtsp'):
        return 'rtsp'
    ext = determine_ext(url)
    if ext == 'm3u8':
        return 'm3u8'
    elif ext == 'f4m':
        return 'f4m'
    return compat_urllib_parse_urlparse(url).scheme

def render_table(header_row, data):
    """""" Render a list of rows, each as a list of values """"""
    table = [header_row] + data
    max_lens = [max(len(compat_str(v)) for v in col) for col in zip(*table)]
    format_str = ' '.join('%-' + compat_str(ml + 1) + 's' for ml in max_lens[:-1]) + '%s'
    return '\n'.join(format_str % tuple(row) for row in table)

def _match_one(filter_part, dct):
    COMPARISON_OPERATORS = {
        '<': operator.lt,
        '<=': operator.le,
        '>': operator.gt,
        '>=': operator.ge,
        '=': operator.eq,
        '!=': operator.ne,
    }
    operator_rex = re.compile(r'''(?x)\s*
        (?P<key>[a-z_]+)
        \s*(?P<op>%s)(?P<none_inclusive>\s*\?)?\s*
        (?:
            (?P<intval>[0-9.]+(?:[kKmMgGtTpPeEzZyY]i?[Bb]?)?)|
            (?P<strval>(?![0-9.])[a-z0-9A-Z]*)
        )
        \s*$
        ''' % '|'.join(map(re.escape, COMPARISON_OPERATORS.keys())))
    m = operator_rex.search(filter_part)
    if m:
        op = COMPARISON_OPERATORS[m.group('op')]
        if m.group('strval') is not None:
            if m.group('op') not in ('=', '!='):
                raise ValueError(
                    'Operator %s does not support string values!' % m.group('op'))
            comparison_value = m.group('strval')
        else:
            try:
                comparison_value = int(m.group('intval'))
            except ValueError:
                comparison_value = parse_filesize(m.group('intval'))
                if comparison_value is None:
                    comparison_value = parse_filesize(m.group('intval') + 'B')
                if comparison_value is None:
                    raise ValueError(
                        'Invalid integer value %r in filter part %r' % (
                            m.group('intval'), filter_part))
        actual_value = dct.get(m.group('key'))
        if actual_value is None:
            return m.group('none_inclusive')
        return op(actual_value, comparison_value)
    UNARY_OPERATORS = {
        '': lambda v: v is not None,
        '!': lambda v: v is None,
    }
    operator_rex = re.compile(r'''(?x)\s*
        (?P<op>%s)\s*(?P<key>[a-z_]+)
        \s*$
        ''' % '|'.join(map(re.escape, UNARY_OPERATORS.keys())))
    m = operator_rex.search(filter_part)
    if m:
        op = UNARY_OPERATORS[m.group('op')]
        actual_value = dct.get(m.group('key'))
        return op(actual_value)
    raise ValueError('Invalid filter part %r' % filter_part)

def match_str(filter_str, dct):
    """""" Filter a dictionary with a simple string syntax. Returns True (=passes filter) or false """"""
    return all(
        _match_one(filter_part, dct) for filter_part in filter_str.split('&'))

def match_filter_func(filter_str):
    def _match_func(info_dict):
        if match_str(filter_str, info_dict):
            return None
        else:
            video_title = info_dict.get('title', info_dict.get('id', 'video'))
            return '%s does not pass filter %s, skipping ..' % (video_title, filter_str)
    return _match_func

def parse_dfxp_time_expr(time_expr):
    if not time_expr:
        return
    mobj = re.match(r'^(?P<time_offset>\d+(?:\.\d+)?)s?$', time_expr)
    if mobj:
        return float(mobj.group('time_offset'))
    mobj = re.match(r'^(\d+):(\d\d):(\d\d(?:\.\d+)?)$', time_expr)
    if mobj:","[125, 127]"
"        class   animal   locomotion
        mammal  cat      walks         0
                dog      walks         0
                bat      flies         2
        bird    penguin  walks         2
        Name: num_wings, dtype: int64
        """"""
        axis = self._get_axis_number(axis)
        labels = self._get_axis(axis)
        if level is not None:
            loc, new_ax = labels.get_loc_level(key, level=level, drop_level=drop_level)
            # create the tuple of the indexer
            _indexer = [slice(None)] * self.ndim
            _indexer[axis] = loc
            indexer = tuple(_indexer)
            result = self.iloc[indexer]
            setattr(result, result._get_axis_name(axis), new_ax)
            return result
        if axis == 1:
            return self[key]
        self._consolidate_inplace()
        index = self.index
        if isinstance(index, MultiIndex):
            loc, new_index = self.index.get_loc_level(key, drop_level=drop_level)
        else:
            loc = self.index.get_loc(key)
            if isinstance(loc, np.ndarray):
                if loc.dtype == np.bool_:
                    (inds,) = loc.nonzero()
                    return self._take_with_is_copy(inds, axis=axis)
                else:
                    return self._take_with_is_copy(loc, axis=axis)
            if not is_scalar(loc):
                new_index = self.index[loc]
        if is_scalar(loc):
            new_values = self._data.fast_xs(loc)
            # may need to box a datelike-scalar
            #
            # if we encounter an array-like and we only have 1 dim
            # that means that their are list/ndarrays inside the Series!
            # so just return them (GH 6394)
            if not is_list_like(new_values) or self.ndim == 1:
                return com.maybe_box_datetimelike(new_values)
            result = self._constructor_sliced(
                new_values,
                index=self.columns,
                name=self.index[loc],
                dtype=new_values.dtype,
            )
        else:
            result = self.iloc[loc]
            result.index = new_index
        # this could be a view
        # but only in a single-dtyped view sliceable case
        result._set_is_copy(self, copy=not result._is_view)
        return result
    _xs: Callable = xs
    def __getitem__(self, item):
        raise AbstractMethodError(self)
    def _get_item_cache(self, item):
        """"""Return the cached item, item represents a label indexer.""""""
        cache = self._item_cache
        res = cache.get(item)
        if res is None:
            values = self._data.get(item)
            res = self._box_item_values(item, values)
            cache[item] = res
            res._set_as_cached(item, self)
            # for a chain
            res._is_copy = self._is_copy
        return res
    def _iget_item_cache(self, item):
        """"""Return the cached item, item represents a positional indexer.""""""
        ax = self._info_axis
        if ax.is_unique:
            lower = self._get_item_cache(ax[item])
        else:
            lower = self._take_with_is_copy(item, axis=self._info_axis_number)
        return lower
    def _box_item_values(self, key, values):
        raise AbstractMethodError(self)
    def _slice(self: FrameOrSeries, slobj: slice, axis=0, kind=None) -> FrameOrSeries:
        """"""
        Construct a slice of this container.
        kind parameter is maintained for compatibility with Series slicing.
        """"""
        axis = self._get_block_manager_axis(axis)
        result = self._constructor(self._data.get_slice(slobj, axis=axis))
        result = result.__finalize__(self)
        # this could be a view
        # but only in a single-dtyped view sliceable case
        is_copy = axis != 0 or result._is_view
        result._set_is_copy(self, copy=is_copy)
        return result
    def _set_item(self, key, value) -> None:
        self._data.set(key, value)
        self._clear_item_cache()
    def _set_is_copy(self, ref, copy: bool_t = True) -> None:
        if not copy:
            self._is_copy = None
        else:
            assert ref is not None
            self._is_copy = weakref.ref(ref)
","[43, 50, 51]"
"""""""Common IO api utilities""""""
import bz2
from collections import abc
import gzip
from io import BufferedIOBase, BytesIO
import mmap
import os
import pathlib
from typing import IO, Any, AnyStr, Dict, List, Mapping, Optional, Tuple, Union
from urllib.parse import (  # noqa
    urlencode,
    urljoin,
    urlparse as parse_url,
    uses_netloc,
    uses_params,
    uses_relative,
)
import zipfile
from pandas._typing import FilePathOrBuffer
from pandas.compat import _get_lzma_file, _import_lzma
from pandas.errors import (  # noqa
    AbstractMethodError,
    DtypeWarning,
    EmptyDataError,
    ParserError,
    ParserWarning,
)
from pandas.core.dtypes.common import is_file_like
lzma = _import_lzma()

_VALID_URLS = set(uses_relative + uses_netloc + uses_params)
_VALID_URLS.discard("""")

def is_url(url) -> bool:
    """"""
    Check to see if a URL has a valid protocol.
    Parameters
    ----------
    url : str or unicode
    Returns
    -------
    isurl : bool
        If `url` has a valid protocol return True otherwise False.
    """"""
    if not isinstance(url, str):
        return False
    return parse_url(url).scheme in _VALID_URLS

def _expand_user(
    filepath_or_buffer: FilePathOrBuffer[AnyStr],
) -> FilePathOrBuffer[AnyStr]:
    """"""Return the argument with an initial component of ~ or ~user
       replaced by that user's home directory.
    Parameters
    ----------
    filepath_or_buffer : object to be converted if possible
    Returns
    -------
    expanded_filepath_or_buffer : an expanded filepath or the
                                  input if not expandable
    """"""
    if isinstance(filepath_or_buffer, str):
        return os.path.expanduser(filepath_or_buffer)
    return filepath_or_buffer

def validate_header_arg(header) -> None:
    if isinstance(header, bool):
        raise TypeError(
            ""Passing a bool to header is invalid. Use header=None for no header or ""
            ""header=int or list-like of ints to specify ""
            ""the row(s) making up the column names""
        )

def stringify_path(
    filepath_or_buffer: FilePathOrBuffer[AnyStr],
) -> FilePathOrBuffer[AnyStr]:
    """"""Attempt to convert a path-like object to a string.
    Parameters
    ----------
    filepath_or_buffer : object to be converted
    Returns
    -------
    str_filepath_or_buffer : maybe a string version of the object
    Notes
    -----
    Objects supporting the fspath protocol (python 3.6+) are coerced
    according to its __fspath__ method.
    For backwards compatibility with older pythons, pathlib.Path and
    py.path objects are specially coerced.
    Any other object is passed through unchanged, which includes bytes,
    strings, buffers, or anything else that's not even path-like.
    """"""
    if hasattr(filepath_or_buffer, ""__fspath__""):
        # https://github.com/python/mypy/issues/1424
        return filepath_or_buffer.__fspath__()  # type: ignore
    elif isinstance(filepath_or_buffer, pathlib.Path):
        return str(filepath_or_buffer)
    return _expand_user(filepath_or_buffer)

def is_s3_url(url) -> bool:
    """"""Check for an s3, s3n, or s3a url""""""
    if not isinstance(url, str):
        return False
    return parse_url(url).scheme in [""s3"", ""s3n"", ""s3a""]

def is_gcs_url(url) -> bool:
    """"""Check for a gcs url""""""",[5]
"        nan-mask if known
    Returns
    -------
    result : float
        Unless input is a float array, in which case use the same
        precision as the input array.
    Examples
    --------
    >>> import pandas.core.nanops as nanops
    >>> s = pd.Series([1, np.nan, 2, 3])
    >>> nanops.nanstd(s)
    1.0
    """"""
    result = np.sqrt(nanvar(values, axis=axis, skipna=skipna, ddof=ddof, mask=mask))
    return _wrap_results(result, values.dtype)

@disallow(""M8"")
@bottleneck_switch(ddof=1)
def nanvar(values, axis=None, skipna=True, ddof=1, mask=None):
    """"""
    Compute the variance along given axis while ignoring NaNs
    Parameters
    ----------
    values : ndarray
    axis: int, optional
    skipna : bool, default True
    ddof : int, default 1
        Delta Degrees of Freedom. The divisor used in calculations is N - ddof,
        where N represents the number of elements.
    mask : ndarray[bool], optional
        nan-mask if known
    Returns
    -------
    result : float
        Unless input is a float array, in which case use the same
        precision as the input array.
    Examples
    --------
    >>> import pandas.core.nanops as nanops
    >>> s = pd.Series([1, np.nan, 2, 3])
    >>> nanops.nanvar(s)
    1.0
    """"""
    values = lib.values_from_object(values)
    dtype = values.dtype
    mask = _maybe_get_mask(values, skipna, mask)
    if is_any_int_dtype(values):
        values = values.astype(""f8"")
        if mask is not None:
            values[mask] = np.nan
    if is_float_dtype(values):
        count, d = _get_counts_nanvar(values.shape, mask, axis, ddof, values.dtype)
    else:
        count, d = _get_counts_nanvar(values.shape, mask, axis, ddof)
    if skipna and mask is not None:
        values = values.copy()
        np.putmask(values, mask, 0)
    # xref GH10242
    # Compute variance via two-pass algorithm, which is stable against
    # cancellation errors and relatively accurate for small numbers of
    # observations.
    #
    # See https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance
    avg = _ensure_numeric(values.sum(axis=axis, dtype=np.float64)) / count
    if axis is not None:
        avg = np.expand_dims(avg, axis)
    sqr = _ensure_numeric((avg - values) ** 2)
    if mask is not None:
        np.putmask(sqr, mask, 0)
    result = sqr.sum(axis=axis, dtype=np.float64) / d
    # Return variance as np.float64 (the datatype used in the accumulator),
    # unless we were dealing with a float array, in which case use the same
    # precision as the original values array.
    if is_float_dtype(dtype):
        result = result.astype(dtype)
    return _wrap_results(result, values.dtype)

@disallow(""M8"", ""m8"")
def nansem(values, axis=None, skipna=True, ddof=1, mask=None):
    """"""
    Compute the standard error in the mean along given axis while ignoring NaNs
    Parameters
    ----------
    values : ndarray
    axis: int, optional
    skipna : bool, default True
    ddof : int, default 1
        Delta Degrees of Freedom. The divisor used in calculations is N - ddof,
        where N represents the number of elements.
    mask : ndarray[bool], optional
        nan-mask if known
    Returns
    -------
    result : float64
        Unless input is a float array, in which case use the same
        precision as the input array.
    Examples
    --------
    >>> import pandas.core.nanops as nanops
    >>> s = pd.Series([1, np.nan, 2, 3])
    >>> nanops.nansem(s)
     0.5773502691896258
    """"""
    # This checks if non-numeric-like data is passed with numeric_only=False
    # and raises a TypeError otherwise
    nanvar(values, axis, skipna, ddof=ddof, mask=mask)
    mask = _maybe_get_mask(values, skipna, mask)
    if not is_float_dtype(values.dtype):
        values = values.astype(""f8"")
    count, _ = _get_counts_nanvar(values.shape, mask, axis, ddof, values.dtype)","[16, 19]"
"# coding: utf8
from __future__ import unicode_literals

def add_codes(err_cls):
    """"""Add error codes to string messages via class attribute names.""""""
    class ErrorsWithCodes(object):
        def __getattribute__(self, code):
            msg = getattr(err_cls, code)
            return ""[{code}] {msg}"".format(code=code, msg=msg)
    return ErrorsWithCodes()

# fmt: off
@add_codes
class Warnings(object):
    W001 = (""As of spaCy v2.0, the keyword argument `path=` is deprecated. ""
            ""You can now call spacy.load with the path as its first argument, ""
            ""and the model's meta.json will be used to determine the language ""
            ""to load. For example:\nnlp = spacy.load('{path}')"")
    W002 = (""Tokenizer.from_list is now deprecated. Create a new Doc object ""
            ""instead and pass in the strings as the `words` keyword argument, ""
            ""for example:\nfrom spacy.tokens import Doc\n""
            ""doc = Doc(nlp.vocab, words=[...])"")
    W003 = (""Positional arguments to Doc.merge are deprecated. Instead, use ""
            ""the keyword arguments, for example tag=, lemma= or ent_type=."")
    W004 = (""No text fixing enabled. Run `pip install ftfy` to enable fixing ""
            ""using ftfy.fix_text if necessary."")
    W005 = (""Doc object not parsed. This means displaCy won't be able to ""
            ""generate a dependency visualization for it. Make sure the Doc ""
            ""was processed with a model that supports dependency parsing, and ""
            ""not just a language class like `English()`. For more info, see ""
            ""the docs:\nhttps://spacy.io/usage/models"")
    W006 = (""No entities to visualize found in Doc object. If this is ""
            ""surprising to you, make sure the Doc was processed using a model ""
            ""that supports named entity recognition, and check the `doc.ents` ""
            ""property manually if necessary."")
    W007 = (""The model you're using has no word vectors loaded, so the result ""
            ""of the {obj}.similarity method will be based on the tagger, ""
            ""parser and NER, which may not give useful similarity judgements. ""
            ""This may happen if you're using one of the small models, e.g. ""
            ""`en_core_web_sm`, which don't ship with word vectors and only ""
            ""use context-sensitive tensors. You can always add your own word ""
            ""vectors, or use one of the larger models instead if available."")
    W008 = (""Evaluating {obj}.similarity based on empty vectors."")
    W009 = (""Custom factory '{name}' provided by entry points of another ""
            ""package overwrites built-in factory."")
    W010 = (""As of v2.1.0, the PhraseMatcher doesn't have a phrase length ""
            ""limit anymore, so the max_length argument is now deprecated. ""
            ""If you did not specify this parameter, make sure you call the ""
            ""constructor with named arguments instead of positional ones."")
    W011 = (""It looks like you're calling displacy.serve from within a ""
            ""Jupyter notebook or a similar environment. This likely means ""
            ""you're already running a local web server, so there's no need to ""
            ""make displaCy start another one. Instead, you should be able to ""
            ""replace displacy.serve with displacy.render to show the ""
            ""visualization."")
    W012 = (""A Doc object you're adding to the PhraseMatcher for pattern ""
            ""'{key}' is parsed and/or tagged, but to match on '{attr}', you ""
            ""don't actually need this information. This means that creating ""
            ""the patterns is potentially much slower, because all pipeline ""
            ""components are applied. To only create tokenized Doc objects, ""
            ""try using `nlp.make_doc(text)` or process all texts as a stream ""
            ""using `list(nlp.tokenizer.pipe(all_texts))`."")
    W013 = (""As of v2.1.0, {obj}.merge is deprecated. Please use the more ""
            ""efficient and less error-prone Doc.retokenize context manager ""
            ""instead."")
    W014 = (""As of v2.1.0, the `disable` keyword argument on the serialization ""
            ""methods is and should be replaced with `exclude`. This makes it ""
            ""consistent with the other serializable objects."")
    W015 = (""As of v2.1.0, the use of keyword arguments to exclude fields from ""
            ""being serialized or deserialized is deprecated. Please use the ""
            ""`exclude` argument instead. For example: exclude=['{arg}']."")
    W016 = (""The keyword argument `n_threads` is now deprecated. As of v2.2.2, ""
            ""the argument `n_process` controls parallel inference via ""
            ""multiprocessing."")
    W017 = (""Alias '{alias}' already exists in the Knowledge Base."")
    W018 = (""Entity '{entity}' already exists in the Knowledge Base - ""
            ""ignoring the duplicate entry."")
    W019 = (""Changing vectors name from {old} to {new}, to avoid clash with ""
            ""previously loaded vectors. See Issue #3853."")
    W020 = (""Unnamed vectors. This won't allow multiple vectors models to be ""
            ""loaded. (Shape: {shape})"")
    W021 = (""Unexpected hash collision in PhraseMatcher. Matches may be ""
            ""incorrect. Modify PhraseMatcher._terminal_hash to fix."")
    W022 = (""Training a new part-of-speech tagger using a model with no ""
            ""lemmatization rules or data. This means that the trained model ""
            ""may not be able to lemmatize correctly. If this is intentional ""
            ""or the language you're using doesn't have lemmatization data. ""
            ""If this is surprising, make sure you have the spacy-lookups-data ""
            ""package installed."")
    W023 = (""Multiprocessing of Language.pipe is not supported in Python 2. ""
            ""'n_process' will be set to 1."")
    W024 = (""Entity '{entity}' - Alias '{alias}' combination already exists in ""
            ""the Knowledge Base."")
    W025 = (""'{name}' requires '{attr}' to be assigned, but none of the ""
            ""previous components in the pipeline declare that they assign it."")
    W026 = (""Unable to set all sentence boundaries from dependency parses."")
    W027 = (""Found a large training file of {size} bytes. Note that it may ""
            ""be more efficient to split your training data into multiple ""
            ""smaller JSON files instead."")
    W028 = (""Doc.from_array was called with a vector of type '{type}', ""
            ""but is expecting one of type 'uint64' instead. This may result ""
            ""in problems with the vocab further on in the pipeline."")
    W029 = (""Unable to align tokens with entities from character offsets. ""
            ""Discarding entity annotation for the text: {text}."")

@add_codes
class Errors(object):
    E001 = (""No component '{name}' found in pipeline. Available names: {opts}"")
    E002 = (""Can't find factory for '{name}'. This usually happens when spaCy ""
            ""calls `nlp.create_pipe` with a component name that's not built ""
            ""in - for example, when constructing the pipeline from a model's ""
            ""meta.json. If you're using a custom component, you can write to ""
            ""`Language.factories['{name}']` or remove it from the model meta ""
            ""and add it via `nlp.add_pipe` instead."")
    E003 = (""Not a valid pipeline component. Expected callable, but ""
            ""got {component} (name: '{name}')."")
    E004 = (""If you meant to add a built-in component, use `create_pipe`: ""
            ""`nlp.add_pipe(nlp.create_pipe('{component}'))`"")
    E005 = (""Pipeline component '{name}' returned None. If you're using a ""
            ""custom component, maybe you forgot to return the processed Doc?"")
    E006 = (""Invalid constraints. You can only set one of the following: ""","[9, 10]"
""""""" implement the TimedeltaIndex """"""
from pandas._libs import NaT, Timedelta, index as libindex
from pandas._typing import Label
from pandas.util._decorators import Appender
from pandas.core.dtypes.common import (
    _TD_DTYPE,
    is_float,
    is_integer,
    is_scalar,
    is_timedelta64_dtype,
    is_timedelta64_ns_dtype,
    pandas_dtype,
)
from pandas.core.dtypes.missing import is_valid_nat_for_dtype
from pandas.core.arrays import datetimelike as dtl
from pandas.core.arrays.timedeltas import TimedeltaArray
import pandas.core.common as com
from pandas.core.indexes.base import Index, InvalidIndexError, maybe_extract_name
from pandas.core.indexes.datetimelike import (
    DatetimeIndexOpsMixin,
    DatetimeTimedeltaMixin,
)
from pandas.core.indexes.extension import inherit_names
from pandas.tseries.frequencies import to_offset

@inherit_names(
    [
        ""_box_values"",
        ""__neg__"",
        ""__pos__"",
        ""__abs__"",
        ""total_seconds"",
        ""round"",
        ""floor"",
        ""ceil"",
    ]
    + TimedeltaArray._field_ops,
    TimedeltaArray,
    wrap=True,
)
@inherit_names(
    [
        ""_bool_ops"",
        ""_object_ops"",
        ""_field_ops"",
        ""_datetimelike_ops"",
        ""_datetimelike_methods"",
        ""_other_ops"",
        ""components"",
        ""to_pytimedelta"",
        ""sum"",
        ""std"",
        ""median"",
        ""_format_native_types"",
        ""freq"",
    ],
    TimedeltaArray,
)
class TimedeltaIndex(DatetimeTimedeltaMixin, dtl.TimelikeOps):
    """"""
    Immutable ndarray of timedelta64 data, represented internally as int64, and
    which can be boxed to timedelta objects.
    Parameters
    ----------
    data  : array-like (1-dimensional), optional
        Optional timedelta-like data to construct index with.
    unit : unit of the arg (D,h,m,s,ms,us,ns) denote the unit, optional
        Which is an integer/float number.
    freq : str or pandas offset object, optional
        One of pandas date offset strings or corresponding objects. The string
        'infer' can be passed in order to set the frequency of the index as the
        inferred frequency upon creation.
    copy  : bool
        Make a copy of input ndarray.
    name : object
        Name to be stored in the index.
    Attributes
    ----------
    days
    seconds
    microseconds
    nanoseconds
    components
    inferred_freq
    Methods
    -------
    to_pytimedelta
    to_series
    round
    floor
    ceil
    to_frame
    mean
    See Also
    --------
    Index : The base pandas Index type.
    Timedelta : Represents a duration between two dates or times.
    DatetimeIndex : Index of datetime64 data.
    PeriodIndex : Index of Period data.
    timedelta_range : Create a fixed-frequency TimedeltaIndex.
    Notes
    -----
    To learn more about the frequency strings, please see `this link
    <https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases>`__.
    """"""
    _typ = ""timedeltaindex""
    _engine_type = libindex.TimedeltaEngine
    _comparables = [""name"", ""freq""]
    _attributes = [""name"", ""freq""]
    _is_numeric_dtype = True
    _infer_as_myclass = True
    _data: TimedeltaArray
",[3]
"            # In this case, we remap the new codes to the original level:
            repeater = self.removed_level_full.get_indexer(self.removed_level)
            if self.lift:
                repeater = np.insert(repeater, 0, -1)
        else:
            # Otherwise, we just use each level item exactly once:
            repeater = np.arange(stride) - self.lift
        # The entire level is then just a repetition of the single chunk:
        new_codes.append(np.tile(repeater, width))
        return MultiIndex(
            levels=new_levels, codes=new_codes, names=new_names, verify_integrity=False
        )
    def get_new_index(self):
        result_codes = [lab.take(self.compressor) for lab in self.sorted_labels[:-1]]
        # construct the new index
        if len(self.new_index_levels) == 1:
            level, level_codes = self.new_index_levels[0], result_codes[0]
            if (level_codes == -1).any():
                level = level.insert(len(level), level._na_value)
            return level.take(level_codes).rename(self.new_index_names[0])
        return MultiIndex(
            levels=self.new_index_levels,
            codes=result_codes,
            names=self.new_index_names,
            verify_integrity=False,
        )

def _unstack_multiple(data, clocs, fill_value=None):
    if len(clocs) == 0:
        return data
    # NOTE: This doesn't deal with hierarchical columns yet
    index = data.index
    clocs = [index._get_level_number(i) for i in clocs]
    rlocs = [i for i in range(index.nlevels) if i not in clocs]
    clevels = [index.levels[i] for i in clocs]
    ccodes = [index.codes[i] for i in clocs]
    cnames = [index.names[i] for i in clocs]
    rlevels = [index.levels[i] for i in rlocs]
    rcodes = [index.codes[i] for i in rlocs]
    rnames = [index.names[i] for i in rlocs]
    shape = [len(x) for x in clevels]
    group_index = get_group_index(ccodes, shape, sort=False, xnull=False)
    comp_ids, obs_ids = compress_group_index(group_index, sort=False)
    recons_codes = decons_obs_group_ids(comp_ids, obs_ids, shape, ccodes, xnull=False)
    if rlocs == []:
        # Everything is in clocs, so the dummy df has a regular index
        dummy_index = Index(obs_ids, name=""__placeholder__"")
    else:
        dummy_index = MultiIndex(
            levels=rlevels + [obs_ids],
            codes=rcodes + [comp_ids],
            names=rnames + [""__placeholder__""],
            verify_integrity=False,
        )
    if isinstance(data, Series):
        dummy = data.copy()
        dummy.index = dummy_index
        unstacked = dummy.unstack(""__placeholder__"", fill_value=fill_value)
        new_levels = clevels
        new_names = cnames
        new_codes = recons_codes
    else:
        if isinstance(data.columns, MultiIndex):
            result = data
            for i in range(len(clocs)):
                val = clocs[i]
                result = result.unstack(val)
                clocs = [v if i > v else v - 1 for v in clocs]
            return result
        dummy = data.copy()
        dummy.index = dummy_index
        unstacked = dummy.unstack(""__placeholder__"", fill_value=fill_value)
        if isinstance(unstacked, Series):
            unstcols = unstacked.index
        else:
            unstcols = unstacked.columns
        new_levels = [unstcols.levels[0]] + clevels
        new_names = [data.columns.name] + cnames
        new_codes = [unstcols.codes[0]]
        for rec in recons_codes:
            new_codes.append(rec.take(unstcols.codes[-1]))
    new_columns = MultiIndex(
        levels=new_levels, codes=new_codes, names=new_names, verify_integrity=False
    )
    if isinstance(unstacked, Series):
        unstacked.index = new_columns
    else:
        unstacked.columns = new_columns
    return unstacked

def unstack(obj, level, fill_value=None):
    if isinstance(level, (tuple, list)):
        if len(level) != 1:
            # _unstack_multiple only handles MultiIndexes,
            # and isn't needed for a single level
            return _unstack_multiple(obj, level, fill_value=fill_value)
        else:
            level = level[0]
    # Prioritize integer interpretation (GH #21677):
    if not is_integer(level) and not level == ""__placeholder__"":
        level = obj.index._get_level_number(level)
    if isinstance(obj, DataFrame):",[81]
"else:
    struct_pack = struct.pack
    struct_unpack = struct.unpack

def read_batch_urls(batch_fd):
    def fixup(url):
        if not isinstance(url, compat_str):
            url = url.decode('utf-8', 'replace')
        BOM_UTF8 = u'\xef\xbb\xbf'
        if url.startswith(BOM_UTF8):
            url = url[len(BOM_UTF8):]
        url = url.strip()
        if url.startswith(('#', ';', ']')):
            return False
        return url
    with contextlib.closing(batch_fd) as fd:
        return [url for url in map(fixup, fd) if url]

def urlencode_postdata(*args, **kargs):
    return compat_urllib_parse.urlencode(*args, **kargs).encode('ascii')

try:
    etree_iter = xml.etree.ElementTree.Element.iter
except AttributeError:  # Python <=2.6
    etree_iter = lambda n: n.findall('.//*')

def parse_xml(s):
    class TreeBuilder(xml.etree.ElementTree.TreeBuilder):
        def doctype(self, name, pubid, system):
            pass  # Ignore doctypes
    parser = xml.etree.ElementTree.XMLParser(target=TreeBuilder())
    kwargs = {'parser': parser} if sys.version_info >= (2, 7) else {}
    tree = xml.etree.ElementTree.XML(s.encode('utf-8'), **kwargs)
    # Fix up XML parser in Python 2.x
    if sys.version_info < (3, 0):
        for n in etree_iter(tree):
            if n.text is not None:
                if not isinstance(n.text, compat_str):
                    n.text = n.text.decode('utf-8')
    return tree

US_RATINGS = {
    'G': 0,
    'PG': 10,
    'PG-13': 13,
    'R': 16,
    'NC': 18,
}

def parse_age_limit(s):
    if s is None:
        return None
    m = re.match(r'^(?P<age>\d{1,2})\+?$', s)
    return int(m.group('age')) if m else US_RATINGS.get(s, None)

def strip_jsonp(code):
    return re.sub(r'(?s)^[a-zA-Z0-9_]+\s*\(\s*(.*)\);?\s*?\s*$', r'\1', code)

def js_to_json(code):
    def fix_kv(m):
        v = m.group(0)
        if v in ('true', 'false', 'null'):
            return v
        if v.startswith('""'):
            return v
        if v.startswith(""'""):
            v = v[1:-1]
            v = re.sub(r""\\\\|\\'|\"""", lambda m: {
                '\\\\': '\\\\',
                ""\\'"": ""'"",
                '""': '\\""',
            }[m.group(0)], v)
        return '""%s""' % v
    res = re.sub(r'''(?x)
        ""(?:[^""\\]*(?:\\\\|\\"")?)*""|
        '(?:[^'\\]*(?:\\\\|\\')?)*'|
        [a-zA-Z_][a-zA-Z_0-9]*
        ''', fix_kv, code)
    res = re.sub(r',(\s*\])', lambda m: m.group(1), res)
    return res

def qualities(quality_ids):
    """""" Get a numeric quality value out of a list of possible values """"""
    def q(qid):
        try:
            return quality_ids.index(qid)
        except ValueError:
            return -1
    return q

DEFAULT_OUTTMPL = '%(title)s-%(id)s.%(ext)s'

def limit_length(s, length):
    """""" Add ellipses to overly long strings """"""
    if s is None:
        return None
    ELLIPSES = '...'
    if len(s) > length:
        return s[:length - len(ELLIPSES)] + ELLIPSES
    return s

def version_tuple(v):
    return [int(e) for e in v.split('.')]

def is_outdated_version(version, limit, assume_new=True):
    if not version:
        return not assume_new
    try:
        return version_tuple(version) < version_tuple(limit)
    except ValueError:",[65]
"        elif c.shape[1] == 4:
            result = c.copy()
            if alpha is not None:
                result[:, -1] = alpha
        if np.any((result < 0) | (result > 1)):
            raise ValueError(""RGBA values should be within 0-1 range"")
        return result
    # Handle single values.
    # Note that this occurs *after* handling inputs that are already arrays, as
    # `to_rgba(c, alpha)` (below) is expensive for such inputs, due to the need
    # to format the array in the ValueError message(!).
    if cbook._str_lower_equal(c, ""none""):
        return np.zeros((0, 4), float)
    try:
        return np.array([to_rgba(c, alpha)], float)
    except (ValueError, TypeError):
        pass
    # Convert one at a time.
    result = np.empty((len(c), 4), float)
    for i, cc in enumerate(c):
        result[i] = to_rgba(cc, alpha)
    return result

def to_rgb(c):
    """"""Convert *c* to an RGB color, silently dropping the alpha channel.""""""
    return to_rgba(c)[:3]

def to_hex(c, keep_alpha=False):
    """"""Convert *c* to a hex color.
    Uses the ``#rrggbb`` format if *keep_alpha* is False (the default),
    ``#rrggbbaa`` otherwise.
    """"""
    c = to_rgba(c)
    if not keep_alpha:
        c = c[:3]
    return ""#"" + """".join(format(int(np.round(val * 255)), ""02x"")
                         for val in c)

### Backwards-compatible color-conversion API

cnames = CSS4_COLORS
hexColorPattern = re.compile(r""\A#[a-fA-F0-9]{6}\Z"")
rgb2hex = to_hex
hex2color = to_rgb

class ColorConverter(object):
    """"""
    This class is only kept for backwards compatibility.
    Its functionality is entirely provided by module-level functions.
    """"""
    colors = _colors_full_map
    cache = _colors_full_map.cache
    to_rgb = staticmethod(to_rgb)
    to_rgba = staticmethod(to_rgba)
    to_rgba_array = staticmethod(to_rgba_array)

colorConverter = ColorConverter()

### End of backwards-compatible color-conversion API

def makeMappingArray(N, data, gamma=1.0):
    """"""Create an *N* -element 1-d lookup table
    *data* represented by a list of x,y0,y1 mapping correspondences.
    Each element in this list represents how a value between 0 and 1
    (inclusive) represented by x is mapped to a corresponding value
    between 0 and 1 (inclusive). The two values of y are to allow
    for discontinuous mapping functions (say as might be found in a
    sawtooth) where y0 represents the value of y for values of x
    <= to that given, and y1 is the value to be used for x > than
    that given). The list must start with x=0, end with x=1, and
    all values of x must be in increasing order. Values between
    the given mapping points are determined by simple linear interpolation.
    Alternatively, data can be a function mapping values between 0 - 1
    to 0 - 1.
    The function returns an array ""result"" where ``result[x*(N-1)]``
    gives the closest value for values of x between 0 and 1.
    """"""
    if callable(data):
        xind = np.linspace(0, 1, N) ** gamma
        lut = np.clip(np.array(data(xind), dtype=float), 0, 1)
        return lut
    try:
        adata = np.array(data)
    except Exception:
        raise TypeError(""data must be convertible to an array"")
    shape = adata.shape
    if len(shape) != 2 or shape[1] != 3:
        raise ValueError(""data must be nx3 format"")
    x = adata[:, 0]
    y0 = adata[:, 1]
    y1 = adata[:, 2]
    if x[0] != 0. or x[-1] != 1.0:
        raise ValueError(
            ""data mapping points must start with x=0 and end with x=1"")
    if (np.diff(x) < 0).any():
        raise ValueError(""data mapping points must have x in increasing order"")
    # begin generation of lookup table
    x = x * (N - 1)
    xind = (N - 1) * np.linspace(0, 1, N) ** gamma
    ind = np.searchsorted(x, xind)[1:-1]
    distance = (xind[1:-1] - x[ind - 1]) / (x[ind] - x[ind - 1])
    lut = np.concatenate([
        [y1[0]],
        distance * (y0[ind] - y1[ind - 1]) + y1[ind - 1],
        [y0[-1]],
    ])
    # ensure that the lut is confined to values between 0 and 1 by clipping it
    return np.clip(lut, 0.0, 1.0)
","[114, 115, 116, 117, 118, 119, 120, 121, 122, 123]"
"    def astype(self, dtype, copy: bool = False, errors: str = ""raise""):
        """"""
        these automatically copy, so copy=True has no effect
        raise on an except if raise == True
        """"""
        dtype = pandas_dtype(dtype)
        # if we are passed a datetime64[ns, tz]
        if is_datetime64tz_dtype(dtype):
            values = self.values
            if copy:
                # this should be the only copy
                values = values.copy()
            if getattr(values, ""tz"", None) is None:
                values = DatetimeArray(values).tz_localize(""UTC"")
            values = values.tz_convert(dtype.tz)
            return self.make_block(values)
        # delegate
        return super().astype(dtype=dtype, copy=copy, errors=errors)
    def _can_hold_element(self, element: Any) -> bool:
        tipo = maybe_infer_dtype_type(element)
        if tipo is not None:
            if self.is_datetimetz:
                # require exact match, since non-nano does not exist
                return is_dtype_equal(tipo, self.dtype) or is_valid_nat_for_dtype(
                    element, self.dtype
                )
            # GH#27419 if we get a non-nano datetime64 object
            return is_datetime64_dtype(tipo)
        elif element is NaT:
            return True
        elif isinstance(element, datetime):
            if self.is_datetimetz:
                return tz_compare(element.tzinfo, self.dtype.tz)
            return element.tzinfo is None
        return is_valid_nat_for_dtype(element, self.dtype)
    def to_native_types(
        self, slicer=None, na_rep=None, date_format=None, quoting=None, **kwargs
    ):
        """""" convert to our native types format, slicing if desired """"""
        values = self.values
        i8values = self.values.view(""i8"")
        if slicer is not None:
            values = values[..., slicer]
            i8values = i8values[..., slicer]
        from pandas.io.formats.format import _get_format_datetime64_from_values
        fmt = _get_format_datetime64_from_values(values, date_format)
        result = tslib.format_array_from_datetime(
            i8values.ravel(),
            tz=getattr(self.values, ""tz"", None),
            format=fmt,
            na_rep=na_rep,
        ).reshape(i8values.shape)
        return np.atleast_2d(result)
    def should_store(self, value) -> bool:
        return is_datetime64_dtype(value.dtype)
    def set(self, locs, values):
        """"""
        Modify Block in-place with new item value
        Returns
        -------
        None
        """"""
        values = conversion.ensure_datetime64ns(values, copy=False)
        self.values[locs] = values
    def external_values(self):
        return np.asarray(self.values.astype(""datetime64[ns]"", copy=False))
    def array_values(self) -> ExtensionArray:
        return DatetimeArray._simple_new(self.values)

class DatetimeTZBlock(ExtensionBlock, DatetimeBlock):
    """""" implement a datetime64 block with a tz attribute """"""
    __slots__ = ()
    is_datetimetz = True
    is_extension = True
    internal_values = Block.internal_values
    _can_hold_element = DatetimeBlock._can_hold_element
    to_native_types = DatetimeBlock.to_native_types
    fill_value = np.datetime64(""NaT"", ""ns"")
    @property
    def _holder(self):
        return DatetimeArray
    def _maybe_coerce_values(self, values):
        """"""
        Input validation for values passed to __init__. Ensure that
        we have datetime64TZ, coercing if necessary.
        Parameters
        ----------
        values : array-like
            Must be convertible to datetime64
        Returns
        -------
        values : DatetimeArray
        """"""
        if not isinstance(values, self._holder):
            values = self._holder(values)
        if values.tz is None:
            raise ValueError(""cannot create a DatetimeTZBlock without a tz"")
        return values
    @property
    def is_view(self) -> bool:","[65, 66, 67, 70, 71, 72, 73, 74]"
"        ----------
        label : object
        takeable : interpret the index as indexers, default False
        Returns
        -------
        scalar value
        """"""
        if takeable:
            return self._values[label]
        # We assume that _convert_scalar_indexer has already been called,
        #  with kind=""loc"", if necessary, by the time we get here
        return self.index.get_value(self, label)
    def __setitem__(self, key, value):
        key = com.apply_if_callable(key, self)
        cacher_needs_updating = self._check_is_chained_assignment_possible()
        try:
            self._set_with_engine(key, value)
        except (KeyError, ValueError):
            values = self._values
            if is_integer(key) and not self.index.inferred_type == ""integer"":
                values[key] = value
            elif key is Ellipsis:
                self[:] = value
            else:
                self.loc[key] = value
        except TypeError as e:
            if isinstance(key, tuple) and not isinstance(self.index, MultiIndex):
                raise ValueError(""Can only tuple-index with a MultiIndex"")
            # python 3 type errors should be raised
            if _is_unorderable_exception(e):
                raise IndexError(key)
            if com.is_bool_indexer(key):
                key = check_bool_indexer(self.index, key)
                try:
                    self._where(~key, value, inplace=True)
                    return
                except InvalidIndexError:
                    pass
            self._set_with(key, value)
        if cacher_needs_updating:
            self._maybe_update_cacher()
    def _set_with_engine(self, key, value):
        # fails with AttributeError for IntervalIndex
        loc = self.index._engine.get_loc(key)
        validate_numeric_casting(self.dtype, value)
        self._values[loc] = value
    def _set_with(self, key, value):
        # other: fancy integer or otherwise
        if isinstance(key, slice):
            indexer = self.index._convert_slice_indexer(key, kind=""getitem"")
            return self._set_values(indexer, value)
        elif is_scalar(key) and not is_integer(key) and key not in self.index:
            # GH#12862 adding an new key to the Series
            # Note: have to exclude integers because that is ambiguously
            #  position-based
            self.loc[key] = value
            return
        else:
            if isinstance(key, tuple):
                try:
                    # TODO: no test cases that get here
                    self._set_values(key, value)
                except Exception:
                    pass
            if is_scalar(key):
                key = [key]
            if isinstance(key, Index):
                key_type = key.inferred_type
                key = key._values
            else:
                key_type = lib.infer_dtype(key, skipna=False)
            if key_type == ""integer"":
                if self.index.inferred_type == ""integer"":
                    self._set_labels(key, value)
                else:
                    return self._set_values(key, value)
            elif key_type == ""boolean"":
                self._set_values(key.astype(np.bool_), value)
            else:
                self._set_labels(key, value)
    def _set_labels(self, key, value):
        key = com.asarray_tuplesafe(key)
        indexer: np.ndarray = self.index.get_indexer(key)
        mask = indexer == -1
        if mask.any():
            raise ValueError(f""{key[mask]} not contained in the index"")
        self._set_values(indexer, value)
    def _set_values(self, key, value):
        if isinstance(key, Series):
            key = key._values
        self._data = self._data.setitem(indexer=key, value=value)
        self._maybe_update_cacher()
    def _set_value(self, label, value, takeable: bool = False):
        """"""
        Quickly set single value at passed label.
        If label is not contained, a new object is created with the label
        placed at the end of the result index.
        Parameters
        ----------
        label : object
            Partial indexing with MultiIndex not allowed.
        value : object
            Scalar value.
        takeable : interpret the index as indexers, default False
        """"""
        try:",[13]
"        @wrap_settings({'apt': '/usr/bin/apt'})
        def match(command, settings):
            print(settings.apt)
    """"""
    def _wrap_settings(fn, command, settings):
        return fn(command, settings.update(**params))
    return decorator(_wrap_settings)

def get_closest(word, possibilities, n=3, cutoff=0.6, fallback_to_first=True):
    """"""Returns closest match or just first from possibilities.""""""
    possibilities = list(possibilities)
    try:
        return get_close_matches(word, possibilities, n, cutoff)[0]
    except IndexError:
        if fallback_to_first:
            return possibilities[0]

@memoize
def get_all_executables():
    from thefuck.shells import thefuck_alias, get_aliases
    def _safe(fn, fallback):
        try:
            return fn()
        except OSError:
            return fallback
    tf_alias = thefuck_alias()
    return [exe.name
            for path in os.environ.get('PATH', '').split(':')
            for exe in _safe(lambda: list(Path(path).iterdir()), [])
            if not _safe(exe.is_dir, True)] + [
                alias for alias in get_aliases() if alias != tf_alias]

def replace_argument(script, from_, to):
    """"""Replaces command line argument.""""""
    replaced_in_the_end = re.sub(u' {}$'.format(from_), u' {}'.format(to),
                                 script, count=1)
    if replaced_in_the_end != script:
        return replaced_in_the_end
    else:
        return script.replace(
            u' {} '.format(from_), u' {} '.format(to), 1)

@decorator
def eager(fn, *args, **kwargs):
    return list(fn(*args, **kwargs))

@eager
def get_all_matched_commands(stderr, separator='Did you mean'):
    should_yield = False
    for line in stderr.split('\n'):
        if separator in line:
            should_yield = True
        elif should_yield and line:
            yield line.strip()

def replace_command(command, broken, matched):
    """"""Helper for *_no_command rules.""""""
    new_cmds = get_close_matches(broken, matched, cutoff=0.1)
    return [replace_argument(command.script, broken, new_cmd.strip())
            for new_cmd in new_cmds]

@memoize
def is_app(command, *app_names):
    """"""Returns `True` if command is call to one of passed app names.""""""
    for name in app_names:
        if command.script == name \
                or command.script.startswith(u'{} '.format(name)):
            return True
    return False

def for_app(*app_names):
    """"""Specifies that matching script is for on of app names.""""""
    def _for_app(fn, command, settings):
        if is_app(command, *app_names):
            return fn(command, settings)
        else:
            return False
    return decorator(_for_app)

def cache(*depends_on):
    """"""Caches function result in temporary file.
    Cache will be expired when modification date of files from `depends_on`
    will be changed.
    Function wrapped in `cache` should be arguments agnostic.
    """"""
    def _get_mtime(name):
        path = os.path.join(os.path.expanduser('~'), name)
        try:
            return str(os.path.getmtime(path))
        except OSError:
            return '0'
    @decorator
    def _cache(fn, *args, **kwargs):
        if cache.disabled:
            return fn(*args, **kwargs)
        cache_path = os.path.join(tempfile.gettempdir(), '.thefuck-cache')
        key = '{}.{}'.format(fn.__module__, repr(fn).split('at')[0])
        etag = '.'.join(_get_mtime(name) for name in depends_on)
        with shelve.open(cache_path) as db:
            if db.get(key, {}).get('etag') == etag:
                return db[key]['value']
            else:
                value = fn(*args, **kwargs)
                db[key] = {'etag': etag, 'value': value}
                return value
    return _cache",[118]
"    `inner` signifies that there were a pair of brackets somewhere around the
    current `line`, possibly transitively. This means we can fallback to splitting
    by delimiters if the LHS/RHS don't yield any results.
    If `py36` is True, splitting may generate syntax that is only compatible
    with Python 3.6 and later.
    """"""
    if line.is_comment:
        yield line
        return
    line_str = str(line).strip(""\n"")
    if not line.should_explode and is_line_short_enough(
        line, line_length=line_length, line_str=line_str
    ):
        yield line
        return
    split_funcs: List[SplitFunc]
    if line.is_def:
        split_funcs = [left_hand_split]
    else:
        def rhs(line: Line, py36: bool = False) -> Iterator[Line]:
            for omit in generate_trailers_to_omit(line, line_length):
                lines = list(right_hand_split(line, line_length, py36, omit=omit))
                if is_line_short_enough(lines[0], line_length=line_length):
                    yield from lines
                    return
            # All splits failed, best effort split with no omits.
            # This mostly happens to multiline strings that are by definition
            # reported as not fitting a single line.
            yield from right_hand_split(line, py36)
        if line.inside_brackets:
            split_funcs = [delimiter_split, standalone_comment_split, rhs]
        else:
            split_funcs = [rhs]
    for split_func in split_funcs:
        # We are accumulating lines in `result` because we might want to abort
        # mission and return the original line in the end, or attempt a different
        # split altogether.
        result: List[Line] = []
        try:
            for l in split_func(line, py36):
                if str(l).strip(""\n"") == line_str:
                    raise CannotSplit(""Split function returned an unchanged result"")
                result.extend(
                    split_line(l, line_length=line_length, inner=True, py36=py36)
                )
        except CannotSplit:
            continue
        else:
            yield from result
            break
    else:
        yield line

def left_hand_split(line: Line, py36: bool = False) -> Iterator[Line]:
    """"""Split line into many lines, starting with the first matching bracket pair.
    Note: this usually looks weird, only use this for function definitions.
    Prefer RHS otherwise.  This is why this function is not symmetrical with
    :func:`right_hand_split` which also handles optional parentheses.
    """"""
    tail_leaves: List[Leaf] = []
    body_leaves: List[Leaf] = []
    head_leaves: List[Leaf] = []
    current_leaves = head_leaves
    matching_bracket = None
    for leaf in line.leaves:
        if (
            current_leaves is body_leaves
            and leaf.type in CLOSING_BRACKETS
            and leaf.opening_bracket is matching_bracket
        ):
            current_leaves = tail_leaves if body_leaves else head_leaves
        current_leaves.append(leaf)
        if current_leaves is head_leaves:
            if leaf.type in OPENING_BRACKETS:
                matching_bracket = leaf
                current_leaves = body_leaves
    if not matching_bracket:
        raise CannotSplit(""No brackets found"")
    head = bracket_split_build_line(head_leaves, line, matching_bracket)
    body = bracket_split_build_line(body_leaves, line, matching_bracket, is_body=True)
    tail = bracket_split_build_line(tail_leaves, line, matching_bracket)
    bracket_split_succeeded_or_raise(head, body, tail)
    for result in (head, body, tail):
        if result:
            yield result

def right_hand_split(
    line: Line, line_length: int, py36: bool = False, omit: Collection[LeafID] = ()
) -> Iterator[Line]:
    """"""Split line into many lines, starting with the last matching bracket pair.
    If the split was by optional parentheses, attempt splitting without them, too.
    `omit` is a collection of closing bracket IDs that shouldn't be considered for
    this split.
    Note: running this function modifies `bracket_depth` on the leaves of `line`.
    """"""
    tail_leaves: List[Leaf] = []
    body_leaves: List[Leaf] = []
    head_leaves: List[Leaf] = []
    current_leaves = tail_leaves
    opening_bracket = None
    closing_bracket = None
    for leaf in reversed(line.leaves):
        if current_leaves is body_leaves:
            if leaf is opening_bracket:
                current_leaves = head_leaves if body_leaves else tail_leaves
        current_leaves.append(leaf)
        if current_leaves is tail_leaves:
            if leaf.type in CLOSING_BRACKETS and id(leaf) not in omit:
                opening_bracket = leaf.opening_bracket
                closing_bracket = leaf
                current_leaves = body_leaves
    if not (opening_bracket and closing_bracket and head_leaves):","[12, 13]"
"            if e:
                return e
    return mimetype2ext(getheader('Content-Type'))

def encode_data_uri(data, mime_type):
    return 'data:%s;base64,%s' % (mime_type, base64.b64encode(data).decode('ascii'))

def age_restricted(content_limit, age_limit):
    """""" Returns True iff the content should be blocked """"""
    if age_limit is None:  # No limit set
        return False
    if content_limit is None:
        return False  # Content available for everyone
    return age_limit < content_limit

def is_html(first_bytes):
    """""" Detect whether a file contains HTML by examining its first bytes. """"""
    BOMS = [
        (b'\xef\xbb\xbf', 'utf-8'),
        (b'\x00\x00\xfe\xff', 'utf-32-be'),
        (b'\xff\xfe\x00\x00', 'utf-32-le'),
        (b'\xff\xfe', 'utf-16-le'),
        (b'\xfe\xff', 'utf-16-be'),
    ]
    for bom, enc in BOMS:
        if first_bytes.startswith(bom):
            s = first_bytes[len(bom):].decode(enc, 'replace')
            break
    else:
        s = first_bytes.decode('utf-8', 'replace')
    return re.match(r'^\s*<', s)

def determine_protocol(info_dict):
    protocol = info_dict.get('protocol')
    if protocol is not None:
        return protocol
    url = info_dict['url']
    if url.startswith('rtmp'):
        return 'rtmp'
    elif url.startswith('mms'):
        return 'mms'
    elif url.startswith('rtsp'):
        return 'rtsp'
    ext = determine_ext(url)
    if ext == 'm3u8':
        return 'm3u8'
    elif ext == 'f4m':
        return 'f4m'
    return compat_urllib_parse_urlparse(url).scheme

def render_table(header_row, data):
    """""" Render a list of rows, each as a list of values """"""
    table = [header_row] + data
    max_lens = [max(len(compat_str(v)) for v in col) for col in zip(*table)]
    format_str = ' '.join('%-' + compat_str(ml + 1) + 's' for ml in max_lens[:-1]) + '%s'
    return '\n'.join(format_str % tuple(row) for row in table)

def _match_one(filter_part, dct):
    COMPARISON_OPERATORS = {
        '<': operator.lt,
        '<=': operator.le,
        '>': operator.gt,
        '>=': operator.ge,
        '=': operator.eq,
        '!=': operator.ne,
    }
    operator_rex = re.compile(r'''(?x)\s*
        (?P<key>[a-z_]+)
        \s*(?P<op>%s)(?P<none_inclusive>\s*\?)?\s*
        (?:
            (?P<intval>[0-9.]+(?:[kKmMgGtTpPeEzZyY]i?[Bb]?)?)|
            (?P<quote>[""\'])(?P<quotedstrval>(?:\\.|(?!(?P=quote)|\\).)+?)(?P=quote)|
            (?P<strval>(?![0-9.])[a-z0-9A-Z]*)
        )
        \s*$
        ''' % '|'.join(map(re.escape, COMPARISON_OPERATORS.keys())))
    m = operator_rex.search(filter_part)
    if m:
        op = COMPARISON_OPERATORS[m.group('op')]
        actual_value = dct.get(m.group('key'))
        if (m.group('quotedstrval') is not None or
            m.group('strval') is not None or
            # If the original field is a string and matching comparisonvalue is
            # a number we should respect the origin of the original field
            # and process comparison value as a string (see
            # https://github.com/rg3/youtube-dl/issues/11082).
            actual_value is not None and m.group('intval') is not None and
                isinstance(actual_value, compat_str)):
            if m.group('op') not in ('=', '!='):
                raise ValueError(
                    'Operator %s does not support string values!' % m.group('op'))
            comparison_value = m.group('quotedstrval') or m.group('strval') or m.group('intval')
            quote = m.group('quote')
            if quote is not None:
                comparison_value = comparison_value.replace(r'\%s' % quote, quote)
        else:
            try:
                comparison_value = int(m.group('intval'))
            except ValueError:
                comparison_value = parse_filesize(m.group('intval'))
                if comparison_value is None:
                    comparison_value = parse_filesize(m.group('intval') + 'B')
                if comparison_value is None:
                    raise ValueError(
                        'Invalid integer value %r in filter part %r' % (
                            m.group('intval'), filter_part))
        if actual_value is None:
            return m.group('none_inclusive')
        return op(actual_value, comparison_value)
    UNARY_OPERATORS = {
        '': lambda v: v is not None,
        '!': lambda v: v is None,
    }","[124, 125]"
"                    '{task} failed {failures} times in the last {window} seconds, so it is being '
                    'disabled for {persist} seconds'.format(
                        failures=config.disable_failures,
                        task=task.id,
                        window=config.disable_window,
                        persist=config.disable_persist,
                    ))
        elif new_status == DISABLED:
            task.scheduler_disable_time = None
        if new_status != task.status:
            self._status_tasks[task.status].pop(task.id)
            self._status_tasks[new_status][task.id] = task
            task.status = new_status
            task.updated = time.time()
    def fail_dead_worker_task(self, task, config, assistants):
        # If a running worker disconnects, tag all its jobs as FAILED and subject it to the same retry logic
        if task.status == RUNNING and task.worker_running and task.worker_running not in task.stakeholders | assistants:
            logger.info(""Task %r is marked as running by disconnected worker %r -> marking as ""
                        ""FAILED with retry delay of %rs"", task.id, task.worker_running,
                        config.retry_delay)
            task.worker_running = None
            self.set_status(task, FAILED, config)
            task.retry = time.time() + config.retry_delay
    def update_status(self, task, config):
        # Mark tasks with no remaining active stakeholders for deletion
        if not task.stakeholders:
            if task.remove is None:
                logger.info(""Task %r has stakeholders %r but none remain connected -> might remove ""
                            ""task in %s seconds"", task.id, task.stakeholders, config.remove_delay)
                task.remove = time.time() + config.remove_delay
        # Re-enable task after the disable time expires
        if task.status == DISABLED and task.scheduler_disable_time is not None:
            if time.time() - task.scheduler_disable_time > config.disable_persist:
                self.re_enable(task, config)
        # Reset FAILED tasks to PENDING if max timeout is reached, and retry delay is >= 0
        if task.status == FAILED and config.retry_delay >= 0 and task.retry < time.time():
            self.set_status(task, PENDING, config)
    def may_prune(self, task):
        return task.remove and time.time() > task.remove
    def inactivate_tasks(self, delete_tasks):
        # The terminology is a bit confusing: we used to ""delete"" tasks when they became inactive,
        # but with a pluggable state storage, you might very well want to keep some history of
        # older tasks as well. That's why we call it ""inactivate"" (as in the verb)
        for task in delete_tasks:
            task_obj = self._tasks.pop(task)
            self._status_tasks[task_obj.status].pop(task)
    def get_active_workers(self, last_active_lt=None, last_get_work_gt=None):
        for worker in six.itervalues(self._active_workers):
            if last_active_lt is not None and worker.last_active >= last_active_lt:
                continue
            last_get_work = getattr(worker, 'last_get_work', None)
            if last_get_work_gt is not None and (
                    last_get_work is None or last_get_work <= last_get_work_gt):
                continue
            yield worker
    def get_assistants(self, last_active_lt=None):
        return filter(lambda w: w.assistant, self.get_active_workers(last_active_lt))
    def get_worker_ids(self):
        return self._active_workers.keys()  # only used for unit tests
    def get_worker(self, worker_id):
        return self._active_workers.setdefault(worker_id, Worker(worker_id))
    def inactivate_workers(self, delete_workers):
        # Mark workers as inactive
        for worker in delete_workers:
            self._active_workers.pop(worker)
        self._remove_workers_from_tasks(delete_workers)
    def _remove_workers_from_tasks(self, workers, remove_stakeholders=True):
        for task in self.get_active_tasks():
            if remove_stakeholders:
                task.stakeholders.difference_update(workers)
            task.workers.difference_update(workers)
    def disable_workers(self, workers):
        self._remove_workers_from_tasks(workers, remove_stakeholders=False)
        for worker in workers:
            self.get_worker(worker).disabled = True
    def get_necessary_tasks(self):
        necessary_tasks = set()
        for task in self.get_active_tasks():
            if task.status not in (DONE, DISABLED) or \
                    getattr(task, 'scheduler_disable_time', None) is not None:
                necessary_tasks.update(task.deps)
                necessary_tasks.add(task.id)
        return necessary_tasks

class CentralPlannerScheduler(Scheduler):
    """"""
    Async scheduler that can handle multiple workers, etc.
    Can be run locally or on a server (using RemoteScheduler + server.Server).
    """"""
    def __init__(self, config=None, resources=None, task_history_impl=None, **kwargs):
        """"""
        Keyword Arguments:
        :param config: an object of class ""scheduler"" or None (in which the global instance will be used)
        :param resources: a dict of str->int constraints
        :param task_history_override: ignore config and use this object as the task history
        """"""
        self._config = config or scheduler(**kwargs)
        self._state = SimpleTaskState(self._config.state_path)
        if task_history_impl:
            self._task_history = task_history_impl
        elif self._config.record_task_history:
            from luigi import db_task_history  # Needs sqlalchemy, thus imported here
            self._task_history = db_task_history.DbTaskHistory()
        else:
            self._task_history = history.NopHistory()
        self._resources = resources or configuration.get_config().getintdict('resources')  # TODO: Can we make this a Parameter?
        self._make_task = functools.partial(
            Task, disable_failures=self._config.disable_failures,","[93, 94]"
"        mouseevent = MouseEvent(s, self, x, y, self._button, self._key,
                                step=step, guiEvent=guiEvent)
        self.callbacks.process(s, mouseevent)
    def button_press_event(self, x, y, button, dblclick=False, guiEvent=None):
        """"""
        Backend derived classes should call this function on any mouse
        button press.  (*x*, *y*) are the canvas coords ((0, 0) is lower left).
        button and key are as defined in :class:`MouseEvent`.
        This method will be call all functions connected to the
        'button_press_event' with a :class:`MouseEvent` instance.
        """"""
        self._button = button
        s = 'button_press_event'
        mouseevent = MouseEvent(s, self, x, y, button, self._key,
                                dblclick=dblclick, guiEvent=guiEvent)
        self.callbacks.process(s, mouseevent)
    def button_release_event(self, x, y, button, guiEvent=None):
        """"""
        Backend derived classes should call this function on any mouse
        button release.
        This method will call all functions connected to the
        'button_release_event' with a :class:`MouseEvent` instance.
        Parameters
        ----------
        x : float
            The canvas coordinates where 0=left.
        y : float
            The canvas coordinates where 0=bottom.
        guiEvent
            The native UI event that generated the Matplotlib event.
        """"""
        s = 'button_release_event'
        event = MouseEvent(s, self, x, y, button, self._key, guiEvent=guiEvent)
        self.callbacks.process(s, event)
        self._button = None
    def motion_notify_event(self, x, y, guiEvent=None):
        """"""
        Backend derived classes should call this function on any
        motion-notify-event.
        This method will call all functions connected to the
        'motion_notify_event' with a :class:`MouseEvent` instance.
        Parameters
        ----------
        x : float
            The canvas coordinates where 0=left.
        y : float
            The canvas coordinates where 0=bottom.
        guiEvent
            The native UI event that generated the Matplotlib event.
        """"""
        self._lastx, self._lasty = x, y
        s = 'motion_notify_event'
        event = MouseEvent(s, self, x, y, self._button, self._key,
                           guiEvent=guiEvent)
        self.callbacks.process(s, event)
    def leave_notify_event(self, guiEvent=None):
        """"""
        Backend derived classes should call this function when leaving
        canvas
        Parameters
        ----------
        guiEvent
            The native UI event that generated the Matplotlib event.
        """"""
        self.callbacks.process('figure_leave_event', LocationEvent.lastevent)
        LocationEvent.lastevent = None
        self._lastx, self._lasty = None, None
    def enter_notify_event(self, guiEvent=None, xy=None):
        """"""
        Backend derived classes should call this function when entering
        canvas
        Parameters
        ----------
        guiEvent
            The native UI event that generated the Matplotlib event.
        xy : (float, float)
            The coordinate location of the pointer when the canvas is entered.
        """"""
        if xy is not None:
            x, y = xy
            self._lastx, self._lasty = x, y
        else:
            x = None
            y = None
            cbook.warn_deprecated(
                '3.0', message='enter_notify_event expects a location but '
                'your backend did not pass one.')
        event = LocationEvent('figure_enter_event', self, x, y, guiEvent)
        self.callbacks.process('figure_enter_event', event)
    def inaxes(self, xy):
        """"""
        Check if a point is in an axes.
        Parameters
        ----------
        xy : tuple or list
            (x, y) coordinates.
            x position - pixels from left of canvas.
            y position - pixels from bottom of canvas.
        Returns
        -------
        axes: topmost axes containing the point, or None if no axes.
        """"""
        axes_list = [a for a in self.figure.get_axes()
                     if a.patch.contains_point(xy)]
        if axes_list:
            axes = cbook._topmost_artist(axes_list)
        else:
            axes = None
","[105, 116, 117, 120, 121]"
"            RuntimeError: if the model was never compiled.
        """"""
        if not self.built:
            raise RuntimeError('The model needs to be compiled '
                               'before being used.')
        return self.model.test_on_batch(x, y,
                                        sample_weight=sample_weight)
    def predict_proba(self, x, batch_size=32, verbose=0):
        """"""Generates class probability predictions for the input samples.
        The input samples are processed batch by batch.
        # Arguments
            x: input data, as a Numpy array or list of Numpy arrays
                (if the model has multiple inputs).
            batch_size: integer.
            verbose: verbosity mode, 0 or 1.
        # Returns
            A Numpy array of probability predictions.
        """"""
        preds = self.predict(x, batch_size, verbose)
        if preds.min() < 0. or preds.max() > 1.:
            warnings.warn('Network returning invalid probability values. '
                          'The last layer might not normalize predictions '
                          'into probabilities '
                          '(like softmax or sigmoid would).')
        return preds
    def predict_classes(self, x, batch_size=32, verbose=0):
        """"""Generate class predictions for the input samples.
        The input samples are processed batch by batch.
        # Arguments
            x: input data, as a Numpy array or list of Numpy arrays
                (if the model has multiple inputs).
            batch_size: integer.
            verbose: verbosity mode, 0 or 1.
        # Returns
            A numpy array of class predictions.
        """"""
        proba = self.predict(x, batch_size=batch_size, verbose=verbose)
        if proba.shape[-1] > 1:
            return proba.argmax(axis=-1)
        else:
            return (proba > 0.5).astype('int32')
    @interfaces.legacy_generator_methods_support
    def fit_generator(self, generator,
                      steps_per_epoch,
                      epochs=1,
                      verbose=1,
                      callbacks=None,
                      validation_data=None,
                      validation_steps=None,
                      class_weight=None,
                      max_queue_size=10,
                      workers=1,
                      use_multiprocessing=False,
                      shuffle=True,
                      initial_epoch=0):
        """"""Fits the model on data generated batch-by-batch by a Python generator.
        The generator is run in parallel to the model, for efficiency.
        For instance, this allows you to do real-time data augmentation
        on images on CPU in parallel to training your model on GPU.
        # Arguments
            generator: A generator.
                The output of the generator must be either
                - a tuple (inputs, targets)
                - a tuple (inputs, targets, sample_weights).
                All arrays should contain the same number of samples.
                The generator is expected to loop over its data
                indefinitely. An epoch finishes when `steps_per_epoch`
                batches have been seen by the model.
            steps_per_epoch: Total number of steps (batches of samples)
                to yield from `generator` before declaring one epoch
                finished and starting the next epoch. It should typically
                be equal to the number of samples of your dataset
                divided by the batch size.
            epochs: Integer, total number of iterations on the data.
                Note that in conjunction with initial_epoch, the parameter
                epochs is to be understood as ""final epoch"". The model is
                not trained for n steps given by epochs, but until the
                epoch epochs is reached.
            verbose: Verbosity mode, 0, 1, or 2.
            callbacks: List of callbacks to be called during training.
            validation_data: This can be either
                - A generator for the validation data
                - A tuple (inputs, targets)
                - A tuple (inputs, targets, sample_weights).
            validation_steps: Only relevant if `validation_data`
                is a generator.
                Number of steps to yield from validation generator
                at the end of every epoch. It should typically
                be equal to the number of samples of your
                validation dataset divided by the batch size.
            class_weight: Dictionary mapping class indices to a weight
                for the class.
            max_queue_size: Maximum size for the generator queue
            workers: Maximum number of processes to spin up
            use_multiprocessing: if True, use process based threading.
                Note that because
                this implementation relies on multiprocessing,
                you should not pass
                non picklable arguments to the generator
                as they can't be passed
                easily to children processes.
            shuffle: Whether to shuffle the order of the batches at
                the beginning of each epoch. Only used with instances
                of `Sequence` (keras.utils.Sequence).
            initial_epoch: Epoch at which to start training
                (useful for resuming a previous training run).
        # Returns
            A `History` object.
        # Raises
            RuntimeError: if the model was never compiled.
        # Example
        ```python",[52]
"        Parameters
        ----------
        method : {'pearson', 'kendall', 'spearman'} or callable
            Method of correlation:
            * pearson : standard correlation coefficient
            * kendall : Kendall Tau correlation coefficient
            * spearman : Spearman rank correlation
            * callable: callable with input two 1d ndarrays
                and returning a float. Note that the returned matrix from corr
                will have 1 along the diagonals and will be symmetric
                regardless of the callable's behavior.
                .. versionadded:: 0.24.0
        min_periods : int, optional
            Minimum number of observations required per pair of columns
            to have a valid result. Currently only available for Pearson
            and Spearman correlation.
        Returns
        -------
        DataFrame
            Correlation matrix.
        See Also
        --------
        DataFrame.corrwith : Compute pairwise correlation with another
            DataFrame or Series.
        Series.corr : Compute the correlation between two Series.
        Examples
        --------
        >>> def histogram_intersection(a, b):
        ...     v = np.minimum(a, b).sum().round(decimals=1)
        ...     return v
        >>> df = pd.DataFrame([(.2, .3), (.0, .6), (.6, .0), (.2, .1)],
        ...                   columns=['dogs', 'cats'])
        >>> df.corr(method=histogram_intersection)
              dogs  cats
        dogs   1.0   0.3
        cats   0.3   1.0
        """"""
        numeric_df = self._get_numeric_data()
        cols = numeric_df.columns
        idx = cols.copy()
        mat = numeric_df.values
        if method == ""pearson"":
            correl = libalgos.nancorr(ensure_float64(mat), minp=min_periods)
        elif method == ""spearman"":
            correl = libalgos.nancorr_spearman(ensure_float64(mat), minp=min_periods)
        elif method == ""kendall"" or callable(method):
            if min_periods is None:
                min_periods = 1
            mat = ensure_float64(mat).T
            corrf = nanops.get_corr_func(method)
            K = len(cols)
            correl = np.empty((K, K), dtype=float)
            mask = np.isfinite(mat)
            for i, ac in enumerate(mat):
                for j, bc in enumerate(mat):
                    if i > j:
                        continue
                    valid = mask[i] & mask[j]
                    if valid.sum() < min_periods:
                        c = np.nan
                    elif i == j:
                        c = 1.0
                    elif not valid.all():
                        c = corrf(ac[valid], bc[valid])
                    else:
                        c = corrf(ac, bc)
                    correl[i, j] = c
                    correl[j, i] = c
        else:
            raise ValueError(
                ""method must be either 'pearson', ""
                ""'spearman', 'kendall', or a callable, ""
                f""'{method}' was supplied""
            )
        return self._constructor(correl, index=idx, columns=cols)
    def cov(self, min_periods=None) -> ""DataFrame"":
        """"""
        Compute pairwise covariance of columns, excluding NA/null values.
        Compute the pairwise covariance among the series of a DataFrame.
        The returned data frame is the `covariance matrix
        <https://en.wikipedia.org/wiki/Covariance_matrix>`__ of the columns
        of the DataFrame.
        Both NA and null values are automatically excluded from the
        calculation. (See the note below about bias from missing values.)
        A threshold can be set for the minimum number of
        observations for each value created. Comparisons with observations
        below this threshold will be returned as ``NaN``.
        This method is generally used for the analysis of time series data to
        understand the relationship between different measures
        across time.
        Parameters
        ----------
        min_periods : int, optional
            Minimum number of observations required per pair of columns
            to have a valid result.
        Returns
        -------
        DataFrame
            The covariance matrix of the series of the DataFrame.
        See Also
        --------
        Series.cov : Compute covariance with another Series.
        core.window.EWM.cov: Exponential weighted sample covariance.
        core.window.Expanding.cov : Expanding sample covariance.
        core.window.Rolling.cov : Rolling sample covariance.
        Notes
        -----
        Returns the covariance matrix of the DataFrame's time series.
        The covariance is normalized by N-1.
","[46, 49, 51, 55]"
"            key = ensure_python_int(key)
        except TypeError:
            return False
        return key in self._range
    @Appender(_index_shared_docs[""get_loc""])
    def get_loc(self, key, method=None, tolerance=None):
        if is_integer(key) and method is None and tolerance is None:
            new_key = int(key)
            try:
                return self._range.index(new_key)
            except ValueError:
                raise KeyError(key)
        return super().get_loc(key, method=method, tolerance=tolerance)
    @Appender(_index_shared_docs[""get_indexer""])
    def get_indexer(self, target, method=None, limit=None, tolerance=None):
        if com.any_not_none(method, tolerance, limit) or not is_list_like(target):
            return super().get_indexer(
                target, method=method, tolerance=tolerance, limit=limit
            )
        if self.step > 0:
            start, stop, step = self.start, self.stop, self.step
        else:
            # Work on reversed range for simplicity:
            start, stop, step = (self.stop - self.step, self.start + 1, -self.step)
        target_array = np.asarray(target)
        if not (is_integer_dtype(target_array) and target_array.ndim == 1):
            # checks/conversions/roundings are delegated to general method
            return super().get_indexer(target, method=method, tolerance=tolerance)
        locs = target_array - start
        valid = (locs % step == 0) & (locs >= 0) & (target_array < stop)
        locs[~valid] = -1
        locs[valid] = locs[valid] / step
        if step != self.step:
            # We reversed this range: transform to original locs
            locs[valid] = len(self) - 1 - locs[valid]
        return ensure_platform_int(locs)
    def tolist(self):
        return list(self._range)
    @Appender(_index_shared_docs[""_shallow_copy""])
    def _shallow_copy(self, values=None, **kwargs):
        if values is None:
            name = kwargs.get(""name"", self.name)
            return self._simple_new(self._range, name=name)
        else:
            kwargs.setdefault(""name"", self.name)
            return self._int64index._shallow_copy(values, **kwargs)
    @Appender(ibase._index_shared_docs[""copy""])
    def copy(self, name=None, deep=False, dtype=None, **kwargs):
        self._validate_dtype(dtype)
        if name is None:
            name = self.name
        return self.from_range(self._range, name=name)
    def _minmax(self, meth):
        no_steps = len(self) - 1
        if no_steps == -1:
            return np.nan
        elif (meth == ""min"" and self.step > 0) or (meth == ""max"" and self.step < 0):
            return self.start
        return self.start + self.step * no_steps
    def min(self, axis=None, skipna=True, *args, **kwargs):
        """"""The minimum value of the RangeIndex""""""
        nv.validate_minmax_axis(axis)
        nv.validate_min(args, kwargs)
        return self._minmax(""min"")
    def max(self, axis=None, skipna=True, *args, **kwargs):
        """"""The maximum value of the RangeIndex""""""
        nv.validate_minmax_axis(axis)
        nv.validate_max(args, kwargs)
        return self._minmax(""max"")
    def argsort(self, *args, **kwargs):
        """"""
        Returns the indices that would sort the index and its
        underlying data.
        Returns
        -------
        argsorted : numpy array
        See Also
        --------
        numpy.ndarray.argsort
        """"""
        nv.validate_argsort(args, kwargs)
        if self._range.step > 0:
            return np.arange(len(self))
        else:
            return np.arange(len(self) - 1, -1, -1)
    def equals(self, other):
        """"""
        Determines if two Index objects contain the same elements.
        """"""
        if isinstance(other, RangeIndex):
            return self._range == other._range
        return super().equals(other)
    def intersection(self, other, sort=False):
        """"""
        Form the intersection of two Index objects.
        Parameters
        ----------
        other : Index or array-like
        sort : False or None, default False
            Sort the resulting index if possible
            .. versionadded:: 0.24.0
            .. versionchanged:: 0.24.1
               Changed the default to ``False`` to match the behaviour
               from before 0.24.0.",[26]
"        if go_backwards:
            mask = reverse(mask, 1)
        if len(int_shape(mask)) == 2:
            mask = expand_dims(mask)
        mask = C.to_sequence_like(mask, rnn_inputs)
    states = tuple(initial)
    with C.default_options(axis_offset=1):
        def _recurrence(x, states, m):
            # create place holder
            place_holders = [C.placeholder(dynamic_axes=x.dynamic_axes) for _ in states]
            past_values = []
            for s, p in zip(states, place_holders):
                past_values.append(C.sequence.past_value(p, s))
            new_output, new_states = step_function(
                x, tuple(past_values) + tuple(rnn_constants))
            if getattr(new_output, '_uses_learning_phase', False):
                global uses_learning_phase
                uses_learning_phase = True
            if m is not None:
                new_states = [C.element_select(m, n, s) for n, s in zip(new_states, past_values)]
            n_s = []
            for o, p in zip(new_states, place_holders):
                n_s.append(o.replace_placeholders({p: o.output}))
            if len(n_s) > 0:
                new_output = n_s[0]
            return new_output, n_s
        final_output, final_states = _recurrence(rnn_inputs, states, mask)
        last_output = C.sequence.last(final_output)
        last_states = [C.sequence.last(s) for s in final_states]
    if need_convert:
        final_output = C.sequence.unpack(final_output, 0, no_mask_output=True)
        if num_time_step is not None and num_time_step is not C.FreeDimension:
            final_output = _reshape_sequence(final_output, num_time_step)
    f_stats = []
    for l_s, i_s in zip(last_states, initial_states):
        if _get_dynamic_axis_num(i_s) == 0 and _get_dynamic_axis_num(l_s) == 1:
            if hasattr(C, 'unpack_batch'):
                f_stats.append(C.unpack_batch(l_s))
            else:
                f_stats.append(C.user_function(ConvertToStatic(l_s, batch_size=i_s.shape[0])))
        else:
            f_stats.append(l_s)
    last_output._uses_learning_phase = uses_learning_phase
    return last_output, final_output, f_stats

def has_seq_axis(x):
    return hasattr(x, 'dynamic_axes') and len(x.dynamic_axes) > 1

def l2_normalize(x, axis=None):
    axis = [axis]
    axis = _normalize_axis(axis, x)
    norm = C.sqrt(C.reduce_sum(C.square(x), axis=axis[0]))
    return x / norm

def hard_sigmoid(x):
    x = (0.2 * x) + 0.5
    x = C.clip(x, 0.0, 1.0)
    return x

def conv1d(x, kernel, strides=1, padding='valid',
           data_format=None, dilation_rate=1):
    data_format = normalize_data_format(data_format)
    if padding == 'causal':
        # causal (dilated) convolution:
        left_pad = dilation_rate * (kernel.shape[0] - 1)
        x = temporal_padding(x, (left_pad, 0))
        padding = 'valid'
    if data_format == 'channels_last':
        x = C.swapaxes(x, 0, 1)
        kernel = C.swapaxes(kernel, 0, 2)
    padding = _preprocess_border_mode(padding)
    if dev.type() == 0 and dilation_rate != 1:
        raise ValueError('Dilated convolution on CPU is not supported by CNTK backend. '
                         'Please set `dilation_rate` to 1. You passed: %s' % (dilation_rate,))
    x = C.convolution(
        kernel,
        x,
        strides=strides,
        auto_padding=[False, padding],
        dilation=dilation_rate)
    if data_format == 'channels_last':
        x = C.swapaxes(x, 0, 1)
    return x

def conv2d(x, kernel, strides=(1, 1), padding='valid',
           data_format=None, dilation_rate=(1, 1)):
    data_format = normalize_data_format(data_format)
    x = _preprocess_conv2d_input(x, data_format)
    kernel = _preprocess_conv2d_kernel(kernel, data_format)
    padding = _preprocess_border_mode(padding)
    if dev.type() == 0 and dilation_rate != (1, 1):
        raise ValueError('Dilated convolution on CPU is not supported by CNTK backend. '
                         'Please set `dilation_rate` to (1, 1). '
                         'You passed: %s' % (dilation_rate,))
    x = C.convolution(kernel,
                      x,
                      strides,
                      auto_padding=[False, padding, padding],
                      dilation=dilation_rate)
    return _postprocess_conv2d_output(x, data_format)

def separable_conv1d(x, depthwise_kernel, pointwise_kernel, strides=1,
                     padding='valid', data_format=None, dilation_rate=1):",[28]
"import base64
from six.moves.urllib.request import getproxies, proxy_bypass
from six.moves.urllib.parse import unquote
try:
    from urllib2 import _parse_proxy
except ImportError:
    from urllib.request import _parse_proxy
from six.moves.urllib.parse import urlunparse
from scrapy.utils.httpobj import urlparse_cached
from scrapy.exceptions import NotConfigured

class HttpProxyMiddleware(object):
    def __init__(self):
        self.proxies = {}
        for type, url in getproxies().items():
            self.proxies[type] = self._get_proxy(url, type)
        if not self.proxies:
            raise NotConfigured
    def _get_proxy(self, url, orig_type):
        proxy_type, user, password, hostport = _parse_proxy(url)
        proxy_url = urlunparse((proxy_type or orig_type, hostport, '', '', '', ''))
        if user:
            user_pass = '%s:%s' % (unquote(user), unquote(password))
            creds = base64.b64encode(user_pass).strip()
        else:
            creds = None
        return creds, proxy_url
    def process_request(self, request, spider):
        # ignore if proxy is already seted
        if 'proxy' in request.meta:
            return
        parsed = urlparse_cached(request)
        scheme = parsed.scheme
        # 'no_proxy' is only supported by http schemes
        if scheme in ('http', 'https') and proxy_bypass(parsed.hostname):
            return
        if scheme in self.proxies:
            self._set_proxy(request, scheme)
    def _set_proxy(self, request, scheme):
        creds, proxy = self.proxies[scheme]
        request.meta['proxy'] = proxy
        if creds:
            request.headers['Proxy-Authorization'] = 'Basic ' + creds","[11, 28]"
"            (isinstance(right_ax, MultiIndex) and len(join_keys) == right_ax.nlevels)
        ):
            raise AssertionError(
                ""If more than one join key is given then ""
                ""'right_ax' must be a MultiIndex and the ""
                ""number of join keys must be the number of levels in right_ax""
            )
        left_indexer, right_indexer = _get_multiindex_indexer(
            join_keys, right_ax, sort=sort
        )
    else:
        jkey = join_keys[0]
        left_indexer, right_indexer = _get_single_indexer(jkey, right_ax, sort=sort)
    if sort or len(left_ax) != len(left_indexer):
        # if asked to sort or there are 1-to-many matches
        join_index = left_ax.take(left_indexer)
        return join_index, left_indexer, right_indexer
    # left frame preserves order & length of its index
    return left_ax, None, right_indexer

def _right_outer_join(x, y, max_groups):
    right_indexer, left_indexer = libjoin.left_outer_join(y, x, max_groups)
    return left_indexer, right_indexer

def _factorize_keys(lk, rk, sort=True):
    # Some pre-processing for non-ndarray lk / rk
    lk = extract_array(lk, extract_numpy=True)
    rk = extract_array(rk, extract_numpy=True)
    if is_datetime64tz_dtype(lk.dtype) and is_datetime64tz_dtype(rk.dtype):
        # Extract the ndarray (UTC-localized) values
        # Note: we dont need the dtypes to match, as these can still be compared
        lk, _ = lk._values_for_factorize()
        rk, _ = rk._values_for_factorize()
    elif (
        is_categorical_dtype(lk) and is_categorical_dtype(rk) and lk.is_dtype_equal(rk)
    ):
        if lk.categories.equals(rk.categories):
            # if we exactly match in categories, allow us to factorize on codes
            rk = rk.codes
        else:
            # Same categories in different orders -> recode
            rk = _recode_for_categories(rk.codes, rk.categories, lk.categories)
        lk = ensure_int64(lk.codes)
        rk = ensure_int64(rk)
    elif is_extension_array_dtype(lk.dtype) and is_dtype_equal(lk.dtype, rk.dtype):
        lk, _ = lk._values_for_factorize()
        rk, _ = rk._values_for_factorize()
    if is_integer_dtype(lk) and is_integer_dtype(rk):
        # GH#23917 TODO: needs tests for case where lk is integer-dtype
        #  and rk is datetime-dtype
        klass = libhashtable.Int64Factorizer
        lk = ensure_int64(np.asarray(lk))
        rk = ensure_int64(np.asarray(rk))
    elif needs_i8_conversion(lk.dtype) and is_dtype_equal(lk.dtype, rk.dtype):
        # GH#23917 TODO: Needs tests for non-matching dtypes
        klass = libhashtable.Int64Factorizer
        lk = ensure_int64(np.asarray(lk, dtype=np.int64))
        rk = ensure_int64(np.asarray(rk, dtype=np.int64))
    else:
        klass = libhashtable.Factorizer
        lk = ensure_object(lk)
        rk = ensure_object(rk)
    rizer = klass(max(len(lk), len(rk)))
    llab = rizer.factorize(lk)
    rlab = rizer.factorize(rk)
    count = rizer.get_count()
    if sort:
        uniques = rizer.uniques.to_array()
        llab, rlab = _sort_labels(uniques, llab, rlab)
    # NA group
    lmask = llab == -1
    lany = lmask.any()
    rmask = rlab == -1
    rany = rmask.any()
    if lany or rany:
        if lany:
            np.putmask(llab, lmask, count)
        if rany:
            np.putmask(rlab, rmask, count)
        count += 1
    return llab, rlab, count

def _sort_labels(uniques: np.ndarray, left, right):
    llength = len(left)
    labels = np.concatenate([left, right])
    _, new_labels = algos.safe_sort(uniques, labels, na_sentinel=-1)
    new_labels = ensure_int64(new_labels)
    new_left, new_right = new_labels[:llength], new_labels[llength:]
    return new_left, new_right

def _get_join_keys(llab, rlab, shape, sort: bool):
    # how many levels can be done without overflow
    pred = lambda i: not is_int64_overflow_possible(shape[:i])
    nlev = next(filter(pred, range(len(shape), 0, -1)))
    # get keys for the first `nlev` levels
    stride = np.prod(shape[1:nlev], dtype=""i8"")
    lkey = stride * llab[0].astype(""i8"", subok=False, copy=False)
    rkey = stride * rlab[0].astype(""i8"", subok=False, copy=False)
    for i in range(1, nlev):","[30, 42]"
"        # group boundaries are where group ids change
        idx = np.r_[0, 1 + np.nonzero(ids[1:] != ids[:-1])[0]]
        # new values are where sorted labels change
        lchanges = llab(lab, slice(1, None)) != llab(lab, slice(None, -1))
        inc = np.r_[True, lchanges]
        inc[idx] = True  # group boundaries are also new values
        out = np.diff(np.nonzero(np.r_[inc, True])[0])  # value counts
        # num. of times each group should be repeated
        rep = partial(np.repeat, repeats=np.add.reduceat(inc, idx))
        # multi-index components
        codes = self.grouper.reconstructed_codes
        codes = [rep(level_codes) for level_codes in codes] + [llab(lab, inc)]
        levels = [ping.group_index for ping in self.grouper.groupings] + [lev]
        names = self.grouper.names + [self._selection_name]
        if dropna:
            mask = codes[-1] != -1
            if mask.all():
                dropna = False
            else:
                out, codes = out[mask], [level_codes[mask] for level_codes in codes]
        if normalize:
            out = out.astype(""float"")
            d = np.diff(np.r_[idx, len(ids)])
            if dropna:
                m = ids[lab == -1]
                np.add.at(d, m, -1)
                acc = rep(d)[mask]
            else:
                acc = rep(d)
            out /= acc
        if sort and bins is None:
            cat = ids[inc][mask] if dropna else ids[inc]
            sorter = np.lexsort((out if ascending else -out, cat))
            out, codes[-1] = out[sorter], codes[-1][sorter]
        if bins is None:
            mi = MultiIndex(
                levels=levels, codes=codes, names=names, verify_integrity=False
            )
            if is_integer_dtype(out):
                out = ensure_int64(out)
            return Series(out, index=mi, name=self._selection_name)
        # for compat. with libgroupby.value_counts need to ensure every
        # bin is present at every index level, null filled with zeros
        diff = np.zeros(len(out), dtype=""bool"")
        for level_codes in codes[:-1]:
            diff |= np.r_[True, level_codes[1:] != level_codes[:-1]]
        ncat, nbin = diff.sum(), len(levels[-1])
        left = [np.repeat(np.arange(ncat), nbin), np.tile(np.arange(nbin), ncat)]
        right = [diff.cumsum() - 1, codes[-1]]
        _, idx = _get_join_indexers(left, right, sort=False, how=""left"")
        out = np.where(idx != -1, out[idx], 0)
        if sort:
            sorter = np.lexsort((out if ascending else -out, left[0]))
            out, left[-1] = out[sorter], left[-1][sorter]
        # build the multi-index w/ full levels
        def build_codes(lev_codes: np.ndarray) -> np.ndarray:
            return np.repeat(lev_codes[diff], nbin)
        codes = [build_codes(lev_codes) for lev_codes in codes[:-1]]
        codes.append(left[-1])
        mi = MultiIndex(levels=levels, codes=codes, names=names, verify_integrity=False)
        if is_integer_dtype(out):
            out = ensure_int64(out)
        return Series(out, index=mi, name=self._selection_name)
    def count(self) -> Series:
        """"""
        Compute count of group, excluding missing values.
        Returns
        -------
        Series
            Count of values within each group.
        """"""
        ids, _, ngroups = self.grouper.group_info
        val = self.obj._internal_get_values()
        mask = (ids != -1) & ~isna(val)
        ids = ensure_platform_int(ids)
        minlength = ngroups or 0
        out = np.bincount(ids[mask], minlength=minlength)
        return Series(
            out,
            index=self.grouper.result_index,
            name=self._selection_name,
            dtype=""int64"",
        )
    def _apply_to_column_groupbys(self, func):
        """""" return a pass thru """"""
        return func(self)
    def pct_change(self, periods=1, fill_method=""pad"", limit=None, freq=None):
        """"""Calculate pct_change of each value to previous entry in group""""""
        # TODO: Remove this conditional when #23918 is fixed
        if freq:
            return self.apply(
                lambda x: x.pct_change(
                    periods=periods, fill_method=fill_method, limit=limit, freq=freq
                )
            )
        filled = getattr(self, fill_method)(limit=limit)
        fill_grp = filled.groupby(self.grouper.codes)
        shifted = fill_grp.shift(periods=periods, freq=freq)
        return (filled / shifted) - 1
",[100]
"            key = _tuplify(self.ndim, key)
        if len(key) != self.ndim:
            raise ValueError(""Not enough indexers for scalar access (setting)!"")
        key = list(self._convert_key(key, is_setter=True))
        self.obj._set_value(*key, value=value, takeable=self._takeable)

@doc(IndexingMixin.at)
class _AtIndexer(_ScalarAccessIndexer):
    _takeable = False
    def _convert_key(self, key, is_setter: bool = False):
        """"""
        Require they keys to be the same type as the index. (so we don't
        fallback)
        """"""
        # allow arbitrary setting
        if is_setter:
            return list(key)
        return key
    @property
    def _axes_are_unique(self) -> bool:
        # Only relevant for self.ndim == 2
        assert self.ndim == 2
        return self.obj.index.is_unique and self.obj.columns.is_unique
    def __getitem__(self, key):
        if self.ndim == 2 and not self._axes_are_unique:
            # GH#33041 fall back to .loc
            if not isinstance(key, tuple) or not all(is_scalar(x) for x in key):
                raise ValueError(""Invalid call for scalar access (getting)!"")
            return self.obj.loc[key]
        return super().__getitem__(key)
    def __setitem__(self, key, value):
        if self.ndim == 2 and not self._axes_are_unique:
            # GH#33041 fall back to .loc
            if not isinstance(key, tuple) or not all(is_scalar(x) for x in key):
                raise ValueError(""Invalid call for scalar access (setting)!"")
            self.obj.loc[key] = value
            return
        return super().__setitem__(key, value)

@doc(IndexingMixin.iat)
class _iAtIndexer(_ScalarAccessIndexer):
    _takeable = True
    def _convert_key(self, key, is_setter: bool = False):
        """"""
        Require integer args. (and convert to label arguments)
        """"""
        for a, i in zip(self.obj.axes, key):
            if not is_integer(i):
                raise ValueError(""iAt based indexing can only have integer indexers"")
        return key

def _tuplify(ndim: int, loc: Hashable) -> Tuple[Union[Hashable, slice], ...]:
    """"""
    Given an indexer for the first dimension, create an equivalent tuple
    for indexing over all dimensions.
    Parameters
    ----------
    ndim : int
    loc : object
    Returns
    -------
    tuple
    """"""
    _tup: List[Union[Hashable, slice]]
    _tup = [slice(None, None) for _ in range(ndim)]
    _tup[0] = loc
    return tuple(_tup)

def convert_to_index_sliceable(obj: ""DataFrame"", key):
    """"""
    If we are index sliceable, then return my slicer, otherwise return None.
    """"""
    idx = obj.index
    if isinstance(key, slice):
        return idx._convert_slice_indexer(key, kind=""getitem"")
    elif isinstance(key, str):
        # we are an actual column
        if key in obj.columns:
            return None
        # We might have a datetimelike string that we can translate to a
        # slice here via partial string indexing
        if idx._supports_partial_string_indexing:
            try:
                return idx._get_string_slice(key)
            except (KeyError, ValueError, NotImplementedError):
                return None
    return None

def check_bool_indexer(index: Index, key) -> np.ndarray:
    """"""
    Check if key is a valid boolean indexer for an object with such index and
    perform reindexing or conversion if needed.
    This function assumes that is_bool_indexer(key) == True.
    Parameters
    ----------
    index : Index
        Index of the object on which the indexing is done.
    key : list-like
        Boolean indexer to check.
    Returns
    -------
    np.array",[4]
"        self._encoding = encoding
        self._lines_read = 0
        self._index = index
        self._chunksize = chunksize
        if isinstance(filepath_or_buffer, str):
            (
                filepath_or_buffer,
                encoding,
                compression,
                should_close,
            ) = get_filepath_or_buffer(filepath_or_buffer, encoding=encoding)
        if isinstance(filepath_or_buffer, (str, bytes)):
            self.filepath_or_buffer = open(filepath_or_buffer, ""rb"")
        else:
            # Copy to BytesIO, and ensure no encoding
            contents = filepath_or_buffer.read()
            try:
                contents = contents.encode(self._encoding)
            except UnicodeEncodeError:
                pass
            self.filepath_or_buffer = BytesIO(contents)
        self._read_header()
    def close(self):
        self.filepath_or_buffer.close()
    def _get_row(self):
        return self.filepath_or_buffer.read(80).decode()
    def _read_header(self):
        self.filepath_or_buffer.seek(0)
        # read file header
        line1 = self._get_row()
        if line1 != _correct_line1:
            self.close()
            raise ValueError(""Header record is not an XPORT file."")
        line2 = self._get_row()
        fif = [[""prefix"", 24], [""version"", 8], [""OS"", 8], [""_"", 24], [""created"", 16]]
        file_info = _split_line(line2, fif)
        if file_info[""prefix""] != ""SAS     SAS     SASLIB"":
            self.close()
            raise ValueError(""Header record has invalid prefix."")
        file_info[""created""] = _parse_date(file_info[""created""])
        self.file_info = file_info
        line3 = self._get_row()
        file_info[""modified""] = _parse_date(line3[:16])
        # read member header
        header1 = self._get_row()
        header2 = self._get_row()
        headflag1 = header1.startswith(_correct_header1)
        headflag2 = header2 == _correct_header2
        if not (headflag1 and headflag2):
            self.close()
            raise ValueError(""Member header not found"")
        # usually 140, could be 135
        fieldnamelength = int(header1[-5:-2])
        # member info
        mem = [
            [""prefix"", 8],
            [""set_name"", 8],
            [""sasdata"", 8],
            [""version"", 8],
            [""OS"", 8],
            [""_"", 24],
            [""created"", 16],
        ]
        member_info = _split_line(self._get_row(), mem)
        mem = [[""modified"", 16], [""_"", 16], [""label"", 40], [""type"", 8]]
        member_info.update(_split_line(self._get_row(), mem))
        member_info[""modified""] = _parse_date(member_info[""modified""])
        member_info[""created""] = _parse_date(member_info[""created""])
        self.member_info = member_info
        # read field names
        types = {1: ""numeric"", 2: ""char""}
        fieldcount = int(self._get_row()[54:58])
        datalength = fieldnamelength * fieldcount
        # round up to nearest 80
        if datalength % 80:
            datalength += 80 - datalength % 80
        fielddata = self.filepath_or_buffer.read(datalength)
        fields = []
        obs_length = 0
        while len(fielddata) >= fieldnamelength:
            # pull data for one field
            field, fielddata = (
                fielddata[:fieldnamelength],
                fielddata[fieldnamelength:],
            )
            # rest at end gets ignored, so if field is short, pad out
            # to match struct pattern below
            field = field.ljust(140)
            fieldstruct = struct.unpack("">hhhh8s40s8shhh2s8shhl52s"", field)
            field = dict(zip(_fieldkeys, fieldstruct))
            del field[""_""]
            field[""ntype""] = types[field[""ntype""]]
            fl = field[""field_length""]
            if field[""ntype""] == ""numeric"" and ((fl < 2) or (fl > 8)):
                self.close()
                msg = f""Floating field width {fl} is not between 2 and 8.""
                raise TypeError(msg)
            for k, v in field.items():
                try:
                    field[k] = v.strip()
                except AttributeError:
                    pass
            obs_length += field[""field_length""]
            fields += [field]
        header = self._get_row()
        if not header == _correct_obs_header:
            self.close()
            raise ValueError(""Observation header not found."")
        self.fields = fields","[17, 18, 19, 20, 21, 22]"
"        if partition_cols is not None:
            kwargs[""file_scheme""] = ""hive""
        if is_s3_url(path):
            # path is s3:// so we need to open the s3file in 'wb' mode.
            # TODO: Support 'ab'
            path, _, _, _ = get_filepath_or_buffer(path, mode=""wb"")
            # And pass the opened s3file to the fastparquet internal impl.
            kwargs[""open_with""] = lambda path, _: path
        else:
            path, _, _, _ = get_filepath_or_buffer(path)
        with catch_warnings(record=True):
            self.api.write(
                path,
                df,
                compression=compression,
                write_index=index,
                partition_on=partition_cols,
                **kwargs
            )
    def read(self, path, columns=None, **kwargs):
        if is_s3_url(path):
            from pandas.io.s3 import get_file_and_filesystem
            # When path is s3:// an S3File is returned.
            # We need to retain the original path(str) while also
            # pass the S3File().open function to fsatparquet impl.
            s3, filesystem = get_file_and_filesystem(path)
            try:
                parquet_file = self.api.ParquetFile(path, open_with=filesystem.open)
            finally:
                s3.close()
        else:
            path, _, _, _ = get_filepath_or_buffer(path)
            parquet_file = self.api.ParquetFile(path)
        return parquet_file.to_pandas(columns=columns, **kwargs)

def to_parquet(
    df,
    path,
    engine=""auto"",
    compression=""snappy"",
    index=None,
    partition_cols=None,
    **kwargs
):
    """"""
    Write a DataFrame to the parquet format.
    Parameters
    ----------
    path : str
        File path or Root Directory path. Will be used as Root Directory path
        while writing a partitioned dataset.
        .. versionchanged:: 0.24.0
    engine : {'auto', 'pyarrow', 'fastparquet'}, default 'auto'
        Parquet library to use. If 'auto', then the option
        ``io.parquet.engine`` is used. The default ``io.parquet.engine``
        behavior is to try 'pyarrow', falling back to 'fastparquet' if
        'pyarrow' is unavailable.
    compression : {'snappy', 'gzip', 'brotli', None}, default 'snappy'
        Name of the compression to use. Use ``None`` for no compression.
    index : bool, default None
        If ``True``, include the dataframe's index(es) in the file output. If
        ``False``, they will not be written to the file.
        If ``None``, similar to ``True`` the dataframe's index(es)
        will be saved. However, instead of being saved as values,
        the RangeIndex will be stored as a range in the metadata so it
        doesn't require much space and is faster. Other indexes will
        be included as columns in the file output.
        .. versionadded:: 0.24.0
    partition_cols : list, optional, default None
        Column names by which to partition the dataset
        Columns are partitioned in the order they are given
        .. versionadded:: 0.24.0
    kwargs
        Additional keyword arguments passed to the engine
    """"""
    impl = get_engine(engine)
    return impl.write(
        df,
        path,
        compression=compression,
        index=index,
        partition_cols=partition_cols,
        **kwargs
    )

def read_parquet(path, engine=""auto"", columns=None, **kwargs):
    """"""
    Load a parquet object from the file path, returning a DataFrame.
    .. versionadded:: 0.21.0
    Parameters
    ----------
    path : str, path object or file-like object
        Any valid string path is acceptable. The string could be a URL. Valid
        URL schemes include http, ftp, s3, and file. For file URLs, a host is
        expected. A local file could be:
        ``file://localhost/path/to/table.parquet``.
        If you want to pass in a path object, pandas accepts any
        ``os.PathLike``.
        By file-like object, we refer to objects with a ``read()`` method,
        such as a file handler (e.g. via builtin ``open`` function)
        or ``StringIO``.
    engine : {'auto', 'pyarrow', 'fastparquet'}, default 'auto'
        Parquet library to use. If 'auto', then the option
        ``io.parquet.engine`` is used. The default ``io.parquet.engine``
        behavior is to try 'pyarrow', falling back to 'fastparquet' if
        'pyarrow' is unavailable.
    columns : list, default=None",[4]
"        elif (
            other.freq is None
            or self.freq is None
            or other.freq != self.freq
            or not other.freq.is_anchored()
            or (not self.is_monotonic or not other.is_monotonic)
        ):
            result = Index.intersection(self, other, sort=sort)
            # Invalidate the freq of `result`, which may not be correct at
            # this point, depending on the values.
            result._set_freq(None)
            result = self._shallow_copy(result._data, name=result.name)
            if result.freq is None:
                result._set_freq(""infer"")
            return result
        # to make our life easier, ""sort"" the two ranges
        if self[0] <= other[0]:
            left, right = self, other
        else:
            left, right = other, self
        # after sorting, the intersection always starts with the right index
        # and ends with the index of which the last elements is smallest
        end = min(left[-1], right[-1])
        start = right[0]
        if end < start:
            return type(self)(data=[])
        else:
            lslice = slice(*left.slice_locs(start, end))
            left_chunk = left.values[lslice]
            return self._shallow_copy(left_chunk)
    def _can_fast_union(self, other) -> bool:
        if not isinstance(other, type(self)):
            return False
        freq = self.freq
        if freq is None or freq != other.freq:
            return False
        if not self.is_monotonic or not other.is_monotonic:
            return False
        if len(self) == 0 or len(other) == 0:
            return True
        # to make our life easier, ""sort"" the two ranges
        if self[0] <= other[0]:
            left, right = self, other
        else:
            left, right = other, self
        right_start = right[0]
        left_end = left[-1]
        # Only need to ""adjoin"", not overlap
        try:
            return (right_start == left_end + freq) or right_start in left
        except ValueError:
            # if we are comparing a freq that does not propagate timezones
            # this will raise
            return False
    def _fast_union(self, other, sort=None):
        if len(other) == 0:
            return self.view(type(self))
        if len(self) == 0:
            return other.view(type(self))
        # to make our life easier, ""sort"" the two ranges
        if self[0] <= other[0]:
            left, right = self, other
        elif sort is False:
            # TDIs are not in the ""correct"" order and we don't want
            #  to sort but want to remove overlaps
            left, right = self, other
            left_start = left[0]
            loc = right.searchsorted(left_start, side=""left"")
            right_chunk = right._values[:loc]
            dates = concat_compat([left._values, right_chunk])
            result = self._shallow_copy(dates)
            result._set_freq(""infer"")
            # TODO: can we infer that it has self.freq?
            return result
        else:
            left, right = other, self
        left_end = left[-1]
        right_end = right[-1]
        # concatenate
        if left_end < right_end:
            loc = right.searchsorted(left_end, side=""right"")
            right_chunk = right._values[loc:]
            dates = concat_compat([left._values, right_chunk])
            result = self._shallow_copy(dates)
            result._set_freq(""infer"")
            # TODO: can we infer that it has self.freq?
            return result
        else:
            return left
    def _union(self, other, sort):
        if not len(other) or self.equals(other) or not len(self):
            return super()._union(other, sort=sort)
        # We are called by `union`, which is responsible for this validation
        assert isinstance(other, type(self))
        this, other = self._maybe_utc_convert(other)
        if this._can_fast_union(other):
            result = this._fast_union(other, sort=sort)
            if result.freq is None:
                result._set_freq(""infer"")
            return result
        else:
            i8self = Int64Index._simple_new(self.asi8, name=self.name)
            i8other = Int64Index._simple_new(other.asi8, name=other.name)
            i8result = i8self._union(i8other, sort=sort)
            result = type(self)(i8result, dtype=self.dtype, freq=""infer"")","[30, 33]"
"                                        request=request, spider=spider)
    def download(self, request, spider):
        slot = self.slot
        slot.add_request(request)
        d = self._download(request, spider)
        d.addBoth(self._downloaded, slot, request, spider)
        return d
    def _downloaded(self, response, slot, request, spider):
        slot.remove_request(request)
        return self.download(response, spider) \
                if isinstance(response, Request) else response
    def _download(self, request, spider):
        slot = self.slot
        slot.add_request(request)
        def _on_success(response):
            assert isinstance(response, (Response, Request))
            if isinstance(response, Response):
                response.request = request # tie request to response received
                logkws = self.logformatter.crawled(request, response, spider)
                logger.log(*logformatter_adapter(logkws), extra={'spider': spider})
                self.signals.send_catch_log(signal=signals.response_received, \
                    response=response, request=request, spider=spider)
            return response
        def _on_complete(_):
            slot.nextcall.schedule()
            return _
        dwld = self.downloader.fetch(request, spider)
        dwld.addCallbacks(_on_success)
        dwld.addBoth(_on_complete)
        return dwld
    @defer.inlineCallbacks
    def open_spider(self, spider, start_requests=(), close_if_idle=True):
        assert self.has_capacity(), ""No free spider slot when opening %r"" % \
            spider.name
        logger.info(""Spider opened"", extra={'spider': spider})
        nextcall = CallLaterOnce(self._next_request, spider)
        scheduler = self.scheduler_cls.from_crawler(self.crawler)
        start_requests = yield self.scraper.spidermw.process_start_requests(start_requests, spider)
        slot = Slot(start_requests, close_if_idle, nextcall, scheduler)
        self.slot = slot
        self.spider = spider
        yield scheduler.open(spider)
        yield self.scraper.open_spider(spider)
        self.crawler.stats.open_spider(spider)
        yield self.signals.send_catch_log_deferred(signals.spider_opened, spider=spider)
        slot.nextcall.schedule()
    def _spider_idle(self, spider):
        """"""Called when a spider gets idle. This function is called when there
        are no remaining pages to download or schedule. It can be called
        multiple times. If some extension raises a DontCloseSpider exception
        (in the spider_idle signal handler) the spider is not closed until the
        next loop and this function is guaranteed to be called (at least) once
        again for this spider.
        """"""
        res = self.signals.send_catch_log(signal=signals.spider_idle, \
            spider=spider, dont_log=DontCloseSpider)
        if any(isinstance(x, Failure) and isinstance(x.value, DontCloseSpider) \
                for _, x in res):
            self.slot.nextcall.schedule(5)
            return
        if self.spider_is_idle(spider):
            self.close_spider(spider, reason='finished')
    def close_spider(self, spider, reason='cancelled'):
        """"""Close (cancel) spider and clear all its outstanding requests""""""
        slot = self.slot
        if slot.closing:
            return slot.closing
        logger.info(""Closing spider (%(reason)s)"",
                    {'reason': reason},
                    extra={'spider': spider})
        dfd = slot.close()
        def log_failure(msg):
            def errback(failure):
                logger.error(msg, extra={'spider': spider, 'failure': failure})
            return errback
        dfd.addBoth(lambda _: self.downloader.close())
        dfd.addErrback(log_failure('Downloader close failure'))
        dfd.addBoth(lambda _: self.scraper.close_spider(spider))
        dfd.addErrback(log_failure('Scraper close failure'))
        dfd.addBoth(lambda _: slot.scheduler.close(reason))
        dfd.addErrback(log_failure('Scheduler close failure'))
        dfd.addBoth(lambda _: self.signals.send_catch_log_deferred(
            signal=signals.spider_closed, spider=spider, reason=reason))
        dfd.addErrback(log_failure('Error while sending spider_close signal'))
        dfd.addBoth(lambda _: self.crawler.stats.close_spider(spider, reason=reason))
        dfd.addErrback(log_failure('Stats close failure'))
        dfd.addBoth(lambda _: logger.info(""Spider closed (%(reason)s)"",
                                          {'reason': reason},
                                          extra={'spider': spider}))
        dfd.addBoth(lambda _: setattr(self, 'slot', None))
        dfd.addErrback(log_failure('Error while unassigning slot'))
        dfd.addBoth(lambda _: setattr(self, 'spider', None))
        dfd.addErrback(log_failure('Error while unassigning spider'))
        dfd.addBoth(lambda _: self._spider_closed_callback(spider))
        return dfd
    def _close_all_spiders(self):
        dfds = [self.close_spider(s, reason='shutdown') for s in self.open_spiders]
        dlist = defer.DeferredList(dfds)
        return dlist
    @defer.inlineCallbacks
    def _finish_stopping_engine(self):
        yield self.signals.send_catch_log_deferred(signal=signals.engine_stopped)",[85]
"        >>> df2_transposed = df2.T # or df2.transpose()
        >>> df2_transposed
                      0     1
        name      Alice   Bob
        score       9.5     8
        employed  False  True
        kids          0     0
        When the DataFrame has mixed dtypes, we get a transposed DataFrame with
        the `object` dtype:
        >>> df2.dtypes
        name         object
        score       float64
        employed       bool
        kids          int64
        dtype: object
        >>> df2_transposed.dtypes
        0    object
        1    object
        dtype: object
        """"""
        nv.validate_transpose(args, dict())
        return super().transpose(1, 0, **kwargs)
    T = property(transpose)
    # ----------------------------------------------------------------------
    # Indexing Methods
    def _ixs(self, i: int, axis: int = 0):
        """"""
        Parameters
        ----------
        i : int
        axis : int
        Notes
        -----
        If slice passed, the resulting data will be a view.
        """"""
        # irow
        if axis == 0:
            new_values = self._data.fast_xs(i)
            # if we are a copy, mark as such
            copy = isinstance(new_values, np.ndarray) and new_values.base is None
            result = self._constructor_sliced(
                new_values,
                index=self.columns,
                name=self.index[i],
                dtype=new_values.dtype,
            )
            result._set_is_copy(self, copy=copy)
            return result
        # icol
        else:
            label = self.columns[i]
            # if the values returned are not the same length
            # as the index (iow a not found value), iget returns
            # a 0-len ndarray. This is effectively catching
            # a numpy error (as numpy should really raise)
            values = self._data.iget(i)
            if len(self.index) and not len(values):
                values = np.array([np.nan] * len(self.index), dtype=object)
            result = self._box_col_values(values, label)
            # this is a cached value, mark it so
            result._set_as_cached(label, self)
            return result
    def __getitem__(self, key):
        key = lib.item_from_zerodim(key)
        key = com.apply_if_callable(key, self)
        if is_hashable(key):
            # shortcut if the key is in columns
            if self.columns.is_unique and key in self.columns:
                if self.columns.nlevels > 1:
                    return self._getitem_multilevel(key)
                return self._get_item_cache(key)
        # Do we have a slicer (on rows)?
        indexer = convert_to_index_sliceable(self, key)
        if indexer is not None:
            # either we have a slice or we have a string that can be converted
            #  to a slice for partial-string date indexing
            return self._slice(indexer, axis=0)
        # Do we have a (boolean) DataFrame?
        if isinstance(key, DataFrame):
            return self.where(key)
        # Do we have a (boolean) 1d indexer?
        if com.is_bool_indexer(key):
            return self._getitem_bool_array(key)
        # We are left with two options: a single key, and a collection of keys,
        # We interpret tuples as collections only for non-MultiIndex
        is_single_key = isinstance(key, tuple) or not is_list_like(key)
        if is_single_key:
            if self.columns.nlevels > 1:
                return self._getitem_multilevel(key)
            indexer = self.columns.get_loc(key)
            if is_integer(indexer):
                indexer = [indexer]
        else:
            if is_iterator(key):
                key = list(key)
            indexer = self.loc._get_listlike_indexer(key, axis=1, raise_missing=True)[1]
        # take() does not accept boolean indexers
        if getattr(indexer, ""dtype"", None) == bool:
            indexer = np.where(indexer)[0]
        data = self.take(indexer, axis=1)
        if is_single_key:
            # What does looking for a single key in a non-unique index return?
            # The behavior is inconsistent. It returns a Series, except when
            # - the key itself is repeated (test on data.shape, #9519), or",[24]
"            'processor': [
                '0', 'GenuineIntel', 'Intel(R) Core(TM) i7-4800MQ CPU @ 2.70GHz',
                '1', 'GenuineIntel', 'Intel(R) Core(TM) i7-4800MQ CPU @ 2.70GHz',
                '2', 'GenuineIntel', 'Intel(R) Core(TM) i7-4800MQ CPU @ 2.70GHz',
                '3', 'GenuineIntel', 'Intel(R) Core(TM) i7-4800MQ CPU @ 2.70GHz',
                '4', 'GenuineIntel', 'Intel(R) Core(TM) i7-4800MQ CPU @ 2.70GHz',
                '5', 'GenuineIntel', 'Intel(R) Core(TM) i7-4800MQ CPU @ 2.70GHz',
                '6', 'GenuineIntel', 'Intel(R) Core(TM) i7-4800MQ CPU @ 2.70GHz',
                '7', 'GenuineIntel', 'Intel(R) Core(TM) i7-4800MQ CPU @ 2.70GHz',
            ],
            'processor_cores': 4,
            'processor_count': 1,
            'processor_threads_per_core': 2,
            'processor_vcpus': 8},
    },
    {
        'architecture': 'arm64',
        'cpuinfo': open(os.path.join(os.path.dirname(__file__), '../fixtures/cpuinfo/arm64-4cpu-cpuinfo')).readlines(),
        'expected_result': {
            'processor': ['0', '1', '2', '3'],
            'processor_cores': 1,
            'processor_count': 4,
            'processor_threads_per_core': 1,
            'processor_vcpus': 4},
    },
    {
        'architecture': 'armv71',
        'cpuinfo': open(os.path.join(os.path.dirname(__file__), '../fixtures/cpuinfo/armv7-rev3-8cpu-cpuinfo')).readlines(),
        'expected_result': {
            'processor': [
                '0', 'ARMv7 Processor rev 3 (v7l)',
                '1', 'ARMv7 Processor rev 3 (v7l)',
                '2', 'ARMv7 Processor rev 3 (v7l)',
                '3', 'ARMv7 Processor rev 3 (v7l)',
                '4', 'ARMv7 Processor rev 3 (v7l)',
                '5', 'ARMv7 Processor rev 3 (v7l)',
                '6', 'ARMv7 Processor rev 3 (v7l)',
                '7', 'ARMv7 Processor rev 3 (v7l)',
            ],
            'processor_cores': 1,
            'processor_count': 8,
            'processor_threads_per_core': 1,
            'processor_vcpus': 8},
    },
    {
        'architecture': 'x86_64',
        'cpuinfo': open(os.path.join(os.path.dirname(__file__), '../fixtures/cpuinfo/x86_64-2cpu-cpuinfo')).readlines(),
        'expected_result': {
            'processor': [
                '0', 'GenuineIntel', 'Intel(R) Xeon(R) CPU E5-2680 v2 @ 2.80GHz',
                '1', 'GenuineIntel', 'Intel(R) Xeon(R) CPU E5-2680 v2 @ 2.80GHz',
            ],
            'processor_cores': 1,
            'processor_count': 2,
            'processor_threads_per_core': 1,
            'processor_vcpus': 2},
    },
    {
        'cpuinfo': open(os.path.join(os.path.dirname(__file__), '../fixtures/cpuinfo/ppc64-power7-rhel7-8cpu-cpuinfo')).readlines(),
        'architecture': 'ppc64',
        'expected_result': {
            'processor': [
                '0', 'POWER7 (architected), altivec supported',
                '1', 'POWER7 (architected), altivec supported',
                '2', 'POWER7 (architected), altivec supported',
                '3', 'POWER7 (architected), altivec supported',
                '4', 'POWER7 (architected), altivec supported',
                '5', 'POWER7 (architected), altivec supported',
                '6', 'POWER7 (architected), altivec supported',
                '7', 'POWER7 (architected), altivec supported'
            ],
            'processor_cores': 1,
            'processor_count': 16,
            'processor_threads_per_core': 1,
            'processor_vcpus': 16
        },
    },
    {
        'cpuinfo': open(os.path.join(os.path.dirname(__file__), '../fixtures/cpuinfo/ppc64le-power8-24cpu-cpuinfo')).readlines(),
        'architecture': 'ppc64le',
        'expected_result': {
            'processor': [
                '0', 'POWER8 (architected), altivec supported',
                '1', 'POWER8 (architected), altivec supported',
                '2', 'POWER8 (architected), altivec supported',
                '3', 'POWER8 (architected), altivec supported',
                '4', 'POWER8 (architected), altivec supported',
                '5', 'POWER8 (architected), altivec supported',
                '6', 'POWER8 (architected), altivec supported',
                '7', 'POWER8 (architected), altivec supported',
                '8', 'POWER8 (architected), altivec supported',
                '9', 'POWER8 (architected), altivec supported',
                '10', 'POWER8 (architected), altivec supported',
                '11', 'POWER8 (architected), altivec supported',
                '12', 'POWER8 (architected), altivec supported',
                '13', 'POWER8 (architected), altivec supported',
                '14', 'POWER8 (architected), altivec supported',
                '15', 'POWER8 (architected), altivec supported',
                '16', 'POWER8 (architected), altivec supported',
                '17', 'POWER8 (architected), altivec supported',
                '18', 'POWER8 (architected), altivec supported',
                '19', 'POWER8 (architected), altivec supported',
                '20', 'POWER8 (architected), altivec supported',
                '21', 'POWER8 (architected), altivec supported',
                '22', 'POWER8 (architected), altivec supported',
                '23', 'POWER8 (architected), altivec supported',
            ],
            'processor_cores': 1,
            'processor_count': 48,
            'processor_threads_per_core': 1,
            'processor_vcpus': 48
        },
    },
    {
        'cpuinfo': open(os.path.join(os.path.dirname(__file__), '../fixtures/cpuinfo/sparc-t5-debian-ldom-24vcpu')).readlines(),
        'architecture': 'sparc64',
        'expected_result': {
            'processor': [
                'UltraSparc T5 (Niagara5)',
            ],
            'processor_cores': 1,
            'processor_count': 24,
            'processor_threads_per_core': 1,
            'processor_vcpus': 24
        },
    },","[72, 74, 108, 110]"
"
def init_ndarray(values, index, columns, dtype=None, copy=False):
    # input must be a ndarray, list, Series, index
    if isinstance(values, ABCSeries):
        if columns is None:
            if values.name is not None:
                columns = [values.name]
        if index is None:
            index = values.index
        else:
            values = values.reindex(index)
        # zero len case (GH #2234)
        if not len(values) and columns is not None and len(columns):
            values = np.empty((0, 1), dtype=object)
    # we could have a categorical type passed or coerced to 'category'
    # recast this to an arrays_to_mgr
    if is_categorical_dtype(getattr(values, ""dtype"", None)) or is_categorical_dtype(
        dtype
    ):
        if not hasattr(values, ""dtype""):
            values = prep_ndarray(values, copy=copy)
            values = values.ravel()
        elif copy:
            values = values.copy()
        index, columns = _get_axes(len(values), 1, index, columns)
        return arrays_to_mgr([values], columns, index, columns, dtype=dtype)
    elif is_extension_array_dtype(values) or is_extension_array_dtype(dtype):
        # GH#19157
        if columns is None:
            columns = [0]
        return arrays_to_mgr([values], columns, index, columns, dtype=dtype)
    # by definition an array here
    # the dtypes will be coerced to a single dtype
    values = prep_ndarray(values, copy=copy)
    if dtype is not None:
        if not is_dtype_equal(values.dtype, dtype):
            try:
                values = values.astype(dtype)
            except Exception as orig:
                # e.g. ValueError when trying to cast object dtype to float64
                raise ValueError(
                    f""failed to cast to '{dtype}' (Exception was: {orig})""
                ) from orig
    index, columns = _get_axes(*values.shape, index=index, columns=columns)
    values = values.T
    # if we don't have a dtype specified, then try to convert objects
    # on the entire block; this is to convert if we have datetimelike's
    # embedded in an object type
    if dtype is None and is_object_dtype(values):
        if values.ndim == 2 and values.shape[0] != 1:
            # transpose and separate blocks
            dvals_list = [maybe_infer_to_datetimelike(row) for row in values]
            for n in range(len(dvals_list)):
                if isinstance(dvals_list[n], np.ndarray):
                    dvals_list[n] = dvals_list[n].reshape(1, -1)
            from pandas.core.internals.blocks import make_block
            # TODO: What about re-joining object columns?
            block_values = [
                make_block(dvals_list[n], placement=[n]) for n in range(len(dvals_list))
            ]
        else:
            datelike_vals = maybe_infer_to_datetimelike(values)
            block_values = [datelike_vals]
    else:
        block_values = [values]
    return create_block_manager_from_blocks(block_values, [columns, index])

def init_dict(data, index, columns, dtype=None):
    """"""
    Segregate Series based on type and coerce into matrices.
    Needs to handle a lot of exceptional cases.
    """"""
    if columns is not None:
        from pandas.core.series import Series
        arrays = Series(data, index=columns, dtype=object)
        data_names = arrays.index
        missing = arrays.isna()
        if index is None:
            # GH10856
            # raise ValueError if only scalars in dict
            index = extract_index(arrays[~missing])
        else:
            index = ensure_index(index)
        # no obvious ""empty"" int column
        if missing.any() and not is_integer_dtype(dtype):
            if dtype is None or np.issubdtype(dtype, np.flexible):
                # GH#1783
                nan_dtype = object
            else:
                nan_dtype = dtype
            val = construct_1d_arraylike_from_scalar(np.nan, len(index), nan_dtype)
            arrays.loc[missing] = [val] * missing.sum()
    else:
        keys = list(data.keys())
        columns = data_names = Index(keys)
        arrays = (com.maybe_iterable_to_list(data[k]) for k in keys)
        # GH#24096 need copy to be deep for datetime64tz case
        # TODO: See if we can avoid these copies
        arrays = [
            arr if not isinstance(arr, ABCIndexClass) else arr._data for arr in arrays
        ]
        arrays = [
            arr if not is_datetime64tz_dtype(arr) else arr.copy() for arr in arrays
        ]
    return arrays_to_mgr(arrays, data_names, index, columns, dtype=dtype)
","[35, 36]"
"                        sample_weight = None
                    elif len(generator_output) == 3:
                        x, y, sample_weight = generator_output
                    else:
                        raise ValueError('Output of generator should be '
                                         'a tuple `(x, y, sample_weight)` '
                                         'or `(x, y)`. Found: ' +
                                         str(generator_output))
                    # build batch logs
                    batch_logs = {}
                    if isinstance(x, list):
                        batch_size = x[0].shape[0]
                    elif isinstance(x, dict):
                        batch_size = list(x.values())[0].shape[0]
                    else:
                        batch_size = x.shape[0]
                    batch_logs['batch'] = batch_index
                    batch_logs['size'] = batch_size
                    callbacks.on_batch_begin(batch_index, batch_logs)
                    outs = self.train_on_batch(x, y,
                                               sample_weight=sample_weight,
                                               class_weight=class_weight)
                    if not isinstance(outs, list):
                        outs = [outs]
                    for l, o in zip(out_labels, outs):
                        batch_logs[l] = o
                    callbacks.on_batch_end(batch_index, batch_logs)
                    # Construct epoch logs.
                    epoch_logs = {}
                    batch_index += 1
                    steps_done += 1
                    # Epoch finished.
                    if steps_done >= steps_per_epoch and do_validation:
                        if val_gen:
                            val_outs = self.evaluate_generator(
                                validation_data,
                                validation_steps,
                                max_queue_size=max_queue_size,
                                workers=workers,
                                use_multiprocessing=use_multiprocessing)
                        else:
                            # No need for try/except because
                            # data has already been validated.
                            val_outs = self.evaluate(
                                val_x, val_y,
                                batch_size=batch_size,
                                sample_weight=val_sample_weights,
                                verbose=0)
                        if not isinstance(val_outs, list):
                            val_outs = [val_outs]
                        # Same labels assumed.
                        for l, o in zip(out_labels, val_outs):
                            epoch_logs['val_' + l] = o
                    if callback_model.stop_training:
                        break
                callbacks.on_epoch_end(epoch, epoch_logs)
                epoch += 1
                if callback_model.stop_training:
                    break
        finally:
            if enqueuer is not None:
                enqueuer.stop()
        callbacks.on_train_end()
        return self.history
    @interfaces.legacy_generator_methods_support
    def evaluate_generator(self, generator, steps,
                           max_queue_size=10,
                           workers=1,
                           use_multiprocessing=False):
        """"""Evaluates the model on a data generator.
        The generator should return the same kind of data
        as accepted by `test_on_batch`.
        # Arguments
            generator: Generator yielding tuples (inputs, targets)
                or (inputs, targets, sample_weights)
                or an instance of Sequence (keras.utils.Sequence)
                    object in order to avoid duplicate data
                    when using multiprocessing.
            steps: Total number of steps (batches of samples)
                to yield from `generator` before stopping.
                Not used if using Sequence.
            max_queue_size: maximum size for the generator queue
            workers: maximum number of processes to spin up
                when using process based threading
            use_multiprocessing: if True, use process based threading.
                Note that because
                this implementation relies on multiprocessing,
                you should not pass
                non picklable arguments to the generator
                as they can't be passed
                easily to children processes.
        # Returns
            Scalar test loss (if the model has a single output and no metrics)
            or list of scalars (if the model has multiple outputs
            and/or metrics). The attribute `model.metrics_names` will give you
            the display labels for the scalar outputs.
        # Raises
            ValueError: In case the generator yields
                data in an invalid format.
        """"""
        self._make_test_function()
        steps_done = 0
        wait_time = 0.01
        all_outs = []
        batch_sizes = []
        is_sequence = isinstance(generator, Sequence)
        if not is_sequence and use_multiprocessing and workers > 1:
            warnings.warn(
                UserWarning('Using a generator with `use_multiprocessing=True`'
                            ' and multiple workers may duplicate your data.'
                            ' Please consider using the`keras.utils.Sequence'
                            ' class.'))","[75, 92, 127]"
"    .. code-block:: console
        $ luigi --module my_tasks MyTask --book_locations '((12,3),(4,15),(52,1))'
    """"""
    def parse(self, x):
        """"""
        Parse an individual value from the input.
        :param str x: the value to parse.
        :return: the parsed value.
        """"""
        # Since the result of json.dumps(tuple) differs from a tuple string, we must handle either case.
        # A tuple string may come from a config file or from cli execution.
        # t = ((1, 2), (3, 4))
        # t_str = '((1,2),(3,4))'
        # t_json_str = json.dumps(t)
        # t_json_str == '[[1, 2], [3, 4]]'
        # json.loads(t_json_str) == t
        # json.loads(t_str) == ValueError: No JSON object could be decoded
        # Therefore, if json.loads(x) returns a ValueError, try ast.literal_eval(x).
        # ast.literal_eval(t_str) == t
        try:
            # loop required to parse tuple of tuples
            return tuple(tuple(x) for x in json.loads(x, object_pairs_hook=_FrozenOrderedDict))
        except ValueError:
            return literal_eval(x)  # if this causes an error, let that error be raised.

class NumericalParameter(Parameter):
    """"""
    Parameter whose value is a number of the specified type, e.g. ``int`` or
    ``float`` and in the range specified.
    In the task definition, use
    .. code-block:: python
        class MyTask(luigi.Task):
            my_param_1 = luigi.NumericalParameter(
                var_type=int, min_value=-3, max_value=7) # -3 <= my_param_1 < 7
            my_param_2 = luigi.NumericalParameter(
                var_type=int, min_value=-3, max_value=7, left_op=operator.lt, right_op=operator.le) # -3 < my_param_2 <= 7
    At the command line, use
    .. code-block:: console
        $ luigi --module my_tasks MyTask --my-param-1 -3 --my-param-2 -2
    """"""
    def __init__(self, left_op=operator.le, right_op=operator.lt, *args, **kwargs):
        """"""
        :param function var_type: The type of the input variable, e.g. int or float.
        :param min_value: The minimum value permissible in the accepted values
                          range.  May be inclusive or exclusive based on left_op parameter.
                          This should be the same type as var_type.
        :param max_value: The maximum value permissible in the accepted values
                          range.  May be inclusive or exclusive based on right_op parameter.
                          This should be the same type as var_type.
        :param function left_op: The comparison operator for the left-most comparison in
                                 the expression ``min_value left_op value right_op value``.
                                 This operator should generally be either
                                 ``operator.lt`` or ``operator.le``.
                                 Default: ``operator.le``.
        :param function right_op: The comparison operator for the right-most comparison in
                                  the expression ``min_value left_op value right_op value``.
                                  This operator should generally be either
                                  ``operator.lt`` or ``operator.le``.
                                  Default: ``operator.lt``.
        """"""
        if ""var_type"" not in kwargs:
            raise ParameterException(""var_type must be specified"")
        self._var_type = kwargs.pop(""var_type"")
        if ""min_value"" not in kwargs:
            raise ParameterException(""min_value must be specified"")
        self._min_value = kwargs.pop(""min_value"")
        if ""max_value"" not in kwargs:
            raise ParameterException(""max_value must be specified"")
        self._max_value = kwargs.pop(""max_value"")
        self._left_op = left_op
        self._right_op = right_op
        self._permitted_range = (
            ""{var_type} in {left_endpoint}{min_value}, {max_value}{right_endpoint}"".format(
                var_type=self._var_type.__name__,
                min_value=self._min_value, max_value=self._max_value,
                left_endpoint=""["" if left_op == operator.le else ""("",
                right_endpoint="")"" if right_op == operator.lt else ""]""))
        super(NumericalParameter, self).__init__(*args, **kwargs)
        if self.description:
            self.description += "" ""
        else:
            self.description = """"
        self.description += ""permitted values: "" + self._permitted_range
    def parse(self, s):
        value = self._var_type(s)
        if (self._left_op(self._min_value, value) and self._right_op(value, self._max_value)):
            return value
        else:
            raise ValueError(
                ""{s} is not in the set of {permitted_range}"".format(
                    s=s, permitted_range=self._permitted_range))

class ChoiceParameter(Parameter):
    """"""
    A parameter which takes two values:
        1. an instance of :class:`~collections.Iterable` and
        2. the class of the variables to convert to.
    In the task definition, use
    .. code-block:: python
        class MyTask(luigi.Task):
            my_param = luigi.ChoiceParameter(choices=[0.1, 0.2, 0.3], var_type=float)
    At the command line, use
    .. code-block:: console
        $ luigi --module my_tasks MyTask --my-param 0.1
    Consider using :class:`~luigi.EnumParameter` for a typed, structured","[27, 28]"
"        finally:
            if lock:
                lock.release()
    return True

def format_stdin_to_stdout(
    line_length: int,
    fast: bool,
    write_back: WriteBack = WriteBack.NO,
    mode: FileMode = FileMode.AUTO_DETECT,
) -> bool:
    """"""Format file on stdin. Return True if changed.
    If `write_back` is True, write reformatted code back to stdout.
    `line_length`, `fast`, `is_pyi`, and `force_py36` arguments are passed to
    :func:`format_file_contents`.
    """"""
    then = datetime.utcnow()
    src, encoding, newline = decode_bytes(sys.stdin.buffer.read())
    dst = src
    try:
        dst = format_file_contents(src, line_length=line_length, fast=fast, mode=mode)
        return True
    except NothingChanged:
        return False
    finally:
        f = io.TextIOWrapper(
            sys.stdout.buffer, encoding=encoding, newline=newline, write_through=True
        )
        if write_back == WriteBack.YES:
            f.write(dst)
        elif write_back == WriteBack.DIFF:
            now = datetime.utcnow()
            src_name = f""STDIN\t{then} +0000""
            dst_name = f""STDOUT\t{now} +0000""
            f.write(diff(src, dst, src_name, dst_name))
        f.detach()

def format_file_contents(
    src_contents: str,
    *,
    line_length: int,
    fast: bool,
    mode: FileMode = FileMode.AUTO_DETECT,
) -> FileContent:
    """"""Reformat contents a file and return new contents.
    If `fast` is False, additionally confirm that the reformatted code is
    valid by calling :func:`assert_equivalent` and :func:`assert_stable` on it.
    `line_length` is passed to :func:`format_str`.
    """"""
    if src_contents.strip() == """":
        raise NothingChanged
    dst_contents = format_str(src_contents, line_length=line_length, mode=mode)
    if src_contents == dst_contents:
        raise NothingChanged
    if not fast:
        assert_equivalent(src_contents, dst_contents)
        assert_stable(src_contents, dst_contents, line_length=line_length, mode=mode)
    return dst_contents

def format_str(
    src_contents: str, line_length: int, *, mode: FileMode = FileMode.AUTO_DETECT
) -> FileContent:
    """"""Reformat a string and return new contents.
    `line_length` determines how many characters per line are allowed.
    """"""
    src_node = lib2to3_parse(src_contents)
    dst_contents = """"
    future_imports = get_future_imports(src_node)
    is_pyi = bool(mode & FileMode.PYI)
    py36 = bool(mode & FileMode.PYTHON36) or is_python36(src_node)
    normalize_strings = not bool(mode & FileMode.NO_STRING_NORMALIZATION)
    lines = LineGenerator(
        remove_u_prefix=py36 or ""unicode_literals"" in future_imports,
        is_pyi=is_pyi,
        normalize_strings=normalize_strings,
    )
    elt = EmptyLineTracker(is_pyi=is_pyi)
    empty_line = Line()
    after = 0
    for current_line in lines.visit(src_node):
        for _ in range(after):
            dst_contents += str(empty_line)
        before, after = elt.maybe_empty_lines(current_line)
        for _ in range(before):
            dst_contents += str(empty_line)
        for line in split_line(current_line, line_length=line_length, py36=py36):
            dst_contents += str(line)
    return dst_contents

def decode_bytes(src: bytes) -> Tuple[FileContent, Encoding, NewLine]:
    """"""Return a tuple of (decoded_contents, encoding, newline).
    `newline` is either CRLF or LF but `decoded_contents` is decoded with
    universal newlines (i.e. only contains LF).
    """"""
    srcbuf = io.BytesIO(src)
    encoding, lines = tokenize.detect_encoding(srcbuf.readline)
    newline = ""\r\n"" if b""\r\n"" == lines[0][-2:] else ""\n""
    srcbuf.seek(0)
    with io.TextIOWrapper(srcbuf, encoding) as tiow:
        return tiow.read(), encoding, newline

GRAMMARS = [
    pygram.python_grammar_no_print_statement_no_exec_statement,
    pygram.python_grammar_no_print_statement,
    pygram.python_grammar,
]

def lib2to3_parse(src_txt: str) -> Node:
    """"""Given a string with source, return the lib2to3 Node.""""""
    grammar = pygram.python_grammar_no_print_statement
    if src_txt[-1] != ""\n"":
        src_txt += ""\n""
    for grammar in GRAMMARS:",[124]
"    ndim_cond = ndim(condition)
    ndim_expr = ndim(then_expression)
    if ndim_cond > ndim_expr:
        raise ValueError('Rank of condition should be less'
                         ' than or equal to rank of then and'
                         ' else expressions. ndim(condition)=' +
                         str(ndim_cond) + ', ndim(then_expression)'
                         '=' + str(ndim_expr))
    elif ndim_cond < ndim_expr:
        shape_expr = int_shape(then_expression)
        ndim_diff = ndim_expr - ndim_cond
        for i in range(ndim_diff):
            condition = expand_dims(condition)
            condition = tile(condition, shape_expr[ndim_cond + i])
    return C.element_select(condition,
                            then_expression,
                            else_expression)

def elu(x, alpha=1.):
    res = C.elu(x)
    if alpha == 1:
        return res
    else:
        return C.element_select(C.greater(x, 0), res, alpha * res)

def in_top_k(predictions, targets, k):
    _targets = C.one_hot(targets, predictions.shape[-1])
    result = C.classification_error(predictions, _targets, topN=k)
    return 1 - C.reshape(result, shape=())

def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),
                     padding='valid', data_format=None):
    data_format = normalize_data_format(data_format)
    x = _preprocess_conv2d_input(x, data_format)
    kernel = _preprocess_conv2d_kernel(kernel, data_format)
    padding = _preprocess_border_mode(padding)
    strides = (1,) + strides
    # cntk output_shape does not include batch axis
    output_shape = output_shape[1:]
    # in keras2, need handle output shape in different format
    if data_format == 'channels_last':
        output_shape = transpose_shape(output_shape, 'channels_first',
                                       spatial_axes=(0, 1))
    x = C.convolution_transpose(
        kernel,
        x,
        strides,
        auto_padding=[
            False,
            padding,
            padding],
        output_shape=output_shape)
    return _postprocess_conv2d_output(x, data_format)

def identity(x, name=None):
    if name is None:
        name = '%s_alias' % x.name
    return C.alias(x, name=name)

def _preprocess_conv2d_input(x, data_format):
    if data_format == 'channels_last':
        # TF uses the last dimension as channel dimension,
        # instead of the 2nd one.
        # TH input shape: (samples, input_depth, rows, cols)
        # TF input shape: (samples, rows, cols, input_depth)
        x = C.transpose(x, (2, 0, 1))
    return x

def _preprocess_conv2d_kernel(kernel, data_format):
    # As of Keras 2.0.0, all kernels are normalized
    # on the format `(rows, cols, input_depth, depth)`,
    # independently of `data_format`.
    # CNTK expects `(depth, input_depth, rows, cols)`.
    kernel = C.transpose(kernel, (3, 2, 0, 1))
    return kernel

def _preprocess_border_mode(padding):
    if padding == 'same':
        padding = True
    elif padding == 'valid':
        padding = False
    else:
        raise ValueError('Invalid border mode: ' + str(padding))
    return padding

def _postprocess_conv2d_output(x, data_format):
    if data_format == 'channels_last':
        x = C.transpose(x, (1, 2, 0))
    return x

def _preprocess_conv3d_input(x, data_format):
    if data_format == 'channels_last':
        # TF uses the last dimension as channel dimension,
        # instead of the 2nd one.
        # TH input shape: (samples, input_depth, conv_dim1, conv_dim2, conv_dim3)
        # TF input shape: (samples, conv_dim1, conv_dim2, conv_dim3,
        # input_depth)
        x = C.transpose(x, (3, 0, 1, 2))
    return x

def _preprocess_conv3d_kernel(kernel, dim_ordering):
    kernel = C.transpose(kernel, (4, 3, 0, 1, 2))
    return kernel

def _postprocess_conv3d_output(x, dim_ordering):
    if dim_ordering == 'channels_last':
        x = C.transpose(x, (1, 2, 3, 0))
    return x

def _get_dynamic_axis_num(x):
    if hasattr(x, 'dynamic_axes'):
        return len(x.dynamic_axes)
    else:","[34, 56]"
"from subprocess import Popen, PIPE
import os
from ..conf import settings
from ..utils import DEVNULL, memoize, cache
from .generic import Generic

class Bash(Generic):
    def app_alias(self, fuck):
        alias = ""TF_ALIAS={0}"" \
                "" alias {0}='PYTHONIOENCODING=utf-8"" \
                "" TF_CMD=$(thefuck $(fc -ln -1)) && "" \
                "" eval $TF_CMD"".format(fuck)
        if settings.alter_history:
            return alias + "" && history -s $TF_CMD'""
        else:
            return alias + ""'""
    def _parse_alias(self, alias):
        name, value = alias.replace('alias ', '', 1).split('=', 1)
        if value[0] == value[-1] == '""' or value[0] == value[-1] == ""'"":
            value = value[1:-1]
        return name, value
    @memoize
    @cache('.bashrc', '.bash_profile')
    def get_aliases(self):
        proc = Popen(['bash', '-ic', 'alias'], stdout=PIPE, stderr=DEVNULL)
        return dict(
                self._parse_alias(alias)
                for alias in proc.stdout.read().decode('utf-8').split('\n')
                if alias and '=' in alias)
    def _get_history_file_name(self):
        return os.environ.get(""HISTFILE"",
                              os.path.expanduser('~/.bash_history'))
    def _get_history_line(self, command_script):
        return u'{}\n'.format(command_script)
    def how_to_configure(self):
        if os.path.join(os.path.expanduser('~'), '.bashrc'):
            config = '~/.bashrc'
        elif os.path.join(os.path.expanduser('~'), '.bash_profile'):
            config = '~/.bashrc'
        else:
            config = 'bash config'
        return 'eval $(thefuck --alias)', config","[0, 3, 11, 26, 28, 29, 30, 31, 32]"
"                else:
                    entries = list(itertools.islice(
                        ie_entries, playliststart, playlistend))
                n_entries = len(entries)
                report_download(n_entries)
            if self.params.get('playlistreverse', False):
                entries = entries[::-1]
            if self.params.get('playlistrandom', False):
                random.shuffle(entries)
            x_forwarded_for = ie_result.get('__x_forwarded_for_ip')
            for i, entry in enumerate(entries, 1):
                self.to_screen('[download] Downloading video %s of %s' % (i, n_entries))
                # This __x_forwarded_for_ip thing is a bit ugly but requires
                # minimal changes
                if x_forwarded_for:
                    entry['__x_forwarded_for_ip'] = x_forwarded_for
                extra = {
                    'n_entries': n_entries,
                    'playlist': playlist,
                    'playlist_id': ie_result.get('id'),
                    'playlist_title': ie_result.get('title'),
                    'playlist_uploader': ie_result.get('uploader'),
                    'playlist_uploader_id': ie_result.get('uploader_id'),
                    'playlist_index': i + playliststart,
                    'extractor': ie_result['extractor'],
                    'webpage_url': ie_result['webpage_url'],
                    'webpage_url_basename': url_basename(ie_result['webpage_url']),
                    'extractor_key': ie_result['extractor_key'],
                }
                reason = self._match_entry(entry, incomplete=True)
                if reason is not None:
                    self.to_screen('[download] ' + reason)
                    continue
                entry_result = self.process_ie_result(entry,
                                                      download=download,
                                                      extra_info=extra)
                playlist_results.append(entry_result)
            ie_result['entries'] = playlist_results
            self.to_screen('[download] Finished downloading playlist: %s' % playlist)
            return ie_result
        elif result_type == 'compat_list':
            self.report_warning(
                'Extractor %s returned a compat_list result. '
                'It needs to be updated.' % ie_result.get('extractor'))
            def _fixup(r):
                self.add_extra_info(
                    r,
                    {
                        'extractor': ie_result['extractor'],
                        'webpage_url': ie_result['webpage_url'],
                        'webpage_url_basename': url_basename(ie_result['webpage_url']),
                        'extractor_key': ie_result['extractor_key'],
                    }
                )
                return r
            ie_result['entries'] = [
                self.process_ie_result(_fixup(r), download, extra_info)
                for r in ie_result['entries']
            ]
            return ie_result
        else:
            raise Exception('Invalid result type: %s' % result_type)
    def _build_format_filter(self, filter_spec):
        "" Returns a function to filter the formats according to the filter_spec ""
        OPERATORS = {
            '<': operator.lt,
            '<=': operator.le,
            '>': operator.gt,
            '>=': operator.ge,
            '=': operator.eq,
            '!=': operator.ne,
        }
        operator_rex = re.compile(r'''(?x)\s*
            (?P<key>width|height|tbr|abr|vbr|asr|filesize|filesize_approx|fps)
            \s*(?P<op>%s)(?P<none_inclusive>\s*\?)?\s*
            (?P<value>[0-9.]+(?:[kKmMgGtTpPeEzZyY]i?[Bb]?)?)
            $
            ''' % '|'.join(map(re.escape, OPERATORS.keys())))
        m = operator_rex.search(filter_spec)
        if m:
            try:
                comparison_value = int(m.group('value'))
            except ValueError:
                comparison_value = parse_filesize(m.group('value'))
                if comparison_value is None:
                    comparison_value = parse_filesize(m.group('value') + 'B')
                if comparison_value is None:
                    raise ValueError(
                        'Invalid value %r in format specification %r' % (
                            m.group('value'), filter_spec))
            op = OPERATORS[m.group('op')]
        if not m:
            STR_OPERATORS = {
                '=': operator.eq,
                '^=': lambda attr, value: attr.startswith(value),
                '$=': lambda attr, value: attr.endswith(value),
                '*=': lambda attr, value: value in attr,
            }
            str_operator_rex = re.compile(r'''(?x)
                \s*(?P<key>ext|acodec|vcodec|container|protocol|format_id)
                \s*(?P<negation>!\s*)?(?P<op>%s)(?P<none_inclusive>\s*\?)?
                \s*(?P<value>[a-zA-Z0-9._-]+)
                \s*$
                ''' % '|'.join(map(re.escape, STR_OPERATORS.keys())))
            m = str_operator_rex.search(filter_spec)
            if m:
                comparison_value = m.group('value')
                str_op = STR_OPERATORS[m.group('op')]
                if m.group('negation'):
                    op = lambda attr, value: not str_op
                else:
                    op = str_op
        if not m:
            raise ValueError('Invalid filter specification %r' % filter_spec)
        def _filter(f):",[119]
"""""""
Base and utility classes for tseries type pandas objects.
""""""
from datetime import datetime
from typing import Any, List, Optional, Union
import numpy as np
from pandas._libs import NaT, iNaT, join as libjoin, lib
from pandas._libs.tslibs import timezones
from pandas._typing import Label
from pandas.compat.numpy import function as nv
from pandas.errors import AbstractMethodError
from pandas.util._decorators import Appender, cache_readonly, doc
from pandas.core.dtypes.common import (
    ensure_int64,
    is_bool_dtype,
    is_categorical_dtype,
    is_dtype_equal,
    is_integer,
    is_list_like,
    is_period_dtype,
    is_scalar,
)
from pandas.core.dtypes.concat import concat_compat
from pandas.core.dtypes.generic import ABCIndex, ABCIndexClass, ABCSeries
from pandas.core.dtypes.missing import is_valid_nat_for_dtype, isna
from pandas.core import algorithms
from pandas.core.arrays import DatetimeArray, PeriodArray, TimedeltaArray
from pandas.core.arrays.datetimelike import DatetimeLikeArrayMixin
from pandas.core.base import IndexOpsMixin
import pandas.core.indexes.base as ibase
from pandas.core.indexes.base import Index, _index_shared_docs
from pandas.core.indexes.extension import (
    ExtensionIndex,
    inherit_names,
    make_wrapped_arith_op,
)
from pandas.core.indexes.numeric import Int64Index
from pandas.core.ops import get_op_result_name
from pandas.core.tools.timedeltas import to_timedelta
from pandas.tseries.frequencies import DateOffset, to_offset
_index_doc_kwargs = dict(ibase._index_doc_kwargs)

def _join_i8_wrapper(joinf, with_indexers: bool = True):
    """"""
    Create the join wrapper methods.
    """"""
    @staticmethod  # type: ignore
    def wrapper(left, right):
        if isinstance(left, (np.ndarray, ABCIndex, ABCSeries, DatetimeLikeArrayMixin)):
            left = left.view(""i8"")
        if isinstance(right, (np.ndarray, ABCIndex, ABCSeries, DatetimeLikeArrayMixin)):
            right = right.view(""i8"")
        results = joinf(left, right)
        if with_indexers:
            # dtype should be timedelta64[ns] for TimedeltaIndex
            #  and datetime64[ns] for DatetimeIndex
            dtype = left.dtype.base
            join_index, left_indexer, right_indexer = results
            join_index = join_index.view(dtype)
            return join_index, left_indexer, right_indexer
        return results
    return wrapper

@inherit_names(
    [""inferred_freq"", ""_isnan"", ""_resolution"", ""resolution""],
    DatetimeLikeArrayMixin,
    cache=True,
)
@inherit_names(
    [""mean"", ""freq"", ""freqstr"", ""asi8"", ""_box_values"", ""_box_func""],
    DatetimeLikeArrayMixin,
)
class DatetimeIndexOpsMixin(ExtensionIndex):
    """"""
    Common ops mixin to support a unified interface datetimelike Index.
    """"""
    _data: Union[DatetimeArray, TimedeltaArray, PeriodArray]
    freq: Optional[DateOffset]
    freqstr: Optional[str]
    _resolution: int
    _bool_ops: List[str] = []
    _field_ops: List[str] = []
    hasnans = cache_readonly(DatetimeLikeArrayMixin._hasnans.fget)  # type: ignore
    _hasnans = hasnans  # for index / array -agnostic code
    @property
    def is_all_dates(self) -> bool:
        return True
    # ------------------------------------------------------------------------
    # Abstract data attributes
    @property
    def values(self):
        # Note: PeriodArray overrides this to return an ndarray of objects.
        return self._data._data
    def __array_wrap__(self, result, context=None):
        """"""
        Gets called after a ufunc.
        """"""
        result = lib.item_from_zerodim(result)
        if is_bool_dtype(result) or lib.is_scalar(result):
            return result
        attrs = self._get_attributes_dict()
        if not is_period_dtype(self) and attrs[""freq""]:
            # no need to infer if freq is None
            attrs[""freq""] = ""infer""
        return Index(result, **attrs)
    # ------------------------------------------------------------------------
","[10, 34]"
"    @defer.inlineCallbacks
    def stop(self):
        if self.crawling:
            self.crawling = False
            yield defer.maybeDeferred(self.engine.stop)

class CrawlerRunner(object):
    def __init__(self, settings):
        if isinstance(settings, dict):
            settings = Settings(settings)
        self.settings = settings
        self.spider_loader = _get_spider_loader(settings)
        self.crawlers = set()
        self._active = set()
    @property
    def spiders(self):
        warnings.warn(""CrawlerRunner.spiders attribute is renamed to ""
                      ""CrawlerRunner.spider_loader."",
                      category=ScrapyDeprecationWarning, stacklevel=2)
        return self.spider_loader
    def crawl(self, crawler_or_spidercls, *args, **kwargs):
        crawler = crawler_or_spidercls
        if not isinstance(crawler_or_spidercls, Crawler):
            crawler = self._create_crawler(crawler_or_spidercls)
            self._setup_crawler_logging(crawler)
        self.crawlers.add(crawler)
        d = crawler.crawl(*args, **kwargs)
        self._active.add(d)
        def _done(result):
            self.crawlers.discard(crawler)
            self._active.discard(d)
            return result
        return d.addBoth(_done)
    def _create_crawler(self, spidercls):
        if isinstance(spidercls, six.string_types):
            spidercls = self.spider_loader.load(spidercls)
        return Crawler(spidercls, self.settings)
    def _setup_crawler_logging(self, crawler):
        log_observer = log.start_from_crawler(crawler)
        if log_observer:
            crawler.signals.connect(log_observer.stop, signals.engine_stopped)
    def stop(self):
        return defer.DeferredList([c.stop() for c in list(self.crawlers)])
    @defer.inlineCallbacks
    def join(self):
        """"""Wait for all managed crawlers to complete""""""
        while self._active:
            yield defer.DeferredList(self._active)

class CrawlerProcess(CrawlerRunner):
    """"""A class to run multiple scrapy crawlers in a process simultaneously""""""
    def __init__(self, settings):
        super(CrawlerProcess, self).__init__(settings)
        install_shutdown_handlers(self._signal_shutdown)
        self.stopping = False
        self.log_observer = log.start_from_settings(self.settings)
        log.scrapy_info(settings)
    def _signal_shutdown(self, signum, _):
        install_shutdown_handlers(self._signal_kill)
        signame = signal_names[signum]
        log.msg(format=""Received %(signame)s, shutting down gracefully. Send again to force "",
                level=log.INFO, signame=signame)
        reactor.callFromThread(self.stop)
    def _signal_kill(self, signum, _):
        install_shutdown_handlers(signal.SIG_IGN)
        signame = signal_names[signum]
        log.msg(format='Received %(signame)s twice, forcing unclean shutdown',
                level=log.INFO, signame=signame)
        self._stop_logging()
        reactor.callFromThread(self._stop_reactor)
    def start(self, stop_after_crawl=True):
        if stop_after_crawl:
            d = self.join()
            # Don't start the reactor if the deferreds are already fired
            if d.called:
                return
            d.addBoth(lambda _: self._stop_reactor())
        cache_size = self.settings.getint('DNSCACHE_SIZE') if self.settings.getbool('DNSCACHE_ENABLED') else 0
        reactor.installResolver(CachingThreadedResolver(reactor, cache_size,
                                                            self.settings.getfloat('DNS_TIMEOUT')))
        tp = reactor.getThreadPool()
        tp.adjustPoolsize(maxthreads=self.settings.getint('REACTOR_THREADPOOL_MAXSIZE'))
        reactor.addSystemEventTrigger('before', 'shutdown', self.stop)
        reactor.run(installSignalHandlers=False)  # blocking call
    def _stop_logging(self):
        if self.log_observer:
            self.log_observer.stop()
    def _stop_reactor(self, _=None):
        try:
            reactor.stop()
        except RuntimeError:  # raised if already stopped or in shutdown stage
            pass

def _get_spider_loader(settings):
    """""" Get SpiderLoader instance from settings """"""
    if settings.get('SPIDER_MANAGER_CLASS'):
        warnings.warn(
            'SPIDER_MANAGER_CLASS option is deprecated. '
            'Please use SPIDER_LOADER_CLASS.',
            category=ScrapyDeprecationWarning, stacklevel=2
        )
    cls_path = settings.get('SPIDER_LOADER_CLASS',
                            settings.get('SPIDER_MANAGER_CLASS'))
    loader_cls = load_object(cls_path)
    verifyClass(ISpiderLoader, loader_cls)","[122, 123]"
"                args.extend(['--password', password])
            if consumer_type:
                args.extend(['--type', consumer_type])
            if consumer_name:
                args.extend(['--name', consumer_name])
            if consumer_id:
                args.extend(['--consumerid', consumer_id])
            if environment:
                args.extend(['--environment', environment])
        if release:
            args.extend(['--release', release])
        rc, stderr, stdout = self.module.run_command(args, check_rc=True, expand_user_and_vars=False)
    def unsubscribe(self, serials=None):
        '''
            Unsubscribe a system from subscribed channels
            Args:
              serials(list or None): list of serials to unsubscribe. If
                                     serials is none or an empty list, then
                                     all subscribed channels will be removed.
            Raises:
              * Exception - if error occurs while running command
        '''
        items = []
        if serials is not None and serials:
            items = [""--serial=%s"" % s for s in serials]
        if serials is None:
            items = [""--all""]
        if items:
            args = [SUBMAN_CMD, 'unsubscribe'] + items
            rc, stderr, stdout = self.module.run_command(args, check_rc=True)
        return serials
    def unregister(self):
        '''
            Unregister a currently registered system
            Raises:
              * Exception - if error occurs while running command
        '''
        args = [SUBMAN_CMD, 'unregister']
        rc, stderr, stdout = self.module.run_command(args, check_rc=True)
        self.update_plugin_conf('rhnplugin', False)
        self.update_plugin_conf('subscription-manager', False)
    def subscribe(self, regexp):
        '''
            Subscribe current system to available pools matching the specified
            regular expression. It matches regexp against available pool ids first.
            If any pool ids match, subscribe to those pools and return.
            If no pool ids match, then match regexp against available pool product
            names. Note this can still easily match many many pools. Then subscribe
            to those pools.
            Since a pool id is a more specific match, we only fallback to matching
            against names if we didn't match pool ids.
            Raises:
              * Exception - if error occurs while running command
        '''
        # See https://github.com/ansible/ansible/issues/19466
        # subscribe to pools whose pool id matches regexp (and only the pool id)
        subscribed_pool_ids = self.subscribe_pool(regexp)
        # If we found any matches, we are done
        # Don't attempt to match pools by product name
        if subscribed_pool_ids:
            return subscribed_pool_ids
        # We didn't match any pool ids.
        # Now try subscribing to pools based on product name match
        # Note: This can match lots of product names.
        subscribed_by_product_pool_ids = self.subscribe_product(regexp)
        if subscribed_by_product_pool_ids:
            return subscribed_by_product_pool_ids
        # no matches
        return []
    def subscribe_by_pool_ids(self, pool_ids):
        """"""
        Try to subscribe to the list of pool IDs
        """"""
        available_pools = RhsmPools(self.module)
        available_pool_ids = [p.get_pool_id() for p in available_pools]
        for pool_id, quantity in sorted(pool_ids.items()):
            if pool_id in available_pool_ids:
                args = [SUBMAN_CMD, 'attach', '--pool', pool_id, '--quantity', quantity]
                rc, stderr, stdout = self.module.run_command(args, check_rc=True)
            else:
                self.module.fail_json(msg='Pool ID: %s not in list of available pools' % pool_id)
        return pool_ids
    def subscribe_pool(self, regexp):
        '''
            Subscribe current system to available pools matching the specified
            regular expression
            Raises:
              * Exception - if error occurs while running command
        '''
        # Available pools ready for subscription
        available_pools = RhsmPools(self.module)
        subscribed_pool_ids = []
        for pool in available_pools.filter_pools(regexp):
            pool.subscribe()
            subscribed_pool_ids.append(pool.get_pool_id())
        return subscribed_pool_ids
    def subscribe_product(self, regexp):
        '''
            Subscribe current system to available pools matching the specified
            regular expression
            Raises:
              * Exception - if error occurs while running command
        '''
        # Available pools ready for subscription
        available_pools = RhsmPools(self.module)
",[93]
"""""""
Read a SAS XPort format file into a Pandas DataFrame.
Based on code from Jack Cushman (github.com/jcushman/xport).
The file format is defined here:
https://support.sas.com/techsup/technote/ts140.pdf
""""""
from collections import abc
from datetime import datetime
from io import BytesIO
import struct
import warnings
import numpy as np
from pandas.util._decorators import Appender
import pandas as pd
from pandas.io.common import get_filepath_or_buffer
_correct_line1 = (
    ""HEADER RECORD*******LIBRARY HEADER RECORD!!!!!!!""
    ""000000000000000000000000000000  ""
)
_correct_header1 = (
    ""HEADER RECORD*******MEMBER  HEADER RECORD!!!!!!!000000000000000001600000000""
)
_correct_header2 = (
    ""HEADER RECORD*******DSCRPTR HEADER RECORD!!!!!!!""
    ""000000000000000000000000000000  ""
)
_correct_obs_header = (
    ""HEADER RECORD*******OBS     HEADER RECORD!!!!!!!""
    ""000000000000000000000000000000  ""
)
_fieldkeys = [
    ""ntype"",
    ""nhfun"",
    ""field_length"",
    ""nvar0"",
    ""name"",
    ""label"",
    ""nform"",
    ""nfl"",
    ""num_decimals"",
    ""nfj"",
    ""nfill"",
    ""niform"",
    ""nifl"",
    ""nifd"",
    ""npos"",
    ""_"",
]

_base_params_doc = """"""\
Parameters
----------
filepath_or_buffer : string or file-like object
    Path to SAS file or object implementing binary read method.""""""
_params2_doc = """"""\
index : identifier of index column
    Identifier of column that should be used as index of the DataFrame.
encoding : string
    Encoding for text data.
chunksize : int
    Read file `chunksize` lines at a time, returns iterator.""""""
_format_params_doc = """"""\
format : string
    File format, only `xport` is currently supported.""""""
_iterator_doc = """"""\
iterator : boolean, default False
    Return XportReader object for reading file incrementally.""""""

_read_sas_doc = f""""""Read a SAS file into a DataFrame.
{_base_params_doc}
{_format_params_doc}
{_params2_doc}
{_iterator_doc}
Returns
-------
DataFrame or XportReader
Examples
--------
Read a SAS Xport file:
>>> df = pd.read_sas('filename.XPT')
Read a Xport file in 10,000 line chunks:
>>> itr = pd.read_sas('filename.XPT', chunksize=10000)
>>> for chunk in itr:
>>>     do_something(chunk)
""""""
_xport_reader_doc = f""""""\
Class for reading SAS Xport files.
{_base_params_doc}
{_params2_doc}
Attributes
----------
member_info : list
    Contains information about the file
fields : list
    Contains information about the variables in the file
""""""
_read_method_doc = """"""\
Read observations from SAS Xport file, returning as data frame.
Parameters
----------
nrows : int
    Number of rows to read from data file; if None, read whole",[11]
"        os.path.join(output_dir, rendered_dirname)
    )
    output_dir_exists = os.path.exists(dir_to_create)
    if overwrite_if_exists:
        if output_dir_exists:
            logging.debug('Output directory {} already exists,'
                          'overwriting it'.format(dir_to_create))
    else:
        if output_dir_exists:
            msg = 'Error: ""{}"" directory already exists'.format(dir_to_create)
            raise OutputDirExistsException(msg)
    make_sure_path_exists(dir_to_create)
    return dir_to_create

def ensure_dir_is_templated(dirname):
    """"""
    Ensures that dirname is a templated directory name.
    """"""
    if '{{' in dirname and '}}' in dirname:
        return True
    else:
        raise NonTemplatedInputDirException

def generate_files(repo_dir, context=None, output_dir='.',
                   overwrite_if_exists=False):
    """"""
    Renders the templates and saves them to files.
    :param repo_dir: Project template input directory.
    :param context: Dict for populating the template's variables.
    :param output_dir: Where to output the generated project dir into.
    :param overwrite_if_exists: Overwrite the contents of the output directory
        if it exists
    """"""
    template_dir = find_template(repo_dir)
    logging.debug('Generating project from {0}...'.format(template_dir))
    context = context or {}
    unrendered_dir = os.path.split(template_dir)[1]
    ensure_dir_is_templated(unrendered_dir)
    project_dir = render_and_create_dir(unrendered_dir,
                                        context,
                                        output_dir,
                                        overwrite_if_exists)
    # We want the Jinja path and the OS paths to match. Consequently, we'll:
    #   + CD to the template folder
    #   + Set Jinja's path to '.'
    #
    #  In order to build our files to the correct folder(s), we'll use an
    # absolute path for the target folder (project_dir)
    project_dir = os.path.abspath(project_dir)
    logging.debug('project_dir is {0}'.format(project_dir))
    # run pre-gen hook from repo_dir
    with work_in(repo_dir):
        if run_hook('pre_gen_project', project_dir, context) != EXIT_SUCCESS:
            logging.error(""Stopping generation because pre_gen_project""
                          "" hook script didn't exit sucessfully"")
            return
    with work_in(template_dir):
        env = Environment(keep_trailing_newline=True)
        env.loader = FileSystemLoader('.')
        for root, dirs, files in os.walk('.'):
            # We must separate the two types of dirs into different lists.
            # The reason is that we don't want ``os.walk`` to go through the
            # unrendered directories, since they will just be copied.
            copy_dirs = []
            render_dirs = []
            for d in dirs:
                d_ = os.path.normpath(os.path.join(root, d))
                # We check the full path, because that's how it can be
                # specified in the ``_copy_without_render`` setting, but
                # we store just the dir name
                if copy_without_render(d_, context):
                    copy_dirs.append(d)
                else:
                    render_dirs.append(d)
            for copy_dir in copy_dirs:
                indir = os.path.normpath(os.path.join(root, copy_dir))
                outdir = os.path.normpath(os.path.join(project_dir, indir))
                logging.debug(
                    'Copying dir {0} to {1} without rendering'
                    ''.format(indir, outdir)
                )
                shutil.copytree(indir, outdir)
            # We mutate ``dirs``, because we only want to go through these dirs
            # recursively
            dirs[:] = render_dirs
            for d in dirs:
                unrendered_dir = os.path.join(project_dir, root, d)
                render_and_create_dir(unrendered_dir, context, output_dir,
                                      overwrite_if_exists)
            for f in files:
                infile = os.path.normpath(os.path.join(root, f))
                if copy_without_render(infile, context):
                    outfile_tmpl = Template(infile)
                    outfile_rendered = outfile_tmpl.render(**context)
                    outfile = os.path.join(project_dir, outfile_rendered)
                    logging.debug(
                        'Copying file {0} to {1} without rendering'
                        ''.format(infile, outfile)
                    )
                    shutil.copyfile(infile, outfile)
                    shutil.copymode(infile, outfile)
                    continue
                logging.debug('f is {0}'.format(f))
                generate_file(project_dir, infile, context, env)
    # run post-gen hook from repo_dir
    with work_in(repo_dir):
        run_hook('post_gen_project', project_dir, context)
",[63]
"            # No extra dependencies were resolved, exit loop
            if deps_exhausted:
                break
        # Now we have resolved the deps to our best extent, now select the latest version for collections with
        # multiple versions found and go from there
        deps_not_checked = set(dependency_map.keys()).difference(checked_parents)
        for collection in deps_not_checked:
            dependency_map[collection].set_latest_version()
            if no_deps or len(dependency_map[collection].dependencies) == 0:
                checked_parents.add(collection)
    return dependency_map

def _get_collection_info(dep_map, existing_collections, collection, requirement, source, b_temp_path, apis,
                         validate_certs, force, parent=None):
    dep_msg = """"
    if parent:
        dep_msg = "" - as dependency of %s"" % parent
    display.vvv(""Processing requirement collection '%s'%s"" % (to_text(collection), dep_msg))
    b_tar_path = None
    if os.path.isfile(to_bytes(collection, errors='surrogate_or_strict')):
        display.vvvv(""Collection requirement '%s' is a tar artifact"" % to_text(collection))
        b_tar_path = to_bytes(collection, errors='surrogate_or_strict')
    elif urlparse(collection).scheme.lower() in ['http', 'https']:
        display.vvvv(""Collection requirement '%s' is a URL to a tar artifact"" % collection)
        try:
            b_tar_path = _download_file(collection, b_temp_path, None, validate_certs)
        except urllib_error.URLError as err:
            raise AnsibleError(""Failed to download collection tar from '%s': %s""
                               % (to_native(collection), to_native(err)))
    if b_tar_path:
        req = CollectionRequirement.from_tar(b_tar_path, force, parent=parent)
        collection_name = to_text(req)
        if collection_name in dep_map:
            collection_info = dep_map[collection_name]
            collection_info.add_requirement(None, req.latest_version)
        else:
            collection_info = req
    else:
        validate_collection_name(collection)
        display.vvvv(""Collection requirement '%s' is the name of a collection"" % collection)
        if collection in dep_map:
            collection_info = dep_map[collection]
            collection_info.add_requirement(parent, requirement)
        else:
            apis = [source] if source else apis
            collection_info = CollectionRequirement.from_name(collection, apis, requirement, force, parent=parent)
    existing = [c for c in existing_collections if to_text(c) == to_text(collection_info)]
    if existing and not collection_info.force:
        # Test that the installed collection fits the requirement
        existing[0].add_requirement(to_text(collection_info), requirement)
        collection_info = existing[0]
    dep_map[to_text(collection_info)] = collection_info

def _download_file(url, b_path, expected_hash, validate_certs, headers=None):
    bufsize = 65536
    digest = sha256()
    urlsplit = os.path.splitext(to_text(url.rsplit('/', 1)[1]))
    b_file_name = to_bytes(urlsplit[0], errors='surrogate_or_strict')
    b_file_ext = to_bytes(urlsplit[1], errors='surrogate_or_strict')
    b_file_path = tempfile.NamedTemporaryFile(dir=b_path, prefix=b_file_name, suffix=b_file_ext, delete=False).name
    display.vvv(""Downloading %s to %s"" % (url, to_text(b_path)))
    # Galaxy redirs downloads to S3 which reject the request if an Authorization header is attached so don't redir that
    resp = open_url(to_native(url, errors='surrogate_or_strict'), validate_certs=validate_certs, headers=headers,
                    unredirected_headers=['Authorization'], http_agent=user_agent())
    with open(b_file_path, 'wb') as download_file:
        data = resp.read(bufsize)
        while data:
            digest.update(data)
            download_file.write(data)
            data = resp.read(bufsize)
    if expected_hash:
        actual_hash = digest.hexdigest()
        display.vvvv(""Validating downloaded file hash %s with expected hash %s"" % (actual_hash, expected_hash))
        if expected_hash != actual_hash:
            raise AnsibleError(""Mismatch artifact hash with downloaded file"")
    return b_file_path

def _extract_tar_file(tar, filename, b_dest, b_temp_path, expected_hash=None):
    n_filename = to_native(filename, errors='surrogate_or_strict')
    try:
        member = tar.getmember(n_filename)
    except KeyError:
        raise AnsibleError(""Collection tar at '%s' does not contain the expected file '%s'."" % (to_native(tar.name),
                                                                                                n_filename))
    with tempfile.NamedTemporaryFile(dir=b_temp_path, delete=False) as tmpfile_obj:
        bufsize = 65536
        sha256_digest = sha256()
        with _tarfile_extract(tar, member) as tar_obj:
            data = tar_obj.read(bufsize)
            while data:
                tmpfile_obj.write(data)
                tmpfile_obj.flush()
                sha256_digest.update(data)
                data = tar_obj.read(bufsize)
        actual_hash = sha256_digest.hexdigest()
        if expected_hash and actual_hash != expected_hash:
            raise AnsibleError(""Checksum mismatch for '%s' inside collection at '%s'""
                               % (n_filename, to_native(tar.name)))
        b_dest_filepath = os.path.join(b_dest, to_bytes(filename, errors='surrogate_or_strict'))
        b_parent_dir = os.path.split(b_dest_filepath)[0]
        if not os.path.exists(b_parent_dir):
            # Seems like Galaxy does not validate if all file entries have a corresponding dir ftype entry. This check
            # makes sure we create the parent directory even if it wasn't set in the metadata.
            os.makedirs(b_parent_dir)
",[58]
"                if col in this and col in other:
                    dleft = this[col].to_dense()
                    dright = other[col].to_dense()
                    result = dleft._binop(dright, func, fill_value=fill_value)
                    result = result.to_sparse(fill_value=this[col].fill_value)
                    new_data[col] = result
        else:
            for col in new_columns:
                if col in this and col in other:
                    new_data[col] = func(this[col], other[col])
        new_fill_value = self._get_op_result_fill_value(other, func)
        return self._constructor(
            data=new_data,
            index=new_index,
            columns=new_columns,
            default_fill_value=new_fill_value,
        ).__finalize__(self)
    def _combine_match_index(self, other, func, level=None):
        if level is not None:
            raise NotImplementedError(""'level' argument is not supported"")
        this, other = self.align(other, join=""outer"", axis=0, level=level, copy=False)
        new_data = {}
        for col, series in this.items():
            new_data[col] = func(series.values, other.values)
        fill_value = self._get_op_result_fill_value(other, func)
        return self._constructor(
            new_data,
            index=this.index,
            columns=self.columns,
            default_fill_value=fill_value,
        ).__finalize__(self)
    def _combine_match_columns(self, other, func, level=None):
        # patched version of DataFrame._combine_match_columns to account for
        # NumPy circumventing __rsub__ with float64 types, e.g.: 3.0 - series,
        # where 3.0 is numpy.float64 and series is a SparseSeries. Still
        # possible for this to happen, which is bothersome
        if level is not None:
            raise NotImplementedError(""'level' argument is not supported"")
        left, right = self.align(other, join=""outer"", axis=1, level=level, copy=False)
        assert left.columns.equals(right.index)
        new_data = {}
        for col in left.columns:
            new_data[col] = func(left[col], float(right[col]))
        return self._constructor(
            new_data,
            index=left.index,
            columns=left.columns,
            default_fill_value=self.default_fill_value,
        ).__finalize__(self)
    def _combine_const(self, other, func):
        return self._apply_columns(lambda x: func(x, other))
    def _get_op_result_fill_value(self, other, func):
        own_default = self.default_fill_value
        if isinstance(other, DataFrame):
            # i.e. called from _combine_frame
            other_default = getattr(other, ""default_fill_value"", np.nan)
            # if the fill values are the same use them? or use a valid one
            if own_default == other_default:
                # TOOD: won't this evaluate as False if both are np.nan?
                fill_value = own_default
            elif np.isnan(own_default) and not np.isnan(other_default):
                fill_value = other_default
            elif not np.isnan(own_default) and np.isnan(other_default):
                fill_value = own_default
            else:
                fill_value = None
        elif isinstance(other, SparseSeries):
            # i.e. called from _combine_match_index
            # fill_value is a function of our operator
            if isna(other.fill_value) or isna(own_default):
                fill_value = np.nan
            else:
                fill_value = func(np.float64(own_default), np.float64(other.fill_value))
                fill_value = item_from_zerodim(fill_value)
        else:
            raise NotImplementedError(type(other))
        return fill_value
    def _reindex_index(
        self, index, method, copy, level, fill_value=np.nan, limit=None, takeable=False
    ):
        if level is not None:
            raise TypeError(""Reindex by level not supported for sparse"")
        if self.index.equals(index):
            if copy:
                return self.copy()
            else:
                return self
        if len(self.index) == 0:
            return self._constructor(index=index, columns=self.columns).__finalize__(
                self
            )
        indexer = self.index.get_indexer(index, method, limit=limit)
        indexer = ensure_platform_int(indexer)
        mask = indexer == -1
        need_mask = mask.any()
        new_series = {}
        for col, series in self.items():
            if mask.all():
                continue","[29, 30, 56]"
"import re
import os
from thefuck.utils import memoize
from thefuck import shells

patterns = (
        # js, node:
        '^    at {file}:{line}:{col}',
        # cargo:
        '^   {file}:{line}:{col}',
        # python, thefuck:
        '^  File ""{file}"", line {line}',
        # awk:
        '^awk: {file}:{line}:',
        # git
        '^fatal: bad config file line {line} in {file}',
        # llc:
        '^llc: {file}:{line}:{col}:',
        # lua:
        '^lua: {file}:{line}:',
        # fish:
        '^{file} \(line {line}\):',
        # bash, sh, ssh:
        '^{file}: line {line}: ',
        # ghc, make, ruby, zsh:
        '^{file}:{line}:',
        # cargo, clang, gcc, go, rustc:
        '^{file}:{line}:{col}',
        # perl:
        'at {file} line {line}',
    )

# for the sake of readability do not use named groups above
def _make_pattern(pattern):
    pattern = pattern.replace('{file}', '(?P<file>[^:\n]+)')
    pattern = pattern.replace('{line}', '(?P<line>[0-9]+)')
    pattern = pattern.replace('{col}',  '(?P<col>[0-9]+)')
    return re.compile(pattern, re.MULTILINE)
patterns = [_make_pattern(p) for p in patterns]

@memoize
def _search(stderr):
    for pattern in patterns:
        m = re.search(pattern, stderr)
        if m:
            return m

def match(command, settings):
    return 'EDITOR' in os.environ and _search(command.stderr)

def get_new_command(command, settings):
    m = _search(command.stderr)
    # Note: there does not seem to be a standard for columns, so they are just
    # ignored for now
    return shells.and_('{} {} +{}'.format(os.environ['EDITOR'], m.group('file'), m.group('line')),
                       command.script)",[52]
"import warnings
import numpy as np
from pandas._libs import index as libindex
from pandas.util._decorators import Appender, cache_readonly
from pandas.core.dtypes.cast import astype_nansafe
from pandas.core.dtypes.common import (
    is_bool,
    is_bool_dtype,
    is_dtype_equal,
    is_extension_array_dtype,
    is_float,
    is_float_dtype,
    is_integer_dtype,
    is_scalar,
    needs_i8_conversion,
    pandas_dtype,
)
from pandas.core.dtypes.generic import (
    ABCFloat64Index,
    ABCInt64Index,
    ABCRangeIndex,
    ABCSeries,
    ABCUInt64Index,
)
from pandas.core.dtypes.missing import isna
from pandas.core import algorithms
import pandas.core.common as com
from pandas.core.indexes.base import Index, InvalidIndexError, _index_shared_docs
from pandas.core.ops import get_op_result_name
_num_index_shared_docs = dict()

class NumericIndex(Index):
    """"""
    Provide numeric type operations.
    This is an abstract class.
    """"""
    _is_numeric_dtype = True
    def __new__(cls, data=None, dtype=None, copy=False, name=None, fastpath=None):
        if fastpath is not None:
            warnings.warn(
                ""The 'fastpath' keyword is deprecated, and will be ""
                ""removed in a future version."",
                FutureWarning,
                stacklevel=2,
            )
            if fastpath:
                return cls._simple_new(data, name=name)
        # Coerce to ndarray if not already ndarray or Index
        if not isinstance(data, (np.ndarray, Index)):
            if is_scalar(data):
                raise cls._scalar_data_error(data)
            # other iterable of some kind
            if not isinstance(data, (ABCSeries, list, tuple)):
                data = list(data)
            data = np.asarray(data, dtype=dtype)
        if issubclass(data.dtype.type, str):
            cls._string_data_error(data)
        if copy or not is_dtype_equal(data.dtype, cls._default_dtype):
            subarr = np.array(data, dtype=cls._default_dtype, copy=copy)
            cls._assert_safe_casting(data, subarr)
        else:
            subarr = data
        if name is None and hasattr(data, ""name""):
            name = data.name
        return cls._simple_new(subarr, name=name)
    @Appender(_index_shared_docs[""_maybe_cast_slice_bound""])
    def _maybe_cast_slice_bound(self, label, side, kind):
        assert kind in [""ix"", ""loc"", ""getitem"", None]
        # we will try to coerce to integers
        return self._maybe_cast_indexer(label)
    @Appender(_index_shared_docs[""_shallow_copy""])
    def _shallow_copy(self, values=None, **kwargs):
        if values is not None and not self._can_hold_na:
            # Ensure we are not returning an Int64Index with float data:
            return self._shallow_copy_with_infer(values=values, **kwargs)
        return super()._shallow_copy(values=values, **kwargs)
    def _convert_for_op(self, value):
        """"""
        Convert value to be insertable to ndarray.
        """"""
        if is_bool(value) or is_bool_dtype(value):
            # force conversion to object
            # so we don't lose the bools
            raise TypeError
        return value
    def _convert_tolerance(self, tolerance, target):
        tolerance = np.asarray(tolerance)
        if target.size != tolerance.size and tolerance.size > 1:
            raise ValueError(""list-like tolerance size must match target index size"")
        if not np.issubdtype(tolerance.dtype, np.number):
            if tolerance.ndim > 0:
                raise ValueError(
                    (
                        ""tolerance argument for %s must contain ""
                        ""numeric elements if it is list type""
                    )
                    % (type(self).__name__,)
                )
            else:
                raise ValueError(
                    (
                        ""tolerance argument for %s must be numeric ""
                        ""if it is a scalar: %r""
                    )",[47]
"        # cannot pass _simple_new as it is
        return type(self)(result, freq=self.freq, name=self.name)
    def asof_locs(self, where, mask: np.ndarray) -> np.ndarray:
        """"""
        where : array of timestamps
        mask : array of booleans where data is not NA
        """"""
        where_idx = where
        if isinstance(where_idx, DatetimeIndex):
            where_idx = PeriodIndex(where_idx.values, freq=self.freq)
        elif not isinstance(where_idx, PeriodIndex):
            raise TypeError(""asof_locs `where` must be DatetimeIndex or PeriodIndex"")
        elif where_idx.freq != self.freq:
            raise raise_on_incompatible(self, where_idx)
        locs = self.asi8[mask].searchsorted(where_idx.asi8, side=""right"")
        locs = np.where(locs > 0, locs - 1, 0)
        result = np.arange(len(self))[mask].take(locs)
        first = mask.argmax()
        result[(locs == 0) & (where_idx.asi8 < self.asi8[first])] = -1
        return result
    @Appender(Index.astype.__doc__)
    def astype(self, dtype, copy=True, how=""start""):
        dtype = pandas_dtype(dtype)
        if is_datetime64_any_dtype(dtype):
            # 'how' is index-specific, isn't part of the EA interface.
            tz = getattr(dtype, ""tz"", None)
            return self.to_timestamp(how=how).tz_localize(tz)
        # TODO: should probably raise on `how` here, so we don't ignore it.
        return super().astype(dtype, copy=copy)
    @property
    def is_full(self) -> bool:
        """"""
        Returns True if this PeriodIndex is range-like in that all Periods
        between start and end are present, in order.
        """"""
        if len(self) == 0:
            return True
        if not self.is_monotonic:
            raise ValueError(""Index is not monotonic"")
        values = self.asi8
        return ((values[1:] - values[:-1]) < 2).all()
    @property
    def inferred_type(self) -> str:
        # b/c data is represented as ints make sure we can't have ambiguous
        # indexing
        return ""period""
    @Appender(_index_shared_docs[""get_indexer""] % _index_doc_kwargs)
    def get_indexer(self, target, method=None, limit=None, tolerance=None):
        target = ensure_index(target)
        if isinstance(target, PeriodIndex):
            if target.freq != self.freq:
                # No matches
                no_matches = -1 * np.ones(self.shape, dtype=np.intp)
                return no_matches
            target = target.asi8
            self_index = self._int64index
        else:
            self_index = self
        if tolerance is not None:
            tolerance = self._convert_tolerance(tolerance, target)
            if self_index is not self:
                # convert tolerance to i8
                tolerance = self._maybe_convert_timedelta(tolerance)
        return Index.get_indexer(self_index, target, method, limit, tolerance)
    @Appender(_index_shared_docs[""get_indexer_non_unique""] % _index_doc_kwargs)
    def get_indexer_non_unique(self, target):
        target = ensure_index(target)
        if isinstance(target, PeriodIndex):
            if target.freq != self.freq:
                no_matches = -1 * np.ones(self.shape, dtype=np.intp)
                return no_matches, no_matches
            target = target.asi8
        indexer, missing = self._int64index.get_indexer_non_unique(target)
        return ensure_platform_int(indexer), missing
    def get_loc(self, key, method=None, tolerance=None):
        """"""
        Get integer location for requested label.
        Parameters
        ----------
        key : Period, NaT, str, or datetime
            String or datetime key must be parseable as Period.
        Returns
        -------
        loc : int or ndarray[int64]
        Raises
        ------
        KeyError
            Key is not present in the index.
        TypeError
            If key is listlike or otherwise not hashable.
        """"""
        orig_key = key
        if not is_scalar(key):
            raise InvalidIndexError(key)
        if isinstance(key, str):
            try:
                loc = self._get_string_slice(key)
                return loc
            except (TypeError, ValueError):
                pass
","[84, 85, 86, 87, 89]"
"import os
import zipfile
from thefuck.utils import for_app

def _is_bad_zip(file):
    with zipfile.ZipFile(file, 'r') as archive:
        return len(archive.namelist()) > 1

def _zip_file(command):
    # unzip works that way:
    # unzip [-flags] file[.zip] [file(s) ...] [-x file(s) ...]
    #                ^          ^ files to unzip from the archive
    #                archive to unzip
    for c in command.script.split()[1:]:
        if not c.startswith('-'):
            if c.endswith('.zip'):
                return c
            else:
                return '{}.zip'.format(c)

@for_app('unzip')
def match(command):
    return ('-d' not in command.script
            and _is_bad_zip(_zip_file(command)))

def get_new_command(command):
    return '{} -d {}'.format(command.script, _zip_file(command)[:-4])

def side_effect(old_cmd, command):
    with zipfile.ZipFile(_zip_file(old_cmd), 'r') as archive:
        for file in archive.namelist():
            try:
                os.remove(file)
            except OSError:
                # does not try to remove directories as we cannot know if they
                # already existed before
                pass

requires_output = False","[15, 30]"
"    ""-="",
    ""*="",
    ""@="",
    ""/="",
    ""%="",
    ""&="",
    ""|="",
    ""^="",
    ""<<="",
    "">>="",
    ""**="",
    ""//="",
}
COMPREHENSION_PRIORITY = 20
COMMA_PRIORITY = 18
TERNARY_PRIORITY = 16
LOGIC_PRIORITY = 14
STRING_PRIORITY = 12
COMPARATOR_PRIORITY = 10
MATH_PRIORITIES = {
    token.VBAR: 9,
    token.CIRCUMFLEX: 8,
    token.AMPER: 7,
    token.LEFTSHIFT: 6,
    token.RIGHTSHIFT: 6,
    token.PLUS: 5,
    token.MINUS: 5,
    token.STAR: 4,
    token.SLASH: 4,
    token.DOUBLESLASH: 4,
    token.PERCENT: 4,
    token.AT: 4,
    token.TILDE: 3,
    token.DOUBLESTAR: 2,
}
DOT_PRIORITY = 1

@dataclass
class BracketTracker:
    """"""Keeps track of brackets on a line.""""""
    depth: int = 0
    bracket_match: Dict[Tuple[Depth, NodeType], Leaf] = Factory(dict)
    delimiters: Dict[LeafID, Priority] = Factory(dict)
    previous: Optional[Leaf] = None
    _for_loop_variable: int = 0
    _lambda_arguments: int = 0
    def mark(self, leaf: Leaf) -> None:
        """"""Mark `leaf` with bracket-related metadata. Keep track of delimiters.
        All leaves receive an int `bracket_depth` field that stores how deep
        within brackets a given leaf is. 0 means there are no enclosing brackets
        that started on this line.
        If a leaf is itself a closing bracket, it receives an `opening_bracket`
        field that it forms a pair with. This is a one-directional link to
        avoid reference cycles.
        If a leaf is a delimiter (a token on which Black can split the line if
        needed) and it's on depth 0, its `id()` is stored in the tracker's
        `delimiters` field.
        """"""
        if leaf.type == token.COMMENT:
            return
        self.maybe_decrement_after_for_loop_variable(leaf)
        self.maybe_decrement_after_lambda_arguments(leaf)
        if leaf.type in CLOSING_BRACKETS:
            self.depth -= 1
            opening_bracket = self.bracket_match.pop((self.depth, leaf.type))
            leaf.opening_bracket = opening_bracket
        leaf.bracket_depth = self.depth
        if self.depth == 0:
            delim = is_split_before_delimiter(leaf, self.previous)
            if delim and self.previous is not None:
                self.delimiters[id(self.previous)] = delim
            else:
                delim = is_split_after_delimiter(leaf, self.previous)
                if delim:
                    self.delimiters[id(leaf)] = delim
        if leaf.type in OPENING_BRACKETS:
            self.bracket_match[self.depth, BRACKET[leaf.type]] = leaf
            self.depth += 1
        self.previous = leaf
        self.maybe_increment_lambda_arguments(leaf)
        self.maybe_increment_for_loop_variable(leaf)
    def any_open_brackets(self) -> bool:
        """"""Return True if there is an yet unmatched open bracket on the line.""""""
        return bool(self.bracket_match)
    def max_delimiter_priority(self, exclude: Iterable[LeafID] = ()) -> int:
        """"""Return the highest priority of a delimiter found on the line.
        Values are consistent with what `is_split_*_delimiter()` return.
        Raises ValueError on no delimiters.
        """"""
        return max(v for k, v in self.delimiters.items() if k not in exclude)
    def delimiter_count_with_priority(self, priority: int = 0) -> int:
        """"""Return the number of delimiters with the given `priority`.
        If no `priority` is passed, defaults to max priority on the line.
        """"""
        if not self.delimiters:
            return 0
        priority = priority or self.max_delimiter_priority()
        return sum(1 for p in self.delimiters.values() if p == priority)
    def maybe_increment_for_loop_variable(self, leaf: Leaf) -> bool:
        """"""In a for loop, or comprehension, the variables are often unpacks.
        To avoid splitting on the comma in this situation, increase the depth of
        tokens between `for` and `in`.
        """"""
        if leaf.type == token.NAME and leaf.value == ""for"":
            self.depth += 1
            self._for_loop_variable += 1
            return True
        return False
    def maybe_decrement_after_for_loop_variable(self, leaf: Leaf) -> bool:
        """"""See `maybe_increment_for_loop_variable` above for explanation.""""""","[46, 47, 120, 127]"
"""""""
This module provides some useful functions for working with
scrapy.http.Request objects
""""""
from __future__ import print_function
import hashlib
import weakref
from six.moves.urllib.parse import urlunparse
from twisted.internet.defer import Deferred
from w3lib.http import basic_auth_header
from scrapy.utils.python import to_bytes, to_native_str
from scrapy.utils.url import canonicalize_url
from scrapy.utils.httpobj import urlparse_cached

_fingerprint_cache = weakref.WeakKeyDictionary()
def request_fingerprint(request, include_headers=None):
    """"""
    Return the request fingerprint.
    The request fingerprint is a hash that uniquely identifies the resource the
    request points to. For example, take the following two urls:
    http://www.example.com/query?id=111&cat=222
    http://www.example.com/query?cat=222&id=111
    Even though those are two different URLs both point to the same resource
    and are equivalent (ie. they should return the same response).
    Another example are cookies used to store session ids. Suppose the
    following page is only accesible to authenticated users:
    http://www.example.com/members/offers.html
    Lot of sites use a cookie to store the session id, which adds a random
    component to the HTTP Request and thus should be ignored when calculating
    the fingerprint.
    For this reason, request headers are ignored by default when calculating
    the fingeprint. If you want to include specific headers use the
    include_headers argument, which is a list of Request headers to include.
    """"""
    if include_headers:
        include_headers = tuple([to_bytes(h.lower())
                                 for h in sorted(include_headers)])
    cache = _fingerprint_cache.setdefault(request, {})
    if include_headers not in cache:
        fp = hashlib.sha1()
        fp.update(to_bytes(request.method))
        fp.update(to_bytes(canonicalize_url(request.url)))
        fp.update(request.body or b'')
        if include_headers:
            for hdr in include_headers:
                if hdr in request.headers:
                    fp.update(hdr)
                    for v in request.headers.getlist(hdr):
                        fp.update(v)
        cache[include_headers] = fp.hexdigest()
    return cache[include_headers]

def request_authenticate(request, username, password):
    """"""Autenticate the given request (in place) using the HTTP basic access
    authentication mechanism (RFC 2617) and the given username and password
    """"""
    request.headers['Authorization'] = basic_auth_header(username, password)

def request_httprepr(request):
    """"""Return the raw HTTP representation (as bytes) of the given request.
    This is provided only for reference since it's not the actual stream of
    bytes that will be send when performing the request (that's controlled
    by Twisted).
    """"""
    parsed = urlparse_cached(request)
    path = urlunparse(('', '', parsed.path or '/', parsed.params, parsed.query, ''))
    s = to_bytes(request.method) + b"" "" + to_bytes(path) + b"" HTTP/1.1\r\n""
    s += b""Host: "" + to_bytes(parsed.hostname) + b""\r\n""
    if request.headers:
        s += request.headers.to_string() + b""\r\n""
    s += b""\r\n""
    s += request.body
    return s
",[81]
"        else:
            raise ValueError(""Not a valid argument for validate"")

def _get_join_indexers(
    left_keys, right_keys, sort: bool = False, how: str = ""inner"", **kwargs
):
    """"""
    Parameters
    ----------
    left_keys: ndarray, Index, Series
    right_keys: ndarray, Index, Series
    sort: bool, default False
    how: string {'inner', 'outer', 'left', 'right'}, default 'inner'
    Returns
    -------
    tuple of (left_indexer, right_indexer)
        indexers into the left_keys, right_keys
    """"""
    assert len(left_keys) == len(
        right_keys
    ), ""left_key and right_keys must be the same length""
    # get left & right join labels and num. of levels at each location
    mapped = (
        _factorize_keys(left_keys[n], right_keys[n], sort=sort)
        for n in range(len(left_keys))
    )
    zipped = zip(*mapped)
    llab, rlab, shape = [list(x) for x in zipped]
    # get flat i8 keys from label lists
    lkey, rkey = _get_join_keys(llab, rlab, shape, sort)
    # factorize keys to a dense i8 space
    # `count` is the num. of unique keys
    # set(lkey) | set(rkey) == range(count)
    lkey, rkey, count = _factorize_keys(lkey, rkey, sort=sort)
    # preserve left frame order if how == 'left' and sort == False
    kwargs = copy.copy(kwargs)
    if how == ""left"":
        kwargs[""sort""] = sort
    join_func = {
        ""inner"": libjoin.inner_join,
        ""left"": libjoin.left_outer_join,
        ""right"": _right_outer_join,
        ""outer"": libjoin.full_outer_join,
    }[how]
    return join_func(lkey, rkey, count, **kwargs)

def _restore_dropped_levels_multijoin(
    left: MultiIndex,
    right: MultiIndex,
    dropped_level_names,
    join_index,
    lindexer,
    rindexer,
):
    """"""
    *this is an internal non-public method*
    Returns the levels, labels and names of a multi-index to multi-index join.
    Depending on the type of join, this method restores the appropriate
    dropped levels of the joined multi-index.
    The method relies on lidx, rindexer which hold the index positions of
    left and right, where a join was feasible
    Parameters
    ----------
    left : MultiIndex
        left index
    right : MultiIndex
        right index
    dropped_level_names : str array
        list of non-common level names
    join_index : MultiIndex
        the index of the join between the
        common levels of left and right
    lindexer : intp array
        left indexer
    rindexer : intp array
        right indexer
    Returns
    -------
    levels : list of Index
        levels of combined multiindexes
    labels : intp array
        labels of combined multiindexes
    names : str array
        names of combined multiindexes
    """"""
    def _convert_to_mulitindex(index) -> MultiIndex:
        if isinstance(index, MultiIndex):
            return index
        else:
            return MultiIndex.from_arrays([index._values], names=[index.name])
    # For multi-multi joins with one overlapping level,
    # the returned index if of type Index
    # Assure that join_index is of type MultiIndex
    # so that dropped levels can be appended
    join_index = _convert_to_mulitindex(join_index)
    join_levels = join_index.levels
    join_codes = join_index.codes
    join_names = join_index.names
    # lindexer and rindexer hold the indexes where the join occurred
    # for left and right respectively. If left/right is None then
    # the join occurred on all indices of left/right
    if lindexer is None:
        lindexer = range(left.size)
    if rindexer is None:
        rindexer = range(right.size)
    # Iterate through the levels that must be restored","[29, 41]"
"from datetime import datetime, timedelta
import functools
import inspect
import re
from typing import Any, List
import warnings
import numpy as np
from pandas._libs import NaT, algos as libalgos, lib, tslib, writers
from pandas._libs.index import convert_scalar
import pandas._libs.internals as libinternals
from pandas._libs.tslibs import Timedelta, conversion
from pandas._libs.tslibs.timezones import tz_compare
from pandas.util._validators import validate_bool_kwarg
from pandas.core.dtypes.cast import (
    astype_nansafe,
    find_common_type,
    infer_dtype_from,
    infer_dtype_from_scalar,
    maybe_downcast_numeric,
    maybe_downcast_to_dtype,
    maybe_infer_dtype_type,
    maybe_promote,
    maybe_upcast,
    soft_convert_objects,
)
from pandas.core.dtypes.common import (
    _NS_DTYPE,
    _TD_DTYPE,
    ensure_platform_int,
    is_bool_dtype,
    is_categorical,
    is_categorical_dtype,
    is_datetime64_dtype,
    is_datetime64tz_dtype,
    is_dtype_equal,
    is_extension_array_dtype,
    is_float_dtype,
    is_integer,
    is_integer_dtype,
    is_interval_dtype,
    is_list_like,
    is_object_dtype,
    is_period_dtype,
    is_re,
    is_re_compilable,
    is_sparse,
    is_timedelta64_dtype,
    pandas_dtype,
)
from pandas.core.dtypes.concat import concat_categorical, concat_datetime
from pandas.core.dtypes.dtypes import CategoricalDtype, ExtensionDtype
from pandas.core.dtypes.generic import (
    ABCDataFrame,
    ABCExtensionArray,
    ABCPandasArray,
    ABCSeries,
)
from pandas.core.dtypes.missing import (
    _isna_compat,
    array_equivalent,
    is_valid_nat_for_dtype,
    isna,
)
import pandas.core.algorithms as algos
from pandas.core.arrays import (
    Categorical,
    DatetimeArray,
    ExtensionArray,
    PandasArray,
    PandasDtype,
    TimedeltaArray,
)
from pandas.core.base import PandasObject
import pandas.core.common as com
from pandas.core.construction import extract_array
from pandas.core.indexers import (
    check_setitem_lengths,
    is_empty_indexer,
    is_scalar_indexer,
)
import pandas.core.missing as missing
from pandas.core.nanops import nanpercentile
from pandas.io.formats.printing import pprint_thing

class Block(PandasObject):
    """"""
    Canonical n-dimensional unit of homogeneous dtype contained in a pandas
    data structure
    Index-ignorant; let the container take care of that
    """"""
    __slots__ = [""_mgr_locs"", ""values"", ""ndim""]
    is_numeric = False
    is_float = False
    is_integer = False
    is_complex = False
    is_datetime = False
    is_datetimetz = False
    is_timedelta = False
    is_bool = False
    is_object = False
    is_categorical = False
    is_extension = False
    _can_hold_na = False
    _can_consolidate = True
    _verify_integrity = True
    _validate_ndim = True
    _ftype = ""dense""
    _concatenator = staticmethod(np.concatenate)
    def __init__(self, values, placement, ndim=None):
        self.ndim = self._check_ndim(values, ndim)
        self.mgr_locs = placement
        self.values = values
        if self._validate_ndim and self.ndim and len(self.mgr_locs) != len(self.values):
            raise ValueError(
                f""Wrong number of items passed {len(self.values)}, ""
                f""placement implies {len(self.mgr_locs)}""
            )",[9]
"            return m.group('none_inclusive')
        return op(actual_value, comparison_value)
    UNARY_OPERATORS = {
        '': lambda v: v is not None,
        '!': lambda v: v is None,
    }
    operator_rex = re.compile(r'''(?x)\s*
        (?P<op>%s)\s*(?P<key>[a-z_]+)
        \s*$
        ''' % '|'.join(map(re.escape, UNARY_OPERATORS.keys())))
    m = operator_rex.search(filter_part)
    if m:
        op = UNARY_OPERATORS[m.group('op')]
        actual_value = dct.get(m.group('key'))
        return op(actual_value)
    raise ValueError('Invalid filter part %r' % filter_part)

def match_str(filter_str, dct):
    """""" Filter a dictionary with a simple string syntax. Returns True (=passes filter) or false """"""
    return all(
        _match_one(filter_part, dct) for filter_part in filter_str.split('&'))

def match_filter_func(filter_str):
    def _match_func(info_dict):
        if match_str(filter_str, info_dict):
            return None
        else:
            video_title = info_dict.get('title', info_dict.get('id', 'video'))
            return '%s does not pass filter %s, skipping ..' % (video_title, filter_str)
    return _match_func

def parse_dfxp_time_expr(time_expr):
    if not time_expr:
        return
    mobj = re.match(r'^(?P<time_offset>\d+(?:\.\d+)?)s?$', time_expr)
    if mobj:
        return float(mobj.group('time_offset'))
    mobj = re.match(r'^(\d+):(\d\d):(\d\d(?:(?:\.|:)\d+)?)$', time_expr)
    if mobj:
        return 3600 * int(mobj.group(1)) + 60 * int(mobj.group(2)) + float(mobj.group(3).replace(':', '.'))

def srt_subtitles_timecode(seconds):
    return '%02d:%02d:%02d,%03d' % (seconds / 3600, (seconds % 3600) / 60, seconds % 60, (seconds % 1) * 1000)

def dfxp2srt(dfxp_data):
    LEGACY_NAMESPACES = (
        ('http://www.w3.org/ns/ttml', [
            'http://www.w3.org/2004/11/ttaf1',
            'http://www.w3.org/2006/04/ttaf1',
            'http://www.w3.org/2006/10/ttaf1',
        ]),
        ('http://www.w3.org/ns/ttml#styling', [
            'http://www.w3.org/ns/ttml#style',
        ]),
    )
    SUPPORTED_STYLING = [
        'color',
        'fontFamily',
        'fontSize',
        'fontStyle',
        'fontWeight',
        'textDecoration'
    ]
    _x = functools.partial(xpath_with_ns, ns_map={
        'ttml': 'http://www.w3.org/ns/ttml',
        'tts': 'http://www.w3.org/ns/ttml#styling',
    })
    styles = {}
    default_style = {}
    class TTMLPElementParser(object):
        _out = ''
        _unclosed_elements = []
        _applied_styles = []
        def start(self, tag, attrib):
            if tag in (_x('ttml:br'), 'br'):
                self._out += '\n'
            else:
                unclosed_elements = []
                style = {}
                element_style_id = attrib.get('style')
                if default_style:
                    style.update(default_style)
                if element_style_id:
                    style.update(styles.get(element_style_id, {}))
                for prop in SUPPORTED_STYLING:
                    prop_val = attrib.get(_x('tts:' + prop))
                    if prop_val:
                        style[prop] = prop_val
                if style:
                    font = ''
                    for k, v in sorted(style.items()):
                        if self._applied_styles and self._applied_styles[-1].get(k) == v:
                            continue
                        if k == 'color':
                            font += ' color=""%s""' % v
                        elif k == 'fontSize':
                            font += ' size=""%s""' % v
                        elif k == 'fontFamily':
                            font += ' face=""%s""' % v
                        elif k == 'fontWeight' and v == 'bold':
                            self._out += '<b>'
                            unclosed_elements.append('b')
                        elif k == 'fontStyle' and v == 'italic':
                            self._out += '<i>'
                            unclosed_elements.append('i')
                        elif k == 'textDecoration' and v == 'underline':
                            self._out += '<u>'
                            unclosed_elements.append('u')
                    if font:
                        self._out += '<font' + font + '>'
                        unclosed_elements.append('font')
                    applied_style = {}","[56, 57, 58, 59, 61, 62]"
"import http.client
from typing import Any, Dict, List, Optional, Sequence, Tuple, Type, cast
from fastapi import routing
from fastapi.dependencies.models import Dependant
from fastapi.dependencies.utils import get_flat_dependant
from fastapi.encoders import jsonable_encoder
from fastapi.openapi.constants import (
    METHODS_WITH_BODY,
    REF_PREFIX,
    STATUS_CODES_WITH_NO_BODY,
)
from fastapi.openapi.models import OpenAPI
from fastapi.params import Body, Param
from fastapi.utils import (
    generate_operation_id_for_path,
    get_field_info,
    get_flat_models_from_routes,
    get_model_definitions,
)
from pydantic import BaseModel
from pydantic.schema import field_schema, get_model_name_map
from pydantic.utils import lenient_issubclass
from starlette.responses import JSONResponse
from starlette.routing import BaseRoute
from starlette.status import HTTP_422_UNPROCESSABLE_ENTITY
try:
    from pydantic.fields import ModelField
except ImportError:  # pragma: nocover
    # TODO: remove when removing support for Pydantic < 1.0.0
    from pydantic.fields import Field as ModelField  # type: ignore
validation_error_definition = {
    ""title"": ""ValidationError"",
    ""type"": ""object"",
    ""properties"": {
        ""loc"": {""title"": ""Location"", ""type"": ""array"", ""items"": {""type"": ""string""}},
        ""msg"": {""title"": ""Message"", ""type"": ""string""},
        ""type"": {""title"": ""Error Type"", ""type"": ""string""},
    },
    ""required"": [""loc"", ""msg"", ""type""],
}
validation_error_response_definition = {
    ""title"": ""HTTPValidationError"",
    ""type"": ""object"",
    ""properties"": {
        ""detail"": {
            ""title"": ""Detail"",
            ""type"": ""array"",
            ""items"": {""$ref"": REF_PREFIX + ""ValidationError""},
        }
    },
}
status_code_ranges: Dict[str, str] = {
    ""1XX"": ""Information"",
    ""2XX"": ""Success"",
    ""3XX"": ""Redirection"",
    ""4XX"": ""Client Error"",
    ""5XX"": ""Server Error"",
    ""DEFAULT"": ""Default Response"",
}

def get_openapi_params(dependant: Dependant) -> List[ModelField]:
    flat_dependant = get_flat_dependant(dependant, skip_repeats=True)
    return (
        flat_dependant.path_params
        + flat_dependant.query_params
        + flat_dependant.header_params
        + flat_dependant.cookie_params
    )

def get_openapi_security_definitions(flat_dependant: Dependant) -> Tuple[Dict, List]:
    security_definitions = {}
    operation_security = []
    for security_requirement in flat_dependant.security_requirements:
        security_definition = jsonable_encoder(
            security_requirement.security_scheme.model,
            by_alias=True,
            include_none=False,
        )
        security_name = security_requirement.security_scheme.scheme_name
        security_definitions[security_name] = security_definition
        operation_security.append({security_name: security_requirement.scopes})
    return security_definitions, operation_security

def get_openapi_operation_parameters(
    all_route_params: Sequence[ModelField],
) -> List[Dict[str, Any]]:
    parameters = []
    for param in all_route_params:
        field_info = get_field_info(param)
        field_info = cast(Param, field_info)
        parameter = {
            ""name"": param.alias,
            ""in"": field_info.in_.value,
            ""required"": param.required,
            ""schema"": field_schema(param, model_name_map={})[0],
        }
        if field_info.description:
            parameter[""description""] = field_info.description
        if field_info.deprecated:
            parameter[""deprecated""] = field_info.deprecated
        parameters.append(parameter)
    return parameters

def get_openapi_operation_request_body(
    *, body_field: Optional[ModelField], model_name_map: Dict[Type[BaseModel], str]
) -> Optional[Dict]:
    if not body_field:
        return None
    assert isinstance(body_field, ModelField)
    body_schema, _, _ = field_schema(
        body_field, model_name_map=model_name_map, ref_prefix=REF_PREFIX
    )
    field_info = cast(Body, get_field_info(body_field))
    request_media_type = field_info.media_type
    required = body_field.required
    request_body_oai: Dict[str, Any] = {}
    if required:
        request_body_oai[""required""] = required",[83]
"                    'service_level_agreement': {},
                    'addons': {'type': 'list'},
                    'sync': {'type': 'bool', 'default': False}
                }
            }
        },
        required_together=[['username', 'password'],
                           ['server_proxy_hostname', 'server_proxy_port'],
                           ['server_proxy_user', 'server_proxy_password']],
        mutually_exclusive=[['activationkey', 'username'],
                            ['activationkey', 'consumer_id'],
                            ['activationkey', 'environment'],
                            ['activationkey', 'autosubscribe'],
                            ['force', 'consumer_id'],
                            ['pool', 'pool_ids']],
        required_if=[['state', 'present', ['username', 'activationkey'], True]],
    )
    rhsm.module = module
    state = module.params['state']
    username = module.params['username']
    password = module.params['password']
    server_hostname = module.params['server_hostname']
    server_insecure = module.params['server_insecure']
    rhsm_baseurl = module.params['rhsm_baseurl']
    rhsm_repo_ca_cert = module.params['rhsm_repo_ca_cert']
    auto_attach = module.params['auto_attach']
    activationkey = module.params['activationkey']
    org_id = module.params['org_id']
    if activationkey and not org_id:
        module.fail_json(msg='org_id is required when using activationkey')
    environment = module.params['environment']
    pool = module.params['pool']
    pool_ids = {}
    for value in module.params['pool_ids']:
        if isinstance(value, dict):
            if len(value) != 1:
                module.fail_json(msg='Unable to parse pool_ids option.')
            pool_id, quantity = list(value.items())[0]
        else:
            pool_id, quantity = value, 1
        pool_ids[pool_id] = str(quantity)
    consumer_type = module.params[""consumer_type""]
    consumer_name = module.params[""consumer_name""]
    consumer_id = module.params[""consumer_id""]
    force_register = module.params[""force_register""]
    server_proxy_hostname = module.params['server_proxy_hostname']
    server_proxy_port = module.params['server_proxy_port']
    server_proxy_user = module.params['server_proxy_user']
    server_proxy_password = module.params['server_proxy_password']
    release = module.params['release']
    syspurpose = module.params['syspurpose']
    global SUBMAN_CMD
    SUBMAN_CMD = module.get_bin_path('subscription-manager', True)
    syspurpose_changed = False
    if syspurpose is not None:
        try:
            syspurpose_changed = SysPurpose().update_syspurpose(syspurpose)
        except Exception as err:
            module.fail_json(msg=""Failed to update syspurpose attributes: %s"" % to_native(err))
    # Ensure system is registered
    if state == 'present':
        # Register system
        if rhsm.is_registered and not force_register:
            if syspurpose and 'sync' in syspurpose and syspurpose['sync'] is True:
                try:
                    rhsm.sync_syspurpose()
                except Exception as e:
                    module.fail_json(msg=""Failed to synchronize syspurpose attributes: %s"" % to_native(e))
            if pool != '^$' or pool_ids:
                try:
                    if pool_ids:
                        result = rhsm.update_subscriptions_by_pool_ids(pool_ids)
                    else:
                        result = rhsm.update_subscriptions(pool)
                except Exception as e:
                    module.fail_json(msg=""Failed to update subscriptions for '%s': %s"" % (server_hostname, to_native(e)))
                else:
                    module.exit_json(**result)
            else:
                if syspurpose_changed is True:
                    module.exit_json(changed=True, msg=""Syspurpose attributes changed."")
                else:
                    module.exit_json(changed=False, msg=""System already registered."")
        else:
            try:
                rhsm.enable()
                rhsm.configure(**module.params)
                rhsm.register(username, password, auto_attach, activationkey, org_id,
                              consumer_type, consumer_name, consumer_id, force_register,
                              environment, rhsm_baseurl, server_insecure, server_hostname,
                              server_proxy_hostname, server_proxy_port, server_proxy_user, server_proxy_password, release)
                if syspurpose and 'sync' in syspurpose and syspurpose['sync'] is True:
                    rhsm.sync_syspurpose()
                if pool_ids:
                    subscribed_pool_ids = rhsm.subscribe_by_pool_ids(pool_ids)
                elif pool != '^$':
                    subscribed_pool_ids = rhsm.subscribe(pool)
                else:
                    subscribed_pool_ids = []
            except Exception as e:
                module.fail_json(msg=""Failed to register with '%s': %s"" % (server_hostname, to_native(e)))
            else:
                module.exit_json(changed=True,
                                 msg=""System successfully registered to '%s'."" % server_hostname,
                                 subscribed_pool_ids=subscribed_pool_ids)
    # Ensure system is *not* registered
    if state == 'absent':
        if not rhsm.is_registered:
            module.exit_json(changed=False, msg=""System already unregistered."")
        else:
            try:
                rhsm.unsubscribe()
                rhsm.unregister()
            except Exception as e:
                module.fail_json(msg=""Failed to unregister: %s"" % to_native(e))
            else:
                module.exit_json(changed=True, msg=""System successfully unregistered from %s."" % server_hostname)

if __name__ == '__main__':","[40, 41]"
"    if limit_direction == ""forward"":
        preserve_nans = start_nans | set(_interp_limit(invalid, limit, 0))
    elif limit_direction == ""backward"":
        preserve_nans = end_nans | set(_interp_limit(invalid, 0, limit))
    else:
        # both directions... just use _interp_limit
        preserve_nans = set(_interp_limit(invalid, limit, limit))
    # if limit_area is set, add either mid or outside indices
    # to preserve_nans GH #16284
    if limit_area == ""inside"":
        # preserve NaNs on the outside
        preserve_nans |= start_nans | end_nans
    elif limit_area == ""outside"":
        # preserve NaNs on the inside
        preserve_nans |= mid_nans
    # sort preserve_nans and covert to list
    preserve_nans = sorted(preserve_nans)
    xvalues = getattr(xvalues, ""values"", xvalues)
    yvalues = getattr(yvalues, ""values"", yvalues)
    result = yvalues.copy()
    if method in [""linear"", ""time"", ""index"", ""values""]:
        if method in (""values"", ""index""):
            inds = np.asarray(xvalues)
            # hack for DatetimeIndex, #1646
            if needs_i8_conversion(inds.dtype.type):
                inds = inds.view(np.int64)
            if inds.dtype == np.object_:
                inds = lib.maybe_convert_objects(inds)
        else:
            inds = xvalues
        result[invalid] = np.interp(inds[invalid], inds[valid], yvalues[valid])
        result[preserve_nans] = np.nan
        return result
    sp_methods = [
        ""nearest"",
        ""zero"",
        ""slinear"",
        ""quadratic"",
        ""cubic"",
        ""barycentric"",
        ""krogh"",
        ""spline"",
        ""polynomial"",
        ""from_derivatives"",
        ""piecewise_polynomial"",
        ""pchip"",
        ""akima"",
    ]
    if method in sp_methods:
        inds = np.asarray(xvalues)
        # hack for DatetimeIndex, #1646
        if issubclass(inds.dtype.type, np.datetime64):
            inds = inds.view(np.int64)
        result[invalid] = _interpolate_scipy_wrapper(
            inds[valid],
            yvalues[valid],
            inds[invalid],
            method=method,
            fill_value=fill_value,
            bounds_error=bounds_error,
            order=order,
            **kwargs,
        )
        result[preserve_nans] = np.nan
        return result

def _interpolate_scipy_wrapper(
    x, y, new_x, method, fill_value=None, bounds_error=False, order=None, **kwargs
):
    """"""
    Passed off to scipy.interpolate.interp1d. method is scipy's kind.
    Returns an array interpolated at new_x.  Add any new methods to
    the list in _clean_interp_method.
    """"""
    extra = ""{method} interpolation requires SciPy."".format(method=method)
    import_optional_dependency(""scipy"", extra=extra)
    from scipy import interpolate
    new_x = np.asarray(new_x)
    # ignores some kwargs that could be passed along.
    alt_methods = {
        ""barycentric"": interpolate.barycentric_interpolate,
        ""krogh"": interpolate.krogh_interpolate,
        ""from_derivatives"": _from_derivatives,
        ""piecewise_polynomial"": _from_derivatives,
    }
    if getattr(x, ""is_all_dates"", False):
        # GH 5975, scipy.interp1d can't handle datetime64s
        x, new_x = x._values.astype(""i8""), new_x.astype(""i8"")
    if method == ""pchip"":
        try:
            alt_methods[""pchip""] = interpolate.pchip_interpolate
        except AttributeError:
            raise ImportError(
                ""Your version of Scipy does not support PCHIP interpolation.""
            )
    elif method == ""akima"":
        alt_methods[""akima""] = _akima_interpolate
    interp1d_methods = [
        ""nearest"",
        ""zero"",
        ""slinear"",
        ""quadratic"",
        ""cubic"",
        ""polynomial"",
    ]
    if method in interp1d_methods:
        if method == ""polynomial"":
            method = order
        terp = interpolate.interp1d(
            x, y, kind=method, fill_value=fill_value, bounds_error=bounds_error
        )
        new_y = terp(new_x)
    elif method == ""spline"":
        # GH #10633, #24014
        if isna(order) or (order <= 0):",[34]
"    dtype: float64
    The below example shows a similar rolling calculation on a
    DataFrame using the pairwise option.
    >>> matrix = np.array([[51., 35.], [49., 30.], [47., 32.],\
    [46., 31.], [50., 36.]])
    >>> print(np.corrcoef(matrix[:-1,0], matrix[:-1,1]).round(7))
    [[1.         0.6263001]
     [0.6263001  1.       ]]
    >>> print(np.corrcoef(matrix[1:,0], matrix[1:,1]).round(7))
    [[1.         0.5553681]
     [0.5553681  1.        ]]
    >>> df = pd.DataFrame(matrix, columns=['X','Y'])
    >>> df
          X     Y
    0  51.0  35.0
    1  49.0  30.0
    2  47.0  32.0
    3  46.0  31.0
    4  50.0  36.0
    >>> df.rolling(4).corr(pairwise=True)
                X         Y
    0 X       NaN       NaN
      Y       NaN       NaN
    1 X       NaN       NaN
      Y       NaN       NaN
    2 X       NaN       NaN
      Y       NaN       NaN
    3 X  1.000000  0.626300
      Y  0.626300  1.000000
    4 X  1.000000  0.555368
      Y  0.555368  1.000000
    """"""
    )
    def corr(self, other=None, pairwise=None, **kwargs):
        if other is None:
            other = self._selected_obj
            # only default unset
            pairwise = True if pairwise is None else pairwise
        other = self._shallow_copy(other)
        window = self._get_window(other)
        def _get_corr(a, b):
            a = a.rolling(
                window=window, min_periods=self.min_periods, center=self.center
            )
            b = b.rolling(
                window=window, min_periods=self.min_periods, center=self.center
            )
            return a.cov(b, **kwargs) / (a.std(**kwargs) * b.std(**kwargs))
        return _flex_binary_moment(
            self._selected_obj, other._selected_obj, _get_corr, pairwise=bool(pairwise)
        )

class Rolling(_Rolling_and_Expanding):
    @cache_readonly
    def is_datetimelike(self) -> bool:
        return isinstance(
            self._on, (ABCDatetimeIndex, ABCTimedeltaIndex, ABCPeriodIndex)
        )
    @cache_readonly
    def _on(self) -> Index:
        if self.on is None:
            if self.axis == 0:
                return self.obj.index
            else:
                # i.e. self.axis == 1
                return self.obj.columns
        elif isinstance(self.on, Index):
            return self.on
        elif isinstance(self.obj, ABCDataFrame) and self.on in self.obj.columns:
            return Index(self.obj[self.on])
        else:
            raise ValueError(
                f""invalid on specified as {self.on}, ""
                ""must be a column (of DataFrame), an Index or None""
            )
    def validate(self):
        super().validate()
        # we allow rolling on a datetimelike index
        if (self.obj.empty or self.is_datetimelike) and isinstance(
            self.window, (str, ABCDateOffset, timedelta)
        ):
            self._validate_monotonic()
            freq = self._validate_freq()
            # we don't allow center
            if self.center:
                raise NotImplementedError(
                    ""center is not implemented for ""
                    ""datetimelike and offset based windows""
                )
            # this will raise ValueError on non-fixed freqs
            self.win_freq = self.window
            self.window = freq.nanos
            self.win_type = ""freq""
            # min_periods must be an integer
            if self.min_periods is None:
                self.min_periods = 1
        elif isinstance(self.window, BaseIndexer):
            # Passed BaseIndexer subclass should handle all other rolling kwargs
            return
        elif not is_integer(self.window):
            raise ValueError(""window must be an integer"")
        elif self.window < 0:
            raise ValueError(""window must be non-negative"")
        if not self.is_datetimelike and self.closed is not None:
            raise ValueError(
                ""closed only implemented for datetimelike and offset based windows""
            )
    def _validate_monotonic(self):
        """"""
        Validate monotonic (increasing or decreasing).",[42]
"        if (self.mode == 'min' or
           (self.mode == 'auto' and 'acc' not in self.monitor)):
            self.monitor_op = lambda a, b: np.less(a, b - self.epsilon)
            self.best = np.Inf
        else:
            self.monitor_op = lambda a, b: np.greater(a, b + self.epsilon)
            self.best = -np.Inf
        self.cooldown_counter = 0
        self.wait = 0
    def on_train_begin(self, logs=None):
        self._reset()
    def on_epoch_end(self, epoch, logs=None):
        logs = logs or {}
        logs['lr'] = K.get_value(self.model.optimizer.lr)
        current = logs.get(self.monitor)
        if current is None:
            warnings.warn(
                'Reduce LR on plateau conditioned on metric `%s` '
                'which is not available. Available metrics are: %s' %
                (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning
            )
        else:
            if self.in_cooldown():
                self.cooldown_counter -= 1
                self.wait = 0
            if self.monitor_op(current, self.best):
                self.best = current
                self.wait = 0
            elif not self.in_cooldown():
                if self.wait >= self.patience:
                    old_lr = float(K.get_value(self.model.optimizer.lr))
                    if old_lr > self.min_lr:
                        new_lr = old_lr * self.factor
                        new_lr = max(new_lr, self.min_lr)
                        K.set_value(self.model.optimizer.lr, new_lr)
                        if self.verbose > 0:
                            print('\nEpoch %05d: ReduceLROnPlateau reducing learning '
                                  'rate to %s.' % (epoch + 1, new_lr))
                        self.cooldown_counter = self.cooldown
                        self.wait = 0
                self.wait += 1
    def in_cooldown(self):
        return self.cooldown_counter > 0

class CSVLogger(Callback):
    """"""Callback that streams epoch results to a csv file.
    Supports all values that can be represented as a string,
    including 1D iterables such as np.ndarray.
    # Example
    ```python
    csv_logger = CSVLogger('training.log')
    model.fit(X_train, Y_train, callbacks=[csv_logger])
    ```
    # Arguments
        filename: filename of the csv file, e.g. 'run/log.csv'.
        separator: string used to separate elements in the csv file.
        append: True: append if file exists (useful for continuing
            training). False: overwrite existing file,
    """"""
    def __init__(self, filename, separator=',', append=False):
        self.sep = separator
        self.filename = filename
        self.append = append
        self.writer = None
        self.keys = None
        self.append_header = True
        self.file_flags = 'b' if six.PY2 and os.name == 'nt' else ''
        super(CSVLogger, self).__init__()
    def on_train_begin(self, logs=None):
        if self.append:
            if os.path.exists(self.filename):
                with open(self.filename, 'r' + self.file_flags) as f:
                    self.append_header = not bool(len(f.readline()))
            self.csv_file = open(self.filename, 'a' + self.file_flags)
        else:
            self.csv_file = open(self.filename, 'w' + self.file_flags)
    def on_epoch_end(self, epoch, logs=None):
        logs = logs or {}
        def handle_value(k):
            is_zero_dim_ndarray = isinstance(k, np.ndarray) and k.ndim == 0
            if isinstance(k, six.string_types):
                return k
            elif isinstance(k, Iterable) and not is_zero_dim_ndarray:
                return '""[%s]""' % (', '.join(map(str, k)))
            else:
                return k
        if self.keys is None:
            self.keys = sorted(logs.keys())
        if self.model.stop_training:
            # We set NA so that csv parsers do not fail for this last epoch.
            logs = dict([(k, logs[k]) if k in logs else (k, 'NA') for k in self.keys])
        if not self.writer:
            class CustomDialect(csv.excel):
                delimiter = self.sep
            self.writer = csv.DictWriter(self.csv_file,
                                         fieldnames=['epoch'] + self.keys, dialect=CustomDialect)
            if self.append_header:
                self.writer.writeheader()
        row_dict = OrderedDict({'epoch': epoch})
        row_dict.update((key, handle_value(logs[key])) for key in self.keys)
        self.writer.writerow(row_dict)
        self.csv_file.flush()
    def on_train_end(self, logs=None):
        self.csv_file.close()
        self.writer = None
","[2, 5, 44]"
"        Examples
        --------
        >>> dtypes = ['int64', 'float64', 'complex128', 'object', 'bool']
        >>> data = dict([(t, np.ones(shape=5000).astype(t))
        ...              for t in dtypes])
        >>> df = pd.DataFrame(data)
        >>> df.head()
           int64  float64            complex128  object  bool
        0      1      1.0    1.000000+0.000000j       1  True
        1      1      1.0    1.000000+0.000000j       1  True
        2      1      1.0    1.000000+0.000000j       1  True
        3      1      1.0    1.000000+0.000000j       1  True
        4      1      1.0    1.000000+0.000000j       1  True
        >>> df.memory_usage()
        Index           128
        int64         40000
        float64       40000
        complex128    80000
        object        40000
        bool           5000
        dtype: int64
        >>> df.memory_usage(index=False)
        int64         40000
        float64       40000
        complex128    80000
        object        40000
        bool           5000
        dtype: int64
        The memory footprint of `object` dtype columns is ignored by default:
        >>> df.memory_usage(deep=True)
        Index            128
        int64          40000
        float64        40000
        complex128     80000
        object        160000
        bool            5000
        dtype: int64
        Use a Categorical for efficient storage of an object-dtype column with
        many repeated values.
        >>> df['object'].astype('category').memory_usage(deep=True)
        5216
        """"""
        result = Series(
            [c.memory_usage(index=False, deep=deep) for col, c in self.items()],
            index=self.columns,
        )
        if index:
            result = Series(self.index.memory_usage(deep=deep), index=[""Index""]).append(
                result
            )
        return result
    def transpose(self, *args, **kwargs):
        """"""
        Transpose index and columns.
        Reflect the DataFrame over its main diagonal by writing rows as columns
        and vice-versa. The property :attr:`.T` is an accessor to the method
        :meth:`transpose`.
        Parameters
        ----------
        *args, **kwargs
            Additional arguments and keywords have no effect but might be
            accepted for compatibility with numpy.
        Returns
        -------
        DataFrame
            The transposed DataFrame.
        See Also
        --------
        numpy.transpose : Permute the dimensions of a given array.
        Notes
        -----
        Transposing a DataFrame with mixed dtypes will result in a homogeneous
        DataFrame with the `object` dtype. In such a case, a copy of the data
        is always made.
        Examples
        --------
        **Square DataFrame with homogeneous dtype**
        >>> d1 = {'col1': [1, 2], 'col2': [3, 4]}
        >>> df1 = pd.DataFrame(data=d1)
        >>> df1
           col1  col2
        0     1     3
        1     2     4
        >>> df1_transposed = df1.T # or df1.transpose()
        >>> df1_transposed
              0  1
        col1  1  2
        col2  3  4
        When the dtype is homogeneous in the original DataFrame, we get a
        transposed DataFrame with the same dtype:
        >>> df1.dtypes
        col1    int64
        col2    int64
        dtype: object
        >>> df1_transposed.dtypes
        0    int64
        1    int64
        dtype: object
        **Non-square DataFrame with mixed dtypes**
        >>> d2 = {'name': ['Alice', 'Bob'],
        ...       'score': [9.5, 8],
        ...       'employed': [False, True],
        ...       'kids': [0, 0]}
        >>> df2 = pd.DataFrame(data=d2)
        >>> df2
            name  score  employed  kids
        0  Alice    9.5     False     0
        1    Bob    8.0      True     0","[58, 69, 70]"
"""""""
This is a middleware to respect robots.txt policies. To activate it you must
enable this middleware and enable the ROBOTSTXT_OBEY setting.
""""""
import logging
from six.moves.urllib import robotparser
from scrapy.exceptions import NotConfigured, IgnoreRequest
from scrapy.http import Request
from scrapy.utils.httpobj import urlparse_cached
logger = logging.getLogger(__name__)

class RobotsTxtMiddleware(object):
    DOWNLOAD_PRIORITY = 1000
    def __init__(self, crawler):
        if not crawler.settings.getbool('ROBOTSTXT_OBEY'):
            raise NotConfigured
        self.crawler = crawler
        self._useragent = crawler.settings.get('USER_AGENT')
        self._parsers = {}
    @classmethod
    def from_crawler(cls, crawler):
        return cls(crawler)
    def process_request(self, request, spider):
        if request.meta.get('dont_obey_robotstxt'):
            return
        rp = self.robot_parser(request, spider)
        if rp and not rp.can_fetch(self._useragent, request.url):
            logger.debug(""Forbidden by robots.txt: %(request)s"",
                         {'request': request}, extra={'spider': spider})
            raise IgnoreRequest
    def robot_parser(self, request, spider):
        url = urlparse_cached(request)
        netloc = url.netloc
        if netloc not in self._parsers:
            self._parsers[netloc] = None
            robotsurl = ""%s://%s/robots.txt"" % (url.scheme, url.netloc)
            robotsreq = Request(
                robotsurl,
                priority=self.DOWNLOAD_PRIORITY,
                meta={'dont_obey_robotstxt': True}
            )
            dfd = self.crawler.engine.download(robotsreq, spider)
            dfd.addCallback(self._parse_robots)
            dfd.addErrback(self._logerror, robotsreq, spider)
        return self._parsers[netloc]
    def _logerror(self, failure, request, spider):
        if failure.type is not IgnoreRequest:
            logger.error(""Error downloading %(request)s: %(f_exception)s"",
                         {'request': request, 'f_exception': failure.value},
                         extra={'spider': spider, 'failure': failure})
    def _parse_robots(self, response):
        rp = robotparser.RobotFileParser(response.url)
        rp.parse(response.body.splitlines())
        self._parsers[urlparse_cached(response).netloc] = rp",[61]
"from datetime import date, datetime, time, timedelta, tzinfo
import operator
from typing import Optional
import warnings
import numpy as np
from pandas._libs import NaT, Period, Timestamp, index as libindex, lib, tslib as libts
from pandas._libs.tslibs import fields, parsing, timezones
from pandas._typing import Label
from pandas.util._decorators import cache_readonly
from pandas.core.dtypes.common import _NS_DTYPE, is_float, is_integer, is_scalar
from pandas.core.dtypes.missing import is_valid_nat_for_dtype
from pandas.core.arrays.datetimes import DatetimeArray, tz_to_dtype
import pandas.core.common as com
from pandas.core.indexes.base import Index, InvalidIndexError, maybe_extract_name
from pandas.core.indexes.datetimelike import DatetimeTimedeltaMixin
from pandas.core.indexes.extension import inherit_names
import pandas.core.tools.datetimes as tools
from pandas.tseries.frequencies import Resolution, to_offset
from pandas.tseries.offsets import prefix_mapping

def _new_DatetimeIndex(cls, d):
    """"""
    This is called upon unpickling, rather than the default which doesn't
    have arguments and breaks __new__
    """"""
    if ""data"" in d and not isinstance(d[""data""], DatetimeIndex):
        # Avoid need to verify integrity by calling simple_new directly
        data = d.pop(""data"")
        if not isinstance(data, DatetimeArray):
            # For backward compat with older pickles, we may need to construct
            #  a DatetimeArray to adapt to the newer _simple_new signature
            tz = d.pop(""tz"")
            freq = d.pop(""freq"")
            dta = DatetimeArray._simple_new(data, dtype=tz_to_dtype(tz), freq=freq)
        else:
            dta = data
            for key in [""tz"", ""freq""]:
                # These are already stored in our DatetimeArray; if they are
                #  also in the pickle and don't match, we have a problem.
                if key in d:
                    assert d.pop(key) == getattr(dta, key)
        result = cls._simple_new(dta, **d)
    else:
        with warnings.catch_warnings():
            # TODO: If we knew what was going in to **d, we might be able to
            #  go through _simple_new instead
            warnings.simplefilter(""ignore"")
            result = cls.__new__(cls, **d)
    return result

@inherit_names(
    [""to_period"", ""to_perioddelta"", ""to_julian_date"", ""strftime""]
    + DatetimeArray._field_ops
    + DatetimeArray._datetimelike_methods,
    DatetimeArray,
    wrap=True,
)
@inherit_names([""_timezone"", ""is_normalized"", ""_resolution""], DatetimeArray, cache=True)
@inherit_names(
    [
        ""_bool_ops"",
        ""_object_ops"",
        ""_field_ops"",
        ""_datetimelike_ops"",
        ""_datetimelike_methods"",
        ""tz"",
        ""tzinfo"",
        ""dtype"",
        ""to_pydatetime"",
        ""_local_timestamps"",
        ""_has_same_tz"",
        ""_format_native_types"",
        ""date"",
        ""time"",
        ""timetz"",
    ]
    + DatetimeArray._bool_ops,
    DatetimeArray,
)
class DatetimeIndex(DatetimeTimedeltaMixin):
    """"""
    Immutable ndarray-like of datetime64 data.
    Represented internally as int64, and which can be boxed to Timestamp objects
    that are subclasses of datetime and carry metadata.
    Parameters
    ----------
    data : array-like (1-dimensional), optional
        Optional datetime-like data to construct index with.
    freq : str or pandas offset object, optional
        One of pandas date offset strings or corresponding objects. The string
        'infer' can be passed in order to set the frequency of the index as the
        inferred frequency upon creation.
    tz : pytz.timezone or dateutil.tz.tzfile or datetime.tzinfo or str
        Set the Timezone of the data.
    normalize : bool, default False
        Normalize start/end dates to midnight before generating date range.
    closed : {'left', 'right'}, optional
        Set whether to include `start` and `end` that are on the
        boundary. The default includes boundary points on either end.
    ambiguous : 'infer', bool-ndarray, 'NaT', default 'raise'
        When clocks moved backward due to DST, ambiguous times may arise.
        For example in Central European Time (UTC+01), when going from 03:00
        DST to 02:00 non-DST, 02:30:00 local time occurs both at 00:30:00 UTC
        and at 01:30:00 UTC. In such a situation, the `ambiguous` parameter
        dictates how ambiguous times should be handled.
        - 'infer' will attempt to infer fall dst-transition hours based on
          order
        - bool-ndarray where True signifies a DST time, False signifies a
          non-DST time (note that this flag is only applicable for ambiguous
          times)
        - 'NaT' will return NaT where there are ambiguous times
        - 'raise' will raise an AmbiguousTimeError if there are ambiguous times.
    dayfirst : bool, default False
        If True, parse dates in `data` with the day first order.
    yearfirst : bool, default False
        If True parse dates in `data` with the year first order.","[9, 12]"
"    @interfaces.legacy_generator_methods_support
    def predict_generator(self, generator, steps=None,
                          max_queue_size=10,
                          workers=1,
                          use_multiprocessing=False,
                          verbose=0):
        """"""Generates predictions for the input samples from a data generator.
        The generator should return the same kind of data as accepted by
        `predict_on_batch`.
        # Arguments
            generator: Generator yielding batches of input samples
                or an instance of Sequence (keras.utils.Sequence)
                object in order to avoid duplicate data
                when using multiprocessing.
            steps: Total number of steps (batches of samples)
                to yield from `generator` before stopping.
                Optional for `Sequence`: if unspecified, will use
                the `len(generator)` as a number of steps.
            max_queue_size: Maximum size for the generator queue.
            workers: Integer. Maximum number of processes to spin up
                when using process based threading.
                If unspecified, `workers` will default to 1. If 0, will
                execute the generator on the main thread.
            use_multiprocessing: If `True`, use process based threading.
                Note that because
                this implementation relies on multiprocessing,
                you should not pass
                non picklable arguments to the generator
                as they can't be passed
                easily to children processes.
            verbose: verbosity mode, 0 or 1.
        # Returns
            Numpy array(s) of predictions.
        # Raises
            ValueError: In case the generator yields
                data in an invalid format.
        """"""
        self._make_predict_function()
        steps_done = 0
        wait_time = 0.01
        all_outs = []
        is_sequence = isinstance(generator, Sequence)
        if not is_sequence and use_multiprocessing and workers > 1:
            warnings.warn(
                UserWarning('Using a generator with `use_multiprocessing=True`'
                            ' and multiple workers may duplicate your data.'
                            ' Please consider using the`keras.utils.Sequence'
                            ' class.'))
        if steps is None:
            if is_sequence:
                steps = len(generator)
            else:
                raise ValueError('`steps=None` is only valid for a generator'
                                 ' based on the `keras.utils.Sequence` class.'
                                 ' Please specify `steps` or use the'
                                 ' `keras.utils.Sequence` class.')
        enqueuer = None
        try:
            if workers > 0:
                if is_sequence:
                    enqueuer = OrderedEnqueuer(generator,
                                               use_multiprocessing=use_multiprocessing)
                else:
                    enqueuer = GeneratorEnqueuer(generator,
                                                 use_multiprocessing=use_multiprocessing,
                                                 wait_time=wait_time)
                enqueuer.start(workers=workers, max_queue_size=max_queue_size)
                output_generator = enqueuer.get()
            else:
                output_generator = generator
            if verbose == 1:
                progbar = Progbar(target=steps)
            while steps_done < steps:
                generator_output = next(output_generator)
                if isinstance(generator_output, tuple):
                    # Compatibility with the generators
                    # used for training.
                    if len(generator_output) == 2:
                        x, _ = generator_output
                    elif len(generator_output) == 3:
                        x, _, _ = generator_output
                    else:
                        raise ValueError('Output of generator should be '
                                         'a tuple `(x, y, sample_weight)` '
                                         'or `(x, y)`. Found: ' +
                                         str(generator_output))
                else:
                    # Assumes a generator that only
                    # yields inputs (not targets and sample weights).
                    x = generator_output
                outs = self.predict_on_batch(x)
                if not isinstance(outs, list):
                    outs = [outs]
                if not all_outs:
                    for out in outs:
                        all_outs.append([])
                for i, out in enumerate(outs):
                    all_outs[i].append(out)
                steps_done += 1
                if verbose == 1:
                    progbar.update(steps_done)
        finally:
            if enqueuer is not None:
                enqueuer.stop()
        if len(all_outs) == 1:
            if steps_done == 1:
                return all_outs[0][0]
            else:
                return np.concatenate(all_outs[0])
        if steps_done == 1:
            return [out[0] for out in all_outs]
        else:",[76]
"        def _filter(f):
            actual_value = f.get(m.group('key'))
            if actual_value is None:
                return m.group('none_inclusive')
            return op(actual_value, comparison_value)
        return _filter
    def build_format_selector(self, format_spec):
        def syntax_error(note, start):
            message = (
                'Invalid format specification: '
                '{0}\n\t{1}\n\t{2}^'.format(note, format_spec, ' ' * start[1]))
            return SyntaxError(message)
        PICKFIRST = 'PICKFIRST'
        MERGE = 'MERGE'
        SINGLE = 'SINGLE'
        GROUP = 'GROUP'
        FormatSelector = collections.namedtuple('FormatSelector', ['type', 'selector', 'filters'])
        def _parse_filter(tokens):
            filter_parts = []
            for type, string, start, _, _ in tokens:
                if type == tokenize.OP and string == ']':
                    return ''.join(filter_parts)
                else:
                    filter_parts.append(string)
        def _parse_format_selection(tokens, inside_merge=False, inside_choice=False, inside_group=False):
            selectors = []
            current_selector = None
            for type, string, start, _, _ in tokens:
                # ENCODING is only defined in python 3.x
                if type == getattr(tokenize, 'ENCODING', None):
                    continue
                elif type in [tokenize.NAME, tokenize.NUMBER]:
                    current_selector = FormatSelector(SINGLE, string, [])
                elif type == tokenize.OP:
                    if string == ')':
                        if not inside_group:
                            # ')' will be handled by the parentheses group
                            tokens.restore_last_token()
                        break
                    elif inside_merge and string in ['/', ',']:
                        tokens.restore_last_token()
                        break
                    elif inside_choice and string == ',':
                        tokens.restore_last_token()
                        break
                    elif string == ',':
                        selectors.append(current_selector)
                        current_selector = None
                    elif string == '/':
                        first_choice = current_selector
                        second_choice = _parse_format_selection(tokens, inside_choice=True)
                        current_selector = None
                        selectors.append(FormatSelector(PICKFIRST, (first_choice, second_choice), []))
                    elif string == '[':
                        if not current_selector:
                            current_selector = FormatSelector(SINGLE, 'best', [])
                        format_filter = _parse_filter(tokens)
                        current_selector.filters.append(format_filter)
                    elif string == '(':
                        if current_selector:
                            raise syntax_error('Unexpected ""(""', start)
                        group = _parse_format_selection(tokens, inside_group=True)
                        current_selector = FormatSelector(GROUP, group, [])
                    elif string == '+':
                        video_selector = current_selector
                        audio_selector = _parse_format_selection(tokens, inside_merge=True)
                        current_selector = FormatSelector(MERGE, (video_selector, audio_selector), [])
                    else:
                        raise syntax_error('Operator not recognized: ""{0}""'.format(string), start)
                elif type == tokenize.ENDMARKER:
                    break
            if current_selector:
                selectors.append(current_selector)
            return selectors
        def _build_selector_function(selector):
            if isinstance(selector, list):
                fs = [_build_selector_function(s) for s in selector]
                def selector_function(formats):
                    for f in fs:
                        for format in f(formats):
                            yield format
                return selector_function
            elif selector.type == GROUP:
                selector_function = _build_selector_function(selector.selector)
            elif selector.type == PICKFIRST:
                fs = [_build_selector_function(s) for s in selector.selector]
                def selector_function(formats):
                    for f in fs:
                        picked_formats = list(f(formats))
                        if picked_formats:
                            return picked_formats
                    return []
            elif selector.type == SINGLE:
                format_spec = selector.selector
                def selector_function(formats):
                    if format_spec == 'all':
                        for f in formats:
                            yield f
                    elif format_spec in ['best', 'worst', None]:
                        format_idx = 0 if format_spec == 'worst' else -1
                        audiovideo_formats = [
                            f for f in formats
                            if f.get('vcodec') != 'none' and f.get('acodec') != 'none']
                        if audiovideo_formats:
                            yield audiovideo_formats[format_idx]
                        # for audio only (soundcloud) or video only (imgur) urls, select the best/worst audio format
                        elif (all(f.get('acodec') != 'none' for f in formats) or
                              all(f.get('vcodec') != 'none' for f in formats)):
                            yield formats[format_idx]
                    elif format_spec == 'bestaudio':
                        audio_formats = [
                            f for f in formats
                            if f.get('vcodec') == 'none']
                        if audio_formats:
                            yield audio_formats[-1]
                    elif format_spec == 'worstaudio':
                        audio_formats = [
                            f for f in formats","[56, 57]"
"from typing import Any, List
import warnings
import numpy as np
from pandas._config import get_option
from pandas._libs import index as libindex
from pandas._libs.hashtable import duplicated_int64
from pandas._libs.lib import no_default
from pandas._typing import Label
from pandas.util._decorators import Appender, cache_readonly, doc
from pandas.core.dtypes.common import (
    ensure_platform_int,
    is_categorical_dtype,
    is_interval_dtype,
    is_list_like,
    is_scalar,
)
from pandas.core.dtypes.dtypes import CategoricalDtype
from pandas.core.dtypes.missing import isna
from pandas.core import accessor
from pandas.core.algorithms import take_1d
from pandas.core.arrays.categorical import Categorical, contains, recode_for_categories
import pandas.core.common as com
from pandas.core.construction import extract_array
import pandas.core.indexes.base as ibase
from pandas.core.indexes.base import Index, _index_shared_docs, maybe_extract_name
from pandas.core.indexes.extension import ExtensionIndex, inherit_names
import pandas.core.missing as missing
from pandas.core.ops import get_op_result_name
_index_doc_kwargs = dict(ibase._index_doc_kwargs)
_index_doc_kwargs.update(dict(target_klass=""CategoricalIndex""))

@inherit_names(
    [
        ""argsort"",
        ""_internal_get_values"",
        ""tolist"",
        ""codes"",
        ""categories"",
        ""ordered"",
        ""_reverse_indexer"",
        ""searchsorted"",
        ""is_dtype_equal"",
        ""min"",
        ""max"",
    ],
    Categorical,
)
@accessor.delegate_names(
    delegate=Categorical,
    accessors=[
        ""rename_categories"",
        ""reorder_categories"",
        ""add_categories"",
        ""remove_categories"",
        ""remove_unused_categories"",
        ""set_categories"",
        ""as_ordered"",
        ""as_unordered"",
    ],
    typ=""method"",
    overwrite=True,
)
class CategoricalIndex(ExtensionIndex, accessor.PandasDelegate):
    """"""
    Index based on an underlying :class:`Categorical`.
    CategoricalIndex, like Categorical, can only take on a limited,
    and usually fixed, number of possible values (`categories`). Also,
    like Categorical, it might have an order, but numerical operations
    (additions, divisions, ...) are not possible.
    Parameters
    ----------
    data : array-like (1-dimensional)
        The values of the categorical. If `categories` are given, values not in
        `categories` will be replaced with NaN.
    categories : index-like, optional
        The categories for the categorical. Items need to be unique.
        If the categories are not given here (and also not in `dtype`), they
        will be inferred from the `data`.
    ordered : bool, optional
        Whether or not this categorical is treated as an ordered
        categorical. If not given here or in `dtype`, the resulting
        categorical will be unordered.
    dtype : CategoricalDtype or ""category"", optional
        If :class:`CategoricalDtype`, cannot be used together with
        `categories` or `ordered`.
    copy : bool, default False
        Make a copy of input ndarray.
    name : object, optional
        Name to be stored in the index.
    Attributes
    ----------
    codes
    categories
    ordered
    Methods
    -------
    rename_categories
    reorder_categories
    add_categories
    remove_categories
    remove_unused_categories
    set_categories
    as_ordered
    as_unordered
    map
    Raises
    ------
    ValueError
        If the categories do not validate.
    TypeError
        If an explicit ``ordered=True`` is given but no `categories` and the
        `values` are not sortable.
    See Also
    --------",[21]
"        results = []
        exclude: List[Scalar] = []
        for i, b in enumerate(blocks):
            try:
                values = self._prep_values(b.values)
            except (TypeError, NotImplementedError) as err:
                if isinstance(obj, ABCDataFrame):
                    exclude.extend(b.columns)
                    del block_list[i]
                    continue
                else:
                    raise DataError(""No numeric types to aggregate"") from err
            if values.size == 0:
                results.append(values.copy())
                continue
            # calculation function
            offset = calculate_center_offset(window) if center else 0
            additional_nans = np.array([np.nan] * offset)
            if not is_weighted:
                def calc(x):
                    x = np.concatenate((x, additional_nans))
                    if not isinstance(window, BaseIndexer):
                        min_periods = calculate_min_periods(
                            window, self.min_periods, len(x), require_min_periods, floor
                        )
                    else:
                        min_periods = calculate_min_periods(
                            self.min_periods or 1,
                            self.min_periods,
                            len(x),
                            require_min_periods,
                            floor,
                        )
                    start, end = window_indexer.get_window_bounds(
                        num_values=len(x),
                        min_periods=self.min_periods,
                        center=self.center,
                        closed=self.closed,
                    )
                    return func(x, start, end, min_periods)
            else:
                def calc(x):
                    x = np.concatenate((x, additional_nans))
                    return func(x, window, self.min_periods)
            with np.errstate(all=""ignore""):
                if values.ndim > 1:
                    result = np.apply_along_axis(calc, self.axis, values)
                else:
                    result = calc(values)
                    result = np.asarray(result)
            if use_numba_cache:
                NUMBA_FUNC_CACHE[(kwargs[""original_func""], ""rolling_apply"")] = func
            if center:
                result = self._center_window(result, window)
            results.append(result)
        return self._wrap_results(results, block_list, obj, exclude)
    def aggregate(self, func, *args, **kwargs):
        result, how = self._aggregate(func, *args, **kwargs)
        if result is None:
            return self.apply(func, raw=False, args=args, kwargs=kwargs)
        return result
    agg = aggregate
    _shared_docs[""sum""] = dedent(
        """"""
    Calculate %(name)s sum of given DataFrame or Series.
    Parameters
    ----------
    *args, **kwargs
        For compatibility with other %(name)s methods. Has no effect
        on the computed value.
    Returns
    -------
    Series or DataFrame
        Same type as the input, with the same index, containing the
        %(name)s sum.
    See Also
    --------
    Series.sum : Reducing sum for Series.
    DataFrame.sum : Reducing sum for DataFrame.
    Examples
    --------
    >>> s = pd.Series([1, 2, 3, 4, 5])
    >>> s
    0    1
    1    2
    2    3
    3    4
    4    5
    dtype: int64
    >>> s.rolling(3).sum()
    0     NaN
    1     NaN
    2     6.0
    3     9.0
    4    12.0
    dtype: float64
    >>> s.expanding(3).sum()
    0     NaN
    1     NaN
    2     6.0
    3    10.0
    4    15.0
    dtype: float64
    >>> s.rolling(3, center=True).sum()
    0     NaN","[26, 32]"
"        (useful for using with keyword arguments in subclasses constructors)
        """"""
        self.fields_to_export = options.pop('fields_to_export', None)
        self.export_empty_fields = options.pop('export_empty_fields', False)
        self.encoding = options.pop('encoding', 'utf-8')
        if not dont_fail and options:
            raise TypeError(""Unexpected options: %s"" % ', '.join(options.keys()))
    def export_item(self, item):
        raise NotImplementedError
    def serialize_field(self, field, name, value):
        serializer = field.get('serializer', lambda x: x)
        return serializer(value)
    def start_exporting(self):
        pass
    def finish_exporting(self):
        pass
    def _get_serialized_fields(self, item, default_value=None, include_empty=None):
        """"""Return the fields to export as an iterable of tuples
        (name, serialized_value)
        """"""
        if include_empty is None:
            include_empty = self.export_empty_fields
        if self.fields_to_export is None:
            if include_empty and not isinstance(item, dict):
                field_iter = six.iterkeys(item.fields)
            else:
                field_iter = six.iterkeys(item)
        else:
            if include_empty:
                field_iter = self.fields_to_export
            else:
                field_iter = (x for x in self.fields_to_export if x in item)
        for field_name in field_iter:
            if field_name in item:
                field = {} if isinstance(item, dict) else item.fields[field_name]
                value = self.serialize_field(field, field_name, item[field_name])
            else:
                value = default_value
            yield field_name, value

class JsonLinesItemExporter(BaseItemExporter):
    def __init__(self, file, **kwargs):
        self._configure(kwargs, dont_fail=True)
        self.file = file
        self.encoder = ScrapyJSONEncoder(**kwargs)
    def export_item(self, item):
        itemdict = dict(self._get_serialized_fields(item))
        self.file.write(to_bytes(self.encoder.encode(itemdict) + '\n'))

class JsonItemExporter(BaseItemExporter):
    def __init__(self, file, **kwargs):
        self._configure(kwargs, dont_fail=True)
        self.file = file
        self.encoder = ScrapyJSONEncoder(**kwargs)
        self.first_item = True
    def start_exporting(self):
        self.file.write(b""["")
    def finish_exporting(self):
        self.file.write(b""]"")
    def export_item(self, item):
        if self.first_item:
            self.first_item = False
        else:
            self.file.write(b',\n')
        itemdict = dict(self._get_serialized_fields(item))
        self.file.write(to_bytes(self.encoder.encode(itemdict)))

class XmlItemExporter(BaseItemExporter):
    def __init__(self, file, **kwargs):
        self.item_element = kwargs.pop('item_element', 'item')
        self.root_element = kwargs.pop('root_element', 'items')
        self._configure(kwargs)
        self.xg = XMLGenerator(file, encoding=self.encoding)
    def start_exporting(self):
        self.xg.startDocument()
        self.xg.startElement(self.root_element, {})
    def export_item(self, item):
        self.xg.startElement(self.item_element, {})
        for name, value in self._get_serialized_fields(item, default_value=''):
            self._export_xml_field(name, value)
        self.xg.endElement(self.item_element)
    def finish_exporting(self):
        self.xg.endElement(self.root_element)
        self.xg.endDocument()
    def _export_xml_field(self, name, serialized_value):
        self.xg.startElement(name, {})
        if hasattr(serialized_value, 'items'):
            for subname, value in serialized_value.items():
                self._export_xml_field(subname, value)
        elif is_listlike(serialized_value):
            for value in serialized_value:
                self._export_xml_field('value', value)
        else:
            self._xg_characters(serialized_value)
        self.xg.endElement(name)
    # Workaround for http://bugs.python.org/issue17606
    # Before Python 2.7.4 xml.sax.saxutils required bytes;
    # since 2.7.4 it requires unicode. The bug is likely to be
    # fixed in 2.7.6, but 2.7.6 will still support unicode,
    # and Python 3.x will require unicode, so "">= 2.7.4"" should be fine.
    if sys.version_info[:3] >= (2, 7, 4):
        def _xg_characters(self, serialized_value):
            if not isinstance(serialized_value, six.text_type):
                serialized_value = serialized_value.decode(self.encoding)
            return self.xg.characters(serialized_value)",[113]
"            for inputs/kernels/outputs.
        dilation_rate: tuple of 2 integers.
    # Returns
        A tensor, result of 2D convolution.
    # Raises
        ValueError: If `data_format` is neither
            `""channels_last""` nor `""channels_first""`.
    """"""
    data_format = normalize_data_format(data_format)
    x, tf_data_format = _preprocess_conv2d_input(x, data_format)
    padding = _preprocess_padding(padding)
    x = tf.nn.convolution(
        input=x,
        filter=kernel,
        dilation_rate=dilation_rate,
        strides=strides,
        padding=padding,
        data_format=tf_data_format)
    if data_format == 'channels_first' and tf_data_format == 'NHWC':
        x = tf.transpose(x, (0, 3, 1, 2))  # NHWC -> NCHW
    return x

def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),
                     padding='valid', data_format=None):
    """"""2D deconvolution (i.e. transposed convolution).
    # Arguments
        x: Tensor or variable.
        kernel: kernel tensor.
        output_shape: 1D int tensor for the output shape.
        strides: strides tuple.
        padding: string, `""same""` or `""valid""`.
        data_format: string, `""channels_last""` or `""channels_first""`.
            Whether to use Theano or TensorFlow/CNTK data format
            for inputs/kernels/outputs.
    # Returns
        A tensor, result of transposed 2D convolution.
    # Raises
        ValueError: If `data_format` is neither
            `""channels_last""` nor `""channels_first""`.
    """"""
    data_format = normalize_data_format(data_format)
    if isinstance(output_shape, (tuple, list)):
        output_shape = tf.stack(output_shape)
    x, tf_data_format = _preprocess_conv2d_input(x, data_format)
    if data_format == 'channels_first' and tf_data_format == 'NHWC':
        output_shape = (output_shape[0],
                        output_shape[2],
                        output_shape[3],
                        output_shape[1])
    if output_shape[0] is None:
        output_shape = (tf.shape(x)[0],) + tuple(output_shape[1:])
        output_shape = tf.stack(list(output_shape))
    padding = _preprocess_padding(padding)
    if tf_data_format == 'NHWC':
        strides = (1,) + strides + (1,)
    else:
        strides = (1, 1) + strides
    x = tf.nn.conv2d_transpose(x, kernel, output_shape, strides,
                               padding=padding,
                               data_format=tf_data_format)
    if data_format == 'channels_first' and tf_data_format == 'NHWC':
        x = tf.transpose(x, (0, 3, 1, 2))  # NHWC -> NCHW
    return x

def separable_conv1d(x, depthwise_kernel, pointwise_kernel, strides=1,
                     padding='valid', data_format=None, dilation_rate=1):
    """"""1D convolution with separable filters.
    # Arguments
        x: input tensor
        depthwise_kernel: convolution kernel for the depthwise convolution.
        pointwise_kernel: kernel for the 1x1 convolution.
        strides: stride integer.
        padding: string, `""same""` or `""valid""`.
        data_format: string, `""channels_last""` or `""channels_first""`.
        dilation_rate: integer dilation rate.
    # Returns
        Output tensor.
    # Raises
        ValueError: If `data_format` is neither
            `""channels_last""` nor `""channels_first""`.
    """"""
    data_format = normalize_data_format(data_format)
    if isinstance(strides, int):
        strides = (strides,)
    if isinstance(dilation_rate, int):
        dilation_rate = (dilation_rate,)
    x, tf_data_format = _preprocess_conv1d_input(x, data_format)
    if tf_data_format == 'NWC':
        tf_data_format = 'NHWC'
    else:
        tf_data_format = 'NCHW'
    padding = _preprocess_padding(padding)
    if tf_data_format == 'NHWC':
        spatial_start_dim = 1
        strides = (1,) + strides * 2 + (1,)
    else:
        spatial_start_dim = 2
        strides = (1, 1) + strides * 2
    x = tf.expand_dims(x, spatial_start_dim)
    depthwise_kernel = tf.expand_dims(depthwise_kernel, 0)
    pointwise_kernel = tf.expand_dims(pointwise_kernel, 0)
    dilation_rate = (1,) + dilation_rate
    x = tf.nn.separable_conv2d(x, depthwise_kernel, pointwise_kernel,
                               strides=strides,
                               padding=padding,
                               rate=dilation_rate,
                               data_format=tf_data_format)
","[29, 53, 70, 71, 72]"
"                        def is_indexed_slices(grad):
                            return type(grad).__name__ == 'IndexedSlices'
                        grads = [
                            grad.values if is_indexed_slices(grad) else grad
                            for grad in grads]
                        tf.summary.histogram('{}_grad'.format(mapped_weight_name), grads)
                    if self.write_images:
                        w_img = tf.squeeze(weight)
                        shape = K.int_shape(w_img)
                        if len(shape) == 2:  # dense layer kernel case
                            if shape[0] > shape[1]:
                                w_img = tf.transpose(w_img)
                                shape = K.int_shape(w_img)
                            w_img = tf.reshape(w_img, [1,
                                                       shape[0],
                                                       shape[1],
                                                       1])
                        elif len(shape) == 3:  # convnet case
                            if K.image_data_format() == 'channels_last':
                                # switch to channels_first to display
                                # every kernel as a separate image
                                w_img = tf.transpose(w_img, perm=[2, 0, 1])
                                shape = K.int_shape(w_img)
                            w_img = tf.reshape(w_img, [shape[0],
                                                       shape[1],
                                                       shape[2],
                                                       1])
                        elif len(shape) == 1:  # bias case
                            w_img = tf.reshape(w_img, [1,
                                                       shape[0],
                                                       1,
                                                       1])
                        else:
                            # not possible to handle 3D convnets etc.
                            continue
                        shape = K.int_shape(w_img)
                        assert len(shape) == 4 and shape[-1] in [1, 3, 4]
                        tf.summary.image(mapped_weight_name, w_img)
                if hasattr(layer, 'output'):
                    tf.summary.histogram('{}_out'.format(layer.name),
                                         layer.output)
        self.merged = tf.summary.merge_all()
        if self.write_graph:
            self.writer = tf.summary.FileWriter(self.log_dir,
                                                self.sess.graph)
        else:
            self.writer = tf.summary.FileWriter(self.log_dir)
        if self.embeddings_freq:
            embeddings_layer_names = self.embeddings_layer_names
            if not embeddings_layer_names:
                embeddings_layer_names = [layer.name for layer in self.model.layers
                                          if type(layer).__name__ == 'Embedding']
            embeddings = {layer.name: layer.weights[0]
                          for layer in self.model.layers
                          if layer.name in embeddings_layer_names}
            self.saver = tf.train.Saver(list(embeddings.values()))
            embeddings_metadata = {}
            if not isinstance(self.embeddings_metadata, str):
                embeddings_metadata = self.embeddings_metadata
            else:
                embeddings_metadata = {layer_name: self.embeddings_metadata
                                       for layer_name in embeddings.keys()}
            config = projector.ProjectorConfig()
            self.embeddings_ckpt_path = os.path.join(self.log_dir,
                                                     'keras_embedding.ckpt')
            for layer_name, tensor in embeddings.items():
                embedding = config.embeddings.add()
                embedding.tensor_name = tensor.name
                if layer_name in embeddings_metadata:
                    embedding.metadata_path = embeddings_metadata[layer_name]
            projector.visualize_embeddings(self.writer, config)
    def on_epoch_end(self, epoch, logs=None):
        logs = logs or {}
        if not self.validation_data and self.histogram_freq:
            raise ValueError('If printing histograms, validation_data must be '
                             'provided, and cannot be a generator.')
        if self.validation_data and self.histogram_freq:
            if epoch % self.histogram_freq == 0:
                val_data = self.validation_data
                tensors = (self.model.inputs +
                           self.model.targets +
                           self.model.sample_weights)
                if self.model.uses_learning_phase:
                    tensors += [K.learning_phase()]
                assert len(val_data) == len(tensors)
                val_size = val_data[0].shape[0]
                i = 0
                while i < val_size:
                    step = min(self.batch_size, val_size - i)
                    if self.model.uses_learning_phase:
                        # do not slice the learning phase
                        batch_val = [x[i:i + step] for x in val_data[:-1]]
                        batch_val.append(val_data[-1])
                    else:
                        batch_val = [x[i:i + step] for x in val_data]
                    assert len(batch_val) == len(tensors)
                    feed_dict = dict(zip(tensors, batch_val))
                    result = self.sess.run([self.merged], feed_dict=feed_dict)
                    summary_str = result[0]
                    self.writer.add_summary(summary_str, epoch)
                    i += self.batch_size
        if self.embeddings_freq and self.embeddings_ckpt_path:
            if epoch % self.embeddings_freq == 0:
                self.saver.save(self.sess,
                                self.embeddings_ckpt_path,
                                epoch)
        for name, value in logs.items():","[41, 42]"
"        vmin, vmax = self.vmin, self.vmax
        if vmin > vmax:
            raise ValueError(""minvalue must be less than or equal to maxvalue"")
        elif vmin == vmax:
            result.fill(0)
        else:
            if clip:
                mask = np.ma.getmask(result)
                result = np.ma.array(np.clip(result.filled(vmax), vmin, vmax),
                                     mask=mask)
            # in-place equivalent of above can be much faster
            resdat = self._transform(result.data)
            resdat -= self._lower
            resdat /= (self._upper - self._lower)
        if is_scalar:
            result = result[0]
        return result
    def _transform(self, a):
        """"""Inplace transformation.""""""
        with np.errstate(invalid=""ignore""):
            masked = np.abs(a) > self.linthresh
        sign = np.sign(a[masked])
        log = (self._linscale_adj + np.log(np.abs(a[masked]) / self.linthresh))
        log *= sign * self.linthresh
        a[masked] = log
        a[~masked] *= self._linscale_adj
        return a
    def _inv_transform(self, a):
        """"""Inverse inplace Transformation.""""""
        masked = np.abs(a) > (self.linthresh * self._linscale_adj)
        sign = np.sign(a[masked])
        exp = np.exp(sign * a[masked] / self.linthresh - self._linscale_adj)
        exp *= sign * self.linthresh
        a[masked] = exp
        a[~masked] /= self._linscale_adj
        return a
    def _transform_vmin_vmax(self):
        """"""Calculates vmin and vmax in the transformed system.""""""
        vmin, vmax = self.vmin, self.vmax
        arr = np.array([vmax, vmin]).astype(float)
        self._upper, self._lower = self._transform(arr)
    def inverse(self, value):
        if not self.scaled():
            raise ValueError(""Not invertible until scaled"")
        val = np.ma.asarray(value)
        val = val * (self._upper - self._lower) + self._lower
        return self._inv_transform(val)
    def autoscale(self, A):
        # docstring inherited.
        super().autoscale(A)
        self._transform_vmin_vmax()
    def autoscale_None(self, A):
        # docstring inherited.
        super().autoscale_None(A)
        self._transform_vmin_vmax()

class PowerNorm(Normalize):
    """"""
    Linearly map a given value to the 0-1 range and then apply
    a power-law normalization over that range.
    """"""
    def __init__(self, gamma, vmin=None, vmax=None, clip=False):
        Normalize.__init__(self, vmin, vmax, clip)
        self.gamma = gamma
    def __call__(self, value, clip=None):
        if clip is None:
            clip = self.clip
        result, is_scalar = self.process_value(value)
        self.autoscale_None(result)
        gamma = self.gamma
        vmin, vmax = self.vmin, self.vmax
        if vmin > vmax:
            raise ValueError(""minvalue must be less than or equal to maxvalue"")
        elif vmin == vmax:
            result.fill(0)
        else:
            if clip:
                mask = np.ma.getmask(result)
                result = np.ma.array(np.clip(result.filled(vmax), vmin, vmax),
                                     mask=mask)
            resdat = result.data
            resdat -= vmin
            resdat[resdat < 0] = 0
            np.power(resdat, gamma, resdat)
            resdat /= (vmax - vmin) ** gamma
            result = np.ma.array(resdat, mask=result.mask, copy=False)
        if is_scalar:
            result = result[0]
        return result
    def inverse(self, value):
        if not self.scaled():
            raise ValueError(""Not invertible until scaled"")
        gamma = self.gamma
        vmin, vmax = self.vmin, self.vmax
        if np.iterable(value):
            val = np.ma.asarray(value)
            return np.ma.power(val, 1. / gamma) * (vmax - vmin) + vmin
        else:
            return pow(value, 1. / gamma) * (vmax - vmin) + vmin

class BoundaryNorm(Normalize):
    """"""
    Generate a colormap index based on discrete intervals.
    Unlike `Normalize` or `LogNorm`, `BoundaryNorm` maps values to integers
    instead of to the interval 0-1.
    Mapping to the 0-1 interval could have been done via piece-wise linear
    interpolation, but using integers seems simpler, and reduces the number of
    conversions back and forth between integer and floating point.
    """"""","[25, 35]"
"        -------
        filtered : Series
        """"""
        if isinstance(func, str):
            wrapper = lambda x: getattr(x, func)(*args, **kwargs)
        else:
            wrapper = lambda x: func(x, *args, **kwargs)
        # Interpret np.nan as False.
        def true_and_notna(x, *args, **kwargs) -> bool:
            b = wrapper(x, *args, **kwargs)
            return b and notna(b)
        try:
            indices = [
                self._get_index(name) for name, group in self if true_and_notna(group)
            ]
        except (ValueError, TypeError):
            raise TypeError(""the filter must return a boolean result"")
        filtered = self._apply_filter(indices, dropna)
        return filtered
    def nunique(self, dropna: bool = True) -> Series:
        """"""
        Return number of unique elements in the group.
        Returns
        -------
        Series
            Number of unique values within each group.
        """"""
        ids, _, _ = self.grouper.group_info
        val = self.obj._internal_get_values()
        # GH 27951
        # temporary fix while we wait for NumPy bug 12629 to be fixed
        val[isna(val)] = np.datetime64(""NaT"")
        try:
            sorter = np.lexsort((val, ids))
        except TypeError:  # catches object dtypes
            msg = ""val.dtype must be object, got {}"".format(val.dtype)
            assert val.dtype == object, msg
            val, _ = algorithms.factorize(val, sort=False)
            sorter = np.lexsort((val, ids))
            _isna = lambda a: a == -1
        else:
            _isna = isna
        ids, val = ids[sorter], val[sorter]
        # group boundaries are where group ids change
        # unique observations are where sorted values change
        idx = np.r_[0, 1 + np.nonzero(ids[1:] != ids[:-1])[0]]
        inc = np.r_[1, val[1:] != val[:-1]]
        # 1st item of each group is a new unique observation
        mask = _isna(val)
        if dropna:
            inc[idx] = 1
            inc[mask] = 0
        else:
            inc[mask & np.r_[False, mask[:-1]]] = 0
            inc[idx] = 1
        out = np.add.reduceat(inc, idx).astype(""int64"", copy=False)
        if len(ids):
            # NaN/NaT group exists if the head of ids is -1,
            # so remove it from res and exclude its index from idx
            if ids[0] == -1:
                res = out[1:]
                idx = idx[np.flatnonzero(idx)]
            else:
                res = out
        else:
            res = out[1:]
        ri = self.grouper.result_index
        # we might have duplications among the bins
        if len(res) != len(ri):
            res, out = np.zeros(len(ri), dtype=out.dtype), res
            res[ids[idx]] = out
        return Series(res, index=ri, name=self._selection_name)
    @Appender(Series.describe.__doc__)
    def describe(self, **kwargs):
        result = self.apply(lambda x: x.describe(**kwargs))
        if self.axis == 1:
            return result.T
        return result.unstack()
    def value_counts(
        self, normalize=False, sort=True, ascending=False, bins=None, dropna=True
    ):
        from pandas.core.reshape.tile import cut
        from pandas.core.reshape.merge import _get_join_indexers
        if bins is not None and not np.iterable(bins):
            # scalar bins cannot be done at top level
            # in a backward compatible way
            return self.apply(
                Series.value_counts,
                normalize=normalize,
                sort=sort,
                ascending=ascending,
                bins=bins,
            )
        ids, _, _ = self.grouper.group_info
        val = self.obj._internal_get_values()
        # groupby removes null keys from groupings
        mask = ids != -1
        ids, val = ids[mask], val[mask]
        if bins is None:
            lab, lev = algorithms.factorize(val, sort=True)
            llab = lambda lab, inc: lab[inc]
        else:
            # lab is a Categorical with categories an IntervalIndex
            lab = cut(Series(val), bins, include_lowest=True)
            lev = lab.cat.categories",[85]
"class HiveCommandError(RuntimeError):
    def __init__(self, message, out=None, err=None):
        super(HiveCommandError, self).__init__(message, out, err)
        self.message = message
        self.out = out
        self.err = err

def load_hive_cmd():
    return luigi.configuration.get_config().get('hive', 'command', 'hive')

def get_hive_syntax():
    return luigi.configuration.get_config().get('hive', 'release', 'cdh4')

def run_hive(args, check_return_code=True):
    """"""
    Runs the `hive` from the command line, passing in the given args, and
    returning stdout.
    With the apache release of Hive, so of the table existence checks
    (which are done using DESCRIBE do not exit with a return code of 0
    so we need an option to ignore the return code and just return stdout for parsing
    """"""
    cmd = [load_hive_cmd()] + args
    p = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    stdout, stderr = p.communicate()
    if check_return_code and p.returncode != 0:
        raise HiveCommandError(""Hive command: {0} failed with error code: {1}"".format("" "".join(cmd), p.returncode),
                               stdout, stderr)
    return stdout

def run_hive_cmd(hivecmd, check_return_code=True):
    """"""
    Runs the given hive query and returns stdout.
    """"""
    return run_hive(['-e', hivecmd], check_return_code)

def run_hive_script(script):
    """"""
    Runs the contents of the given script in hive and returns stdout.
    """"""
    if not os.path.isfile(script):
        raise RuntimeError(""Hive script: {0} does not exist."".format(script))
    return run_hive(['-f', script])

@six.add_metaclass(abc.ABCMeta)
class HiveClient(object):  # interface
    @abc.abstractmethod
    def table_location(self, table, database='default', partition=None):
        """"""
        Returns location of db.table (or db.table.partition). partition is a dict of partition key to
        value.
        """"""
        pass
    @abc.abstractmethod
    def table_schema(self, table, database='default'):
        """"""
        Returns list of [(name, type)] for each column in database.table.
        """"""
        pass
    @abc.abstractmethod
    def table_exists(self, table, database='default', partition=None):
        """"""
        Returns true if db.table (or db.table.partition) exists. partition is a dict of partition key to
        value.
        """"""
        pass
    @abc.abstractmethod
    def partition_spec(self, partition):
        """""" Turn a dict into a string partition specification """"""
        pass

class HiveCommandClient(HiveClient):
    """"""
    Uses `hive` invocations to find information.
    """"""
    def table_location(self, table, database='default', partition=None):
        cmd = ""use {0}; describe formatted {1}"".format(database, table)
        if partition is not None:
            cmd += "" PARTITION ({0})"".format(self.partition_spec(partition))
        stdout = run_hive_cmd(cmd)
        for line in stdout.split(""\n""):
            if ""Location:"" in line:
                return line.split(""\t"")[1]
    def table_exists(self, table, database='default', partition=None):
        if partition is None:
            stdout = run_hive_cmd('use {0}; show tables like ""{1}"";'.format(database, table))
            return stdout and table in stdout
        else:
            stdout = run_hive_cmd(""""""use %s; show partitions %s partition
                                (%s)"""""" % (database, table, self.partition_spec(partition)))
            if stdout:
                return True
            else:
                return False
    def table_schema(self, table, database='default'):
        describe = run_hive_cmd(""use {0}; describe {1}"".format(database, table))
        if not describe or ""does not exist"" in describe:
            return None
        return [tuple([x.strip() for x in line.strip().split(""\t"")]) for line in describe.strip().split(""\n"")]
    def partition_spec(self, partition):
        """"""
        Turns a dict into the a Hive partition specification string.
        """"""
        return ','.join([""{0}='{1}'"".format(k, v) for (k, v) in
                         sorted(six.iteritems(partition), key=operator.itemgetter(0))])
",[104]
"                                     str(generator_output))
                # build batch logs
                batch_logs = {}
                if x is None or len(x) == 0:
                    # Handle data tensors support when no input given
                    # step-size = 1 for data tensors
                    batch_size = 1
                elif isinstance(x, list):
                    batch_size = x[0].shape[0]
                elif isinstance(x, dict):
                    batch_size = list(x.values())[0].shape[0]
                else:
                    batch_size = x.shape[0]
                batch_logs['batch'] = batch_index
                batch_logs['size'] = batch_size
                callbacks.on_batch_begin(batch_index, batch_logs)
                outs = model.train_on_batch(x, y,
                                            sample_weight=sample_weight,
                                            class_weight=class_weight)
                outs = to_list(outs)
                for l, o in zip(out_labels, outs):
                    batch_logs[l] = o
                callbacks.on_batch_end(batch_index, batch_logs)
                batch_index += 1
                steps_done += 1
                # Epoch finished.
                if steps_done >= steps_per_epoch and do_validation:
                    if val_gen:
                        val_outs = model.evaluate_generator(
                            val_enqueuer_gen,
                            validation_steps,
                            workers=0)
                    else:
                        # No need for try/except because
                        # data has already been validated.
                        val_outs = model.evaluate(
                            val_x, val_y,
                            batch_size=batch_size,
                            sample_weight=val_sample_weights,
                            verbose=0)
                    val_outs = to_list(val_outs)
                    # Same labels assumed.
                    for l, o in zip(out_labels, val_outs):
                        epoch_logs['val_' + l] = o
                if callback_model.stop_training:
                    break
            callbacks.on_epoch_end(epoch, epoch_logs)
            epoch += 1
            if callback_model.stop_training:
                break
    finally:
        try:
            if enqueuer is not None:
                enqueuer.stop()
        finally:
            if val_enqueuer is not None:
                val_enqueuer.stop()
    callbacks.on_train_end()
    return model.history

def evaluate_generator(model, generator,
                       steps=None,
                       max_queue_size=10,
                       workers=1,
                       use_multiprocessing=False,
                       verbose=0):
    """"""See docstring for `Model.evaluate_generator`.""""""
    model._make_test_function()
    if hasattr(model, 'metrics'):
        for m in model.stateful_metric_functions:
            m.reset_states()
        stateful_metric_indices = [
            i for i, name in enumerate(model.metrics_names)
            if str(name) in model.stateful_metric_names]
    else:
        stateful_metric_indices = []
    steps_done = 0
    outs_per_batch = []
    batch_sizes = []
    is_sequence = isinstance(generator, Sequence)
    if not is_sequence and use_multiprocessing and workers > 1:
        warnings.warn(
            UserWarning('Using a generator with `use_multiprocessing=True`'
                        ' and multiple workers may duplicate your data.'
                        ' Please consider using the`keras.utils.Sequence'
                        ' class.'))
    if steps is None:
        if is_sequence:
            steps = len(generator)
        else:
            raise ValueError('`steps=None` is only valid for a generator'
                             ' based on the `keras.utils.Sequence` class.'
                             ' Please specify `steps` or use the'
                             ' `keras.utils.Sequence` class.')
    enqueuer = None
    try:
        if workers > 0:
            if is_sequence:
                enqueuer = OrderedEnqueuer(
                    generator,
                    use_multiprocessing=use_multiprocessing)
            else:
                enqueuer = GeneratorEnqueuer(
                    generator,
                    use_multiprocessing=use_multiprocessing)
            enqueuer.start(workers=workers, max_queue_size=max_queue_size)
            output_generator = enqueuer.get()
        else:
            if is_sequence:
                output_generator = iter_sequence_infinite(generator)
            else:
                output_generator = generator
        if verbose == 1:","[91, 92, 99, 110, 121]"
"            To be deprecated in 1.0. Use :func:`blueprint` instead.
        :param args: Blueprint object or (list, tuple) thereof
        :param kwargs: option dictionary with blueprint defaults
        :return: None
        """"""
        if self.debug:
            warnings.simplefilter(""default"")
        warnings.warn(
            ""Use of register_blueprint will be deprecated in ""
            ""version 1.0.  Please use the blueprint method""
            "" instead"",
            DeprecationWarning,
        )
        return self.blueprint(*args, **kwargs)
    def url_for(self, view_name: str, **kwargs):
        r""""""Build a URL based on a view name and the values provided.
        In order to build a URL, all request parameters must be supplied as
        keyword arguments, and each parameter must pass the test for the
        specified parameter type. If these conditions are not met, a
        `URLBuildError` will be thrown.
        Keyword arguments that are not request parameters will be included in
        the output URL's query string.
        :param view_name: string referencing the view name
        :param \**kwargs: keys and values that are used to build request
            parameters and query string arguments.
        :return: the built URL
        Raises:
            URLBuildError
        """"""
        # find the route by the supplied view name
        kw: Dict[str, str] = {}
        # special static files url_for
        if view_name == ""static"":
            kw.update(name=kwargs.pop(""name"", ""static""))
        elif view_name.endswith("".static""):  # blueprint.static
            kwargs.pop(""name"", None)
            kw.update(name=view_name)
        uri, route = self.router.find_route_by_view_name(view_name, **kw)
        if not (uri and route):
            raise URLBuildError(
                ""Endpoint with name `{}` was not found"".format(view_name)
            )
        if view_name == ""static"" or view_name.endswith("".static""):
            filename = kwargs.pop(""filename"", None)
            # it's static folder
            if ""<file_uri:"" in uri:
                folder_ = uri.split(""<file_uri:"", 1)[0]
                if folder_.endswith(""/""):
                    folder_ = folder_[:-1]
                if filename.startswith(""/""):
                    filename = filename[1:]
                uri = ""{}/{}"".format(folder_, filename)
        if uri != ""/"" and uri.endswith(""/""):
            uri = uri[:-1]
        out = uri
        # find all the parameters we will need to build in the URL
        matched_params = re.findall(self.router.parameter_pattern, uri)
        # _method is only a placeholder now, don't know how to support it
        kwargs.pop(""_method"", None)
        anchor = kwargs.pop(""_anchor"", """")
        # _external need SERVER_NAME in config or pass _server arg
        external = kwargs.pop(""_external"", False)
        scheme = kwargs.pop(""_scheme"", """")
        if scheme and not external:
            raise ValueError(""When specifying _scheme, _external must be True"")
        netloc = kwargs.pop(""_server"", None)
        if netloc is None and external:
            netloc = self.config.get(""SERVER_NAME"", """")
        if external:
            if not scheme:
                if "":"" in netloc[:8]:
                    scheme = netloc[:8].split("":"", 1)[0]
                else:
                    scheme = ""http""
            if ""://"" in netloc[:8]:
                netloc = netloc.split(""://"", 1)[-1]
        for match in matched_params:
            name, _type, pattern = self.router.parse_parameter_string(match)
            # we only want to match against each individual parameter
            specific_pattern = ""^{}$"".format(pattern)
            supplied_param = None
            if name in kwargs:
                supplied_param = kwargs.get(name)
                del kwargs[name]
            else:
                raise URLBuildError(
                    ""Required parameter `{}` was not passed to url_for"".format(
                        name
                    )
                )
            supplied_param = str(supplied_param)
            # determine if the parameter supplied by the caller passes the test
            # in the URL
            passes_pattern = re.match(specific_pattern, supplied_param)
            if not passes_pattern:
                if _type != str:
                    msg = (
                        'Value ""{}"" for parameter `{}` does not '
                        ""match pattern for type `{}`: {}"".format(
                            supplied_param, name, _type.__name__, pattern
                        )
                    )
                else:
                    msg = (",[84]
"    def _generate_recurrent_dropout_mask(self, inputs, training=None):
        if 0 < self.recurrent_dropout < 1:
            ones = K.ones_like(K.reshape(inputs[:, 0, 0], (-1, 1)))
            ones = K.tile(ones, (1, self.units))
            def dropped_inputs():
                return K.dropout(ones, self.dropout)
            self._recurrent_dropout_mask = [K.in_train_phase(
                dropped_inputs,
                ones,
                training=training)
                for _ in range(4)]
        else:
            self._recurrent_dropout_mask = None
    def call(self, inputs, states, training=None):
        # dropout matrices for input units
        dp_mask = self._dropout_mask
        # dropout matrices for recurrent units
        rec_dp_mask = self._recurrent_dropout_mask
        h_tm1 = states[0]  # previous memory state
        c_tm1 = states[1]  # previous carry state
        if self.implementation == 1:
            if 0 < self.dropout < 1.:
                inputs_i = inputs * dp_mask[0]
                inputs_f = inputs * dp_mask[1]
                inputs_c = inputs * dp_mask[2]
                inputs_o = inputs * dp_mask[3]
            else:
                inputs_i = inputs
                inputs_f = inputs
                inputs_c = inputs
                inputs_o = inputs
            x_i = K.dot(inputs_i, self.kernel_i) + self.bias_i
            x_f = K.dot(inputs_f, self.kernel_f) + self.bias_f
            x_c = K.dot(inputs_c, self.kernel_c) + self.bias_c
            x_o = K.dot(inputs_o, self.kernel_o) + self.bias_o
            if 0 < self.recurrent_dropout < 1.:
                h_tm1_i = h_tm1 * rec_dp_mask[0]
                h_tm1_f = h_tm1 * rec_dp_mask[1]
                h_tm1_c = h_tm1 * rec_dp_mask[2]
                h_tm1_o = h_tm1 * rec_dp_mask[3]
            else:
                h_tm1_i = h_tm1
                h_tm1_f = h_tm1
                h_tm1_c = h_tm1
                h_tm1_o = h_tm1
            i = self.recurrent_activation(x_i + K.dot(h_tm1_i,
                                                      self.recurrent_kernel_i))
            f = self.recurrent_activation(x_f + K.dot(h_tm1_f,
                                                      self.recurrent_kernel_f))
            c = f * c_tm1 + i * self.activation(x_c + K.dot(h_tm1_c,
                                                            self.recurrent_kernel_c))
            o = self.recurrent_activation(x_o + K.dot(h_tm1_o,
                                                      self.recurrent_kernel_o))
        else:
            if 0. < self.dropout < 1.:
                inputs *= dp_mask[0]
            z = K.dot(inputs, self.kernel)
            if 0. < self.recurrent_dropout < 1.:
                h_tm1 *= rec_dp_mask[0]
            z += K.dot(h_tm1, self.recurrent_kernel)
            if self.use_bias:
                z = K.bias_add(z, self.bias)
            z0 = z[:, :self.units]
            z1 = z[:, self.units: 2 * self.units]
            z2 = z[:, 2 * self.units: 3 * self.units]
            z3 = z[:, 3 * self.units:]
            i = self.recurrent_activation(z0)
            f = self.recurrent_activation(z1)
            c = f * c_tm1 + i * self.activation(z2)
            o = self.recurrent_activation(z3)
        h = o * self.activation(c)
        if 0 < self.dropout + self.recurrent_dropout:
            if training is None:
                h._uses_learning_phase = True
        return h, [h, c]

class LSTM(RNN):
    """"""Long-Short Term Memory layer - Hochreiter 1997.
    # Arguments
        units: Positive integer, dimensionality of the output space.
        activation: Activation function to use
            (see [activations](../activations.md)).
            If you pass None, no activation is applied
            (ie. ""linear"" activation: `a(x) = x`).
        recurrent_activation: Activation function to use
            for the recurrent step
            (see [activations](../activations.md)).
        use_bias: Boolean, whether the layer uses a bias vector.
        kernel_initializer: Initializer for the `kernel` weights matrix,
            used for the linear transformation of the inputs.
            (see [initializers](../initializers.md)).
        recurrent_initializer: Initializer for the `recurrent_kernel`
            weights matrix,
            used for the linear transformation of the recurrent state.
            (see [initializers](../initializers.md)).
        bias_initializer: Initializer for the bias vector
            (see [initializers](../initializers.md)).
        unit_forget_bias: Boolean.
            If True, add 1 to the bias of the forget gate at initialization.
            Setting it to true will also force `bias_initializer=""zeros""`.
            This is recommended in [Jozefowicz et al.](http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf)
        kernel_regularizer: Regularizer function applied to
            the `kernel` weights matrix
            (see [regularizer](../regularizers.md)).
        recurrent_regularizer: Regularizer function applied to
            the `recurrent_kernel` weights matrix
            (see [regularizer](../regularizers.md)).
        bias_regularizer: Regularizer function applied to the bias vector
            (see [regularizer](../regularizers.md)).
        activity_regularizer: Regularizer function applied to
            the output of the layer (its ""activation"").
            (see [regularizer](../regularizers.md)).
        kernel_constraint: Constraint function applied to
            the `kernel` weights matrix
            (see [constraints](../constraints.md)).","[37, 38, 39, 40]"
"def validate_vrf(value, module):
    out = run_commands(module, ['show vrf'])
    configured_vrfs = []
    lines = out[0].strip().splitlines()[3:]
    for l in lines:
        if not l:
            continue
        splitted_line = re.split(r'\s{2,}', l.strip())
        if len(splitted_line) > 2:
            configured_vrfs.append(splitted_line[0])
    configured_vrfs.append('default')
    if value not in configured_vrfs:
        module.fail_json(msg='vrf `%s` is not configured on the system' % value)

def map_obj_to_commands(updates, module, warnings):
    commands = list()
    want, have = updates
    def needs_update(x):
        return want.get(x) is not None and (want.get(x) != have.get(x))
    def add(cmd):
        if 'management api http-commands' not in commands:
            commands.insert(0, 'management api http-commands')
        commands.append(cmd)
    if any((needs_update('http'), needs_update('http_port'))):
        if want['http'] is False:
            add('no protocol http')
        else:
            if have['http'] is False and want['http'] in (False, None):
                warnings.append('protocol http is not enabled, not configuring http port value')
            else:
                port = want['http_port'] or 80
                add('protocol http port %s' % port)
    if any((needs_update('https'), needs_update('https_port'))):
        if want['https'] is False:
            add('no protocol https')
        else:
            if have['https'] is False and want['https'] in (False, None):
                warnings.append('protocol https is not enabled, not configuring https port value')
            else:
                port = want['https_port'] or 443
                add('protocol https port %s' % port)
    if any((needs_update('local_http'), needs_update('local_http_port'))):
        if want['local_http'] is False:
            add('no protocol http localhost')
        else:
            if have['local_http'] is False and want['local_http'] in (False, None):
                warnings.append('protocol local_http is not enabled, not configuring local_http port value')
            else:
                port = want['local_http_port'] or 8080
                add('protocol http localhost port %s' % port)
    if any((needs_update('socket'), needs_update('socket'))):
        if want['socket'] is False:
            add('no protocol unix-socket')
        else:
            add('protocol unix-socket')
    if needs_update('state') and not needs_update('vrf'):
        if want['state'] == 'stopped':
            add('shutdown')
        elif want['state'] == 'started':
            add('no shutdown')
    if needs_update('vrf'):
        add('vrf %s' % want['vrf'])
        # switching operational vrfs here
        # need to add the desired state as well
        if want['state'] == 'stopped':
            add('shutdown')
        elif want['state'] == 'started':
            add('no shutdown')
    return commands

def parse_state(data):
    if data[0]['enabled']:
        return 'started'
    else:
        return 'stopped'

def map_config_to_obj(module):
    out = run_commands(module, ['show management api http-commands | json'])
    return {
        'http': out[0]['httpServer']['configured'],
        'http_port': out[0]['httpServer']['port'],
        'https': out[0]['httpsServer']['configured'],
        'https_port': out[0]['httpsServer']['port'],
        'local_http': out[0]['localHttpServer']['configured'],
        'local_http_port': out[0]['localHttpServer']['port'],
        'socket': out[0]['unixSocketServer']['configured'],
        'vrf': out[0]['vrf'] or ""default"",
        'state': parse_state(out)
    }

def map_params_to_obj(module):
    obj = {
        'http': module.params['http'],
        'http_port': module.params['http_port'],
        'https': module.params['https'],
        'https_port': module.params['https_port'],
        'local_http': module.params['local_http'],
        'local_http_port': module.params['local_http_port'],
        'socket': module.params['socket'],
        'vrf': module.params['vrf'],
        'state': module.params['state']
    }
    for key, value in iteritems(obj):
        if value:
            validator = globals().get('validate_%s' % key)
            if validator:
                validator(value, module)
    return obj
",[65]
"    round
    floor
    ceil
    to_frame
    mean
    See Also
    --------
    Index : The base pandas Index type.
    Timedelta : Represents a duration between two dates or times.
    DatetimeIndex : Index of datetime64 data.
    PeriodIndex : Index of Period data.
    timedelta_range : Create a fixed-frequency TimedeltaIndex.
    Notes
    -----
    To learn more about the frequency strings, please see `this link
    <https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases>`__.
    """"""
    _typ = ""timedeltaindex""
    _engine_type = libindex.TimedeltaEngine
    _comparables = [""name"", ""freq""]
    _attributes = [""name"", ""freq""]
    _is_numeric_dtype = True
    _infer_as_myclass = True
    # -------------------------------------------------------------------
    # Constructors
    def __new__(
        cls,
        data=None,
        unit=None,
        freq=None,
        closed=None,
        dtype=_TD_DTYPE,
        copy=False,
        name=None,
    ):
        name = maybe_extract_name(name, data, cls)
        if is_scalar(data):
            raise TypeError(
                f""{cls.__name__}() must be called with a ""
                f""collection of some kind, {repr(data)} was passed""
            )
        if unit in {""Y"", ""y"", ""M""}:
            raise ValueError(
                ""Units 'M' and 'Y' are no longer supported, as they do not ""
                ""represent unambiguous timedelta values durations.""
            )
        if isinstance(data, TimedeltaArray):
            if copy:
                data = data.copy()
            return cls._simple_new(data, name=name, freq=freq)
        if isinstance(data, TimedeltaIndex) and freq is None and name is None:
            if copy:
                return data.copy()
            else:
                return data._shallow_copy()
        # - Cases checked above all return/raise before reaching here - #
        tdarr = TimedeltaArray._from_sequence(
            data, freq=freq, unit=unit, dtype=dtype, copy=copy
        )
        return cls._simple_new(tdarr, name=name)
    @classmethod
    def _simple_new(cls, values, name=None, freq=None, dtype=_TD_DTYPE):
        # `dtype` is passed by _shallow_copy in corner cases, should always
        #  be timedelta64[ns] if present
        if not isinstance(values, TimedeltaArray):
            values = TimedeltaArray._simple_new(values, dtype=dtype, freq=freq)
        else:
            if freq is None:
                freq = values.freq
        assert isinstance(values, TimedeltaArray), type(values)
        assert dtype == _TD_DTYPE, dtype
        assert values.dtype == ""m8[ns]"", values.dtype
        tdarr = TimedeltaArray._simple_new(values._data, freq=freq)
        result = object.__new__(cls)
        result._data = tdarr
        result._name = name
        # For groupby perf. See note in indexes/base about _index_data
        result._index_data = tdarr._data
        result._reset_identity()
        return result
    # -------------------------------------------------------------------
    # Rendering Methods
    @property
    def _formatter_func(self):
        from pandas.io.formats.format import _get_format_timedelta64
        return _get_format_timedelta64(self, box=True)
    # -------------------------------------------------------------------
    @Appender(_index_shared_docs[""astype""])
    def astype(self, dtype, copy=True):
        dtype = pandas_dtype(dtype)
        if is_timedelta64_dtype(dtype) and not is_timedelta64_ns_dtype(dtype):
            # Have to repeat the check for 'timedelta64' (not ns) dtype
            #  so that we can return a numeric index, since pandas will return
            #  a TimedeltaIndex when dtype='timedelta'
            result = self._data.astype(dtype, copy=copy)
            if self.hasnans:
                return Index(result, name=self.name)
            return Index(result.astype(""i8""), name=self.name)
        return DatetimeIndexOpsMixin.astype(self, dtype, copy=copy)
    def _maybe_promote(self, other):
        if other.inferred_type == ""timedelta"":
            other = TimedeltaIndex(other)
        return self, other
",[56]
"""""""Built-in metrics.
""""""
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function
import six
from . import backend as K
from .losses import mean_squared_error
from .losses import mean_absolute_error
from .losses import mean_absolute_percentage_error
from .losses import mean_squared_logarithmic_error
from .losses import hinge
from .losses import logcosh
from .losses import squared_hinge
from .losses import categorical_crossentropy
from .losses import sparse_categorical_crossentropy
from .losses import binary_crossentropy
from .losses import kullback_leibler_divergence
from .losses import poisson
from .losses import cosine_proximity
from .utils.generic_utils import deserialize_keras_object
from .utils.generic_utils import serialize_keras_object

def binary_accuracy(y_true, y_pred):
    return K.mean(K.equal(y_true, K.round(y_pred)), axis=-1)

def categorical_accuracy(y_true, y_pred):
    return K.cast(K.equal(K.argmax(y_true, axis=-1),
                          K.argmax(y_pred, axis=-1)),
                  K.floatx())

def sparse_categorical_accuracy(y_true, y_pred):
    # flatten y_true in case it's in shape (num_samples, 1) instead of (num_samples,)
    return K.cast(K.equal(K.flatten(y_true),
                          K.cast(K.argmax(y_pred, axis=-1), K.floatx())),
                  K.floatx())

def top_k_categorical_accuracy(y_true, y_pred, k=5):
    return K.mean(K.in_top_k(y_pred, K.argmax(y_true, axis=-1), k), axis=-1)

def sparse_top_k_categorical_accuracy(y_true, y_pred, k=5):
    # If the shape of y_true is (num_samples, 1), flatten to (num_samples,)
    return K.mean(K.in_top_k(y_pred, K.cast(K.flatten(y_true), 'int32'), k),
                  axis=-1)

# Aliases
mse = MSE = mean_squared_error
mae = MAE = mean_absolute_error
mape = MAPE = mean_absolute_percentage_error
msle = MSLE = mean_squared_logarithmic_error
cosine = cosine_proximity

def serialize(metric):
    return serialize_keras_object(metric)

def deserialize(config, custom_objects=None):
    return deserialize_keras_object(config,
                                    module_objects=globals(),
                                    custom_objects=custom_objects,
                                    printable_module_name='metric function')

def get(identifier):
    if isinstance(identifier, dict):
        config = {'class_name': str(identifier), 'config': {}}
        return deserialize(config)
    elif isinstance(identifier, six.string_types):
        return deserialize(str(identifier))
    elif callable(identifier):
        return identifier
    else:
        raise ValueError('Could not interpret '
                         'metric function identifier:', identifier)","[37, 38, 39]"
"def normalize_padding(value):
    padding = value.lower()
    allowed = {'valid', 'same', 'causal'}
    if K.backend() == 'theano':
        allowed.add('full')
    if padding not in allowed:
        raise ValueError('The `padding` argument must be one of ""valid"", ""same"" '
                         '(or ""causal"" for Conv1D). Received: ' + str(padding))
    return padding

def convert_kernel(kernel):
    """"""Converts a Numpy kernel matrix from Theano format to TensorFlow format.
    Also works reciprocally, since the transformation is its own inverse.
    # Arguments
        kernel: Numpy array (3D, 4D or 5D).
    # Returns
        The converted kernel.
    # Raises
        ValueError: in case of invalid kernel shape or invalid data_format.
    """"""
    kernel = np.asarray(kernel)
    if not 3 <= kernel.ndim <= 5:
        raise ValueError('Invalid kernel shape:', kernel.shape)
    slices = [slice(None, None, -1) for _ in range(kernel.ndim)]
    no_flip = (slice(None, None), slice(None, None))
    slices[-2:] = no_flip
    return np.copy(kernel[slices])

def conv_output_length(input_length, filter_size,
                       padding, stride, dilation=1):
    """"""Determines output length of a convolution given input length.
    # Arguments
        input_length: integer.
        filter_size: integer.
        padding: one of `""same""`, `""valid""`, `""full""`.
        stride: integer.
        dilation: dilation rate, integer.
    # Returns
        The output length (integer).
    """"""
    if input_length is None:
        return None
    assert padding in {'same', 'valid', 'full', 'causal'}
    dilated_filter_size = filter_size + (filter_size - 1) * (dilation - 1)
    if padding == 'same':
        output_length = input_length
    elif padding == 'valid':
        output_length = input_length - dilated_filter_size + 1
    elif padding == 'causal':
        output_length = input_length
    elif padding == 'full':
        output_length = input_length + dilated_filter_size - 1
    return (output_length + stride - 1) // stride

def conv_input_length(output_length, filter_size, padding, stride):
    """"""Determines input length of a convolution given output length.
    # Arguments
        output_length: integer.
        filter_size: integer.
        padding: one of `""same""`, `""valid""`, `""full""`.
        stride: integer.
    # Returns
        The input length (integer).
    """"""
    if output_length is None:
        return None
    assert padding in {'same', 'valid', 'full'}
    if padding == 'same':
        pad = filter_size // 2
    elif padding == 'valid':
        pad = 0
    elif padding == 'full':
        pad = filter_size - 1
    return (output_length - 1) * stride - 2 * pad + filter_size

def deconv_length(dim_size, stride_size, kernel_size, padding, output_padding):
    """"""Determines output length of a transposed convolution given input length.
    # Arguments
        dim_size: Integer, the input length.
        stride_size: Integer, the stride along the dimension of `dim_size`.
        kernel_size: Integer, the kernel size along the dimension of
            `dim_size`.
        padding: One of `""same""`, `""valid""`, `""full""`.
        output_padding: Integer, amount of padding along the output dimension,
            Can be set to `None` in which case the output length is inferred.
    # Returns
        The output length (integer).
    """"""
    assert padding in {'same', 'valid', 'full'}
    if dim_size is None:
        return None
    # Infer length if output padding is None, else compute the exact length
    if output_padding is None:
        if padding == 'valid':
            dim_size = dim_size * stride_size + max(kernel_size - stride_size, 0)
        elif padding == 'full':
            dim_size = dim_size * stride_size - (stride_size + kernel_size - 2)
        elif padding == 'same':
            dim_size = dim_size * stride_size
    else:
        if padding == 'same':
            pad = kernel_size // 2
        elif padding == 'valid':
            pad = 0
        elif padding == 'full':
            pad = kernel_size - 1
        dim_size = ((dim_size - 1) * stride_size + kernel_size - 2 * pad +
                    output_padding)
",[88]
"    @property
    def table_type(self):
        """"""
        Return table type (i.e. 'temp').
        """"""
        return ''
    @property
    def queries(self):
        """"""
        Override to return a list of queries to be executed in order.
        """"""
        return []
    def truncate_table(self, connection):
        query = ""truncate %s"" % self.table
        cursor = connection.cursor()
        try:
            cursor.execute(query)
        finally:
            cursor.close()
    def prune(self, connection):
        query = ""delete from %s where %s >= %s"" % (self.prune_table, self.prune_column, self.prune_date)
        cursor = connection.cursor()
        try:
            cursor.execute(query)
        finally:
            cursor.close()
    def create_table(self, connection):
        """"""
        Override to provide code for creating the target table.
        By default it will be created using types (optionally)
        specified in columns.
        If overridden, use the provided connection object for
        setting up the table in order to create the table and
        insert data using the same transaction.
        """"""
        if len(self.columns[0]) == 1:
            # only names of columns specified, no types
            raise NotImplementedError(""create_table() not implemented ""
                                      ""for %r and columns types not ""
                                      ""specified"" % self.table)
        elif len(self.columns[0]) == 2:
            # if columns is specified as (name, type) tuples
            coldefs = ','.join(
                '{name} {type}'.format(
                    name=name,
                    type=type) for name, type in self.columns
            )
            query = (""CREATE {type} TABLE ""
                     ""{table} ({coldefs}) ""
                     ""{table_attributes}"").format(
                type=self.table_type,
                table=self.table,
                coldefs=coldefs,
                table_attributes=self.table_attributes)
            connection.cursor().execute(query)
        elif len(self.columns[0]) == 3:
            # if columns is specified as (name, type, encoding) tuples
            # possible column encodings: https://docs.aws.amazon.com/redshift/latest/dg/c_Compression_encodings.html
            coldefs = ','.join(
                '{name} {type} ENCODE {encoding}'.format(
                    name=name,
                    type=type,
                    encoding=encoding) for name, type, encoding in self.columns
            )
            query = (""CREATE {type} TABLE ""
                     ""{table} ({coldefs}) ""
                     ""{table_attributes}"").format(
                type=self.table_type,
                table=self.table,
                coldefs=coldefs,
                table_attributes=self.table_attributes)
            connection.cursor().execute(query)
        else:
            raise ValueError(""create_table() found no columns for %r""
                             % self.table)
    def run(self):
        """"""
        If the target table doesn't exist, self.create_table
        will be called to attempt to create the table.
        """"""
        if not (self.table):
            raise Exception(""table need to be specified"")
        path = self.s3_load_path()
        output = self.output()
        connection = output.connect()
        cursor = connection.cursor()
        self.init_copy(connection)
        self.copy(cursor, path)
        self.post_copy(cursor)
        # update marker table
        output.touch(connection)
        connection.commit()
        # commit and clean up
        connection.close()
    def copy(self, cursor, f):
        """"""
        Defines copying from s3 into redshift.
        If both key-based and role-based credentials are provided, role-based will be used.
        """"""
        logger.info(""Inserting file: %s"", f)
        colnames = ''
        if len(self.columns) > 0:
            colnames = "","".join([x[0] for x in self.columns])
            colnames = '({})'.format(colnames)
        cursor.execute(""""""
         COPY {table} {colnames} from '{source}'
         CREDENTIALS '{creds}'
         {options}
         ;"""""".format(
            table=self.table,
            colnames=colnames,",[116]
".. |m21| image:: /_static/markers/m21.png
.. |m22| image:: /_static/markers/m22.png
.. |m23| image:: /_static/markers/m23.png
.. |m24| image:: /_static/markers/m24.png
.. |m25| image:: /_static/markers/m25.png
.. |m26| image:: /_static/markers/m26.png
.. |m27| image:: /_static/markers/m27.png
.. |m28| image:: /_static/markers/m28.png
.. |m29| image:: /_static/markers/m29.png
.. |m30| image:: /_static/markers/m30.png
.. |m31| image:: /_static/markers/m31.png
.. |m32| image:: /_static/markers/m32.png
.. |m33| image:: /_static/markers/m33.png
.. |m34| image:: /_static/markers/m34.png
.. |m35| image:: /_static/markers/m35.png
.. |m36| image:: /_static/markers/m36.png
.. |m37| image:: /_static/markers/m37.png
""""""
from collections.abc import Sized
import numpy as np
from . import cbook, rcParams
from .path import Path
from .transforms import IdentityTransform, Affine2D
# special-purpose marker identifiers:
(TICKLEFT, TICKRIGHT, TICKUP, TICKDOWN,
 CARETLEFT, CARETRIGHT, CARETUP, CARETDOWN,
 CARETLEFTBASE, CARETRIGHTBASE, CARETUPBASE, CARETDOWNBASE) = range(12)
_empty_path = Path(np.empty((0, 2)))

class MarkerStyle:
    markers = {
        '.': 'point',
        ',': 'pixel',
        'o': 'circle',
        'v': 'triangle_down',
        '^': 'triangle_up',
        '<': 'triangle_left',
        '>': 'triangle_right',
        '1': 'tri_down',
        '2': 'tri_up',
        '3': 'tri_left',
        '4': 'tri_right',
        '8': 'octagon',
        's': 'square',
        'p': 'pentagon',
        '*': 'star',
        'h': 'hexagon1',
        'H': 'hexagon2',
        '+': 'plus',
        'x': 'x',
        'D': 'diamond',
        'd': 'thin_diamond',
        '|': 'vline',
        '_': 'hline',
        'P': 'plus_filled',
        'X': 'x_filled',
        TICKLEFT: 'tickleft',
        TICKRIGHT: 'tickright',
        TICKUP: 'tickup',
        TICKDOWN: 'tickdown',
        CARETLEFT: 'caretleft',
        CARETRIGHT: 'caretright',
        CARETUP: 'caretup',
        CARETDOWN: 'caretdown',
        CARETLEFTBASE: 'caretleftbase',
        CARETRIGHTBASE: 'caretrightbase',
        CARETUPBASE: 'caretupbase',
        CARETDOWNBASE: 'caretdownbase',
        ""None"": 'nothing',
        None: 'nothing',
        ' ': 'nothing',
        '': 'nothing'
    }
    # Just used for informational purposes.  is_filled()
    # is calculated in the _set_* functions.
    filled_markers = (
        'o', 'v', '^', '<', '>', '8', 's', 'p', '*', 'h', 'H', 'D', 'd',
        'P', 'X')
    fillstyles = ('full', 'left', 'right', 'bottom', 'top', 'none')
    _half_fillstyles = ('left', 'right', 'bottom', 'top')
    # TODO: Is this ever used as a non-constant?
    _point_size_reduction = 0.5
    def __init__(self, marker=None, fillstyle=None):
        """"""
        Attributes
        ----------
        markers : list of known marks
        fillstyles : list of known fillstyles
        filled_markers : list of known filled markers.
        Parameters
        ----------
        marker : str or array-like, default: None
            See the descriptions of possible markers in the module docstring.
        fillstyle : str, default: 'full'
            One of 'full', 'left', 'right', 'bottom', 'top', 'none'.
        """"""
        self._marker_function = None
        self.set_fillstyle(fillstyle)
        self.set_marker(marker)
    def _recache(self):
        if self._marker_function is None:
            return
        self._path = _empty_path
        self._transform = IdentityTransform()
        self._alt_path = None
        self._alt_transform = None
        self._snap_threshold = None
        self._joinstyle = 'round'
        self._capstyle = 'butt'
        self._filled = True
        self._marker_function()",[125]
"                'total': memstats.get('swaptotal'),
                'free': memstats.get('swapfree'),
                'used': memstats.get('swap:used'),
                'cached': memstats.get('swapcached'),
            },
        }
        return memory_facts
    def get_cpu_facts(self, collected_facts=None):
        cpu_facts = {}
        collected_facts = collected_facts or {}
        i = 0
        vendor_id_occurrence = 0
        model_name_occurrence = 0
        processor_occurence = 0
        physid = 0
        coreid = 0
        sockets = {}
        cores = {}
        xen = False
        xen_paravirt = False
        try:
            if os.path.exists('/proc/xen'):
                xen = True
            else:
                for line in get_file_lines('/sys/hypervisor/type'):
                    if line.strip() == 'xen':
                        xen = True
                    # Only interested in the first line
                    break
        except IOError:
            pass
        if not os.access(""/proc/cpuinfo"", os.R_OK):
            return cpu_facts
        cpu_facts['processor'] = []
        for line in get_file_lines('/proc/cpuinfo'):
            data = line.split("":"", 1)
            key = data[0].strip()
            try:
                val = data[1].strip()
            except IndexError:
                val = """"
            if xen:
                if key == 'flags':
                    # Check for vme cpu flag, Xen paravirt does not expose this.
                    #   Need to detect Xen paravirt because it exposes cpuinfo
                    #   differently than Xen HVM or KVM and causes reporting of
                    #   only a single cpu core.
                    if 'vme' not in val:
                        xen_paravirt = True
            # model name is for Intel arch, Processor (mind the uppercase P)
            # works for some ARM devices, like the Sheevaplug.
            # 'ncpus active' is SPARC attribute
            if key in ['model name', 'Processor', 'vendor_id', 'cpu', 'Vendor', 'processor']:
                if 'processor' not in cpu_facts:
                    cpu_facts['processor'] = []
                cpu_facts['processor'].append(val)
                if key == 'vendor_id':
                    vendor_id_occurrence += 1
                if key == 'model name':
                    model_name_occurrence += 1
                if key == 'processor':
                    processor_occurence += 1
                i += 1
            elif key == 'physical id':
                physid = val
                if physid not in sockets:
                    sockets[physid] = 1
            elif key == 'core id':
                coreid = val
                if coreid not in sockets:
                    cores[coreid] = 1
            elif key == 'cpu cores':
                sockets[physid] = int(val)
            elif key == 'siblings':
                cores[coreid] = int(val)
            elif key == '# processors':
                cpu_facts['processor_cores'] = int(val)
            elif key == 'ncpus active':
                i = int(val)
        # Skip for platforms without vendor_id/model_name in cpuinfo (e.g ppc64le)
        if vendor_id_occurrence > 0:
            if vendor_id_occurrence == model_name_occurrence:
                i = vendor_id_occurrence
        # The fields for ARM CPUs do not always include 'vendor_id' or 'model name',
        # and sometimes includes both 'processor' and 'Processor'.
        # Always use 'processor' count for ARM systems
        if collected_facts.get('ansible_architecture', '').startswith(('armv', 'aarch')):
            i = processor_occurence
        # FIXME
        if collected_facts.get('ansible_architecture') != 's390x':
            if xen_paravirt:
                cpu_facts['processor_count'] = i
                cpu_facts['processor_cores'] = i
                cpu_facts['processor_threads_per_core'] = 1
                cpu_facts['processor_vcpus'] = i
            else:
                if sockets:
                    cpu_facts['processor_count'] = len(sockets)
                else:
                    cpu_facts['processor_count'] = i
                socket_values = list(sockets.values())
                if socket_values and socket_values[0]:
                    cpu_facts['processor_cores'] = socket_values[0]
                else:
                    cpu_facts['processor_cores'] = 1
                core_values = list(cores.values())
                if core_values:
                    cpu_facts['processor_threads_per_core'] = core_values[0] // cpu_facts['processor_cores']
                else:
                    cpu_facts['processor_threads_per_core'] = 1 // cpu_facts['processor_cores']
                cpu_facts['processor_vcpus'] = (cpu_facts['processor_threads_per_core'] *
                                                cpu_facts['processor_count'] * cpu_facts['processor_cores'])",[97]
""""""" parquet compat """"""
from warnings import catch_warnings
from pandas.compat._optional import import_optional_dependency
from pandas.errors import AbstractMethodError
from pandas import DataFrame, get_option
from pandas.io.common import get_filepath_or_buffer, is_s3_url

def get_engine(engine):
    """""" return our implementation """"""
    if engine == ""auto"":
        engine = get_option(""io.parquet.engine"")
    if engine == ""auto"":
        # try engines in this order
        try:
            return PyArrowImpl()
        except ImportError:
            pass
        try:
            return FastParquetImpl()
        except ImportError:
            pass
        raise ImportError(
            ""Unable to find a usable engine; ""
            ""tried using: 'pyarrow', 'fastparquet'.\n""
            ""pyarrow or fastparquet is required for parquet ""
            ""support""
        )
    if engine not in [""pyarrow"", ""fastparquet""]:
        raise ValueError(""engine must be one of 'pyarrow', 'fastparquet'"")
    if engine == ""pyarrow"":
        return PyArrowImpl()
    elif engine == ""fastparquet"":
        return FastParquetImpl()

class BaseImpl:
    api = None  # module
    @staticmethod
    def validate_dataframe(df):
        if not isinstance(df, DataFrame):
            raise ValueError(""to_parquet only supports IO with DataFrames"")
        # must have value column names (strings only)
        if df.columns.inferred_type not in {""string"", ""unicode"", ""empty""}:
            raise ValueError(""parquet must have string column names"")
        # index level names must be strings
        valid_names = all(
            isinstance(name, str) for name in df.index.names if name is not None
        )
        if not valid_names:
            raise ValueError(""Index level names must be strings"")
    def write(self, df, path, compression, **kwargs):
        raise AbstractMethodError(self)
    def read(self, path, columns=None, **kwargs):
        raise AbstractMethodError(self)

class PyArrowImpl(BaseImpl):
    def __init__(self):
        pyarrow = import_optional_dependency(
            ""pyarrow"", extra=""pyarrow is required for parquet support.""
        )
        import pyarrow.parquet
        self.api = pyarrow
    def write(
        self,
        df,
        path,
        compression=""snappy"",
        coerce_timestamps=""ms"",
        index=None,
        partition_cols=None,
        **kwargs
    ):
        self.validate_dataframe(df)
        path, _, _, _ = get_filepath_or_buffer(path, mode=""wb"")
        if index is None:
            from_pandas_kwargs = {}
        else:
            from_pandas_kwargs = {""preserve_index"": index}
        table = self.api.Table.from_pandas(df, **from_pandas_kwargs)
        if partition_cols is not None:
            self.api.parquet.write_to_dataset(
                table,
                path,
                compression=compression,
                coerce_timestamps=coerce_timestamps,
                partition_cols=partition_cols,
                **kwargs
            )
        else:
            self.api.parquet.write_table(
                table,
                path,
                compression=compression,
                coerce_timestamps=coerce_timestamps,
                **kwargs
            )
    def read(self, path, columns=None, **kwargs):
        path, _, _, should_close = get_filepath_or_buffer(path)
        kwargs[""use_pandas_metadata""] = True
        result = self.api.parquet.read_table(
            path, columns=columns, **kwargs
        ).to_pandas()
        if should_close:",[9]
"        self.tracking_url = tracking_url
        self.status_message = status_message
        self.scheduler_disable_time = None
        self.runnable = False
    def __repr__(self):
        return ""Task(%r)"" % vars(self)
    def add_failure(self):
        self.failures.add_failure()
    def has_excessive_failures(self):
        if (self.failures.first_failure_time is not None and
                self.disable_hard_timeout):
            if (time.time() >= self.failures.first_failure_time +
                    self.disable_hard_timeout):
                return True
        if self.failures.num_failures() >= self.disable_failures:
            return True
        return False
    def can_disable(self):
        return (self.disable_failures is not None or
                self.disable_hard_timeout is not None)
    @property
    def pretty_id(self):
        param_str = ', '.join('{}={}'.format(key, value) for key, value in self.params.items())
        return '{}({})'.format(self.family, param_str)

class Worker(object):
    """"""
    Structure for tracking worker activity and keeping their references.
    """"""
    def __init__(self, worker_id, last_active=None):
        self.id = worker_id
        self.reference = None  # reference to the worker in the real world. (Currently a dict containing just the host)
        self.last_active = last_active or time.time()  # seconds since epoch
        self.last_get_work = None
        self.started = time.time()  # seconds since epoch
        self.tasks = set()  # task objects
        self.info = {}
        self.disabled = False
    def add_info(self, info):
        self.info.update(info)
    def update(self, worker_reference, get_work=False):
        if worker_reference:
            self.reference = worker_reference
        self.last_active = time.time()
        if get_work:
            self.last_get_work = time.time()
    def prune(self, config):
        # Delete workers that haven't said anything for a while (probably killed)
        if self.last_active + config.worker_disconnect_delay < time.time():
            return True
    def get_pending_tasks(self, state):
        """"""
        Get PENDING (and RUNNING) tasks for this worker.
        You have to pass in the state for optimization reasons.
        """"""
        if len(self.tasks) < state.num_pending_tasks():
            return six.moves.filter(lambda task: task.status in [PENDING, RUNNING],
                                    self.tasks)
        else:
            return state.get_pending_tasks()
    def is_trivial_worker(self, state):
        """"""
        If it's not an assistant having only tasks that are without
        requirements.
        We have to pass the state parameter for optimization reasons.
        """"""
        if self.assistant:
            return False
        return all(not task.resources for task in self.get_pending_tasks(state))
    @property
    def assistant(self):
        return self.info.get('assistant', False)
    def __str__(self):
        return self.id

class SimpleTaskState(object):
    """"""
    Keep track of the current state and handle persistance.
    The point of this class is to enable other ways to keep state, eg. by using a database
    These will be implemented by creating an abstract base class that this and other classes
    inherit from.
    """"""
    def __init__(self, state_path):
        self._state_path = state_path
        self._tasks = {}  # map from id to a Task object
        self._status_tasks = collections.defaultdict(dict)
        self._active_workers = {}  # map from id to a Worker object
    def get_state(self):
        return self._tasks, self._active_workers
    def set_state(self, state):
        self._tasks, self._active_workers = state
    def dump(self):
        try:
            with open(self._state_path, 'wb') as fobj:
                pickle.dump(self.get_state(), fobj)
        except IOError:
            logger.warning(""Failed saving scheduler state"", exc_info=1)
        else:
            logger.info(""Saved state in %s"", self._state_path)
    # prone to lead to crashes when old state is unpickled with updated code. TODO some kind of version control?
    def load(self):
        if os.path.exists(self._state_path):","[12, 13, 23, 24, 25, 26]"
"        if isinstance(index, MultiIndex):
            loc, new_index = self.index.get_loc_level(key, drop_level=drop_level)
        else:
            loc = self.index.get_loc(key)
            if isinstance(loc, np.ndarray):
                if loc.dtype == np.bool_:
                    (inds,) = loc.nonzero()
                    return self._take_with_is_copy(inds, axis=axis)
                else:
                    return self._take_with_is_copy(loc, axis=axis)
            if not is_scalar(loc):
                new_index = self.index[loc]
        if is_scalar(loc):
            # In this case loc should be an integer
            if self.ndim == 1:
                # if we encounter an array-like and we only have 1 dim
                # that means that their are list/ndarrays inside the Series!
                # so just return them (GH 6394)
                return self._values[loc]
            new_values = self._data.fast_xs(loc)
            result = self._constructor_sliced(
                new_values,
                index=self.columns,
                name=self.index[loc],
                dtype=new_values.dtype,
            )
        else:
            result = self.iloc[loc]
            result.index = new_index
        # this could be a view
        # but only in a single-dtyped view sliceable case
        result._set_is_copy(self, copy=not result._is_view)
        return result
    _xs: Callable = xs
    def __getitem__(self, item):
        raise AbstractMethodError(self)
    def _get_item_cache(self, item):
        """"""Return the cached item, item represents a label indexer.""""""
        cache = self._item_cache
        res = cache.get(item)
        if res is None:
            values = self._data.get(item)
            res = self._box_item_values(item, values)
            cache[item] = res
            res._set_as_cached(item, self)
            # for a chain
            res._is_copy = self._is_copy
        return res
    def _iget_item_cache(self, item):
        """"""Return the cached item, item represents a positional indexer.""""""
        ax = self._info_axis
        if ax.is_unique:
            lower = self._get_item_cache(ax[item])
        else:
            lower = self._take_with_is_copy(item, axis=self._info_axis_number)
        return lower
    def _box_item_values(self, key, values):
        raise AbstractMethodError(self)
    def _slice(self: FrameOrSeries, slobj: slice, axis=0) -> FrameOrSeries:
        """"""
        Construct a slice of this container.
        Slicing with this method is *always* positional.
        """"""
        assert isinstance(slobj, slice), type(slobj)
        axis = self._get_block_manager_axis(axis)
        result = self._constructor(self._data.get_slice(slobj, axis=axis))
        result = result.__finalize__(self)
        # this could be a view
        # but only in a single-dtyped view sliceable case
        is_copy = axis != 0 or result._is_view
        result._set_is_copy(self, copy=is_copy)
        return result
    def _set_item(self, key, value) -> None:
        self._data.set(key, value)
        self._clear_item_cache()
    def _set_is_copy(self, ref, copy: bool_t = True) -> None:
        if not copy:
            self._is_copy = None
        else:
            assert ref is not None
            self._is_copy = weakref.ref(ref)
    def _check_is_chained_assignment_possible(self) -> bool_t:
        """"""
        Check if we are a view, have a cacher, and are of mixed type.
        If so, then force a setitem_copy check.
        Should be called just near setting a value
        Will return a boolean if it we are a view and are cached, but a
        single-dtype meaning that the cacher should be updated following
        setting.
        """"""
        if self._is_view and self._is_cached:
            ref = self._get_cacher()
            if ref is not None and ref._is_mixed_type:
                self._check_setitem_copy(stacklevel=4, t=""referant"", force=True)
            return True
        elif self._is_copy:
            self._check_setitem_copy(stacklevel=4, t=""referant"")
        return False
    def _check_setitem_copy(self, stacklevel=4, t=""setting"", force=False):
        """"""
        Parameters
        ----------
        stacklevel : int, default 4
           the level to show of the stack when the error is output","[60, 66]"
"    def get_minpos(self):
        raise NotImplementedError()

def _make_getset_interval(method_name, lim_name, attr_name):
    """"""
    Helper to generate ``get_{data,view}_interval`` and
    ``set_{data,view}_interval`` implementations.
    """"""
    def getter(self):
        # docstring inherited.
        return getattr(getattr(self.axes, lim_name), attr_name)
    def setter(self, vmin, vmax, ignore=False):
        # docstring inherited.
        if ignore:
            setattr(getattr(self.axes, lim_name), attr_name, (vmin, vmax))
        else:
            oldmin, oldmax = getter(self)
            if oldmin < oldmax:
                setter(self, min(vmin, vmax, oldmin), max(vmin, vmax, oldmax),
                       ignore=True)
            else:
                setter(self, max(vmin, vmax, oldmax), min(vmin, vmax, oldmin),
                       ignore=True)
        self.stale = True
    getter.__name__ = f""get_{method_name}_interval""
    setter.__name__ = f""set_{method_name}_interval""
    return getter, setter

class XAxis(Axis):
    __name__ = 'xaxis'
    axis_name = 'x'
    def contains(self, mouseevent):
        """"""Test whether the mouse event occurred in the x axis.
        """"""
        if self._contains is not None:
            return self._contains(self, mouseevent)
        x, y = mouseevent.x, mouseevent.y
        try:
            trans = self.axes.transAxes.inverted()
            xaxes, yaxes = trans.transform_point((x, y))
        except ValueError:
            return False, {}
        l, b = self.axes.transAxes.transform_point((0, 0))
        r, t = self.axes.transAxes.transform_point((1, 1))
        inaxis = 0 <= xaxes <= 1 and (
            b - self.pickradius < y < b or
            t < y < t + self.pickradius)
        return inaxis, {}
    def _get_tick(self, major):
        if major:
            tick_kw = self._major_tick_kw
        else:
            tick_kw = self._minor_tick_kw
        return XTick(self.axes, 0, '', major=major, **tick_kw)
    def _get_label(self):
        # x in axes coords, y in display coords (to be updated at draw
        # time by _update_label_positions)
        label = mtext.Text(x=0.5, y=0,
                           fontproperties=font_manager.FontProperties(
                               size=rcParams['axes.labelsize'],
                               weight=rcParams['axes.labelweight']),
                           color=rcParams['axes.labelcolor'],
                           verticalalignment='top',
                           horizontalalignment='center')
        label.set_transform(mtransforms.blended_transform_factory(
            self.axes.transAxes, mtransforms.IdentityTransform()))
        self._set_artist_props(label)
        self.label_position = 'bottom'
        return label
    def _get_offset_text(self):
        # x in axes coords, y in display coords (to be updated at draw time)
        offsetText = mtext.Text(x=1, y=0,
                                fontproperties=font_manager.FontProperties(
                                    size=rcParams['xtick.labelsize']),
                                color=rcParams['xtick.color'],
                                verticalalignment='top',
                                horizontalalignment='right')
        offsetText.set_transform(mtransforms.blended_transform_factory(
            self.axes.transAxes, mtransforms.IdentityTransform())
        )
        self._set_artist_props(offsetText)
        self.offset_text_position = 'bottom'
        return offsetText
    def set_label_position(self, position):
        """"""
        Set the label position (top or bottom)
        Parameters
        ----------
        position : {'top', 'bottom'}
        """"""
        if position == 'top':
            self.label.set_verticalalignment('baseline')
        elif position == 'bottom':
            self.label.set_verticalalignment('top')
        else:
            raise ValueError(""Position accepts only 'top' or 'bottom'"")
        self.label_position = position
        self.stale = True
    def _get_tick_boxes_siblings(self, renderer):
        """"""
        Get the bounding boxes for this `.axis` and its siblings
        as set by `.Figure.align_xlabels` or  `.Figure.align_ylablels`.
        By default it just gets bboxes for self.
        """"""
        bboxes = []
        bboxes2 = []
        # get the Grouper that keeps track of x-label groups for this figure
        grp = self.figure._align_xlabel_grp
        # if we want to align labels from other axes:
        for nn, axx in enumerate(grp.get_siblings(self.axes)):",[24]
"        """"""
        if cfg.get(""device"", -1) >= 0:
            util.use_gpu(cfg[""device""])
            if self.vocab.vectors.data.shape[1] >= 1:
                self.vocab.vectors.data = Model.ops.asarray(self.vocab.vectors.data)
        link_vectors_to_models(self.vocab)
        if self.vocab.vectors.data.shape[1]:
            cfg[""pretrained_vectors""] = self.vocab.vectors.name
        if sgd is None:
            sgd = create_default_optimizer(Model.ops)
        self._optimizer = sgd
        for name, proc in self.pipeline:
            if hasattr(proc, ""_rehearsal_model""):
                proc._rehearsal_model = deepcopy(proc.model)
        return self._optimizer
    def evaluate(
        self, docs_golds, verbose=False, batch_size=256, scorer=None, component_cfg=None
    ):
        """"""Evaluate a model's pipeline components.
        docs_golds (iterable): Tuples of `Doc` and `GoldParse` objects.
        verbose (bool): Print debugging information.
        batch_size (int): Batch size to use.
        scorer (Scorer): Optional `Scorer` to use. If not passed in, a new one
            will be created.
        component_cfg (dict): An optional dictionary with extra keyword
            arguments for specific components.
        RETURNS (Scorer): The scorer containing the evaluation results.
        DOCS: https://spacy.io/api/language#evaluate
        """"""
        if scorer is None:
            scorer = Scorer(pipeline=self.pipeline)
        if component_cfg is None:
            component_cfg = {}
        docs, golds = zip(*docs_golds)
        docs = [
            self.make_doc(doc) if isinstance(doc, basestring_) else doc for doc in docs
        ]
        golds = list(golds)
        for name, pipe in self.pipeline:
            kwargs = component_cfg.get(name, {})
            kwargs.setdefault(""batch_size"", batch_size)
            if not hasattr(pipe, ""pipe""):
                docs = _pipe(pipe, docs, kwargs)
            else:
                docs = pipe.pipe(docs, **kwargs)
        for doc, gold in zip(docs, golds):
            if not isinstance(gold, GoldParse):
                gold = GoldParse(doc, **gold)
            if verbose:
                print(doc)
            kwargs = component_cfg.get(""scorer"", {})
            kwargs.setdefault(""verbose"", verbose)
            scorer.score(doc, gold, **kwargs)
        return scorer
    @contextmanager
    def use_params(self, params, **cfg):
        """"""Replace weights of models in the pipeline with those provided in the
        params dictionary. Can be used as a contextmanager, in which case,
        models go back to their original weights after the block.
        params (dict): A dictionary of parameters keyed by model ID.
        **cfg: Config parameters.
        EXAMPLE:
            >>> with nlp.use_params(optimizer.averages):
            >>>     nlp.to_disk('/tmp/checkpoint')
        """"""
        contexts = [
            pipe.use_params(params)
            for name, pipe in self.pipeline
            if hasattr(pipe, ""use_params"")
        ]
        # TODO: Having trouble with contextlib
        # Workaround: these aren't actually context managers atm.
        for context in contexts:
            try:
                next(context)
            except StopIteration:
                pass
        yield
        for context in contexts:
            try:
                next(context)
            except StopIteration:
                pass
    def pipe(
        self,
        texts,
        as_tuples=False,
        n_threads=-1,
        batch_size=1000,
        disable=[],
        cleanup=False,
        component_cfg=None,
        n_process=1,
    ):
        """"""Process texts as a stream, and yield `Doc` objects in order.
        texts (iterator): A sequence of texts to process.
        as_tuples (bool): If set to True, inputs should be a sequence of
            (text, context) tuples. Output will then be a sequence of
            (doc, context) tuples. Defaults to False.
        batch_size (int): The number of texts to buffer.
        disable (list): Names of the pipeline components to disable.
        cleanup (bool): If True, unneeded strings are freed to control memory
            use. Experimental.
        component_cfg (dict): An optional dictionary with extra keyword
            arguments for specific components.
        n_process (int): Number of processors to process texts, only supported
            in Python3. If -1, set `multiprocessing.cpu_count()`.
        YIELDS (Doc): Documents in the order of the original text.
        DOCS: https://spacy.io/api/language#pipe
        """"""
        # raw_texts will be used later to stop iterator.
        texts, raw_texts = itertools.tee(texts)
        if is_python2 and n_process != 1:
            user_warning(Warnings.W023)
            n_process = 1
        if n_threads != -1:
            deprecation_warning(Warnings.W016)
        if n_process == -1:",[45]
"            Whether or not this categorical is treated as an ordered
            categorical. If not given here or in `dtype`, the resulting
            categorical will be unordered.
        dtype : CategoricalDtype or ""category"", optional
            If :class:`CategoricalDtype`, cannot be used together with
            `categories` or `ordered`.
            .. versionadded:: 0.24.0
               When `dtype` is provided, neither `categories` nor `ordered`
               should be provided.
        Returns
        -------
        Categorical
        Examples
        --------
        >>> dtype = pd.CategoricalDtype(['a', 'b'], ordered=True)
        >>> pd.Categorical.from_codes(codes=[0, 1, 0, 1], dtype=dtype)
        [a, b, a, b]
        Categories (2, object): [a < b]
        """"""
        dtype = CategoricalDtype._from_values_or_dtype(
            categories=categories, ordered=ordered, dtype=dtype
        )
        if dtype.categories is None:
            msg = (
                ""The categories must be provided in 'categories' or ""
                ""'dtype'. Both were None.""
            )
            raise ValueError(msg)
        codes = np.asarray(codes)  # #21767
        if len(codes) and not is_integer_dtype(codes):
            raise ValueError(""codes need to be array-like integers"")
        if len(codes) and (codes.max() >= len(dtype.categories) or codes.min() < -1):
            raise ValueError(""codes need to be between -1 and len(categories)-1"")
        return cls(codes, dtype=dtype, fastpath=True)
    def _get_codes(self):
        """"""
        Get the codes.
        Returns
        -------
        codes : integer array view
            A non writable view of the `codes` array.
        """"""
        v = self._codes.view()
        v.flags.writeable = False
        return v
    def _set_codes(self, codes):
        """"""
        Not settable by the user directly
        """"""
        raise ValueError(""cannot set Categorical codes directly"")
    codes = property(fget=_get_codes, fset=_set_codes, doc=_codes_doc)
    def _set_categories(self, categories, fastpath=False):
        """"""
        Sets new categories inplace
        Parameters
        ----------
        fastpath : bool, default False
           Don't perform validation of the categories for uniqueness or nulls
        Examples
        --------
        >>> c = pd.Categorical(['a', 'b'])
        >>> c
        [a, b]
        Categories (2, object): [a, b]
        >>> c._set_categories(pd.Index(['a', 'c']))
        >>> c
        [a, c]
        Categories (2, object): [a, c]
        """"""
        if fastpath:
            new_dtype = CategoricalDtype._from_fastpath(categories, self.ordered)
        else:
            new_dtype = CategoricalDtype(categories, ordered=self.ordered)
        if (
            not fastpath
            and self.dtype.categories is not None
            and len(new_dtype.categories) != len(self.dtype.categories)
        ):
            raise ValueError(
                ""new categories need to have the same number of ""
                ""items than the old categories!""
            )
        self._dtype = new_dtype
    def _set_dtype(self, dtype: CategoricalDtype) -> ""Categorical"":
        """"""
        Internal method for directly updating the CategoricalDtype
        Parameters
        ----------
        dtype : CategoricalDtype
        Notes
        -----
        We don't do any validation here. It's assumed that the dtype is
        a (valid) instance of `CategoricalDtype`.
        """"""
        codes = _recode_for_categories(self.codes, self.categories, dtype.categories)
        return type(self)(codes, dtype=dtype, fastpath=True)
    def set_ordered(self, value, inplace=False):
        """"""
        Set the ordered attribute to the boolean value.
        Parameters
        ----------
        value : bool
           Set whether this categorical is ordered (True) or not (False).
        inplace : bool, default False
           Whether or not to set the ordered attribute in-place or return",[33]
"from __future__ import unicode_literals
import base64
import io
import itertools
import os
from struct import unpack, pack
import time
import xml.etree.ElementTree as etree
from .common import FileDownloader
from .http import HttpFD
from ..utils import (
    compat_urllib_request,
    compat_urlparse,
    format_bytes,
    encodeFilename,
    sanitize_open,
)

class FlvReader(io.BytesIO):
    """"""
    Reader for Flv files
    The file format is documented in https://www.adobe.com/devnet/f4v.html
    """"""
    # Utility functions for reading numbers and strings
    def read_unsigned_long_long(self):
        return unpack('!Q', self.read(8))[0]
    def read_unsigned_int(self):
        return unpack('!I', self.read(4))[0]
    def read_unsigned_char(self):
        return unpack('!B', self.read(1))[0]
    def read_string(self):
        res = b''
        while True:
            char = self.read(1)
            if char == b'\x00':
                break
            res += char
        return res
    def read_box_info(self):
        """"""
        Read a box and return the info as a tuple: (box_size, box_type, box_data)
        """"""
        real_size = size = self.read_unsigned_int()
        box_type = self.read(4)
        header_end = 8
        if size == 1:
            real_size = self.read_unsigned_long_long()
            header_end = 16
        return real_size, box_type, self.read(real_size-header_end)
    def read_asrt(self):
        # version
        self.read_unsigned_char()
        # flags
        self.read(3)
        quality_entry_count = self.read_unsigned_char()
        # QualityEntryCount
        for i in range(quality_entry_count):
            self.read_string()
        segment_run_count = self.read_unsigned_int()
        segments = []
        for i in range(segment_run_count):
            first_segment = self.read_unsigned_int()
            fragments_per_segment = self.read_unsigned_int()
            segments.append((first_segment, fragments_per_segment))
        return {
            'segment_run': segments,
        }
    def read_afrt(self):
        # version
        self.read_unsigned_char()
        # flags
        self.read(3)
        # time scale
        self.read_unsigned_int()
        quality_entry_count = self.read_unsigned_char()
        # QualitySegmentUrlModifiers
        for i in range(quality_entry_count):
            self.read_string()
        fragments_count = self.read_unsigned_int()
        fragments = []
        for i in range(fragments_count):
            first = self.read_unsigned_int()
            first_ts = self.read_unsigned_long_long()
            duration = self.read_unsigned_int()
            if duration == 0:
                discontinuity_indicator = self.read_unsigned_char()
            else:
                discontinuity_indicator = None
            fragments.append({
                'first': first,
                'ts': first_ts,
                'duration': duration,
                'discontinuity_indicator': discontinuity_indicator,
            })
        return {
            'fragments': fragments,
        }
    def read_abst(self):
        # version
        self.read_unsigned_char()
        # flags
        self.read(3)
        # BootstrapinfoVersion
        bootstrap_info_version = self.read_unsigned_int()
        # Profile,Live,Update,Reserved
        self.read(1)
        # time scale
        self.read_unsigned_int()
        # CurrentMediaTime
        self.read_unsigned_long_long()
        # SmpteTimeCodeOffset","[6, 29, 32, 35]"
"        $ luigi --module my_tasks MyTask --grades '[100,70]'
    """"""
    def normalize(self, x):
        """"""
        Ensure that list parameter is converted to a tuple so it can be hashed.
        :param str x: the value to parse.
        :return: the normalized (hashable/immutable) value.
        """"""
        return _recursively_freeze(x)
    def parse(self, x):
        """"""
        Parse an individual value from the input.
        :param str x: the value to parse.
        :return: the parsed value.
        """"""
        return list(json.loads(x))
    def serialize(self, x):
        """"""
        Opposite of :py:meth:`parse`.
        Converts the value ``x`` to a string.
        :param x: the value to serialize.
        """"""
        return json.dumps(x)

class TupleParameter(Parameter):
    """"""
    Parameter whose value is a ``tuple`` or ``tuple`` of tuples.
    In the task definition, use
    .. code-block:: python
        class MyTask(luigi.Task):
          book_locations = luigi.TupleParameter()
            def run(self):
                for location in self.book_locations:
                    print(""Go to page %d, line %d"" % (location[0], location[1]))

    At the command line, use
    .. code-block:: console
        $ luigi --module my_tasks MyTask --book_locations <JSON string>
    Simple example with two grades:
    .. code-block:: console
        $ luigi --module my_tasks MyTask --book_locations '((12,3),(4,15),(52,1))'
    """"""
    def parse(self, x):
        """"""
        Parse an individual value from the input.
        :param str x: the value to parse.
        :return: the parsed value.
        """"""
        # Since the result of json.dumps(tuple) differs from a tuple string, we must handle either case.
        # A tuple string may come from a config file or from cli execution.
        # t = ((1, 2), (3, 4))
        # t_str = '((1,2),(3,4))'
        # t_json_str = json.dumps(t)
        # t_json_str == '[[1, 2], [3, 4]]'
        # json.loads(t_json_str) == t
        # json.loads(t_str) == ValueError: No JSON object could be decoded
        # Therefore, if json.loads(x) returns a ValueError, try ast.literal_eval(x).
        # ast.literal_eval(t_str) == t
        try:
            return tuple(tuple(x) for x in json.loads(x))  # loop required to parse tuple of tuples
        except ValueError:
            return literal_eval(x)  # if this causes an error, let that error be raised.
    def serialize(self, x):
        """"""
        Opposite of :py:meth:`parse`.
        Converts the value ``x`` to a string.
        :param x: the value to serialize.
        """"""
        return json.dumps(x)

class NumericalParameter(Parameter):
    """"""
    Parameter whose value is a number of the specified type, e.g. ``int`` or
    ``float`` and in the range specified.
    In the task definition, use
    .. code-block:: python
        class MyTask(luigi.Task):
            my_param_1 = luigi.NumericalParameter(
                var_type=int, min_value=-3, max_value=7) # -3 <= my_param_1 < 7
            my_param_2 = luigi.NumericalParameter(
                var_type=int, min_value=-3, max_value=7, left_op=operator.lt, right_op=operator.le) # -3 < my_param_2 <= 7
    At the command line, use
    .. code-block:: console
        $ luigi --module my_tasks MyTask --my-param-1 -3 --my-param-2 -2
    """"""
    def __init__(self, left_op=operator.le, right_op=operator.lt, *args, **kwargs):
        """"""
        :param function var_type: The type of the input variable, e.g. int or float.
        :param min_value: The minimum value permissible in the accepted values
                          range.  May be inclusive or exclusive based on left_op parameter.
                          This should be the same type as var_type.
        :param max_value: The maximum value permissible in the accepted values
                          range.  May be inclusive or exclusive based on right_op parameter.
                          This should be the same type as var_type.
        :param function left_op: The comparison operator for the left-most comparison in","[5, 19, 29, 32, 60, 81, 85, 87, 88, 89, 90, 91, 93, 94]"
"    if not query:
        return url
    parsed_url = compat_urlparse.urlparse(url)
    qs = compat_parse_qs(parsed_url.query)
    qs.update(query)
    return compat_urlparse.urlunparse(parsed_url._replace(
        query=compat_urllib_parse_urlencode(qs, True)))

def update_Request(req, url=None, data=None, headers={}, query={}):
    req_headers = req.headers.copy()
    req_headers.update(headers)
    req_data = data or req.data
    req_url = update_url_query(url or req.get_full_url(), query)
    req_type = HEADRequest if req.get_method() == 'HEAD' else compat_urllib_request.Request
    new_req = req_type(
        req_url, data=req_data, headers=req_headers,
        origin_req_host=req.origin_req_host, unverifiable=req.unverifiable)
    if hasattr(req, 'timeout'):
        new_req.timeout = req.timeout
    return new_req

def dict_get(d, key_or_keys, default=None, skip_false_values=True):
    if isinstance(key_or_keys, (list, tuple)):
        for key in key_or_keys:
            if key not in d or d[key] is None or skip_false_values and not d[key]:
                continue
            return d[key]
        return default
    return d.get(key_or_keys, default)

def try_get(src, getter, expected_type=None):
    try:
        v = getter(src)
    except (AttributeError, KeyError, TypeError, IndexError):
        pass
    else:
        if expected_type is None or isinstance(v, expected_type):
            return v

def encode_compat_str(string, encoding=preferredencoding(), errors='strict'):
    return string if isinstance(string, compat_str) else compat_str(string, encoding, errors)

US_RATINGS = {
    'G': 0,
    'PG': 10,
    'PG-13': 13,
    'R': 16,
    'NC': 18,
}

def parse_age_limit(s):
    if s is None:
        return None
    m = re.match(r'^(?P<age>\d{1,2})\+?$', s)
    return int(m.group('age')) if m else US_RATINGS.get(s)

def strip_jsonp(code):
    return re.sub(
        r'(?s)^[a-zA-Z0-9_.$]+\s*\(\s*(.*)\);?\s*?(?://[^\n]*)*$', r'\1', code)

def js_to_json(code):
    def fix_kv(m):
        v = m.group(0)
        if v in ('true', 'false', 'null'):
            return v
        elif v.startswith('/*') or v == ',':
            return """"
        if v[0] in (""'"", '""'):
            v = re.sub(r'(?s)\\.|""', lambda m: {
                '""': '\\""',
                ""\\'"": ""'"",
                '\\\n': '',
                '\\x': '\\u00',
            }.get(m.group(0), m.group(0)), v[1:-1])
        INTEGER_TABLE = (
            (r'^0[xX][0-9a-fA-F]+', 16),
            (r'^0+[0-7]+', 8),
        )
        for regex, base in INTEGER_TABLE:
            im = re.match(regex, v)
            if im:
                i = int(im.group(0), base)
                return '""%d"":' % i if v.endswith(':') else '%d' % i
        return '""%s""' % v
    return re.sub(r'''(?sx)
        ""(?:[^""\\]*(?:\\\\|\\['""nurtbfx/\n]))*[^""\\]*""|
        '(?:[^'\\]*(?:\\\\|\\['""nurtbfx/\n]))*[^'\\]*'|
        /\*.*?\*/|,(?=\s*[\]}])|
        [a-zA-Z_][.a-zA-Z_0-9]*|
        (?:0[xX][0-9a-fA-F]+|0+[0-7]+)(?:\s*:)?|
        [0-9]+(?=\s*:)
        ''', fix_kv, code)

def qualities(quality_ids):
    """""" Get a numeric quality value out of a list of possible values """"""
    def q(qid):
        try:
            return quality_ids.index(qid)
        except ValueError:
            return -1
    return q

DEFAULT_OUTTMPL = '%(title)s-%(id)s.%(ext)s'

def limit_length(s, length):
    """""" Add ellipses to overly long strings """"""
    if s is None:
        return None
    ELLIPSES = '...'
    if len(s) > length:
        return s[:length - len(ELLIPSES)] + ELLIPSES",[102]
"    """"""
    This returns a set of the tasks that are being run by other worker
    """"""
    task_sets = _get_external_workers(worker).values()
    return functools.reduce(lambda a, b: a | b, task_sets, set())

def _get_external_workers(worker):
    """"""
    This returns a dict with a set of tasks for all of the other workers
    """"""
    worker_that_blocked_task = collections.defaultdict(set)
    get_work_response_history = worker._get_work_response_history
    for get_work_response in get_work_response_history:
        if get_work_response['task_id'] is None:
            for running_task in get_work_response['running_tasks']:
                other_worker_id = running_task['worker']
                other_task_id = running_task['task_id']
                other_task = worker._scheduled_tasks.get(other_task_id)
                if other_worker_id == worker._id or not other_task:
                    continue
                worker_that_blocked_task[other_worker_id].add(other_task)
    return worker_that_blocked_task

def _group_tasks_by_name_and_status(task_dict):
    """"""
    Takes a dictionary with sets of tasks grouped by their status and
    returns a dictionary with dictionaries with an array of tasks grouped by
    their status and task name
    """"""
    group_status = {}
    for task in task_dict:
        if task.task_family not in group_status:
            group_status[task.task_family] = []
        group_status[task.task_family].append(task)
    return group_status

def _summary_dict(worker):
    set_tasks = _partition_tasks(worker)
    set_tasks[""run_by_other_worker""] = _get_run_by_other_worker(worker)
    _populate_unknown_statuses(set_tasks)
    return set_tasks

def _summary_format(set_tasks, worker):
    group_tasks = {}
    for status, task_dict in set_tasks.items():
        group_tasks[status] = _group_tasks_by_name_and_status(task_dict)
    comments = _get_comments(group_tasks)
    num_all_tasks = sum([len(set_tasks[""already_done""]),
                         len(set_tasks[""completed""]), len(set_tasks[""failed""]),
                         len(set_tasks[""scheduling_error""]),
                         len(set_tasks[""still_pending_ext""]),
                         len(set_tasks[""still_pending_not_ext""])])
    str_output = ''
    str_output += 'Scheduled {0} tasks of which:\n'.format(num_all_tasks)
    for status in _ORDERED_STATUSES:
        if status not in comments:
            continue
        str_output += '{0}'.format(comments[status])
        if status != 'still_pending':
            str_output += '{0}\n'.format(_get_str(group_tasks[status], status in _PENDING_SUB_STATUSES))
    ext_workers = _get_external_workers(worker)
    group_tasks_ext_workers = {}
    for ext_worker, task_dict in ext_workers.items():
        group_tasks_ext_workers[ext_worker] = _group_tasks_by_name_and_status(task_dict)
    if len(ext_workers) > 0:
        str_output += ""\nThe other workers were:\n""
        count = 0
        for ext_worker, task_dict in ext_workers.items():
            if count > 3 and count < len(ext_workers) - 1:
                str_output += ""    and {0} other workers"".format(len(ext_workers) - count)
                break
            str_output += ""    - {0} ran {1} tasks\n"".format(ext_worker, len(task_dict))
            count += 1
        str_output += '\n'
    if num_all_tasks == sum([len(set_tasks[""already_done""]),
                             len(set_tasks[""scheduling_error""]),
                             len(set_tasks[""still_pending_ext""]),
                             len(set_tasks[""still_pending_not_ext""])]):
        if len(ext_workers) == 0:
            str_output += '\n'
        str_output += 'Did not run any tasks'
    smiley = """"
    reason = """"
    if set_tasks[""failed""]:
        smiley = "":(""
        reason = ""there were failed tasks""
        if set_tasks[""scheduling_error""]:
            reason += "" and tasks whose scheduling failed""
    elif set_tasks[""scheduling_error""]:
        smiley = "":(""
        reason = ""there were tasks whose scheduling failed""
    elif set_tasks[""not_run""]:
        smiley = "":|""
        reason = ""there were tasks that were not granted run permission by the scheduler""
    elif set_tasks[""still_pending_ext""]:
        smiley = "":|""
        reason = ""there were missing external dependencies""
    else:
        smiley = "":)""
        reason = ""there were no failed tasks or missing external dependencies""
    str_output += ""\nThis progress looks {0} because {1}"".format(smiley, reason)
    if num_all_tasks == 0:
        str_output = 'Did not schedule any tasks'
    return str_output

def _summary_wrap(str_output):
    return textwrap.dedent(""""""
    ===== Luigi Execution Summary =====
    {str_output}
    ===== Luigi Execution Summary =====
    """""").format(str_output=str_output)

def summary(worker):
    """"""
    Given a worker, return a human readable summary of what the worker have
    done.
    """"""
    return _summary_wrap(_summary_format(_summary_dict(worker), worker))","[87, 88, 89, 90, 91]"
"                self.cell.build([step_input_shape] + constants_shape)
            else:
                self.cell.build(step_input_shape)
        # set or validate state_spec
        if hasattr(self.cell.state_size, '__len__'):
            state_size = list(self.cell.state_size)
        else:
            state_size = [self.cell.state_size]
        if self.state_spec is not None:
            # initial_state was passed in call, check compatibility
            if [spec.shape[-1] for spec in self.state_spec] != state_size:
                raise ValueError(
                    'An `initial_state` was passed that is not compatible with '
                    '`cell.state_size`. Received `state_spec`={}; '
                    'however `cell.state_size` is '
                    '{}'.format(self.state_spec, self.cell.state_size))
        else:
            self.state_spec = [InputSpec(shape=(None, dim))
                               for dim in state_size]
        if self.stateful:
            self.reset_states()
    def get_initial_state(self, inputs):
        # build an all-zero tensor of shape (samples, output_dim)
        initial_state = K.zeros_like(inputs)  # (samples, timesteps, input_dim)
        initial_state = K.sum(initial_state, axis=(1, 2))  # (samples,)
        initial_state = K.expand_dims(initial_state)  # (samples, 1)
        if hasattr(self.cell.state_size, '__len__'):
            return [K.tile(initial_state, [1, dim])
                    for dim in self.cell.state_size]
        else:
            return [K.tile(initial_state, [1, self.cell.state_size])]
    def __call__(self, inputs, initial_state=None, constants=None, **kwargs):
        inputs, initial_state, constants = self._standardize_args(
            inputs, initial_state, constants)
        if initial_state is None and constants is None:
            return super(RNN, self).__call__(inputs, **kwargs)
        # If any of `initial_state` or `constants` are specified and are Keras
        # tensors, then add them to the inputs and temporarily modify the
        # input_spec to include them.
        additional_inputs = []
        additional_specs = []
        if initial_state is not None:
            kwargs['initial_state'] = initial_state
            additional_inputs += initial_state
            self.state_spec = [InputSpec(shape=K.int_shape(state))
                               for state in initial_state]
            additional_specs += self.state_spec
        if constants is not None:
            kwargs['constants'] = constants
            additional_inputs += constants
            self.constants_spec = [InputSpec(shape=K.int_shape(constant))
                                   for constant in constants]
            self._num_constants = len(constants)
            additional_specs += self.constants_spec
        # at this point additional_inputs cannot be empty
        is_keras_tensor = hasattr(additional_inputs[0], '_keras_history')
        for tensor in additional_inputs:
            if hasattr(tensor, '_keras_history') != is_keras_tensor:
                raise ValueError('The initial state or constants of an RNN'
                                 ' layer cannot be specified with a mix of'
                                 ' Keras tensors and non-Keras tensors')
        if is_keras_tensor:
            # Compute the full input spec, including state and constants
            full_input = [inputs] + additional_inputs
            full_input_spec = self.input_spec + additional_specs
            # Perform the call with temporarily replaced input_spec
            original_input_spec = self.input_spec
            self.input_spec = full_input_spec
            output = super(RNN, self).__call__(full_input, **kwargs)
            self.input_spec = original_input_spec
            return output
        else:
            return super(RNN, self).__call__(inputs, **kwargs)
    def call(self,
             inputs,
             mask=None,
             training=None,
             initial_state=None,
             constants=None):
        # input shape: `(samples, time (padded with zeros), input_dim)`
        # note that the .build() method of subclasses MUST define
        # self.input_spec and self.state_spec with complete input shapes.
        if isinstance(inputs, list):
            inputs = inputs[0]
        if initial_state is not None:
            pass
        elif self.stateful:
            initial_state = self.states
        else:
            initial_state = self.get_initial_state(inputs)
        if isinstance(mask, list):
            mask = mask[0]
        if len(initial_state) != len(self.states):
            raise ValueError('Layer has ' + str(len(self.states)) +
                             ' states but was passed ' +
                             str(len(initial_state)) +
                             ' initial states.')
        input_shape = K.int_shape(inputs)
        timesteps = input_shape[1]
        if self.unroll and timesteps in [None, 1]:
            raise ValueError('Cannot unroll a RNN if the '
                             'time dimension is undefined or equal to 1. \n'
                             '- If using a Sequential model, '
                             'specify the time dimension by passing '
                             'an `input_shape` or `batch_input_shape` '
                             'argument to your first layer. If your '
                             'first layer is an Embedding, you can '
                             'also use the `input_length` argument.\n'
                             '- If using the functional API, specify '
                             'the time dimension by passing a `shape` '
                             'or `batch_shape` argument to your Input layer.')
        kwargs = {}
        if has_arg(self.cell.call, 'training'):
            kwargs['training'] = training
","[62, 64, 67]"
"    opening_bracket = None
    closing_bracket = None
    optional_brackets: Set[LeafID] = set()
    inner_brackets: Set[LeafID] = set()
    for index, leaf, leaf_length in enumerate_with_length(line, reversed=True):
        length += leaf_length
        if length > line_length:
            break
        has_inline_comment = leaf_length > len(leaf.value) + len(leaf.prefix)
        if leaf.type == STANDALONE_COMMENT or has_inline_comment:
            break
        optional_brackets.discard(id(leaf))
        if opening_bracket:
            if leaf is opening_bracket:
                opening_bracket = None
            elif leaf.type in CLOSING_BRACKETS:
                inner_brackets.add(id(leaf))
        elif leaf.type in CLOSING_BRACKETS:
            if not leaf.value:
                optional_brackets.add(id(opening_bracket))
                continue
            if index > 0 and line.leaves[index - 1].type in OPENING_BRACKETS:
                # Empty brackets would fail a split so treat them as ""inner""
                # brackets (e.g. only add them to the `omit` set if another
                # pair of brackets was good enough.
                inner_brackets.add(id(leaf))
                continue
            opening_bracket = leaf.opening_bracket
            if closing_bracket:
                omit.add(id(closing_bracket))
                omit.update(inner_brackets)
                inner_brackets.clear()
                yield omit
            closing_bracket = leaf

def get_future_imports(node: Node) -> Set[str]:
    """"""Return a set of __future__ imports in the file.""""""
    imports = set()
    for child in node.children:
        if child.type != syms.simple_stmt:
            break
        first_child = child.children[0]
        if isinstance(first_child, Leaf):
            # Continue looking if we see a docstring; otherwise stop.
            if (
                len(child.children) == 2
                and first_child.type == token.STRING
                and child.children[1].type == token.NEWLINE
            ):
                continue
            else:
                break
        elif first_child.type == syms.import_from:
            module_name = first_child.children[1]
            if not isinstance(module_name, Leaf) or module_name.value != ""__future__"":
                break
            for import_from_child in first_child.children[3:]:
                if isinstance(import_from_child, Leaf):
                    if import_from_child.type == token.NAME:
                        imports.add(import_from_child.value)
                else:
                    assert import_from_child.type == syms.import_as_names
                    for leaf in import_from_child.children:
                        if isinstance(leaf, Leaf) and leaf.type == token.NAME:
                            imports.add(leaf.value)
        else:
            break
    return imports

def gen_python_files_in_dir(
    path: Path,
    root: Path,
    include: Pattern[str],
    exclude: Pattern[str],
    report: ""Report"",
) -> Iterator[Path]:
    """"""Generate all files under `path` whose paths are not excluded by the
    `exclude` regex, but are included by the `include` regex.
    `report` is where output about exclusions goes.
    """"""
    assert root.is_absolute(), f""INTERNAL ERROR: `root` must be absolute but is {root}""
    for child in path.iterdir():
        normalized_path = ""/"" + child.resolve().relative_to(root).as_posix()
        if child.is_dir():
            normalized_path += ""/""
        exclude_match = exclude.search(normalized_path)
        if exclude_match and exclude_match.group(0):
            report.path_ignored(child, f""matches the --exclude regular expression"")
            continue
        if child.is_dir():
            yield from gen_python_files_in_dir(child, root, include, exclude, report)
        elif child.is_file():
            include_match = include.search(normalized_path)
            if include_match:
                yield child

@lru_cache()
def find_project_root(srcs: Iterable[str]) -> Path:
    """"""Return a directory containing .git, .hg, or pyproject.toml.
    That directory can be one of the directories passed in `srcs` or their
    common parent.
    If no directory in the tree contains a marker that would specify it's the
    project root, the root of the file system is returned.
    """"""
    if not srcs:
        return Path(""/"").resolve()
    common_base = min(Path(src).resolve() for src in srcs)
    if common_base.is_dir():
        # Append a fake file so `parents` below returns `common_base_dir`, too.
        common_base /= ""fake-file""
    for directory in common_base.parents:
        if (directory / "".git"").is_dir():
            return directory
",[89]
"    def __setitem__(self, key, value):
        dict.__setitem__(self, self.normkey(key), self.normvalue(value))
    def __delitem__(self, key):
        dict.__delitem__(self, self.normkey(key))
    def __contains__(self, key):
        return dict.__contains__(self, self.normkey(key))
    has_key = __contains__
    def __copy__(self):
        return self.__class__(self)
    copy = __copy__
    def normkey(self, key):
        """"""Method to normalize dictionary key access""""""
        return key.lower()
    def normvalue(self, value):
        """"""Method to normalize values prior to be setted""""""
        return value
    def get(self, key, def_val=None):
        return dict.get(self, self.normkey(key), self.normvalue(def_val))
    def setdefault(self, key, def_val=None):
        return dict.setdefault(self, self.normkey(key), self.normvalue(def_val))
    def update(self, seq):
        seq = seq.items() if isinstance(seq, Mapping) else seq
        iseq = ((self.normkey(k), self.normvalue(v)) for k, v in seq)
        super(CaselessDict, self).update(iseq)
    @classmethod
    def fromkeys(cls, keys, value=None):
        return cls((k, value) for k in keys)
    def pop(self, key, *args):
        return dict.pop(self, self.normkey(key), *args)

class MergeDict(object):
    """"""
    A simple class for creating new ""virtual"" dictionaries that actually look
    up values in more than one dictionary, passed in the constructor.
    If a key appears in more than one of the given dictionaries, only the
    first occurrence will be used.
    """"""
    def __init__(self, *dicts):
        if not six.PY2:
            warnings.warn(
                ""scrapy.utils.datatypes.MergeDict is deprecated in favor ""
                ""of collections.ChainMap (introduced in Python 3.3)"",
                category=ScrapyDeprecationWarning,
                stacklevel=2,
            )
        self.dicts = dicts
    def __getitem__(self, key):
        for dict_ in self.dicts:
            try:
                return dict_[key]
            except KeyError:
                pass
        raise KeyError
    def __copy__(self):
        return self.__class__(*self.dicts)
    def get(self, key, default=None):
        try:
            return self[key]
        except KeyError:
            return default
    def getlist(self, key):
        for dict_ in self.dicts:
            if key in dict_.keys():
                return dict_.getlist(key)
        return []
    def items(self):
        item_list = []
        for dict_ in self.dicts:
            item_list.extend(dict_.items())
        return item_list
    def has_key(self, key):
        for dict_ in self.dicts:
            if key in dict_:
                return True
        return False
    __contains__ = has_key
    def copy(self):
        """"""Returns a copy of this object.""""""
        return self.__copy__()

class LocalCache(collections.OrderedDict):
    """"""Dictionary with a finite number of keys.
    Older items expires first.
    """"""
    def __init__(self, limit=None):
        super(LocalCache, self).__init__()
        self.limit = limit
    def __setitem__(self, key, value):
        while len(self) >= self.limit:
            self.popitem(last=False)
        super(LocalCache, self).__setitem__(key, value)

class SequenceExclude(object):
    """"""Object to test if an item is NOT within some sequence.""""""
    def __init__(self, seq):
        self.seq = seq
    def __contains__(self, item):","[114, 115]"
"            kwtrans['tickdir'] = kw.pop('direction')
        if 'rotation' in kw:
            kwtrans['labelrotation'] = kw.pop('rotation')
        if 'left' in kw:
            kwtrans['tick1On'] = kw.pop('left')
        if 'bottom' in kw:
            kwtrans['tick1On'] = kw.pop('bottom')
        if 'right' in kw:
            kwtrans['tick2On'] = kw.pop('right')
        if 'top' in kw:
            kwtrans['tick2On'] = kw.pop('top')
        if 'labelleft' in kw:
            kwtrans['label1On'] = kw.pop('labelleft')
        if 'labelbottom' in kw:
            kwtrans['label1On'] = kw.pop('labelbottom')
        if 'labelright' in kw:
            kwtrans['label2On'] = kw.pop('labelright')
        if 'labeltop' in kw:
            kwtrans['label2On'] = kw.pop('labeltop')
        if 'colors' in kw:
            c = kw.pop('colors')
            kwtrans['color'] = c
            kwtrans['labelcolor'] = c
        # Maybe move the checking up to the caller of this method.
        for key in kw:
            if key not in kwkeys:
                raise ValueError(
                    ""keyword %s is not recognized; valid keywords are %s""
                    % (key, kwkeys))
            kwtrans.update(kw)
        return kwtrans
    def set_clip_path(self, clippath, transform=None):
        martist.Artist.set_clip_path(self, clippath, transform)
        for child in self.majorTicks + self.minorTicks:
            child.set_clip_path(clippath, transform)
        self.stale = True
    def get_view_interval(self):
        """"""Return the Interval instance for this axis view limits.""""""
        raise NotImplementedError('Derived must override')
    def set_view_interval(self, vmin, vmax, ignore=False):
        """"""
        Set the axis view limits.  This method is for internal use; Matplotlib
        users should typically use e.g. `~Axes.set_xlim` and `~Axes.set_ylim`.
        If *ignore* is False (the default), this method will never reduce the
        preexisting view limits, only expand them if *vmin* or *vmax* are not
        within them.  Moreover, the order of *vmin* and *vmax* does not matter;
        the orientation of the axis will not change.
        If *ignore* is True, the view limits will be set exactly to ``(vmin,
        vmax)`` in that order.
        """"""
        raise NotImplementedError('Derived must override')
    def get_data_interval(self):
        """"""Return the Interval instance for this axis data limits.""""""
        raise NotImplementedError('Derived must override')
    def set_data_interval(self, vmin, vmax, ignore=False):
        """"""
        Set the axis data limits.  This method is for internal use.
        If *ignore* is False (the default), this method will never reduce the
        preexisting data limits, only expand them if *vmin* or *vmax* are not
        within them.  Moreover, the order of *vmin* and *vmax* does not matter;
        the orientation of the axis will not change.
        If *ignore* is True, the data limits will be set exactly to ``(vmin,
        vmax)`` in that order.
        """"""
        raise NotImplementedError('Derived must override')
    def get_inverted(self):
        """"""
        Return whether the axis is oriented in the ""inverse"" direction.
        The ""normal"" direction is increasing to the right for the x-axis and to
        the top for the y-axis; the ""inverse"" direction is increasing to the
        left for the x-axis and to the bottom for the y-axis.
        """"""
        low, high = self.get_view_interval()
        return high < low
    def set_inverted(self, inverted):
        """"""
        Set whether the axis is oriented in the ""inverse"" direction.
        The ""normal"" direction is increasing to the right for the x-axis and to
        the top for the y-axis; the ""inverse"" direction is increasing to the
        left for the x-axis and to the bottom for the y-axis.
        """"""
        a, b = self.get_view_interval()
        if inverted:
            self.set_view_interval(max(a, b), min(a, b), ignore=True)
        else:
            self.set_view_interval(min(a, b), max(a, b), ignore=True)
    def set_default_intervals(self):
        """"""
        Set the default limits for the axis data and view interval if they
        have not been not mutated yet.
        """"""
        # this is mainly in support of custom object plotting.  For
        # example, if someone passes in a datetime object, we do not
        # know automagically how to set the default min/max of the
        # data and view limits.  The unit conversion AxisInfo
        # interface provides a hook for custom types to register
        # default limits through the AxisInfo.default_limits
        # attribute, and the derived code below will check for that
        # and use it if is available (else just use 0..1)
    def _set_artist_props(self, a):
        if a is None:
            return
        a.set_figure(self.figure)
    @cbook.deprecated(""3.1"")
    def iter_ticks(self):
        """"""
        Yield ``(Tick, location, label)`` tuples for major and minor ticks.
        """"""
        major_locs = self.get_majorticklocs()
        major_labels = self.major.formatter.format_ticks(major_locs)
        major_ticks = self.get_major_ticks(len(major_locs))","[94, 95, 96, 97, 98]"
"            return idx
        if self.keys is None:
            concat_axis = _concat_indexes(indexes)
        else:
            concat_axis = _make_concat_multiindex(
                indexes, self.keys, self.levels, self.names
            )
        self._maybe_check_integrity(concat_axis)
        return concat_axis
    def _maybe_check_integrity(self, concat_index: Index):
        if self.verify_integrity:
            if not concat_index.is_unique:
                overlap = concat_index[concat_index.duplicated()].unique()
                raise ValueError(f""Indexes have overlapping values: {overlap}"")

def _concat_indexes(indexes) -> Index:
    return indexes[0].append(indexes[1:])

def _make_concat_multiindex(indexes, keys, levels=None, names=None) -> MultiIndex:
    if (levels is None and isinstance(keys[0], tuple)) or (
        levels is not None and len(levels) > 1
    ):
        zipped = list(zip(*keys))
        if names is None:
            names = [None] * len(zipped)
        if levels is None:
            _, levels = factorize_from_iterables(zipped)
        else:
            levels = [ensure_index(x) for x in levels]
    else:
        zipped = [keys]
        if names is None:
            names = [None]
        if levels is None:
            levels = [ensure_index(keys)]
        else:
            levels = [ensure_index(x) for x in levels]
    if not all_indexes_same(indexes):
        codes_list = []
        # things are potentially different sizes, so compute the exact codes
        # for each level and pass those to MultiIndex.from_arrays
        for hlevel, level in zip(zipped, levels):
            to_concat = []
            for key, index in zip(hlevel, indexes):
                try:
                    i = level.get_loc(key)
                except KeyError as err:
                    raise ValueError(f""Key {key} not in level {level}"") from err
                to_concat.append(np.repeat(i, len(index)))
            codes_list.append(np.concatenate(to_concat))
        concat_index = _concat_indexes(indexes)
        # these go at the end
        if isinstance(concat_index, MultiIndex):
            levels.extend(concat_index.levels)
            codes_list.extend(concat_index.codes)
        else:
            codes, categories = factorize_from_iterable(concat_index)
            levels.append(categories)
            codes_list.append(codes)
        if len(names) == len(levels):
            names = list(names)
        else:
            # make sure that all of the passed indices have the same nlevels
            if not len({idx.nlevels for idx in indexes}) == 1:
                raise AssertionError(
                    ""Cannot concat indices that do not have the same number of levels""
                )
            # also copies
            names = names + get_consensus_names(indexes)
        return MultiIndex(
            levels=levels, codes=codes_list, names=names, verify_integrity=False
        )
    new_index = indexes[0]
    n = len(new_index)
    kpieces = len(indexes)
    # also copies
    new_names = list(names)
    new_levels = list(levels)
    # construct codes
    new_codes = []
    # do something a bit more speedy
    for hlevel, level in zip(zipped, levels):
        hlevel = ensure_index(hlevel)
        mapped = level.get_indexer(hlevel)
        mask = mapped == -1
        if mask.any():
            raise ValueError(f""Values not found in passed level: {hlevel[mask]!s}"")
        new_codes.append(np.repeat(mapped, n))
    if isinstance(new_index, MultiIndex):
        new_levels.extend(new_index.levels)
        new_codes.extend([np.tile(lab, kpieces) for lab in new_index.codes])
    else:
        new_levels.append(new_index)
        new_codes.append(np.tile(np.arange(n), kpieces))
    if len(new_names) < len(new_levels):
        new_names.extend(new_index.names)
    return MultiIndex(
        levels=new_levels, codes=new_codes, names=new_names, verify_integrity=False","[56, 57, 58, 59]"
"    def parse_stream_raw(self, stream, debug=False):
        """"""Parse a stream and return the syntax tree.""""""
        tokens = tokenize.generate_tokens(stream.readline)
        return self.parse_tokens(tokens, debug)
    def parse_stream(self, stream, debug=False):
        """"""Parse a stream and return the syntax tree.""""""
        return self.parse_stream_raw(stream, debug)
    def parse_file(self, filename, encoding=None, debug=False):
        """"""Parse a file and return the syntax tree.""""""
        with io.open(filename, ""r"", encoding=encoding) as stream:
            return self.parse_stream(stream, debug)
    def parse_string(self, text, debug=False):
        """"""Parse a string and return the syntax tree.""""""
        tokens = tokenize.generate_tokens(io.StringIO(text).readline)
        return self.parse_tokens(tokens, debug)
    def _partially_consume_prefix(self, prefix, column):
        lines = []
        current_line = """"
        current_column = 0
        wait_for_nl = False
        for char in prefix:
            current_line += char
            if wait_for_nl:
                if char == '\n':
                    if current_line.strip() and current_column < column:
                        res = ''.join(lines)
                        return res, prefix[len(res):]
                    lines.append(current_line)
                    current_line = """"
                    current_column = 0
                    wait_for_nl = False
            elif char == ' ':
                current_column += 1
            elif char == '\t':
                current_column += 4
            elif char == '\n':
                # unexpected empty line
                current_column = 0
            else:
                # indent is finished
                wait_for_nl = True
        return ''.join(lines), current_line

def _generate_pickle_name(gt, cache_dir=None):
    head, tail = os.path.splitext(gt)
    if tail == "".txt"":
        tail = """"
    name = head + tail + ""."".join(map(str, sys.version_info)) + "".pickle""
    if cache_dir:
        return os.path.join(cache_dir, os.path.basename(name))
    else:
        return name

def load_grammar(gt=""Grammar.txt"", gp=None,
                 save=True, force=False, logger=None):
    """"""Load the grammar (maybe from a pickle).""""""
    if logger is None:
        logger = logging.getLogger()
    gp = _generate_pickle_name(gt) if gp is None else gp
    if force or not _newer(gp, gt):
        logger.info(""Generating grammar tables from %s"", gt)
        g = pgen.generate_grammar(gt)
        if save:
            logger.info(""Writing grammar tables to %s"", gp)
            try:
                g.dump(gp)
            except OSError as e:
                logger.info(""Writing failed: %s"", e)
    else:
        g = grammar.Grammar()
        g.load(gp)
    return g

def _newer(a, b):
    """"""Inquire whether file a was written since file b.""""""
    if not os.path.exists(a):
        return False
    if not os.path.exists(b):
        return True
    return os.path.getmtime(a) >= os.path.getmtime(b)

def load_packaged_grammar(package, grammar_source, cache_dir=None):
    """"""Normally, loads a pickled grammar by doing
        pkgutil.get_data(package, pickled_grammar)
    where *pickled_grammar* is computed from *grammar_source* by adding the
    Python version and using a ``.pickle`` extension.
    However, if *grammar_source* is an extant file, load_grammar(grammar_source)
    is called instead. This facilitates using a packaged grammar file when needed
    but preserves load_grammar's automatic regeneration behavior when possible.
    """"""
    if os.path.isfile(grammar_source):
        gp = _generate_pickle_name(grammar_source, cache_dir) if cache_dir else None
        return load_grammar(grammar_source, gp=gp)
    pickled_name = _generate_pickle_name(os.path.basename(grammar_source), cache_dir)
    data = pkgutil.get_data(package, pickled_name)
    g = grammar.Grammar()
    g.loads(data)
    return g

def main(*args):
    """"""Main program, when run as a script: produce grammar pickle files.
    Calls load_grammar for each argument, a path to a grammar text file.
    """"""
    if not args:
        args = sys.argv[1:]
    logging.basicConfig(level=logging.INFO, stream=sys.stdout,
                        format='%(message)s')
    for gt in args:
        load_grammar(gt, save=True, force=True)
    return True
if __name__ == ""__main__"":","[37, 39, 40]"
"# -*- coding: utf-8 -*-
from __future__ import print_function
from __future__ import unicode_literals
import re
import inspect
import os
import shutil
import keras
from keras import backend as K
from keras.backend import numpy_backend
from docs.structure import EXCLUDE
from docs.structure import PAGES
from docs.structure import ROOT
from docs.structure import template_np_implementation
from docs.structure import template_hidden_np_implementation
import sys
if sys.version[0] == '2':
    reload(sys)
    sys.setdefaultencoding('utf8')

def get_function_signature(function, method=True):
    wrapped = getattr(function, '_original_function', None)
    if wrapped is None:
        signature = inspect.getargspec(function)
    else:
        signature = inspect.getargspec(wrapped)
    defaults = signature.defaults
    if method:
        args = signature.args[1:]
    else:
        args = signature.args
    if defaults:
        kwargs = zip(args[-len(defaults):], defaults)
        args = args[:-len(defaults)]
    else:
        kwargs = []
    st = '%s.%s(' % (clean_module_name(function.__module__), function.__name__)
    for a in args:
        st += str(a) + ', '
    for a, v in kwargs:
        if isinstance(v, str):
            v = '\'' + v + '\''
        st += str(a) + '=' + str(v) + ', '
    if kwargs or args:
        signature = st[:-2] + ')'
    else:
        signature = st + ')'
    return post_process_signature(signature)

def get_class_signature(cls):
    try:
        class_signature = get_function_signature(cls.__init__)
        class_signature = class_signature.replace('__init__', cls.__name__)
    except (TypeError, AttributeError):
        # in case the class inherits from object and does not
        # define __init__
        class_signature = ""{clean_module_name}.{cls_name}()"".format(
            clean_module_name=clean_module_name(cls.__module__),
            cls_name=cls.__name__
        )
    return post_process_signature(class_signature)

def post_process_signature(signature):
    parts = re.split(r'\.(?!\d)', signature)
    if len(parts) >= 4:
        if parts[1] == 'layers':
            signature = 'keras.layers.' + '.'.join(parts[3:])
        if parts[1] == 'utils':
            signature = 'keras.utils.' + '.'.join(parts[3:])
        if parts[1] == 'backend':
            signature = 'keras.backend.' + '.'.join(parts[3:])
    return signature

def clean_module_name(name):
    if name.startswith('keras_applications'):
        name = name.replace('keras_applications', 'keras.applications')
    if name.startswith('keras_preprocessing'):
        name = name.replace('keras_preprocessing', 'keras.preprocessing')
    assert name[:6] == 'keras.', 'Invalid module name: %s' % name
    return name

def class_to_source_link(cls):
    module_name = clean_module_name(cls.__module__)
    path = module_name.replace('.', '/')
    path += '.py'
    line = inspect.getsourcelines(cls)[-1]
    link = ('https://github.com/keras-team/'
            'keras/blob/master/' + path + '#L' + str(line))
    return '[[source]](' + link + ')'

def code_snippet(snippet):
    result = '```python\n'
    result += snippet + '\n'
    result += '```\n'
    return result

def count_leading_spaces(s):
    ws = re.search(r'\S', s)
    if ws:
        return ws.start()
    else:
        return 0

def process_list_block(docstring, starting_point, section_end,
                       leading_spaces, marker):
    ending_point = docstring.find('\n\n', starting_point)
    block = docstring[starting_point:(None if ending_point == -1 else
                                      ending_point - 1)]
    # Place marker for later reinjection.
    docstring_slice = docstring[starting_point:section_end].replace(block, marker)
    docstring = (docstring[:starting_point]
                 + docstring_slice
                 + docstring[section_end:])
    lines = block.split('\n')","[119, 120]"
"    __slots__ = ()
    is_numeric = True
    _can_hold_na = True

class FloatOrComplexBlock(NumericBlock):
    __slots__ = ()
    def equals(self, other) -> bool:
        if self.dtype != other.dtype or self.shape != other.shape:
            return False
        left, right = self.values, other.values
        return ((left == right) | (np.isnan(left) & np.isnan(right))).all()

class FloatBlock(FloatOrComplexBlock):
    __slots__ = ()
    is_float = True
    def _can_hold_element(self, element: Any) -> bool:
        tipo = maybe_infer_dtype_type(element)
        if tipo is not None:
            return issubclass(tipo.type, (np.floating, np.integer)) and not issubclass(
                tipo.type, (np.datetime64, np.timedelta64)
            )
        return isinstance(
            element, (float, int, np.floating, np.int_)
        ) and not isinstance(
            element,
            (bool, np.bool_, datetime, timedelta, np.datetime64, np.timedelta64),
        )
    def to_native_types(
        self,
        slicer=None,
        na_rep="""",
        float_format=None,
        decimal=""."",
        quoting=None,
        **kwargs,
    ):
        """""" convert to our native types format, slicing if desired """"""
        values = self.values
        if slicer is not None:
            values = values[:, slicer]
        # see gh-13418: no special formatting is desired at the
        # output (important for appropriate 'quoting' behaviour),
        # so do not pass it through the FloatArrayFormatter
        if float_format is None and decimal == ""."":
            mask = isna(values)
            if not quoting:
                values = values.astype(str)
            else:
                values = np.array(values, dtype=""object"")
            values[mask] = na_rep
            return values
        from pandas.io.formats.format import FloatArrayFormatter
        formatter = FloatArrayFormatter(
            values,
            na_rep=na_rep,
            float_format=float_format,
            decimal=decimal,
            quoting=quoting,
            fixed_width=False,
        )
        return formatter.get_result_as_array()
    def should_store(self, value) -> bool:
        # when inserting a column should not coerce integers to floats
        # unnecessarily
        return issubclass(value.dtype.type, np.floating) and value.dtype == self.dtype

class ComplexBlock(FloatOrComplexBlock):
    __slots__ = ()
    is_complex = True
    def _can_hold_element(self, element: Any) -> bool:
        tipo = maybe_infer_dtype_type(element)
        if tipo is not None:
            return issubclass(tipo.type, (np.floating, np.integer, np.complexfloating))
        return isinstance(
            element, (float, int, complex, np.float_, np.int_)
        ) and not isinstance(element, (bool, np.bool_))
    def should_store(self, value) -> bool:
        return issubclass(value.dtype.type, np.complexfloating)

class IntBlock(NumericBlock):
    __slots__ = ()
    is_integer = True
    _can_hold_na = False
    def _can_hold_element(self, element: Any) -> bool:
        tipo = maybe_infer_dtype_type(element)
        if tipo is not None:
            return (
                issubclass(tipo.type, np.integer)
                and not issubclass(tipo.type, (np.datetime64, np.timedelta64))
                and self.dtype.itemsize >= tipo.itemsize
            )
        return is_integer(element)
    def should_store(self, value) -> bool:
        return is_integer_dtype(value) and value.dtype == self.dtype

class DatetimeLikeBlockMixin:
    """"""Mixin class for DatetimeBlock, DatetimeTZBlock, and TimedeltaBlock.""""""
    @property
    def _holder(self):
        return DatetimeArray
    @property
    def fill_value(self):
        return np.datetime64(""NaT"", ""ns"")
    def get_values(self, dtype=None):
        """"""
        return object dtype as boxed values, such as Timestamps/Timedelta","[72, 90, 109]"
"from .common import InfoExtractor
from ..utils import (
    compat_http_client,
    compat_str,
    compat_urllib_error,
    compat_urllib_parse,
    compat_urllib_request,
    urlencode_postdata,
    ExtractorError,
)

class FacebookIE(InfoExtractor):
    _VALID_URL = r'''(?x)
        https?://(?:\w+\.)?facebook\.com/
        (?:[^#?]*\#!/)?
        (?:video/video\.php|photo\.php|video/embed)\?(?:.*?)
        (?:v|video_id)=(?P<id>[0-9]+)
        (?:.*)'''
    _LOGIN_URL = 'https://www.facebook.com/login.php?next=http%3A%2F%2Ffacebook.com%2Fhome.php&login_attempt=1'
    _CHECKPOINT_URL = 'https://www.facebook.com/checkpoint/?next=http%3A%2F%2Ffacebook.com%2Fhome.php&_fb_noscript=1'
    _NETRC_MACHINE = 'facebook'
    IE_NAME = 'facebook'
    _TEST = {
        'url': 'https://www.facebook.com/photo.php?v=120708114770723',
        'md5': '48975a41ccc4b7a581abd68651c1a5a8',
        'info_dict': {
            'id': '120708114770723',
            'ext': 'mp4',
            'duration': 279,
            'title': 'PEOPLE ARE AWESOME 2013',
        }
    }
    def _login(self):
        (useremail, password) = self._get_login_info()
        if useremail is None:
            return
        login_page_req = compat_urllib_request.Request(self._LOGIN_URL)
        login_page_req.add_header('Cookie', 'locale=en_US')
        login_page = self._download_webpage(login_page_req, None,
            note='Downloading login page',
            errnote='Unable to download login page')
        lsd = self._search_regex(
            r'<input type=""hidden"" name=""lsd"" value=""([^""]*)""',
            login_page, 'lsd')
        lgnrnd = self._search_regex(r'name=""lgnrnd"" value=""([^""]*?)""', login_page, 'lgnrnd')
        login_form = {
            'email': useremail,
            'pass': password,
            'lsd': lsd,
            'lgnrnd': lgnrnd,
            'next': 'http://facebook.com/home.php',
            'default_persistent': '0',
            'legacy_return': '1',
            'timezone': '-60',
            'trynum': '1',
            }
        request = compat_urllib_request.Request(self._LOGIN_URL, urlencode_postdata(login_form))
        request.add_header('Content-Type', 'application/x-www-form-urlencoded')
        try:
            login_results = self._download_webpage(request, None,
                note='Logging in', errnote='unable to fetch login page')
            if re.search(r'<form(.*)name=""login""(.*)</form>', login_results) is not None:
                self._downloader.report_warning('unable to log in: bad username/password, or exceded login rate limit (~3/min). Check credentials or wait.')
                return
            check_form = {
                'fb_dtsg': self._search_regex(r'name=""fb_dtsg"" value=""(.+?)""', login_results, 'fb_dtsg'),
                'h': self._search_regex(r'name=""h"" value=""(\w*?)""', login_results, 'h'),
                'name_action_selected': 'dont_save',
            }
            check_req = compat_urllib_request.Request(self._CHECKPOINT_URL, urlencode_postdata(check_form))
            check_req.add_header('Content-Type', 'application/x-www-form-urlencoded')
            check_response = self._download_webpage(check_req, None,
                note='Confirming login')
            if re.search(r'id=""checkpointSubmitButton""', check_response) is not None:
                self._downloader.report_warning('Unable to confirm login, you have to login in your brower and authorize the login.')
        except (compat_urllib_error.URLError, compat_http_client.HTTPException, socket.error) as err:
            self._downloader.report_warning('unable to log in: %s' % compat_str(err))
            return
    def _real_initialize(self):
        self._login()
    def _real_extract(self, url):
        mobj = re.match(self._VALID_URL, url)
        video_id = mobj.group('id')
        url = 'https://www.facebook.com/video/video.php?v=%s' % video_id
        webpage = self._download_webpage(url, video_id)
        BEFORE = '{swf.addParam(param[0], param[1]);});\n'
        AFTER = '.forEach(function(variable) {swf.addVariable(variable[0], variable[1]);});'
        m = re.search(re.escape(BEFORE) + '(.*?)' + re.escape(AFTER), webpage)
        if not m:
            m_msg = re.search(r'class=""[^""]*uiInterstitialContent[^""]*""><div>(.*?)</div>', webpage)
            if m_msg is not None:
                raise ExtractorError(
                    'The video is not available, Facebook said: ""%s""' % m_msg.group(1),
                    expected=True)
            else:
                raise ExtractorError('Cannot parse data')
        data = dict(json.loads(m.group(1)))
        params_raw = compat_urllib_parse.unquote(data['params'])
        params = json.loads(params_raw)
        video_data = params['video_data'][0]
        video_url = video_data.get('hd_src')
        if not video_url:
            video_url = video_data['sd_src']
        if not video_url:
            raise ExtractorError('Cannot find video URL')
        video_title = self._html_search_regex(
            r'<h2 class=""uiHeaderTitle"">([^<]*)</h2>', webpage, 'title')
        return {
            'id': video_id,
            'title': video_title,
            'url': video_url,
            'duration': int(video_data['video_duration']),
            'thumbnail': video_data['thumbnail_src'],",[17]
"            % (content_range, resumed_from)
        )
    return last_byte_pos + 1

def filename_from_content_disposition(content_disposition):
    """"""
    Extract and validate filename from a Content-Disposition header.
    :param content_disposition: Content-Disposition value
    :return: the filename if present and valid, otherwise `None`
    """"""
    # attachment; filename=jkbrzt-httpie-0.4.1-20-g40bd8f6.tar.gz
    msg = Message('Content-Disposition: %s' % content_disposition)
    filename = msg.get_filename()
    if filename:
        # Basic sanitation.
        filename = os.path.basename(filename).lstrip('.').strip()
        if filename:
            return filename

def filename_from_url(url, content_type):
    fn = urlsplit(url).path.rstrip('/')
    fn = os.path.basename(fn) if fn else 'index'
    if '.' not in fn and content_type:
        content_type = content_type.split(';')[0]
        if content_type == 'text/plain':
            # mimetypes returns '.ksh'
            ext = '.txt'
        else:
            ext = mimetypes.guess_extension(content_type)
        if ext == '.htm':  # Python 3
            ext = '.html'
        if ext:
            fn += ext
    return fn

def get_unique_filename(filename, exists=os.path.exists):
    attempt = 0
    while True:
        suffix = '-' + str(attempt) if attempt > 0 else ''
        if not exists(filename + suffix):
            return filename + suffix
        attempt += 1

class Downloader(object):
    def __init__(self, output_file=None,
                 resume=False, progress_file=sys.stderr):
        """"""
        :param resume: Should the download resume if partial download
                       already exists.
        :type resume: bool
        :param output_file: The file to store response body in. If not
                            provided, it will be guessed from the response.
        :param progress_file: Where to report download progress.
        """"""
        self._output_file = output_file
        self._resume = resume
        self._resumed_from = 0
        self.finished = False
        self.status = Status()
        self._progress_reporter = ProgressReporterThread(
            status=self.status,
            output=progress_file
        )
    def pre_request(self, request_headers):
        """"""Called just before the HTTP request is sent.
        Might alter `request_headers`.
        :type request_headers: dict
        """"""
        # Ask the server not to encode the content so that we can resume, etc.
        request_headers['Accept-Encoding'] = 'identity'
        if self._resume:
            bytes_have = os.path.getsize(self._output_file.name)
            if bytes_have:
                # Set ``Range`` header to resume the download
                # TODO: Use ""If-Range: mtime"" to make sure it's fresh?
                request_headers['Range'] = 'bytes=%d-' % bytes_have
                self._resumed_from = bytes_have
    def start(self, response):
        """"""
        Initiate and return a stream for `response` body  with progress
        callback attached. Can be called only once.
        :param response: Initiated response object with headers already fetched
        :type response: requests.models.Response
        :return: RawStream, output_file
        """"""
        assert not self.status.time_started
        # FIXME: some servers still might sent Content-Encoding: gzip
        # <https://github.com/jkbrzt/httpie/issues/423>
        try:
            total_size = int(response.headers['Content-Length'])
        except (KeyError, ValueError, TypeError):
            total_size = None
        if self._output_file:
            if self._resume and response.status_code == PARTIAL_CONTENT:
                total_size = parse_content_range(
                    response.headers.get('Content-Range'),
                    self._resumed_from
                )
            else:
                self._resumed_from = 0","[49, 50]"
"    When `pairwise` is set to `False`, only matching columns between `self` and
    `other` will be used.
    When `pairwise` is set to `True`, the output will be a MultiIndex DataFrame
    with the original index on the first level, and the `other` DataFrame
    columns on the second level.
    In the case of missing elements, only complete pairwise observations
    will be used.
    Examples
    --------
    The below example shows a rolling calculation with a window size of
    four matching the equivalent function call using :meth:`numpy.corrcoef`.
    >>> v1 = [3, 3, 3, 5, 8]
    >>> v2 = [3, 4, 4, 4, 8]
    >>> fmt = ""{0:.6f}""  # limit the printed precision to 6 digits
    >>> # numpy returns a 2X2 array, the correlation coefficient
    >>> # is the number at entry [0][1]
    >>> print(fmt.format(np.corrcoef(v1[:-1], v2[:-1])[0][1]))
    0.333333
    >>> print(fmt.format(np.corrcoef(v1[1:], v2[1:])[0][1]))
    0.916949
    >>> s1 = pd.Series(v1)
    >>> s2 = pd.Series(v2)
    >>> s1.rolling(4).corr(s2)
    0         NaN
    1         NaN
    2         NaN
    3    0.333333
    4    0.916949
    dtype: float64
    The below example shows a similar rolling calculation on a
    DataFrame using the pairwise option.
    >>> matrix = np.array([[51., 35.], [49., 30.], [47., 32.],\
    [46., 31.], [50., 36.]])
    >>> print(np.corrcoef(matrix[:-1,0], matrix[:-1,1]).round(7))
    [[1.         0.6263001]
     [0.6263001  1.       ]]
    >>> print(np.corrcoef(matrix[1:,0], matrix[1:,1]).round(7))
    [[1.         0.5553681]
     [0.5553681  1.        ]]
    >>> df = pd.DataFrame(matrix, columns=['X','Y'])
    >>> df
          X     Y
    0  51.0  35.0
    1  49.0  30.0
    2  47.0  32.0
    3  46.0  31.0
    4  50.0  36.0
    >>> df.rolling(4).corr(pairwise=True)
                X         Y
    0 X       NaN       NaN
      Y       NaN       NaN
    1 X       NaN       NaN
      Y       NaN       NaN
    2 X       NaN       NaN
      Y       NaN       NaN
    3 X  1.000000  0.626300
      Y  0.626300  1.000000
    4 X  1.000000  0.555368
      Y  0.555368  1.000000
    """"""
    )
    def corr(self, other=None, pairwise=None, **kwargs):
        if other is None:
            other = self._selected_obj
            # only default unset
            pairwise = True if pairwise is None else pairwise
        other = self._shallow_copy(other)
        window = self._get_window(other)
        def _get_corr(a, b):
            a = a.rolling(
                window=window, min_periods=self.min_periods, center=self.center
            )
            b = b.rolling(
                window=window, min_periods=self.min_periods, center=self.center
            )
            return a.cov(b, **kwargs) / (a.std(**kwargs) * b.std(**kwargs))
        return _flex_binary_moment(
            self._selected_obj, other._selected_obj, _get_corr, pairwise=bool(pairwise)
        )

class Rolling(_Rolling_and_Expanding):
    @cache_readonly
    def is_datetimelike(self):
        return isinstance(
            self._on, (ABCDatetimeIndex, ABCTimedeltaIndex, ABCPeriodIndex)
        )
    @cache_readonly
    def _on(self):
        if self.on is None:
            return self.obj.index
        elif isinstance(self.obj, ABCDataFrame) and self.on in self.obj.columns:
            return Index(self.obj[self.on])
        else:
            raise ValueError(
                ""invalid on specified as {0}, ""
                ""must be a column (if DataFrame) ""
                ""or None"".format(self.on)
            )
    def validate(self):
        super().validate()
        # we allow rolling on a datetimelike index
        if (self.obj.empty or self.is_datetimelike) and isinstance(
            self.window, (str, ABCDateOffset, timedelta)
        ):
            self._validate_monotonic()
            freq = self._validate_freq()
            # we don't allow center
            if self.center:
                raise NotImplementedError(
                    ""center is not implemented """,[102]
"        elif leaf.type in CLOSING_BRACKETS:
            if not leaf.value:
                optional_brackets.add(id(opening_bracket))
                continue
            if index > 0 and line.leaves[index - 1].type in OPENING_BRACKETS:
                # Empty brackets would fail a split so treat them as ""inner""
                # brackets (e.g. only add them to the `omit` set if another
                # pair of brackets was good enough.
                inner_brackets.add(id(leaf))
                continue
            opening_bracket = leaf.opening_bracket
            if closing_bracket:
                omit.add(id(closing_bracket))
                omit.update(inner_brackets)
                inner_brackets.clear()
                yield omit
            closing_bracket = leaf

def get_future_imports(node: Node) -> Set[str]:
    """"""Return a set of __future__ imports in the file.""""""
    imports = set()
    for child in node.children:
        if child.type != syms.simple_stmt:
            break
        first_child = child.children[0]
        if isinstance(first_child, Leaf):
            # Continue looking if we see a docstring; otherwise stop.
            if (
                len(child.children) == 2
                and first_child.type == token.STRING
                and child.children[1].type == token.NEWLINE
            ):
                continue
            else:
                break
        elif first_child.type == syms.import_from:
            module_name = first_child.children[1]
            if not isinstance(module_name, Leaf) or module_name.value != ""__future__"":
                break
            for import_from_child in first_child.children[3:]:
                if isinstance(import_from_child, Leaf):
                    if import_from_child.type == token.NAME:
                        imports.add(import_from_child.value)
                else:
                    assert import_from_child.type == syms.import_as_names
                    for leaf in import_from_child.children:
                        if isinstance(leaf, Leaf) and leaf.type == token.NAME:
                            imports.add(leaf.value)
        else:
            break
    return imports

def gen_python_files_in_dir(
    path: Path,
    root: Path,
    include: Pattern[str],
    exclude: Pattern[str],
    report: ""Report"",
) -> Iterator[Path]:
    """"""Generate all files under `path` whose paths are not excluded by the
    `exclude` regex, but are included by the `include` regex.
    Symbolic links pointing outside of the `root` directory are ignored.
    `report` is where output about exclusions goes.
    """"""
    assert root.is_absolute(), f""INTERNAL ERROR: `root` must be absolute but is {root}""
    for child in path.iterdir():
        try:
            normalized_path = ""/"" + child.resolve().relative_to(root).as_posix()
        except ValueError:
            if child.is_symlink():
                report.path_ignored(
                    child, f""is a symbolic link that points outside {root}""
                )
                continue
            raise
        if child.is_dir():
            normalized_path += ""/""
        exclude_match = exclude.search(normalized_path)
        if exclude_match and exclude_match.group(0):
            report.path_ignored(child, f""matches the --exclude regular expression"")
            continue
        if child.is_dir():
            yield from gen_python_files_in_dir(child, root, include, exclude, report)
        elif child.is_file():
            include_match = include.search(normalized_path)
            if include_match:
                yield child

@lru_cache()
def find_project_root(srcs: Iterable[str]) -> Path:
    """"""Return a directory containing .git, .hg, or pyproject.toml.
    That directory can be one of the directories passed in `srcs` or their
    common parent.
    If no directory in the tree contains a marker that would specify it's the
    project root, the root of the file system is returned.
    """"""
    if not srcs:
        return Path(""/"").resolve()
    common_base = min(Path(src).resolve() for src in srcs)
    if common_base.is_dir():
        # Append a fake file so `parents` below returns `common_base_dir`, too.
        common_base /= ""fake-file""
    for directory in common_base.parents:
        if (directory / "".git"").is_dir():
            return directory
        if (directory / "".hg"").is_dir():
            return directory
        if (directory / ""pyproject.toml"").is_file():
            return directory
    return directory","[23, 42, 43, 44, 45, 46, 47, 48, 49, 50]"
"# coding: utf-8
from __future__ import unicode_literals
import re
import bz2
import logging
import random
import json
from spacy.gold import GoldParse
from bin.wiki_entity_linking import wiki_io as io
from bin.wiki_entity_linking.wiki_namespaces import (
    WP_META_NAMESPACE,
    WP_FILE_NAMESPACE,
    WP_CATEGORY_NAMESPACE,
)
""""""
Process a Wikipedia dump to calculate entity frequencies and prior probabilities in combination with certain mentions.
Write these results to file for downstream KB and training data generation.
Process Wikipedia interlinks to generate a training dataset for the EL algorithm.
""""""
ENTITY_FILE = ""gold_entities.csv""
map_alias_to_link = dict()
logger = logging.getLogger(__name__)
title_regex = re.compile(r""(?<=<title>).*(?=</title>)"")
id_regex = re.compile(r""(?<=<id>)\d*(?=</id>)"")
text_regex = re.compile(r""(?<=<text xml:space=\""preserve\"">).*(?=</text)"")
info_regex = re.compile(r""{[^{]*?}"")
html_regex = re.compile(r""&lt;!--[^-]*--&gt;"")
ref_regex = re.compile(r""&lt;ref.*?&gt;"")  # non-greedy
ref_2_regex = re.compile(r""&lt;/ref.*?&gt;"")  # non-greedy
# find the links
link_regex = re.compile(r""\[\[[^\[\]]*\]\]"")
# match on interwiki links, e.g. `en:` or `:fr:`
ns_regex = r"":?"" + ""[a-z][a-z]"" + "":""
# match on Namespace: optionally preceded by a :
for ns in WP_META_NAMESPACE:
    ns_regex += ""|"" + "":?"" + ns + "":""
ns_regex = re.compile(ns_regex, re.IGNORECASE)
files = r""""
for f in WP_FILE_NAMESPACE:
    files += ""\[\["" + f + "":[^[\]]+]]"" + ""|""
files = files[0 : len(files) - 1]
file_regex = re.compile(files)
cats = r""""
for c in WP_CATEGORY_NAMESPACE:
    cats += ""\[\["" + c + "":[^\[]*]]"" + ""|""
cats = cats[0 : len(cats) - 1]
category_regex = re.compile(cats)

def read_prior_probs(wikipedia_input, prior_prob_output, limit=None):
    """"""
    Read the XML wikipedia data and parse out intra-wiki links to estimate prior probabilities.
    The full file takes about 2-3h to parse 1100M lines.
    It works relatively fast because it runs line by line, irrelevant of which article the intrawiki is from,
    though dev test articles are excluded in order not to get an artificially strong baseline.
    """"""
    cnt = 0
    read_id = False
    current_article_id = None
    with bz2.open(wikipedia_input, mode=""rb"") as file:
        line = file.readline()
        while line and (not limit or cnt < limit):
            if cnt % 25000000 == 0 and cnt > 0:
                logger.info(""processed {} lines of Wikipedia XML dump"".format(cnt))
            clean_line = line.strip().decode(""utf-8"")
            # we attempt at reading the article's ID (but not the revision or contributor ID)
            if ""<revision>"" in clean_line or ""<contributor>"" in clean_line:
                read_id = False
            if ""<page>"" in clean_line:
                read_id = True
            if read_id:
                ids = id_regex.search(clean_line)
                if ids:
                    current_article_id = ids[0]
            # only processing prior probabilities from true training (non-dev) articles
            if not is_dev(current_article_id):
                aliases, entities, normalizations = get_wp_links(clean_line)
                for alias, entity, norm in zip(aliases, entities, normalizations):
                    _store_alias(
                        alias, entity, normalize_alias=norm, normalize_entity=True
                    )
            line = file.readline()
            cnt += 1
        logger.info(""processed {} lines of Wikipedia XML dump"".format(cnt))
    logger.info(""Finished. processed {} lines of Wikipedia XML dump"".format(cnt))
    # write all aliases and their entities and count occurrences to file
    with prior_prob_output.open(""w"", encoding=""utf8"") as outputfile:
        outputfile.write(""alias"" + ""|"" + ""count"" + ""|"" + ""entity"" + ""\n"")
        for alias, alias_dict in sorted(map_alias_to_link.items(), key=lambda x: x[0]):
            s_dict = sorted(alias_dict.items(), key=lambda x: x[1], reverse=True)
            for entity, count in s_dict:
                outputfile.write(alias + ""|"" + str(count) + ""|"" + entity + ""\n"")

def _store_alias(alias, entity, normalize_alias=False, normalize_entity=True):
    alias = alias.strip()
    entity = entity.strip()
    # remove everything after # as this is not part of the title but refers to a specific paragraph
    if normalize_entity:
        # wikipedia titles are always capitalized
        entity = _capitalize_first(entity.split(""#"")[0])
    if normalize_alias:
        alias = alias.split(""#"")[0]
    if alias and entity:
        alias_dict = map_alias_to_link.get(alias, dict())
        entity_count = alias_dict.get(entity, 0)
        alias_dict[entity] = entity_count + 1
        map_alias_to_link[alias] = alias_dict",[32]
"                        for line in f:
                            # create numpy arrays of input data
                            # and labels, from each line in the file
                            x1, x2, y = process_line(line)
                            yield ({'input_1': x1, 'input_2': x2}, {'output': y})
            model.fit_generator(generate_arrays_from_file('/my_file.txt'),
                                steps_per_epoch=10000, epochs=10)
        ```
        """"""
        wait_time = 0.01  # in seconds
        epoch = initial_epoch
        do_validation = bool(validation_data)
        self._make_train_function()
        if do_validation:
            self._make_test_function()
        is_sequence = isinstance(generator, Sequence)
        if not is_sequence and use_multiprocessing and workers > 1:
            warnings.warn(
                UserWarning('Using a generator with `use_multiprocessing=True`'
                            ' and multiple workers may duplicate your data.'
                            ' Please consider using the`keras.utils.Sequence'
                            ' class.'))
        if steps_per_epoch is None:
            if is_sequence:
                steps_per_epoch = len(generator)
            else:
                raise ValueError('`steps_per_epoch=None` is only valid for a'
                                 ' generator based on the `keras.utils.Sequence`'
                                 ' class. Please specify `steps_per_epoch` or use'
                                 ' the `keras.utils.Sequence` class.')
        # python 2 has 'next', 3 has '__next__'
        # avoid any explicit version checks
        val_gen = (hasattr(validation_data, 'next') or
                   hasattr(validation_data, '__next__') or
                   isinstance(validation_data, Sequence))
        if (val_gen and not isinstance(validation_data, Sequence) and
                not validation_steps):
            raise ValueError('`validation_steps=None` is only valid for a'
                             ' generator based on the `keras.utils.Sequence`'
                             ' class. Please specify `validation_steps` or use'
                             ' the `keras.utils.Sequence` class.')
        # Prepare display labels.
        out_labels = self.metrics_names
        callback_metrics = out_labels + ['val_' + n for n in out_labels]
        # prepare callbacks
        self.history = cbks.History()
        _callbacks = [cbks.BaseLogger(
            stateful_metrics=self.stateful_metric_names)]
        if verbose:
            _callbacks.append(
                cbks.ProgbarLogger(
                    count_mode='steps',
                    stateful_metrics=self.stateful_metric_names))
        _callbacks += (callbacks or []) + [self.history]
        callbacks = cbks.CallbackList(_callbacks)
        # it's possible to callback a different model than self:
        if hasattr(self, 'callback_model') and self.callback_model:
            callback_model = self.callback_model
        else:
            callback_model = self
        callbacks.set_model(callback_model)
        callbacks.set_params({
            'epochs': epochs,
            'steps': steps_per_epoch,
            'verbose': verbose,
            'do_validation': do_validation,
            'metrics': callback_metrics,
        })
        callbacks.on_train_begin()
        enqueuer = None
        val_enqueuer = None
        try:
            if do_validation and not val_gen:
                # Prepare data for validation
                if len(validation_data) == 2:
                    val_x, val_y = validation_data
                    val_sample_weight = None
                elif len(validation_data) == 3:
                    val_x, val_y, val_sample_weight = validation_data
                else:
                    raise ValueError('`validation_data` should be a tuple '
                                     '`(val_x, val_y, val_sample_weight)` '
                                     'or `(val_x, val_y)`. Found: ' +
                                     str(validation_data))
                val_x, val_y, val_sample_weights = self._standardize_user_data(
                    val_x, val_y, val_sample_weight)
                val_data = val_x + val_y + val_sample_weights
                if self.uses_learning_phase and not isinstance(K.learning_phase(), int):
                    val_data += [0.]
                for cbk in callbacks:
                    cbk.validation_data = val_data
            if workers > 0:
                if is_sequence:
                    enqueuer = OrderedEnqueuer(generator,
                                               use_multiprocessing=use_multiprocessing,
                                               shuffle=shuffle)
                else:
                    enqueuer = GeneratorEnqueuer(generator,
                                                 use_multiprocessing=use_multiprocessing,
                                                 wait_time=wait_time)
                enqueuer.start(workers=workers, max_queue_size=max_queue_size)
                output_generator = enqueuer.get()
            else:
                if is_sequence:
                    output_generator = iter(generator)
                else:
                    output_generator = generator
            callback_model.stop_training = False
            # Construct epoch logs.
            epoch_logs = {}
            while epoch < epochs:
                for m in self.metrics:
                    if isinstance(m, Layer) and m.stateful:
                        m.reset_states()
                callbacks.on_epoch_begin(epoch)
                steps_done = 0","[122, 123, 124]"
"        Iceland       337000    17036      IS
        Nauru          11300      182      NR
        Tuvalu         11300       38      TV
        Anguilla       11300      311      AI
        In the following example, we will use ``nsmallest`` to select the
        three rows having the smallest values in column ""a"".
        >>> df.nsmallest(3, 'population')
                  population  GDP alpha-2
        Nauru          11300  182      NR
        Tuvalu         11300   38      TV
        Anguilla       11300  311      AI
        When using ``keep='last'``, ties are resolved in reverse order:
        >>> df.nsmallest(3, 'population', keep='last')
                  population  GDP alpha-2
        Anguilla       11300  311      AI
        Tuvalu         11300   38      TV
        Nauru          11300  182      NR
        When using ``keep='all'``, all duplicate items are maintained:
        >>> df.nsmallest(3, 'population', keep='all')
                  population  GDP alpha-2
        Nauru          11300  182      NR
        Tuvalu         11300   38      TV
        Anguilla       11300  311      AI
        To order by the largest values in column ""a"" and then ""c"", we can
        specify multiple columns like in the next example.
        >>> df.nsmallest(3, ['population', 'GDP'])
                  population  GDP alpha-2
        Tuvalu         11300   38      TV
        Nauru          11300  182      NR
        Anguilla       11300  311      AI
        """"""
        return algorithms.SelectNFrame(
            self, n=n, keep=keep, columns=columns
        ).nsmallest()
    def swaplevel(self, i=-2, j=-1, axis=0) -> ""DataFrame"":
        """"""
        Swap levels i and j in a MultiIndex on a particular axis.
        Parameters
        ----------
        i, j : int or str
            Levels of the indices to be swapped. Can pass level name as string.
        Returns
        -------
        DataFrame
        """"""
        result = self.copy()
        axis = self._get_axis_number(axis)
        if not isinstance(result._get_axis(axis), ABCMultiIndex):  # pragma: no cover
            raise TypeError(""Can only swap levels on a hierarchical axis."")
        if axis == 0:
            assert isinstance(result.index, ABCMultiIndex)
            result.index = result.index.swaplevel(i, j)
        else:
            assert isinstance(result.columns, ABCMultiIndex)
            result.columns = result.columns.swaplevel(i, j)
        return result
    def reorder_levels(self, order, axis=0) -> ""DataFrame"":
        """"""
        Rearrange index levels using input order. May not drop or duplicate levels.
        Parameters
        ----------
        order : list of int or list of str
            List representing new level order. Reference level by number
            (position) or by key (label).
        axis : int
            Where to reorder levels.
        Returns
        -------
        DataFrame
        """"""
        axis = self._get_axis_number(axis)
        if not isinstance(self._get_axis(axis), ABCMultiIndex):  # pragma: no cover
            raise TypeError(""Can only reorder levels on a hierarchical axis."")
        result = self.copy()
        if axis == 0:
            assert isinstance(result.index, ABCMultiIndex)
            result.index = result.index.reorder_levels(order)
        else:
            assert isinstance(result.columns, ABCMultiIndex)
            result.columns = result.columns.reorder_levels(order)
        return result
    # ----------------------------------------------------------------------
    # Arithmetic / combination related
    def _combine_frame(self, other, func, fill_value=None, level=None):
        # at this point we have `self._indexed_same(other)`
        if fill_value is None:
            # since _arith_op may be called in a loop, avoid function call
            #  overhead if possible by doing this check once
            _arith_op = func
        else:
            def _arith_op(left, right):
                # for the mixed_type case where we iterate over columns,
                # _arith_op(left, right) is equivalent to
                # left._binop(right, func, fill_value=fill_value)
                left, right = ops.fill_binop(left, right, fill_value)
                return func(left, right)
        if ops.should_series_dispatch(self, other, func):
            # iterate over columns
            new_data = ops.dispatch_to_series(self, other, _arith_op)
        else:
            with np.errstate(all=""ignore""):
                res_values = _arith_op(self.values, other.values)",[104]
"                self.image_shape = self.target_size + (3,)
            else:
                self.image_shape = (3,) + self.target_size
        else:
            if self.data_format == 'channels_last':
                self.image_shape = self.target_size + (1,)
            else:
                self.image_shape = (1,) + self.target_size
        self.classes = classes
        if class_mode not in {'categorical', 'binary', 'sparse',
                              'input', None}:
            raise ValueError('Invalid class_mode:', class_mode,
                             '; expected one of ""categorical"", '
                             '""binary"", ""sparse"", ""input""'
                             ' or None.')
        self.class_mode = class_mode
        self.save_to_dir = save_to_dir
        self.save_prefix = save_prefix
        self.save_format = save_format
        self.interpolation = interpolation
        if subset is not None:
            validation_split = self.image_data_generator._validation_split
            if subset == 'validation':
                split = (0, validation_split)
            elif subset == 'training':
                split = (validation_split, 1)
            else:
                raise ValueError('Invalid subset name: ', subset,
                                 '; expected ""training"" or ""validation""')
        else:
            split = None
        self.subset = subset
        white_list_formats = {'png', 'jpg', 'jpeg', 'bmp', 'ppm', 'tif', 'tiff'}
        # first, count the number of samples and classes
        self.samples = 0
        if not classes:
            classes = []
            for subdir in sorted(os.listdir(directory)):
                if os.path.isdir(os.path.join(directory, subdir)):
                    classes.append(subdir)
        self.num_classes = len(classes)
        self.class_indices = dict(zip(classes, range(len(classes))))
        pool = multiprocessing.pool.ThreadPool()
        function_partial = partial(_count_valid_files_in_directory,
                                   white_list_formats=white_list_formats,
                                   follow_links=follow_links,
                                   split=split)
        self.samples = sum(pool.map(function_partial,
                                    (os.path.join(directory, subdir)
                                     for subdir in classes)))
        print('Found %d images belonging to %d classes.' % (self.samples, self.num_classes))
        # second, build an index of the images in the different class subfolders
        results = []
        self.filenames = []
        self.classes = np.zeros((self.samples,), dtype='int32')
        i = 0
        for dirpath in (os.path.join(directory, subdir) for subdir in classes):
            results.append(pool.apply_async(_list_valid_filenames_in_directory,
                                            (dirpath, white_list_formats, split,
                                             self.class_indices, follow_links)))
        for res in results:
            classes, filenames = res.get()
            self.classes[i:i + len(classes)] = classes
            self.filenames += filenames
            i += len(classes)
        pool.close()
        pool.join()
        super(DirectoryIterator, self).__init__(self.samples, batch_size, shuffle, seed)
    def _get_batches_of_transformed_samples(self, index_array):
        batch_x = np.zeros((len(index_array),) + self.image_shape, dtype=K.floatx())
        grayscale = self.color_mode == 'grayscale'
        # build batch of image data
        for i, j in enumerate(index_array):
            fname = self.filenames[j]
            img = load_img(os.path.join(self.directory, fname),
                           grayscale=grayscale,
                           target_size=self.target_size,
                           interpolation=self.interpolation)
            x = img_to_array(img, data_format=self.data_format)
            x = self.image_data_generator.random_transform(x)
            x = self.image_data_generator.standardize(x)
            batch_x[i] = x
        # optionally save augmented images to disk for debugging purposes
        if self.save_to_dir:
            for i, j in enumerate(index_array):
                img = array_to_img(batch_x[i], self.data_format, scale=True)
                fname = '{prefix}_{index}_{hash}.{format}'.format(prefix=self.save_prefix,
                                                                  index=j,
                                                                  hash=np.random.randint(1e7),
                                                                  format=self.save_format)
                img.save(os.path.join(self.save_to_dir, fname))
        # build batch of labels
        if self.class_mode == 'input':
            batch_y = batch_x.copy()
        elif self.class_mode == 'sparse':
            batch_y = self.classes[index_array]
        elif self.class_mode == 'binary':
            batch_y = self.classes[index_array].astype(K.floatx())
        elif self.class_mode == 'categorical':
            batch_y = np.zeros((len(batch_x), self.num_classes), dtype=K.floatx())
            for i, label in enumerate(self.classes[index_array]):
                batch_y[i, label] = 1.
        else:
            return batch_x
        return batch_x, batch_y
    def next(self):
        """"""For python 2.x.
        # Returns
            The next batch.
        """"""
        with self.lock:
            index_array = next(self.index_generator)
        # The transformation of images is not under thread lock
        # so it can be done in parallel",[86]
"            return func(*self.args, **self.kwds)
        # ufunc
        elif isinstance(self.f, np.ufunc):
            with np.errstate(all=""ignore""):
                results = self.obj._data.apply(""apply"", func=self.f)
            return self.obj._constructor(
                data=results, index=self.index, columns=self.columns, copy=False
            )
        # broadcasting
        if self.result_type == ""broadcast"":
            return self.apply_broadcast()
        # one axis empty
        elif not all(self.obj.shape):
            return self.apply_empty_result()
        # raw
        elif self.raw and not self.obj._is_mixed_type:
            return self.apply_raw()
        return self.apply_standard()
    def apply_empty_result(self):
        """"""
        we have an empty result; at least 1 axis is 0
        we will try to apply the function to an empty
        series in order to see if this is a reduction function
        """"""
        # we are not asked to reduce or infer reduction
        # so just return a copy of the existing object
        if self.result_type not in [""reduce"", None]:
            return self.obj.copy()
        # we may need to infer
        should_reduce = self.result_type == ""reduce""
        from pandas import Series
        if not should_reduce:
            EMPTY_SERIES = Series([])
            try:
                r = self.f(EMPTY_SERIES, *self.args, **self.kwds)
            except Exception:
                pass
            else:
                should_reduce = not isinstance(r, Series)
        if should_reduce:
            return self.obj._constructor_sliced(np.nan, index=self.agg_axis)
        else:
            return self.obj.copy()
    def apply_raw(self):
        """""" apply to the values as a numpy array """"""
        try:
            result = libreduction.compute_reduction(self.values, self.f, axis=self.axis)
        except Exception:
            result = np.apply_along_axis(self.f, self.axis, self.values)
        # TODO: mixed type case
        if result.ndim == 2:
            return self.obj._constructor(result, index=self.index, columns=self.columns)
        else:
            return self.obj._constructor_sliced(result, index=self.agg_axis)
    def apply_broadcast(self, target):
        result_values = np.empty_like(target.values)
        # axis which we want to compare compliance
        result_compare = target.shape[0]
        for i, col in enumerate(target.columns):
            res = self.f(target[col])
            ares = np.asarray(res).ndim
            # must be a scalar or 1d
            if ares > 1:
                raise ValueError(""too many dims to broadcast"")
            elif ares == 1:
                # must match return dim
                if result_compare != len(res):
                    raise ValueError(""cannot broadcast result"")
            result_values[:, i] = res
        # we *always* preserve the original index / columns
        result = self.obj._constructor(
            result_values, index=target.index, columns=target.columns
        )
        return result
    def apply_standard(self):
        # try to reduce first (by default)
        # this only matters if the reduction in values is of different dtype
        # e.g. if we want to apply to a SparseFrame, then can't directly reduce
        # we cannot reduce using non-numpy dtypes,
        # as demonstrated in gh-12244
        if (
            self.result_type in [""reduce"", None]
            and not self.dtypes.apply(is_extension_type).any()
        ):
            # Create a dummy Series from an empty array
            from pandas import Series
            values = self.values
            index = self.obj._get_axis(self.axis)
            labels = self.agg_axis
            empty_arr = np.empty(len(index), dtype=values.dtype)
            dummy = Series(empty_arr, index=index, dtype=values.dtype)
            try:
                result = libreduction.compute_reduction(
                    values, self.f, axis=self.axis, dummy=dummy, labels=labels
                )
                return self.obj._constructor_sliced(result, index=labels)
            except Exception:
                pass","[43, 44, 46, 53]"
"            The current x-axis limits in data coordinates.
        See Also
        --------
        set_xlim
        set_xbound, get_xbound
        invert_xaxis, xaxis_inverted
        Notes
        -----
        The x-axis may be inverted, in which case the *left* value will
        be greater than the *right* value.
        """"""
        return tuple(self.viewLim.intervalx)
    def _validate_converted_limits(self, limit, convert):
        """"""
        Raise ValueError if converted limits are non-finite.
        Note that this function also accepts None as a limit argument.
        Returns
        -------
        The limit value after call to convert(), or None if limit is None.
        """"""
        if limit is not None:
            converted_limit = convert(limit)
            if (isinstance(converted_limit, Real)
                    and not np.isfinite(converted_limit)):
                raise ValueError(""Axis limits cannot be NaN or Inf"")
            return converted_limit
    def set_xlim(self, left=None, right=None, emit=True, auto=False,
                 *, xmin=None, xmax=None):
        """"""
        Set the x-axis view limits.
        .. ACCEPTS: (left: float, right: float)
        Parameters
        ----------
        left : scalar, optional
            The left xlim in data coordinates. Passing *None* leaves the
            limit unchanged.
            The left and right xlims may be passed as the tuple
            (*left*, *right*) as the first positional argument (or as
            the *left* keyword argument).
        right : scalar, optional
            The right xlim in data coordinates. Passing *None* leaves the
            limit unchanged.
        emit : bool, optional
            Whether to notify observers of limit change (default: True).
        auto : bool or None, optional
            Whether to turn on autoscaling of the x-axis. True turns on,
            False turns off (default action), None leaves unchanged.
        xmin, xmax : scalar, optional
            They are equivalent to left and right respectively,
            and it is an error to pass both *xmin* and *left* or
            *xmax* and *right*.
        Returns
        -------
        left, right : (float, float)
            The new x-axis limits in data coordinates.
        See Also
        --------
        get_xlim
        set_xbound, get_xbound
        invert_xaxis, xaxis_inverted
        Notes
        -----
        The *left* value may be greater than the *right* value, in which
        case the x-axis values will decrease from left to right.
        Examples
        --------
        >>> set_xlim(left, right)
        >>> set_xlim((left, right))
        >>> left, right = set_xlim(left, right)
        One limit may be left unchanged.
        >>> set_xlim(right=right_lim)
        Limits may be passed in reverse order to flip the direction of
        the x-axis. For example, suppose *x* represents the number of
        years before present. The x-axis limits might be set like the
        following so 5000 years ago is on the left of the plot and the
        present is on the right.
        >>> set_xlim(5000, 0)
        """"""
        if right is None and np.iterable(left):
            left, right = left
        if xmin is not None:
            if left is not None:
                raise TypeError('Cannot pass both `xmin` and `left`')
            left = xmin
        if xmax is not None:
            if right is not None:
                raise TypeError('Cannot pass both `xmax` and `right`')
            right = xmax
        self._process_unit_info(xdata=(left, right))
        left = self._validate_converted_limits(left, self.convert_xunits)
        right = self._validate_converted_limits(right, self.convert_xunits)
        if left is None or right is None:
            # Axes init calls set_xlim(0, 1) before get_xlim() can be called,
            # so only grab the limits if we really need them.
            old_left, old_right = self.get_xlim()
            if left is None:
                left = old_left
            if right is None:
                right = old_right
        if self.get_xscale() == 'log':
            if left <= 0:",[125]
"        model._make_test_function()
    is_sequence = isinstance(generator, Sequence)
    if not is_sequence and use_multiprocessing and workers > 1:
        warnings.warn(
            UserWarning('Using a generator with `use_multiprocessing=True`'
                        ' and multiple workers may duplicate your data.'
                        ' Please consider using the`keras.utils.Sequence'
                        ' class.'))
    if steps_per_epoch is None:
        if is_sequence:
            steps_per_epoch = len(generator)
        else:
            raise ValueError('`steps_per_epoch=None` is only valid for a'
                             ' generator based on the '
                             '`keras.utils.Sequence`'
                             ' class. Please specify `steps_per_epoch` '
                             'or use the `keras.utils.Sequence` class.')
    # python 2 has 'next', 3 has '__next__'
    # avoid any explicit version checks
    val_gen = (hasattr(validation_data, 'next') or
               hasattr(validation_data, '__next__') or
               isinstance(validation_data, Sequence))
    if (val_gen and not isinstance(validation_data, Sequence) and
            not validation_steps):
        raise ValueError('`validation_steps=None` is only valid for a'
                         ' generator based on the `keras.utils.Sequence`'
                         ' class. Please specify `validation_steps` or use'
                         ' the `keras.utils.Sequence` class.')
    # Prepare display labels.
    out_labels = model.metrics_names
    callback_metrics = out_labels + ['val_' + n for n in out_labels]
    # prepare callbacks
    model.history = cbks.History()
    _callbacks = [cbks.BaseLogger(
        stateful_metrics=model.stateful_metric_names)]
    if verbose:
        _callbacks.append(
            cbks.ProgbarLogger(
                count_mode='steps',
                stateful_metrics=model.stateful_metric_names))
    _callbacks += (callbacks or []) + [model.history]
    callbacks = cbks.CallbackList(_callbacks)
    # it's possible to callback a different model than self:
    if hasattr(model, 'callback_model') and model.callback_model:
        callback_model = model.callback_model
    else:
        callback_model = model
    callbacks.set_model(callback_model)
    callbacks.set_params({
        'epochs': epochs,
        'steps': steps_per_epoch,
        'verbose': verbose,
        'do_validation': do_validation,
        'metrics': callback_metrics,
    })
    callbacks.on_train_begin()
    enqueuer = None
    val_enqueuer = None
    try:
        if do_validation:
            if val_gen and workers > 0:
                # Create an Enqueuer that can be reused
                val_data = validation_data
                if isinstance(val_data, Sequence):
                    val_enqueuer = OrderedEnqueuer(
                        val_data,
                        use_multiprocessing=use_multiprocessing)
                    validation_steps = validation_steps or len(val_data)
                else:
                    val_enqueuer = GeneratorEnqueuer(
                        val_data,
                        use_multiprocessing=use_multiprocessing)
                val_enqueuer.start(workers=workers,
                                   max_queue_size=max_queue_size)
                val_enqueuer_gen = val_enqueuer.get()
            elif val_gen:
                val_data = validation_data
                if isinstance(val_data, Sequence):
                    val_enqueuer_gen = iter_sequence_infinite(val_data)
                    validation_steps = validation_steps or len(val_data)
                else:
                    val_enqueuer_gen = val_data
            else:
                # Prepare data for validation
                if len(validation_data) == 2:
                    val_x, val_y = validation_data
                    val_sample_weight = None
                elif len(validation_data) == 3:
                    val_x, val_y, val_sample_weight = validation_data
                else:
                    raise ValueError('`validation_data` should be a tuple '
                                     '`(val_x, val_y, val_sample_weight)` '
                                     'or `(val_x, val_y)`. Found: ' +
                                     str(validation_data))
                val_x, val_y, val_sample_weights = model._standardize_user_data(
                    val_x, val_y, val_sample_weight)
                val_data = val_x + val_y + val_sample_weights
                if model.uses_learning_phase and not isinstance(K.learning_phase(),
                                                                int):
                    val_data += [0.]
                for cbk in callbacks:
                    cbk.validation_data = val_data
        if workers > 0:
            if is_sequence:
                enqueuer = OrderedEnqueuer(
                    generator,
                    use_multiprocessing=use_multiprocessing,
                    shuffle=shuffle)
            else:
                enqueuer = GeneratorEnqueuer(
                    generator,
                    use_multiprocessing=use_multiprocessing)
            enqueuer.start(workers=workers, max_queue_size=max_queue_size)
            output_generator = enqueuer.get()
        else:
            if is_sequence:
                output_generator = iter_sequence_infinite(generator)
            else:
                output_generator = generator","[2, 3, 10, 23, 24, 70, 84, 111, 123]"
"                if style:
                    font = ''
                    for k, v in sorted(style.items()):
                        if self._applied_styles and self._applied_styles[-1].get(k) == v:
                            continue
                        if k == 'color':
                            font += ' color=""%s""' % v
                        elif k == 'fontSize':
                            font += ' size=""%s""' % v
                        elif k == 'fontFamily':
                            font += ' face=""%s""' % v
                        elif k == 'fontWeight' and v == 'bold':
                            self._out += '<b>'
                            unclosed_elements.append('b')
                        elif k == 'fontStyle' and v == 'italic':
                            self._out += '<i>'
                            unclosed_elements.append('i')
                        elif k == 'textDecoration' and v == 'underline':
                            self._out += '<u>'
                            unclosed_elements.append('u')
                    if font:
                        self._out += '<font' + font + '>'
                        unclosed_elements.append('font')
                    applied_style = {}
                    if self._applied_styles:
                        applied_style.update(self._applied_styles[-1])
                    applied_style.update(style)
                    self._applied_styles.append(applied_style)
                self._unclosed_elements.append(unclosed_elements)
        def end(self, tag):
            if tag not in (_x('ttml:br'), 'br'):
                unclosed_elements = self._unclosed_elements.pop()
                for element in reversed(unclosed_elements):
                    self._out += '</%s>' % element
                if unclosed_elements and self._applied_styles:
                    self._applied_styles.pop()
        def data(self, data):
            self._out += data
        def close(self):
            return self._out.strip()
    def parse_node(node):
        target = TTMLPElementParser()
        parser = xml.etree.ElementTree.XMLParser(target=target)
        parser.feed(xml.etree.ElementTree.tostring(node))
        return parser.close()
    for k, v in LEGACY_NAMESPACES:
        for ns in v:
            dfxp_data = dfxp_data.replace(ns, k)
    dfxp = compat_etree_fromstring(dfxp_data.encode('utf-8'))
    out = []
    paras = dfxp.findall(_x('.//ttml:p')) or dfxp.findall('.//p')
    if not paras:
        raise ValueError('Invalid dfxp/TTML subtitle')
    repeat = False
    while True:
        for style in dfxp.findall(_x('.//ttml:style')):
            style_id = style.get('id')
            parent_style_id = style.get('style')
            if parent_style_id:
                if parent_style_id not in styles:
                    repeat = True
                    continue
                styles[style_id] = styles[parent_style_id].copy()
            for prop in SUPPORTED_STYLING:
                prop_val = style.get(_x('tts:' + prop))
                if prop_val:
                    styles.setdefault(style_id, {})[prop] = prop_val
        if repeat:
            repeat = False
        else:
            break
    for p in ('body', 'div'):
        ele = xpath_element(dfxp, [_x('.//ttml:' + p), './/' + p])
        if ele is None:
            continue
        style = styles.get(ele.get('style'))
        if not style:
            continue
        default_style.update(style)
    for para, index in zip(paras, itertools.count(1)):
        begin_time = parse_dfxp_time_expr(para.attrib.get('begin'))
        end_time = parse_dfxp_time_expr(para.attrib.get('end'))
        dur = parse_dfxp_time_expr(para.attrib.get('dur'))
        if begin_time is None:
            continue
        if not end_time:
            if not dur:
                continue
            end_time = begin_time + dur
        out.append('%d\n%s --> %s\n%s\n\n' % (
            index,
            srt_subtitles_timecode(begin_time),
            srt_subtitles_timecode(end_time),
            parse_node(para)))
    return ''.join(out)

def cli_option(params, command_option, param):
    param = params.get(param)
    if param:
        param = compat_str(param)
    return [command_option, param] if param is not None else []

def cli_bool_option(params, command_option, param, true_value='true', false_value='false', separator=None):
    param = params.get(param)
    if param is None:
        return []
    assert isinstance(param, bool)
    if separator:
        return [command_option + separator + (true_value if param else false_value)]
    return [command_option, true_value if param else false_value]

def cli_valueless_option(params, command_option, param, expected_value=True):
    param = params.get(param)",[54]
"from scrapy.utils.python import to_bytes
from scrapy.utils.trackref import object_ref
from scrapy.utils.url import escape_ajax
from scrapy.http.common import obsolete_setter
from scrapy.utils.curl import curl_to_request_kwargs

class Request(object_ref):
    def __init__(self, url, callback=None, method='GET', headers=None, body=None,
                 cookies=None, meta=None, encoding='utf-8', priority=0,
                 dont_filter=False, errback=None, flags=None, cb_kwargs=None):
        self._encoding = encoding  # this one has to be set first
        self.method = str(method).upper()
        self._set_url(url)
        self._set_body(body)
        assert isinstance(priority, int), ""Request priority not an integer: %r"" % priority
        self.priority = priority
        if callback is not None and not callable(callback):
            raise TypeError('callback must be a callable, got %s' % type(callback).__name__)
        if errback is not None and not callable(errback):
            raise TypeError('errback must be a callable, got %s' % type(errback).__name__)
        assert callback or not errback, ""Cannot use errback without a callback""
        self.callback = callback
        self.errback = errback
        self.cookies = cookies or {}
        self.headers = Headers(headers or {}, encoding=encoding)
        self.dont_filter = dont_filter
        self._meta = dict(meta) if meta else None
        self._cb_kwargs = dict(cb_kwargs) if cb_kwargs else None
        self.flags = [] if flags is None else list(flags)
    @property
    def cb_kwargs(self):
        if self._cb_kwargs is None:
            self._cb_kwargs = {}
        return self._cb_kwargs
    @property
    def meta(self):
        if self._meta is None:
            self._meta = {}
        return self._meta
    def _get_url(self):
        return self._url
    def _set_url(self, url):
        if not isinstance(url, six.string_types):
            raise TypeError('Request url must be str or unicode, got %s:' % type(url).__name__)
        s = safe_url_string(url, self.encoding)
        self._url = escape_ajax(s)
        if ':' not in self._url:
            raise ValueError('Missing scheme in request url: %s' % self._url)
    url = property(_get_url, obsolete_setter(_set_url, 'url'))
    def _get_body(self):
        return self._body
    def _set_body(self, body):
        if body is None:
            self._body = b''
        else:
            self._body = to_bytes(body, self.encoding)
    body = property(_get_body, obsolete_setter(_set_body, 'body'))
    @property
    def encoding(self):
        return self._encoding
    def __str__(self):
        return ""<%s %s>"" % (self.method, self.url)
    __repr__ = __str__
    def copy(self):
        """"""Return a copy of this Request""""""
        return self.replace()
    def replace(self, *args, **kwargs):
        """"""Create a new Request with the same attributes except for those
        given new values.
        """"""
        for x in ['url', 'method', 'headers', 'body', 'cookies', 'meta', 'flags',
                  'encoding', 'priority', 'dont_filter', 'callback', 'errback', 'cb_kwargs']:
            kwargs.setdefault(x, getattr(self, x))
        cls = kwargs.pop('cls', self.__class__)
        return cls(*args, **kwargs)
    @classmethod
    def from_curl(cls, curl_command, ignore_unknown_options=True, **kwargs):
        """"""Create a Request object from a string containing a `cURL
        <https://curl.haxx.se/>`_ command. It populates the HTTP method, the
        URL, the headers, the cookies and the body. It accepts the same
        arguments as the :class:`Request` class, taking preference and
        overriding the values of the same arguments contained in the cURL
        command.
        Unrecognized options are ignored by default. To raise an error when
        finding unknown options call this method by passing
        ``ignore_unknown_options=False``.
        .. caution:: Using :meth:`from_curl` from :class:`~scrapy.http.Request`
                     subclasses, such as :class:`~scrapy.http.JSONRequest`, or
                     :class:`~scrapy.http.XmlRpcRequest`, as well as having
                     :ref:`downloader middlewares <topics-downloader-middleware>`
                     and
                     :ref:`spider middlewares <topics-spider-middleware>`
                     enabled, such as
                     :class:`~scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware`,
                     :class:`~scrapy.downloadermiddlewares.useragent.UserAgentMiddleware`,
                     or
                     :class:`~scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware`,
                     may modify the :class:`~scrapy.http.Request` object.
       """"""
        request_kwargs = curl_to_request_kwargs(curl_command, ignore_unknown_options)
        request_kwargs.update(kwargs)",[58]
"        else:
            axis = sample._get_axis_number(axis)
        # Need to flip BlockManager axis in the DataFrame special case
        self._is_frame = isinstance(sample, DataFrame)
        if self._is_frame:
            axis = 1 if axis == 0 else 0
        self._is_series = isinstance(sample, Series)
        if not 0 <= axis <= sample.ndim:
            raise AssertionError(
                ""axis must be between 0 and {ndim}, input was ""
                ""{axis}"".format(ndim=sample.ndim, axis=axis)
            )
        # if we have mixed ndims, then convert to highest ndim
        # creating column numbers as needed
        if len(ndims) > 1:
            current_column = 0
            max_ndim = sample.ndim
            self.objs, objs = [], self.objs
            for obj in objs:
                ndim = obj.ndim
                if ndim == max_ndim:
                    pass
                elif ndim != max_ndim - 1:
                    raise ValueError(
                        ""cannot concatenate unaligned mixed ""
                        ""dimensional NDFrame objects""
                    )
                else:
                    name = getattr(obj, ""name"", None)
                    if ignore_index or name is None:
                        name = current_column
                        current_column += 1
                    # doing a row-wise concatenation so need everything
                    # to line up
                    if self._is_frame and axis == 1:
                        name = 0
                    obj = sample._constructor({name: obj})
                self.objs.append(obj)
        # note: this is the BlockManager axis (since DataFrame is transposed)
        self.axis = axis
        self.keys = keys
        self.names = names or getattr(keys, ""names"", None)
        self.levels = levels
        self.sort = sort
        self.ignore_index = ignore_index
        self.verify_integrity = verify_integrity
        self.copy = copy
        self.new_axes = self._get_new_axes()
    def get_result(self):
        # series only
        if self._is_series:
            # stack blocks
            if self.axis == 0:
                name = com.consensus_name_attr(self.objs)
                mgr = self.objs[0]._data.concat(
                    [x._data for x in self.objs], self.new_axes
                )
                cons = self.objs[0]._constructor
                return cons(mgr, name=name).__finalize__(self, method=""concat"")
            # combine as columns in a frame
            else:
                data = dict(zip(range(len(self.objs)), self.objs))
                cons = DataFrame
                index, columns = self.new_axes
                df = cons(data, index=index)
                df.columns = columns
                return df.__finalize__(self, method=""concat"")
        # combine block managers
        else:
            mgrs_indexers = []
            for obj in self.objs:
                mgr = obj._data
                indexers = {}
                for ax, new_labels in enumerate(self.new_axes):
                    if ax == self.axis:
                        # Suppress reindexing on concat axis
                        continue
                    obj_labels = mgr.axes[ax]
                    if not new_labels.equals(obj_labels):
                        indexers[ax] = obj_labels.reindex(new_labels)[1]
                mgrs_indexers.append((obj._data, indexers))
            new_data = concatenate_block_managers(
                mgrs_indexers, self.new_axes, concat_axis=self.axis, copy=self.copy
            )
            if not self.copy:
                new_data._consolidate_inplace()
            cons = self.objs[0]._constructor
            return cons(new_data).__finalize__(self, method=""concat"")
    def _get_result_dim(self) -> int:
        if self._is_series and self.axis == 1:
            return 2
        else:
            return self.objs[0].ndim
    def _get_new_axes(self) -> List[Index]:
        ndim = self._get_result_dim()
        return [
            self._get_concat_axis() if i == self.axis else self._get_comb_axis(i)
            for i in range(ndim)
        ]
    def _get_comb_axis(self, i: int) -> Index:
        data_axis = self.objs[0]._get_block_manager_axis(i)
        return get_objs_combined_axis(",[127]
"        #  yarr = isinstance(y, np.ndarray)
        #  yint = is_integer(y) or (yarr and y.dtype.kind == ""i"")
        #  ybool = is_bool(y) or (yarr and y.dtype.kind == ""b"")
        #  xint = x.dtype.kind == ""i""
        #  xbool = x.dtype.kind == ""b""
        # Then Cases where this goes through without raising include:
        #  (xint or xbool) and (yint or bool)
        result = op(x, y)
    except TypeError:
        if isinstance(y, np.ndarray):
            # bool-bool dtype operations should be OK, should not get here
            assert not (is_bool_dtype(x.dtype) and is_bool_dtype(y.dtype))
            x = ensure_object(x)
            y = ensure_object(y)
            result = libops.vec_binop(x, y, op)
        else:
            # let null fall thru
            assert lib.is_scalar(y)
            if not isna(y):
                y = bool(y)
            try:
                result = libops.scalar_binop(x, y, op)
            except (
                TypeError,
                ValueError,
                AttributeError,
                OverflowError,
                NotImplementedError,
            ):
                typ = type(y).__name__
                raise TypeError(
                    f""Cannot perform '{op.__name__}' with a dtyped [{x.dtype}] array ""
                    f""and scalar of type [{typ}]""
                )
    return result

def logical_op(
    left: Union[np.ndarray, ABCExtensionArray], right: Any, op
) -> Union[np.ndarray, ABCExtensionArray]:
    """"""
    Evaluate a logical operation `|`, `&`, or `^`.
    Parameters
    ----------
    left : np.ndarray or ExtensionArray
    right : object
        Cannot be a DataFrame, Series, or Index.
    op : {operator.and_, operator.or_, operator.xor}
        Or one of the reversed variants from roperator.
    Returns
    -------
    ndarrray or ExtensionArray
    """"""
    fill_int = lambda x: x
    def fill_bool(x, left=None):
        # if `left` is specifically not-boolean, we do not cast to bool
        if x.dtype.kind in [""c"", ""f"", ""O""]:
            # dtypes that can hold NA
            mask = isna(x)
            if mask.any():
                x = x.astype(object)
                x[mask] = False
        if left is None or is_bool_dtype(left.dtype):
            x = x.astype(bool)
        return x
    is_self_int_dtype = is_integer_dtype(left.dtype)
    right = lib.item_from_zerodim(right)
    if is_list_like(right) and not hasattr(right, ""dtype""):
        # e.g. list, tuple
        right = construct_1d_object_array_from_listlike(right)
    # NB: We assume extract_array has already been called on left and right
    lvalues = left
    rvalues = right
    if should_extension_dispatch(lvalues, rvalues):
        res_values = dispatch_to_extension_op(op, lvalues, rvalues)
    else:
        if isinstance(rvalues, np.ndarray):
            is_other_int_dtype = is_integer_dtype(rvalues.dtype)
            rvalues = rvalues if is_other_int_dtype else fill_bool(rvalues, lvalues)
        else:
            # i.e. scalar
            is_other_int_dtype = lib.is_integer(rvalues)
        # For int vs int `^`, `|`, `&` are bitwise operators and return
        #   integer dtypes.  Otherwise these are boolean ops
        filler = fill_int if is_self_int_dtype and is_other_int_dtype else fill_bool
        res_values = na_logical_op(lvalues, rvalues, op)
        res_values = filler(res_values)  # type: ignore
    return res_values

def get_array_op(op, str_rep: Optional[str] = None):
    """"""
    Return a binary array operation corresponding to the given operator op.
    Parameters
    ----------
    op : function
        Binary operator from operator or roperator module.
    str_rep : str or None, default None
        str_rep to pass to arithmetic_op
    Returns
    -------
    function
    """"""
    op_name = op.__name__.strip(""_"")
    if op_name in {""eq"", ""ne"", ""lt"", ""le"", ""gt"", ""ge""}:
        return partial(comparison_op, op=op)
    elif op_name in {""and"", ""or"", ""xor"", ""rand"", ""ror"", ""rxor""}:
        return partial(logical_op, op=op)
    else:","[14, 35]"
"""""""
SQL-style merge routines
""""""
import copy
import datetime
from functools import partial
import string
import warnings
import numpy as np
from pandas._libs import hashtable as libhashtable, lib
import pandas._libs.join as libjoin
from pandas.errors import MergeError
from pandas.util._decorators import Appender, Substitution
from pandas.core.dtypes.common import (
    ensure_float64,
    ensure_int64,
    ensure_object,
    is_array_like,
    is_bool,
    is_bool_dtype,
    is_categorical_dtype,
    is_datetime64tz_dtype,
    is_datetimelike,
    is_dtype_equal,
    is_extension_array_dtype,
    is_float_dtype,
    is_int64_dtype,
    is_integer,
    is_integer_dtype,
    is_list_like,
    is_number,
    is_numeric_dtype,
    is_object_dtype,
    needs_i8_conversion,
)
from pandas.core.dtypes.missing import isnull, na_value_for_dtype
from pandas import Categorical, DataFrame, Index, MultiIndex, Series, Timedelta
import pandas.core.algorithms as algos
from pandas.core.arrays.categorical import _recode_for_categories
import pandas.core.common as com
from pandas.core.frame import _merge_doc
from pandas.core.internals import _transform_index, concatenate_block_managers
import pandas.core.sorting as sorting
from pandas.core.sorting import is_int64_overflow_possible

@Substitution(""\nleft : DataFrame"")
@Appender(_merge_doc, indents=0)
def merge(
    left,
    right,
    how=""inner"",
    on=None,
    left_on=None,
    right_on=None,
    left_index=False,
    right_index=False,
    sort=False,
    suffixes=(""_x"", ""_y""),
    copy=True,
    indicator=False,
    validate=None,
):
    op = _MergeOperation(
        left,
        right,
        how=how,
        on=on,
        left_on=left_on,
        right_on=right_on,
        left_index=left_index,
        right_index=right_index,
        sort=sort,
        suffixes=suffixes,
        copy=copy,
        indicator=indicator,
        validate=validate,
    )
    return op.get_result()

if __debug__:
    merge.__doc__ = _merge_doc % ""\nleft : DataFrame""

def _groupby_and_merge(by, on, left, right, _merge_pieces, check_duplicates=True):
    """"""
    groupby & merge; we are always performing a left-by type operation
    Parameters
    ----------
    by: field to group
    on: duplicates field
    left: left frame
    right: right frame
    _merge_pieces: function for merging
    check_duplicates: bool, default True
        should we check & clean duplicates
    """"""
    pieces = []
    if not isinstance(by, (list, tuple)):
        by = [by]
    lby = left.groupby(by, sort=False)
    # if we can groupby the rhs
    # then we can get vastly better perf
    try:
        # we will check & remove duplicates if indicated
        if check_duplicates:
            if on is None:
                on = []
            elif not isinstance(on, (list, tuple)):
                on = [on]
            if right.duplicated(by + on).any():
                right = right.drop_duplicates(by + on, keep=""last"")
        rby = right.groupby(by, sort=False)
    except KeyError:
        rby = None",[30]
"from typing import TYPE_CHECKING, Callable, Dict, List, Tuple, Union
import numpy as np
from pandas.util._decorators import Appender, Substitution
from pandas.core.dtypes.cast import maybe_downcast_to_dtype
from pandas.core.dtypes.common import is_integer_dtype, is_list_like, is_scalar
from pandas.core.dtypes.generic import ABCDataFrame, ABCSeries
import pandas.core.common as com
from pandas.core.frame import _shared_docs
from pandas.core.groupby import Grouper
from pandas.core.indexes.api import Index, MultiIndex, get_objs_combined_axis
from pandas.core.reshape.concat import concat
from pandas.core.reshape.util import cartesian_product
from pandas.core.series import Series
if TYPE_CHECKING:
    from pandas import DataFrame

# Note: We need to make sure `frame` is imported before `pivot`, otherwise
# _shared_docs['pivot_table'] will not yet exist.  TODO: Fix this dependency
@Substitution(""\ndata : DataFrame"")
@Appender(_shared_docs[""pivot_table""], indents=1)
def pivot_table(
    data,
    values=None,
    index=None,
    columns=None,
    aggfunc=""mean"",
    fill_value=None,
    margins=False,
    dropna=True,
    margins_name=""All"",
    observed=False,
) -> ""DataFrame"":
    index = _convert_by(index)
    columns = _convert_by(columns)
    if isinstance(aggfunc, list):
        pieces: List[DataFrame] = []
        keys = []
        for func in aggfunc:
            table = pivot_table(
                data,
                values=values,
                index=index,
                columns=columns,
                fill_value=fill_value,
                aggfunc=func,
                margins=margins,
                dropna=dropna,
                margins_name=margins_name,
                observed=observed,
            )
            pieces.append(table)
            keys.append(getattr(func, ""__name__"", func))
        return concat(pieces, keys=keys, axis=1)
    keys = index + columns
    values_passed = values is not None
    if values_passed:
        if is_list_like(values):
            values_multi = True
            values = list(values)
        else:
            values_multi = False
            values = [values]
        # GH14938 Make sure value labels are in data
        for i in values:
            if i not in data:
                raise KeyError(i)
        to_filter = []
        for x in keys + values:
            if isinstance(x, Grouper):
                x = x.key
            try:
                if x in data:
                    to_filter.append(x)
            except TypeError:
                pass
        if len(to_filter) < len(data.columns):
            data = data[to_filter]
    else:
        values = data.columns
        for key in keys:
            try:
                values = values.drop(key)
            except (TypeError, ValueError, KeyError):
                pass
        values = list(values)
    grouped = data.groupby(keys, observed=observed)
    agged = grouped.agg(aggfunc)
    if dropna and isinstance(agged, ABCDataFrame) and len(agged.columns):
        agged = agged.dropna(how=""all"")
        # gh-21133
        # we want to down cast if
        # the original values are ints
        # as we grouped with a NaN value
        # and then dropped, coercing to floats
        for v in values:
            if (
                v in data
                and is_integer_dtype(data[v])
                and v in agged
                and not is_integer_dtype(agged[v])
            ):
                agged[v] = maybe_downcast_to_dtype(agged[v], data[v].dtype)
    table = agged
    if table.index.nlevels > 1:
        # Related GH #17123
        # If index_names are integers, determine whether the integers refer
        # to the level position or name.
        index_names = agged.index.names[: len(index)]
        to_unstack = []
        for i in range(len(index), len(keys)):
            name = agged.index.names[i]",[119]
"    def dependencies(self):
        if self._metadata:
            return self._metadata.dependencies
        elif len(self.versions) > 1:
            return None
        self._get_metadata()
        return self._metadata.dependencies
    def add_requirement(self, parent, requirement):
        self.required_by.append((parent, requirement))
        new_versions = set(v for v in self.versions if self._meets_requirements(v, requirement, parent))
        if len(new_versions) == 0:
            if self.skip:
                force_flag = '--force-with-deps' if parent else '--force'
                version = self.latest_version if self.latest_version != '*' else 'unknown'
                msg = ""Cannot meet requirement %s:%s as it is already installed at version '%s'. Use %s to overwrite"" \
                      % (to_text(self), requirement, version, force_flag)
                raise AnsibleError(msg)
            elif parent is None:
                msg = ""Cannot meet requirement %s for dependency %s"" % (requirement, to_text(self))
            else:
                msg = ""Cannot meet dependency requirement '%s:%s' for collection %s"" \
                      % (to_text(self), requirement, parent)
            collection_source = to_text(self.b_path, nonstring='passthru') or self.api.api_server
            req_by = ""\n"".join(
                ""\t%s - '%s:%s'"" % (to_text(p) if p else 'base', to_text(self), r)
                for p, r in self.required_by
            )
            versions = "", "".join(sorted(self.versions, key=LooseVersion))
            raise AnsibleError(
                ""%s from source '%s'. Available versions before last requirement added: %s\nRequirements from:\n%s""
                % (msg, collection_source, versions, req_by)
            )
        self.versions = new_versions
    def install(self, path, b_temp_path):
        if self.skip:
            display.display(""Skipping '%s' as it is already installed"" % to_text(self))
            return
        # Install if it is not
        collection_path = os.path.join(path, self.namespace, self.name)
        b_collection_path = to_bytes(collection_path, errors='surrogate_or_strict')
        display.display(""Installing '%s:%s' to '%s'"" % (to_text(self), self.latest_version, collection_path))
        if self.b_path is None:
            download_url = self._metadata.download_url
            artifact_hash = self._metadata.artifact_sha256
            headers = {}
            self.api._add_auth_token(headers, download_url, required=False)
            self.b_path = _download_file(download_url, b_temp_path, artifact_hash, self.api.validate_certs,
                                         headers=headers)
        if os.path.exists(b_collection_path):
            shutil.rmtree(b_collection_path)
        os.makedirs(b_collection_path)
        with tarfile.open(self.b_path, mode='r') as collection_tar:
            files_member_obj = collection_tar.getmember('FILES.json')
            with _tarfile_extract(collection_tar, files_member_obj) as files_obj:
                files = json.loads(to_text(files_obj.read(), errors='surrogate_or_strict'))
            _extract_tar_file(collection_tar, 'MANIFEST.json', b_collection_path, b_temp_path)
            _extract_tar_file(collection_tar, 'FILES.json', b_collection_path, b_temp_path)
            for file_info in files['files']:
                file_name = file_info['name']
                if file_name == '.':
                    continue
                if file_info['ftype'] == 'file':
                    _extract_tar_file(collection_tar, file_name, b_collection_path, b_temp_path,
                                      expected_hash=file_info['chksum_sha256'])
                else:
                    os.makedirs(os.path.join(b_collection_path, to_bytes(file_name, errors='surrogate_or_strict')))
    def set_latest_version(self):
        self.versions = set([self.latest_version])
        self._get_metadata()
    def _get_metadata(self):
        if self._metadata:
            return
        self._metadata = self.api.get_collection_version_metadata(self.namespace, self.name, self.latest_version)
    def _meets_requirements(self, version, requirements, parent):
        """"""
        Supports version identifiers can be '==', '!=', '>', '>=', '<', '<=', '*'. Each requirement is delimited by ','
        """"""
        op_map = {
            '!=': operator.ne,
            '==': operator.eq,
            '=': operator.eq,
            '>=': operator.ge,
            '>': operator.gt,
            '<=': operator.le,
            '<': operator.lt,
        }
        for req in list(requirements.split(',')):
            op_pos = 2 if len(req) > 1 and req[1] == '=' else 1
            op = op_map.get(req[:op_pos])
            requirement = req[op_pos:]
            if not op:
                requirement = req
                op = operator.eq
                # In the case we are checking a new requirement on a base requirement (parent != None) we can't accept
                # version as '*' (unknown version) unless the requirement is also '*'.
                if parent and version == '*' and requirement != '*':
                    break
                elif requirement == '*' or version == '*':
                    continue
            if not op(LooseVersion(version), LooseVersion(requirement)):
                break
        else:
            return True
        # The loop was broken early, it does not meet all the requirements
        return False","[115, 116, 117, 118]"
"        self._state_path = state_path
        self._tasks = {}  # map from id to a Task object
        self._status_tasks = collections.defaultdict(dict)
        self._active_workers = {}  # map from id to a Worker object
    def get_state(self):
        return self._tasks, self._active_workers
    def set_state(self, state):
        self._tasks, self._active_workers = state
    def dump(self):
        try:
            with open(self._state_path, 'wb') as fobj:
                pickle.dump(self.get_state(), fobj)
        except IOError:
            logger.warning(""Failed saving scheduler state"", exc_info=1)
        else:
            logger.info(""Saved state in %s"", self._state_path)
    # prone to lead to crashes when old state is unpickled with updated code. TODO some kind of version control?
    def load(self):
        if os.path.exists(self._state_path):
            logger.info(""Attempting to load state from %s"", self._state_path)
            try:
                with open(self._state_path, 'rb') as fobj:
                    state = pickle.load(fobj)
            except BaseException:
                logger.exception(""Error when loading state. Starting from empty state."")
                return
            self.set_state(state)
            self._status_tasks = collections.defaultdict(dict)
            for task in six.itervalues(self._tasks):
                self._status_tasks[task.status][task.id] = task
        else:
            logger.info(""No prior state file exists at %s. Starting with empty state"", self._state_path)
    def get_active_tasks(self, status=None):
        if status:
            for task in six.itervalues(self._status_tasks[status]):
                yield task
        else:
            for task in six.itervalues(self._tasks):
                yield task
    def get_running_tasks(self):
        return six.itervalues(self._status_tasks[RUNNING])
    def get_pending_tasks(self):
        return itertools.chain.from_iterable(six.itervalues(self._status_tasks[status])
                                             for status in [PENDING, RUNNING])
    def num_pending_tasks(self):
        """"""
        Return how many tasks are PENDING + RUNNING. O(1).
        """"""
        return len(self._status_tasks[PENDING]) + len(self._status_tasks[RUNNING])
    def get_task(self, task_id, default=None, setdefault=None):
        if setdefault:
            task = self._tasks.setdefault(task_id, setdefault)
            self._status_tasks[task.status][task.id] = task
            return task
        else:
            return self._tasks.get(task_id, default)
    def has_task(self, task_id):
        return task_id in self._tasks
    def re_enable(self, task, config=None):
        task.scheduler_disable_time = None
        task.failures.clear()
        if config:
            self.set_status(task, FAILED, config)
            task.failures.clear()
    def set_status(self, task, new_status, config=None):
        if new_status == FAILED:
            assert config is not None
        if new_status == DISABLED and task.status == RUNNING:
            return
        if task.status == DISABLED:
            if new_status == DONE:
                self.re_enable(task)
            # don't allow workers to override a scheduler disable
            elif task.scheduler_disable_time is not None and new_status != DISABLED:
                return
        if new_status == FAILED and task.can_disable() and task.status != DISABLED:
            task.add_failure()
            if task.has_excessive_failures():
                task.scheduler_disable_time = time.time()
                new_status = DISABLED
                notifications.send_error_email(
                    'Luigi Scheduler: DISABLED {task} due to excessive failures'.format(task=task.id),
                    '{task} failed {failures} times in the last {window} seconds, so it is being '
                    'disabled for {persist} seconds'.format(
                        failures=config.disable_failures,
                        task=task.id,
                        window=config.disable_window,
                        persist=config.disable_persist,
                    ))
        elif new_status == DISABLED:
            task.scheduler_disable_time = None
        if new_status != task.status:
            self._status_tasks[task.status].pop(task.id)
            self._status_tasks[new_status][task.id] = task
            task.status = new_status
            task.updated = time.time()
    def fail_dead_worker_task(self, task, config, assistants):
        # If a running worker disconnects, tag all its jobs as FAILED and subject it to the same retry logic
        if task.status == RUNNING and task.worker_running and task.worker_running not in task.stakeholders | assistants:
            logger.info(""Task %r is marked as running by disconnected worker %r -> marking as ""
                        ""FAILED with retry delay of %rs"", task.id, task.worker_running,
                        config.retry_delay)
            task.worker_running = None
            self.set_status(task, FAILED, config)
            task.retry = time.time() + config.retry_delay
    def update_status(self, task, config):
        # Mark tasks with no remaining active stakeholders for deletion",[92]
"""""""Part of the training engine related to Python generators of array data.
""""""
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function
import warnings
import numpy as np
from .training_utils import iter_sequence_infinite
from .. import backend as K
from ..utils.data_utils import Sequence
from ..utils.data_utils import GeneratorEnqueuer
from ..utils.data_utils import OrderedEnqueuer
from ..utils.generic_utils import Progbar
from ..utils.generic_utils import to_list
from ..utils.generic_utils import unpack_singleton
from .. import callbacks as cbks

def fit_generator(model,
                  generator,
                  steps_per_epoch=None,
                  epochs=1,
                  verbose=1,
                  callbacks=None,
                  validation_data=None,
                  validation_steps=None,
                  class_weight=None,
                  max_queue_size=10,
                  workers=1,
                  use_multiprocessing=False,
                  shuffle=True,
                  initial_epoch=0):
    """"""See docstring for `Model.fit_generator`.""""""
    epoch = initial_epoch
    do_validation = bool(validation_data)
    model._make_train_function()
    if do_validation:
        model._make_test_function()
    is_sequence = isinstance(generator, Sequence)
    if not is_sequence and use_multiprocessing and workers > 1:
        warnings.warn(
            UserWarning('Using a generator with `use_multiprocessing=True`'
                        ' and multiple workers may duplicate your data.'
                        ' Please consider using the`keras.utils.Sequence'
                        ' class.'))
    if steps_per_epoch is None:
        if is_sequence:
            steps_per_epoch = len(generator)
        else:
            raise ValueError('`steps_per_epoch=None` is only valid for a'
                             ' generator based on the '
                             '`keras.utils.Sequence`'
                             ' class. Please specify `steps_per_epoch` '
                             'or use the `keras.utils.Sequence` class.')
    # python 2 has 'next', 3 has '__next__'
    # avoid any explicit version checks
    val_gen = (hasattr(validation_data, 'next') or
               hasattr(validation_data, '__next__') or
               isinstance(validation_data, Sequence))
    if (val_gen and not isinstance(validation_data, Sequence) and
            not validation_steps):
        raise ValueError('`validation_steps=None` is only valid for a'
                         ' generator based on the `keras.utils.Sequence`'
                         ' class. Please specify `validation_steps` or use'
                         ' the `keras.utils.Sequence` class.')
    # Prepare display labels.
    out_labels = model.metrics_names
    callback_metrics = out_labels + ['val_' + n for n in out_labels]
    # prepare callbacks
    model.history = cbks.History()
    _callbacks = [cbks.BaseLogger(
        stateful_metrics=model.stateful_metric_names)]
    if verbose:
        _callbacks.append(
            cbks.ProgbarLogger(
                count_mode='steps',
                stateful_metrics=model.stateful_metric_names))
    _callbacks += (callbacks or []) + [model.history]
    callbacks = cbks.CallbackList(_callbacks)
    # it's possible to callback a different model than self:
    if hasattr(model, 'callback_model') and model.callback_model:
        callback_model = model.callback_model
    else:
        callback_model = model
    callbacks.set_model(callback_model)
    callbacks.set_params({
        'epochs': epochs,
        'steps': steps_per_epoch,
        'verbose': verbose,
        'do_validation': do_validation,
        'metrics': callback_metrics,
    })
    callbacks.on_train_begin()
    enqueuer = None
    val_enqueuer = None
    try:
        if do_validation:
            if val_gen and workers > 0:
                # Create an Enqueuer that can be reused
                val_data = validation_data
                if isinstance(val_data, Sequence):
                    val_enqueuer = OrderedEnqueuer(
                        val_data,
                        use_multiprocessing=use_multiprocessing)
                    validation_steps = validation_steps or len(val_data)
                else:
                    val_enqueuer = GeneratorEnqueuer(
                        val_data,
                        use_multiprocessing=use_multiprocessing)
                val_enqueuer.start(workers=workers,
                                   max_queue_size=max_queue_size)
                val_enqueuer_gen = val_enqueuer.get()
            elif val_gen:
                val_data = validation_data
                if isinstance(val_data, Sequence):
                    val_enqueuer_gen = iter_sequence_infinite(val_data)
                    validation_steps = validation_steps or len(val_data)","[42, 43, 50, 63, 64, 110, 124]"
"         >>> x = K.print_tensor(x, message=""x is: "")
     ```
    # Arguments
        x: Tensor to print.
        message: Message to print jointly with the tensor.
    # Returns
        The same tensor `x`, unchanged.
    """"""
    # TODO
    return tf.Print(x, [x], message)

# GRAPH MANIPULATION
def function(inputs, outputs, updates=None, **kwargs):
    if _is_tf_1():
        v1_variable_initialization()
    return tf_keras_backend.function(inputs, outputs,
                                     updates=updates,
                                     **kwargs)

@symbolic
def gradients(loss, variables):
    """"""Returns the gradients of `loss` w.r.t. `variables`.
    # Arguments
        loss: Scalar tensor to minimize.
        variables: List of variables.
    # Returns
        A gradients tensor.
    """"""
    if _is_tf_1():
        return tf.gradients(loss, variables, colocate_gradients_with_ops=True)
    return tf.gradients(loss, variables)

@symbolic
def stop_gradient(variables):
    """"""Returns `variables` but with zero gradient w.r.t. every other variable.
    # Arguments
        variables: tensor or list of tensors to consider constant with respect
            to any other variable.
    # Returns
        A single tensor or a list of tensors (depending on the passed argument)
            that has constant gradient with respect to any other variable.
    """"""
    if isinstance(variables, (list, tuple)):
        return map(tf.stop_gradient, variables)
    else:
        return tf.stop_gradient(variables)

# CONTROL FLOW
def rnn(step_function, inputs, initial_states,
        go_backwards=False, mask=None, constants=None,
        unroll=False, input_length=None):
    """"""Iterates over the time dimension of a tensor.
    # Arguments
        step_function:
            Parameters:
                inputs: Tensor with shape (samples, ...) (no time dimension),
                    representing input for the batch of samples at a certain
                    time step.
                states: List of tensors.
            Returns:
                outputs: Tensor with shape (samples, ...) (no time dimension),
                new_states: List of tensors, same length and shapes
                    as 'states'.
        inputs: Tensor of temporal data of shape (samples, time, ...)
            (at least 3D).
        initial_states: Tensor with shape (samples, ...) (no time dimension),
            containing the initial values for the states used in
            the step function.
        go_backwards: Boolean. If True, do the iteration over the time
            dimension in reverse order and return the reversed sequence.
        mask: Binary tensor with shape (samples, time),
            with a zero for every element that is masked.
        constants: A list of constant values passed at each step.
        unroll: Whether to unroll the RNN or to use a symbolic loop
            (`while_loop` or `scan` depending on backend).
        input_length: Static number of timesteps in the input.
    # Returns
        A tuple, `(last_output, outputs, new_states)`.
        last_output: The latest output of the rnn, of shape `(samples, ...)`
        outputs: Tensor with shape `(samples, time, ...)` where each
            entry `outputs[s, t]` is the output of the step function
            at time `t` for sample `s`.
        new_states: List of tensors, latest states returned by
            the step function, of shape `(samples, ...)`.
    # Raises
        ValueError: If input dimension is less than 3.
        ValueError: If `unroll` is `True`
            but input timestep is not a fixed number.
        ValueError: If `mask` is provided (not `None`)
            but states is not provided (`len(states)` == 0).
    {{np_implementation}}
    """"""
    last_output, outputs, new_states = tf_keras_backend.rnn(
        step_function, inputs, initial_states,
        go_backwards=go_backwards,
        mask=mask,
        constants=constants,
        unroll=unroll,
        input_length=input_length)
    reachable = tf_utils.get_reachable_from_inputs([learning_phase()],
                                                   targets=[last_output])
    if last_output in reachable:
        last_output._uses_learning_phase = True
    return last_output, outputs, new_states

@symbolic
def switch(condition, then_expression, else_expression):
    """"""Switches between two operations depending on a scalar value.
",[11]
"    return compression, compression_args

def infer_compression(
    filepath_or_buffer: FilePathOrBuffer, compression: Optional[str]
) -> Optional[str]:
    """"""
    Get the compression method for filepath_or_buffer. If compression='infer',
    the inferred compression method is returned. Otherwise, the input
    compression method is returned unchanged, unless it's invalid, in which
    case an error is raised.
    Parameters
    ----------
    filepath_or_buffer : str or file handle
        File path or object.
    compression : {'infer', 'gzip', 'bz2', 'zip', 'xz', None}
        If 'infer' and `filepath_or_buffer` is path-like, then detect
        compression from the following extensions: '.gz', '.bz2', '.zip',
        or '.xz' (otherwise no compression).
    Returns
    -------
    string or None
    Raises
    ------
    ValueError on invalid compression specified.
    """"""
    # No compression has been explicitly specified
    if compression is None:
        return None
    # Infer compression
    if compression == ""infer"":
        # Convert all path types (e.g. pathlib.Path) to strings
        filepath_or_buffer = stringify_path(filepath_or_buffer)
        if not isinstance(filepath_or_buffer, str):
            # Cannot infer compression of a buffer, assume no compression
            return None
        # Infer compression from the filename/URL extension
        for compression, extension in _compression_to_extension.items():
            if filepath_or_buffer.endswith(extension):
                return compression
        return None
    # Compression has been specified. Check that it's valid
    if compression in _compression_to_extension:
        return compression
    msg = f""Unrecognized compression type: {compression}""
    valid = [""infer"", None] + sorted(_compression_to_extension)
    msg += f""\nValid compression types are {valid}""
    raise ValueError(msg)

def get_handle(
    path_or_buf,
    mode: str,
    encoding=None,
    compression: Optional[Union[str, Mapping[str, Any]]] = None,
    memory_map: bool = False,
    is_text: bool = True,
):
    """"""
    Get file handle for given path/buffer and mode.
    Parameters
    ----------
    path_or_buf : str or file handle
        File path or object.
    mode : str
        Mode to open path_or_buf with.
    encoding : str or None
        Encoding to use.
    compression : str or dict, default None
        If string, specifies compression mode. If dict, value at key 'method'
        specifies compression mode. Compression mode must be one of {'infer',
        'gzip', 'bz2', 'zip', 'xz', None}. If compression mode is 'infer'
        and `filepath_or_buffer` is path-like, then detect compression from
        the following extensions: '.gz', '.bz2', '.zip', or '.xz' (otherwise
        no compression). If dict and compression mode is 'zip' or inferred as
        'zip', other entries passed as additional compression options.
        .. versionchanged:: 1.0.0
           May now be a dict with key 'method' as compression mode
           and other keys as compression options if compression
           mode is 'zip'.
    memory_map : boolean, default False
        See parsers._parser_params for more information.
    is_text : boolean, default True
        whether file/buffer is in text format (csv, json, etc.), or in binary
        mode (pickle, etc.).
    Returns
    -------
    f : file-like
        A file-like object.
    handles : list of file-like objects
        A list of file-like object that were opened in this function.
    """"""
    try:
        from s3fs import S3File
        need_text_wrapping = (BufferedIOBase, S3File)
    except ImportError:
        need_text_wrapping = BufferedIOBase  # type: ignore
    handles: List[IO] = list()
    f = path_or_buf
    # Convert pathlib.Path/py.path.local or string
    path_or_buf = stringify_path(path_or_buf)
    is_path = isinstance(path_or_buf, str)
    compression, compression_args = get_compression_method(compression)
    if is_path:
        compression = infer_compression(path_or_buf, compression)
    if compression:
        # GZ Compression
        if compression == ""gzip"":","[108, 110]"
"""""""
Base and utility classes for tseries type pandas objects.
""""""
import operator
from typing import List, Optional, Set
import numpy as np
from pandas._libs import NaT, iNaT, join as libjoin, lib
from pandas._libs.algos import unique_deltas
from pandas._libs.tslibs import timezones
from pandas.compat.numpy import function as nv
from pandas.errors import AbstractMethodError
from pandas.util._decorators import Appender, cache_readonly
from pandas.core.dtypes.common import (
    ensure_int64,
    is_bool_dtype,
    is_dtype_equal,
    is_float,
    is_integer,
    is_list_like,
    is_period_dtype,
    is_scalar,
)
from pandas.core.dtypes.concat import concat_compat
from pandas.core.dtypes.generic import ABCIndex, ABCIndexClass, ABCSeries
from pandas.core import algorithms
from pandas.core.accessor import PandasDelegate
from pandas.core.arrays import (
    DatetimeArray,
    ExtensionArray,
    ExtensionOpsMixin,
    TimedeltaArray,
)
from pandas.core.arrays.datetimelike import (
    DatetimeLikeArrayMixin,
    _ensure_datetimelike_to_i8,
)
import pandas.core.indexes.base as ibase
from pandas.core.indexes.base import Index, _index_shared_docs
from pandas.core.indexes.numeric import Int64Index
from pandas.core.ops import get_op_result_name
from pandas.core.tools.timedeltas import to_timedelta
from pandas.tseries.frequencies import DateOffset, to_offset
from .extension import (
    ExtensionIndex,
    inherit_names,
    make_wrapped_arith_op,
    make_wrapped_comparison_op,
)
_index_doc_kwargs = dict(ibase._index_doc_kwargs)

def _join_i8_wrapper(joinf, with_indexers: bool = True):
    """"""
    Create the join wrapper methods.
    """"""
    @staticmethod  # type: ignore
    def wrapper(left, right):
        if isinstance(left, (np.ndarray, ABCIndex, ABCSeries, DatetimeLikeArrayMixin)):
            left = left.view(""i8"")
        if isinstance(right, (np.ndarray, ABCIndex, ABCSeries, DatetimeLikeArrayMixin)):
            right = right.view(""i8"")
        results = joinf(left, right)
        if with_indexers:
            # dtype should be timedelta64[ns] for TimedeltaIndex
            #  and datetime64[ns] for DatetimeIndex
            dtype = left.dtype.base
            join_index, left_indexer, right_indexer = results
            join_index = join_index.view(dtype)
            return join_index, left_indexer, right_indexer
        return results
    return wrapper

@inherit_names(
    [""inferred_freq"", ""_isnan"", ""_resolution"", ""resolution""],
    DatetimeLikeArrayMixin,
    cache=True,
)
@inherit_names(
    [""__iter__"", ""mean"", ""freq"", ""freqstr"", ""_ndarray_values"", ""asi8"", ""_box_values""],
    DatetimeLikeArrayMixin,
)
class DatetimeIndexOpsMixin(ExtensionIndex, ExtensionOpsMixin):
    """"""
    Common ops mixin to support a unified interface datetimelike Index.
    """"""
    _data: ExtensionArray
    freq: Optional[DateOffset]
    freqstr: Optional[str]
    _resolution: int
    _bool_ops: List[str] = []
    _field_ops: List[str] = []
    hasnans = cache_readonly(DatetimeLikeArrayMixin._hasnans.fget)  # type: ignore
    _hasnans = hasnans  # for index / array -agnostic code
    @property
    def is_all_dates(self) -> bool:
        return True
    @classmethod
    def _create_comparison_method(cls, op):
        """"""
        Create a comparison method that dispatches to ``cls.values``.
        """"""
        return make_wrapped_comparison_op(f""__{op.__name__}__"")
    # ------------------------------------------------------------------------
    # Abstract data attributes
    @property
    def values(self):
        # Note: PeriodArray overrides this to return an ndarray of objects.
        return self._data._data
","[36, 37, 38, 39]"
"# -*- coding: utf-8 -*-
""""""Recurrent layers and their base classes.
""""""
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function
import numpy as np
import warnings
from .. import backend as K
from .. import activations
from .. import initializers
from .. import regularizers
from .. import constraints
from ..engine.base_layer import Layer
from ..engine.base_layer import InputSpec
from ..utils.generic_utils import has_arg
# Legacy support.
from ..legacy.layers import Recurrent
from ..legacy import interfaces

class StackedRNNCells(Layer):
    """"""Wrapper allowing a stack of RNN cells to behave as a single cell.
    Used to implement efficient stacked RNNs.
    # Arguments
        cells: List of RNN cell instances.
    # Examples
    ```python
        cells = [
            keras.layers.LSTMCell(output_dim),
            keras.layers.LSTMCell(output_dim),
            keras.layers.LSTMCell(output_dim),
        ]
        inputs = keras.Input((timesteps, input_dim))
        x = keras.layers.RNN(cells)(inputs)
    ```
    """"""
    def __init__(self, cells, **kwargs):
        for cell in cells:
            if not hasattr(cell, 'call'):
                raise ValueError('All cells must have a `call` method. '
                                 'received cells:', cells)
            if not hasattr(cell, 'state_size'):
                raise ValueError('All cells must have a '
                                 '`state_size` attribute. '
                                 'received cells:', cells)
        self.cells = cells
        super(StackedRNNCells, self).__init__(**kwargs)
    @property
    def state_size(self):
        # States are a flat list
        # in reverse order of the cell stack.
        # This allows to preserve the requirement
        # `stack.state_size[0] == output_dim`.
        # e.g. states of a 2-layer LSTM would be
        # `[h2, c2, h1, c1]`
        # (assuming one LSTM has states [h, c])
        state_size = []
        for cell in self.cells[::-1]:
            if hasattr(cell.state_size, '__len__'):
                state_size += list(cell.state_size)
            else:
                state_size.append(cell.state_size)
        return tuple(state_size)
    def call(self, inputs, states, constants=None, **kwargs):
        # Recover per-cell states.
        nested_states = []
        for cell in self.cells[::-1]:
            if hasattr(cell.state_size, '__len__'):
                nested_states.append(states[:len(cell.state_size)])
                states = states[len(cell.state_size):]
            else:
                nested_states.append([states[0]])
                states = states[1:]
        nested_states = nested_states[::-1]
        # Call the cells in order and store the returned states.
        new_nested_states = []
        for cell, states in zip(self.cells, nested_states):
            if has_arg(cell.call, 'constants'):
                inputs, states = cell.call(inputs, states,
                                           constants=constants,
                                           **kwargs)
            else:
                inputs, states = cell.call(inputs, states, **kwargs)
            new_nested_states.append(states)
        # Format the new states as a flat list
        # in reverse cell order.
        states = []
        for cell_states in new_nested_states[::-1]:
            states += cell_states
        return inputs, states
    def build(self, input_shape):
        if isinstance(input_shape, list):
            constants_shape = input_shape[1:]
            input_shape = input_shape[0]
        for cell in self.cells:
            if isinstance(cell, Layer):
                if has_arg(cell.call, 'constants'):
                    cell.build([input_shape] + constants_shape)
                else:
                    cell.build(input_shape)
            if hasattr(cell.state_size, '__len__'):
                output_dim = cell.state_size[0]
            else:
                output_dim = cell.state_size
            input_shape = (input_shape[0], output_dim)
        self.built = True
    def get_config(self):
        cells = []
        for cell in self.cells:
            cells.append({'class_name': cell.__class__.__name__,
                          'config': cell.get_config()})","[68, 78, 85, 100, 101, 102, 103, 115]"
"        These have kind = ""O"" but aren't string dtypes so need to be explicitly excluded
        """"""
        is_excluded_checks = (is_period_dtype, is_interval_dtype)
        return any(is_excluded(dtype) for is_excluded in is_excluded_checks)
    return _is_dtype(arr_or_dtype, condition)

def is_dtype_equal(source, target) -> bool:
    """"""
    Check if two dtypes are equal.
    Parameters
    ----------
    source : The first dtype to compare
    target : The second dtype to compare
    Returns
    -------
    boolean
        Whether or not the two dtypes are equal.
    Examples
    --------
    >>> is_dtype_equal(int, float)
    False
    >>> is_dtype_equal(""int"", int)
    True
    >>> is_dtype_equal(object, ""category"")
    False
    >>> is_dtype_equal(CategoricalDtype(), ""category"")
    True
    >>> is_dtype_equal(DatetimeTZDtype(tz=""UTC""), ""datetime64"")
    False
    """"""
    try:
        source = _get_dtype(source)
        target = _get_dtype(target)
        return source == target
    except (TypeError, AttributeError):
        # invalid comparison
        # object == category will hit this
        return False

def is_any_int_dtype(arr_or_dtype) -> bool:
    """"""
    Check whether the provided array or dtype is of an integer dtype.
    In this function, timedelta64 instances are also considered ""any-integer""
    type objects and will return True.
    This function is internal and should not be exposed in the public API.
    .. versionchanged:: 0.24.0
       The nullable Integer dtypes (e.g. pandas.Int64Dtype) are also considered
       as integer by this function.
    Parameters
    ----------
    arr_or_dtype : array-like
        The array or dtype to check.
    Returns
    -------
    boolean
        Whether or not the array or dtype is of an integer dtype.
    Examples
    --------
    >>> is_any_int_dtype(str)
    False
    >>> is_any_int_dtype(int)
    True
    >>> is_any_int_dtype(float)
    False
    >>> is_any_int_dtype(np.uint64)
    True
    >>> is_any_int_dtype(np.datetime64)
    False
    >>> is_any_int_dtype(np.timedelta64)
    True
    >>> is_any_int_dtype(np.array(['a', 'b']))
    False
    >>> is_any_int_dtype(pd.Series([1, 2]))
    True
    >>> is_any_int_dtype(np.array([], dtype=np.timedelta64))
    True
    >>> is_any_int_dtype(pd.Index([1, 2.]))  # float
    False
    """"""
    return _is_dtype_type(arr_or_dtype, classes(np.integer, np.timedelta64))

def is_integer_dtype(arr_or_dtype) -> bool:
    """"""
    Check whether the provided array or dtype is of an integer dtype.
    Unlike in `in_any_int_dtype`, timedelta64 instances will return False.
    .. versionchanged:: 0.24.0
       The nullable Integer dtypes (e.g. pandas.Int64Dtype) are also considered
       as integer by this function.
    Parameters
    ----------
    arr_or_dtype : array-like
        The array or dtype to check.
    Returns
    -------
    boolean
        Whether or not the array or dtype is of an integer dtype and
        not an instance of timedelta64.
    Examples
    --------
    >>> is_integer_dtype(str)
    False
    >>> is_integer_dtype(int)
    True
    >>> is_integer_dtype(float)
    False
    >>> is_integer_dtype(np.uint64)",[2]
"# -*- coding: utf-8 -*-
import os
import sys
import logging
import warnings
from logging.config import dictConfig
from twisted.python.failure import Failure
from twisted.python import log as twisted_log
import scrapy
from scrapy.settings import overridden_settings, Settings
from scrapy.exceptions import ScrapyDeprecationWarning
logger = logging.getLogger(__name__)

class FailureFormatter(logging.Filter):
    """"""Extract exc_info from Failure instances provided as contextual data
    This filter mimics Twisted log.err formatting for its first `_stuff`
    argument, which means that reprs of non Failure objects are appended to the
    log messages.
    """"""
    def filter(self, record):
        failure = record.__dict__.get('failure')
        if failure:
            if isinstance(failure, Failure):
                record.exc_info = (failure.type, failure.value, failure.tb)
            else:
                record.msg += os.linesep + repr(failure)
        return True

class TopLevelFormatter(logging.Filter):
    """"""Keep only top level loggers's name (direct children from root) from
    records.
    This filter will replace Scrapy loggers' names with 'scrapy'. This mimics
    the old Scrapy log behaviour and helps shortening long names.
    Since it can't be set for just one logger (it won't propagate for its
    children), it's going to be set in the root handler, with a parametrized
    `loggers` list where it should act.
    """"""
    def __init__(self, loggers=None):
        self.loggers = loggers or []
    def filter(self, record):
        if any(record.name.startswith(l + '.') for l in self.loggers):
            record.name = record.name.split('.', 1)[0]
        return True

DEFAULT_LOGGING = {
    'version': 1,
    'disable_existing_loggers': False,
    'filters': {
        'failure_formatter': {
            '()': 'scrapy.utils.log.FailureFormatter',
        },
    },
    'loggers': {
        'scrapy': {
            'level': 'DEBUG',
            'filters': ['failure_formatter'],
        },
        'twisted': {
            'level': 'ERROR',
        },
    }
}

def configure_logging(settings=None):
    """"""Initialize and configure default loggers
    This function does:
      - Route warnings and twisted logging through Python standard logging
      - Set FailureFormatter filter on Scrapy logger
      - Assign DEBUG and ERROR level to Scrapy and Twisted loggers respectively
      - Create a handler for the root logger according to given settings
    """"""
    if not sys.warnoptions:
        # Route warnings through python logging
        logging.captureWarnings(True)
    observer = twisted_log.PythonLoggingObserver('twisted')
    observer.start()
    dictConfig(DEFAULT_LOGGING)
    if isinstance(settings, dict):
        settings = Settings(settings)
    if settings:
        logging.root.setLevel(logging.NOTSET)
        if settings.getbool('LOG_STDOUT'):
            sys.stdout = StreamLogger(logging.getLogger('stdout'))
        # Set up the default log handler
        filename = settings.get('LOG_FILE')
        if filename:
            encoding = settings.get('LOG_ENCODING')
            handler = logging.FileHandler(filename, encoding=encoding)
        elif settings.getbool('LOG_ENABLED'):
            handler = logging.StreamHandler()
        else:
            handler = logging.NullHandler()
        formatter = logging.Formatter(
            fmt=settings.get('LOG_FORMAT'),
            datefmt=settings.get('LOG_DATEFORMAT')
        )
        handler.setFormatter(formatter)
        handler.setLevel(settings.get('LOG_LEVEL'))
        handler.addFilter(TopLevelFormatter(['scrapy']))
        logging.root.addHandler(handler)

def log_scrapy_info(settings):
    logger.info(""Scrapy %(version)s started (bot: %(bot)s)"",
                {'version': scrapy.__version__, 'bot': settings['BOT_NAME']})","[2, 18, 20, 21, 22, 23, 25, 26, 27, 28, 29, 30, 31, 32, 33, 60, 61, 62, 63, 64, 68]"
"    def _convert_key(self, key, is_setter: bool = False):
        raise AbstractMethodError(self)
    def __getitem__(self, key):
        if not isinstance(key, tuple):
            # we could have a convertible item here (e.g. Timestamp)
            if not is_list_like_indexer(key):
                key = tuple([key])
            else:
                raise ValueError(""Invalid call for scalar access (getting)!"")
        key = self._convert_key(key)
        return self.obj._get_value(*key, takeable=self._takeable)
    def __setitem__(self, key, value):
        if isinstance(key, tuple):
            key = tuple(com.apply_if_callable(x, self.obj) for x in key)
        else:
            # scalar callable may return tuple
            key = com.apply_if_callable(key, self.obj)
        if not isinstance(key, tuple):
            key = _tuplify(self.ndim, key)
        if len(key) != self.ndim:
            raise ValueError(""Not enough indexers for scalar access (setting)!"")
        key = list(self._convert_key(key, is_setter=True))
        self.obj._set_value(*key, value=value, takeable=self._takeable)

@Appender(IndexingMixin.at.__doc__)
class _AtIndexer(_ScalarAccessIndexer):
    _takeable = False
    def _convert_key(self, key, is_setter: bool = False):
        """"""
        Require they keys to be the same type as the index. (so we don't
        fallback)
        """"""
        # allow arbitrary setting
        if is_setter:
            return list(key)
        for ax, i in zip(self.obj.axes, key):
            if ax.is_integer():
                if not is_integer(i):
                    raise ValueError(
                        ""At based indexing on an integer index ""
                        ""can only have integer indexers""
                    )
            else:
                if is_integer(i) and not (ax.holds_integer() or ax.is_floating()):
                    raise ValueError(
                        ""At based indexing on an non-integer ""
                        ""index can only have non-integer ""
                        ""indexers""
                    )
        return key

@Appender(IndexingMixin.iat.__doc__)
class _iAtIndexer(_ScalarAccessIndexer):
    _takeable = True
    def _convert_key(self, key, is_setter: bool = False):
        """"""
        Require integer args. (and convert to label arguments)
        """"""
        for a, i in zip(self.obj.axes, key):
            if not is_integer(i):
                raise ValueError(""iAt based indexing can only have integer indexers"")
        return key

def _tuplify(ndim: int, loc: Hashable) -> Tuple[Union[Hashable, slice], ...]:
    """"""
    Given an indexer for the first dimension, create an equivalent tuple
    for indexing over all dimensions.
    Parameters
    ----------
    ndim : int
    loc : object
    Returns
    -------
    tuple
    """"""
    _tup: List[Union[Hashable, slice]]
    _tup = [slice(None, None) for _ in range(ndim)]
    _tup[0] = loc
    return tuple(_tup)

def convert_to_index_sliceable(obj, key):
    """"""
    If we are index sliceable, then return my slicer, otherwise return None.
    """"""
    idx = obj.index
    if isinstance(key, slice):
        return idx._convert_slice_indexer(key, kind=""getitem"")
    elif isinstance(key, str):
        # we are an actual column
        if key in obj._data.items:
            return None
        # We might have a datetimelike string that we can translate to a
        # slice here via partial string indexing
        if idx._supports_partial_string_indexing:
            try:
                return idx._get_string_slice(key)
            except (KeyError, ValueError, NotImplementedError):
                return None
    return None

def check_bool_indexer(index: Index, key) -> np.ndarray:
    """"""
    Check if key is a valid boolean indexer for an object with such index and
    perform reindexing or conversion if needed.
    This function assumes that is_bool_indexer(key) == True.
    Parameters","[43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57]"
"            # don't allow re-scheduling of task while it is running, it must either fail or succeed first
            if status == PENDING or status != task.status:
                # Update the DB only if there was a acctual change, to prevent noise.
                # We also check for status == PENDING b/c that's the default value
                # (so checking for status != task.status woule lie)
                self._update_task_history(task_id, status)
            self._state.set_status(task, PENDING if status == SUSPENDED else status, self._config)
            if status == FAILED:
                task.retry = time.time() + self._config.retry_delay
        if deps is not None:
            task.deps = set(deps)
        if new_deps is not None:
            task.deps.update(new_deps)
        if resources is not None:
            task.resources = resources
        # only assistants should normally schedule tasks as FAILED and not runnable
        if runnable or status != FAILED:
            task.stakeholders.add(worker)
            # Task dependencies might not exist yet. Let's create dummy tasks for them for now.
            # Otherwise the task dependencies might end up being pruned if scheduling takes a long time
            for dep in task.deps or []:
                t = self._state.get_task(dep, setdefault=self._make_task(task_id=dep, status=UNKNOWN, deps=None, priority=priority))
                t.stakeholders.add(worker)
        self._update_priority(task, priority, worker)
        if runnable:
            task.workers.add(worker)
        if expl is not None:
            task.expl = expl
    def add_worker(self, worker, info, **kwargs):
        self._state.get_worker(worker).add_info(info)
    def update_resources(self, **resources):
        if self._resources is None:
            self._resources = {}
        self._resources.update(resources)
    def _has_resources(self, needed_resources, used_resources):
        if needed_resources is None:
            return True
        available_resources = self._resources or {}
        for resource, amount in six.iteritems(needed_resources):
            if amount + used_resources[resource] > available_resources.get(resource, 1):
                return False
        return True
    def _used_resources(self):
        used_resources = collections.defaultdict(int)
        if self._resources is not None:
            for task in self._state.get_active_tasks():
                if task.status == RUNNING and task.resources:
                    for resource, amount in six.iteritems(task.resources):
                        used_resources[resource] += amount
        return used_resources
    def _rank(self):
        """"""
        Return worker's rank function for task scheduling.
        :return:
        """"""
        dependents = collections.defaultdict(int)
        def not_done(t):
            task = self._state.get_task(t, default=None)
            return task is None or task.status != DONE
        for task in self._state.get_pending_tasks():
            if task.status != DONE:
                deps = list(filter(not_done, task.deps))
                inverse_num_deps = 1.0 / max(len(deps), 1)
                for dep in deps:
                    dependents[dep] += inverse_num_deps
        return lambda task: (task.priority, dependents[task.id], -task.time)
    def _schedulable(self, task):
        if task.status != PENDING:
            return False
        for dep in task.deps:
            dep_task = self._state.get_task(dep, default=None)
            if dep_task is None or dep_task.status != DONE:
                return False
        return True
    def get_work(self, worker, host=None, assistant=False, **kwargs):
        # TODO: remove any expired nodes
        # Algo: iterate over all nodes, find the highest priority node no dependencies and available
        # resources.
        # Resource checking looks both at currently available resources and at which resources would
        # be available if all running tasks died and we rescheduled all workers greedily. We do both
        # checks in order to prevent a worker with many low-priority tasks from starving other
        # workers with higher priority tasks that share the same resources.
        # TODO: remove tasks that can't be done, figure out if the worker has absolutely
        # nothing it can wait for
        # Return remaining tasks that have no FAILED descendents
        self.update(worker, {'host': host})
        if assistant:
            self.add_worker(worker, [('assistant', assistant)])
        best_task = None
        locally_pending_tasks = 0
        running_tasks = []
        used_resources = self._used_resources()
        greedy_resources = collections.defaultdict(int)
        n_unique_pending = 0
        greedy_workers = dict((worker.id, worker.info.get('workers', 1))
                              for worker in self._state.get_active_workers())
        tasks = list(self._state.get_pending_tasks())
        tasks.sort(key=self._rank(), reverse=True)
        for task in tasks:
            in_workers = assistant or worker in task.workers
            if task.status == 'RUNNING' and in_workers:",[125]
"                                              name='kernel')
                self.recurrent_kernel = self.add_weight(
                    shape=(self.units, self.units),
                    initializer='uniform',
                    name='recurrent_kernel')
                self.built = True
            def call(self, inputs, states):
                prev_output = states[0]
                h = K.dot(inputs, self.kernel)
                output = h + K.dot(prev_output, self.recurrent_kernel)
                return output, [output]
        # Let's use this cell in a RNN layer:
        cell = MinimalRNNCell(32)
        x = keras.Input((None, 5))
        layer = RNN(cell)
        y = layer(x)
        # Here's how to use the cell to build a stacked RNN:
        cells = [MinimalRNNCell(32), MinimalRNNCell(64)]
        x = keras.Input((None, 5))
        layer = RNN(cells)
        y = layer(x)
    ```
    """"""
    def __init__(self, cell,
                 return_sequences=False,
                 return_state=False,
                 go_backwards=False,
                 stateful=False,
                 unroll=False,
                 **kwargs):
        if isinstance(cell, (list, tuple)):
            cell = StackedRNNCells(cell)
        if not hasattr(cell, 'call'):
            raise ValueError('`cell` should have a `call` method. '
                             'The RNN was passed:', cell)
        if not hasattr(cell, 'state_size'):
            raise ValueError('The RNN cell should have '
                             'an attribute `state_size` '
                             '(tuple of integers, '
                             'one integer per RNN state).')
        super(RNN, self).__init__(**kwargs)
        self.cell = cell
        self.return_sequences = return_sequences
        self.return_state = return_state
        self.go_backwards = go_backwards
        self.stateful = stateful
        self.unroll = unroll
        self.supports_masking = True
        self.input_spec = [InputSpec(ndim=3)]
        self.state_spec = None
        self._states = None
        self.constants_spec = None
        self._num_constants = None
    @property
    def states(self):
        if self._states is None:
            if isinstance(self.cell.state_size, int):
                num_states = 1
            else:
                num_states = len(self.cell.state_size)
            return [None for _ in range(num_states)]
        return self._states
    @states.setter
    def states(self, states):
        self._states = states
    def compute_output_shape(self, input_shape):
        if isinstance(input_shape, list):
            input_shape = input_shape[0]
        if hasattr(self.cell.state_size, '__len__'):
            state_size = self.cell.state_size
        else:
            state_size = [self.cell.state_size]
        output_dim = state_size[0]
        if self.return_sequences:
            output_shape = (input_shape[0], input_shape[1], output_dim)
        else:
            output_shape = (input_shape[0], output_dim)
        if self.return_state:
            state_shape = [(input_shape[0], dim) for dim in state_size]
            return [output_shape] + state_shape
        else:
            return output_shape
    def compute_mask(self, inputs, mask):
        if isinstance(mask, list):
            mask = mask[0]
        output_mask = mask if self.return_sequences else None
        if self.return_state:
            state_mask = [None for _ in self.states]
            return [output_mask] + state_mask
        else:
            return output_mask
    def build(self, input_shape):
        # Note input_shape will be list of shapes of initial states and
        # constants if these are passed in __call__.
        if self._num_constants is not None:
            constants_shape = input_shape[-self._num_constants:]
        else:
            constants_shape = None
        if isinstance(input_shape, list):
            input_shape = input_shape[0]
        batch_size = input_shape[0] if self.stateful else None
        input_dim = input_shape[-1]
        self.input_spec[0] = InputSpec(shape=(batch_size, None, input_dim))
        # allow cell (if layer) to build before we set or validate state_spec
        if isinstance(self.cell, Layer):
            step_input_shape = (input_shape[0],) + input_shape[2:]
            if constants_shape is not None:
                self.cell.build([step_input_shape] + constants_shape)
            else:",[83]
"import operator
from shutil import get_terminal_size
from typing import Dict, Hashable, List, Type, Union, cast
from warnings import warn
import numpy as np
from pandas._config import get_option
from pandas._libs import algos as libalgos, hashtable as htable
from pandas._typing import ArrayLike, Dtype, Ordered, Scalar
from pandas.compat.numpy import function as nv
from pandas.util._decorators import (
    Appender,
    Substitution,
    cache_readonly,
    deprecate_kwarg,
)
from pandas.util._validators import validate_bool_kwarg, validate_fillna_kwargs
from pandas.core.dtypes.cast import coerce_indexer_dtype, maybe_infer_to_datetimelike
from pandas.core.dtypes.common import (
    ensure_int64,
    ensure_object,
    ensure_platform_int,
    is_categorical_dtype,
    is_datetime64_dtype,
    is_dict_like,
    is_dtype_equal,
    is_extension_array_dtype,
    is_integer_dtype,
    is_iterator,
    is_list_like,
    is_object_dtype,
    is_scalar,
    is_sequence,
    is_timedelta64_dtype,
    needs_i8_conversion,
)
from pandas.core.dtypes.dtypes import CategoricalDtype
from pandas.core.dtypes.generic import ABCIndexClass, ABCSeries
from pandas.core.dtypes.inference import is_hashable
from pandas.core.dtypes.missing import isna, notna
from pandas.core import ops
from pandas.core.accessor import PandasDelegate, delegate_names
import pandas.core.algorithms as algorithms
from pandas.core.algorithms import _get_data_algo, factorize, take, take_1d, unique1d
from pandas.core.arrays.base import (
    ExtensionArray,
    _extension_array_shared_docs,
    try_cast_to_ea,
)
from pandas.core.base import NoNewAttributesMixin, PandasObject, _shared_docs
import pandas.core.common as com
from pandas.core.construction import array, extract_array, sanitize_array
from pandas.core.indexers import check_array_indexer, deprecate_ndim_indexing
from pandas.core.missing import interpolate_2d
from pandas.core.ops.common import unpack_zerodim_and_defer
from pandas.core.sorting import nargsort
from pandas.io.formats import console

def _cat_compare_op(op):
    opname = f""__{op.__name__}__""
    @unpack_zerodim_and_defer(opname)
    def func(self, other):
        if is_list_like(other) and len(other) != len(self):
            # TODO: Could this fail if the categories are listlike objects?
            raise ValueError(""Lengths must match."")
        if not self.ordered:
            if opname in [""__lt__"", ""__gt__"", ""__le__"", ""__ge__""]:
                raise TypeError(
                    ""Unordered Categoricals can only compare equality or not""
                )
        if isinstance(other, Categorical):
            # Two Categoricals can only be be compared if the categories are
            # the same (maybe up to ordering, depending on ordered)
            msg = ""Categoricals can only be compared if 'categories' are the same.""
            if len(self.categories) != len(other.categories):
                raise TypeError(msg + "" Categories are different lengths"")
            elif self.ordered and not (self.categories == other.categories).all():
                raise TypeError(msg)
            elif not set(self.categories) == set(other.categories):
                raise TypeError(msg)
            if not (self.ordered == other.ordered):
                raise TypeError(
                    ""Categoricals can only be compared if 'ordered' is the same""
                )
            if not self.ordered and not self.categories.equals(other.categories):
                # both unordered and different order
                other_codes = _get_codes_for_values(other, self.categories)
            else:
                other_codes = other._codes
            f = getattr(self._codes, opname)
            ret = f(other_codes)
            mask = (self._codes == -1) | (other_codes == -1)
            if mask.any():
                # In other series, the leads to False, so do that here too
                ret[mask] = False
            return ret
        if is_scalar(other):
            if other in self.categories:
                i = self.categories.get_loc(other)
                ret = getattr(self._codes, opname)(i)
                if opname not in {""__eq__"", ""__ge__"", ""__gt__""}:
                    # check for NaN needed if we are not equal or larger
                    mask = self._codes == -1
                    ret[mask] = False
                return ret
            else:
                if opname == ""__eq__"":
                    return np.zeros(len(self), dtype=bool)
                elif opname == ""__ne__"":
                    return np.ones(len(self), dtype=bool)
                else:
                    raise TypeError(
                        f""Cannot compare a Categorical for op {opname} with a ""
                        ""scalar, which is not a category.""",[105]
"        """"""
        x = _standardize_input_data(x, self._feed_input_names,
                                    self._feed_input_shapes)
        if self.uses_learning_phase and not isinstance(K.learning_phase(), int):
            ins = x + [0.]
        else:
            ins = x
        self._make_predict_function()
        outputs = self.predict_function(ins)
        if len(outputs) == 1:
            return outputs[0]
        return outputs
    @interfaces.legacy_generator_methods_support
    def fit_generator(self,
                      generator,
                      steps_per_epoch,
                      epochs=1,
                      verbose=1,
                      callbacks=None,
                      validation_data=None,
                      validation_steps=None,
                      class_weight=None,
                      max_queue_size=10,
                      workers=1,
                      use_multiprocessing=False,
                      shuffle=True,
                      initial_epoch=0):
        """"""Fits the model on data yielded batch-by-batch by a Python generator.
        The generator is run in parallel to the model, for efficiency.
        For instance, this allows you to do real-time data augmentation
        on images on CPU in parallel to training your model on GPU.
        The use of `keras.utils.Sequence` guarantees the ordering
        and guarantees the single use of every input per epoch when
        using `use_multiprocessing=True`.
        # Arguments
            generator: A generator or an instance of `Sequence` (`keras.utils.Sequence`)
                    object in order to avoid duplicate data
                    when using multiprocessing.
                The output of the generator must be either
                - a tuple `(inputs, targets)`
                - a tuple `(inputs, targets, sample_weights)`.
                This tuple (a single output of the generator) makes a single batch.
                Therefore, all arrays in this tuple must have the same length (equal
                to the size of this batch). Different batches may have different sizes.
                For example, the last batch of the epoch is commonly smaller than the
                others, if the size of the dataset is not divisible by the batch size.
                The generator is expected to loop over its data
                indefinitely. An epoch finishes when `steps_per_epoch`
                batches have been seen by the model.
            steps_per_epoch: Total number of steps (batches of samples)
                to yield from `generator` before declaring one epoch
                finished and starting the next epoch. It should typically
                be equal to the number of samples of your dataset
                divided by the batch size. Not used if using `Sequence`.
            epochs: Integer, total number of iterations on the data.
            verbose: Verbosity mode, 0, 1, or 2.
            callbacks: List of callbacks to be called during training.
            validation_data: This can be either
                - a generator for the validation data
                - a tuple (inputs, targets)
                - a tuple (inputs, targets, sample_weights).
            validation_steps: Only relevant if `validation_data`
                is a generator. Total number of steps (batches of samples)
                to yield from `generator` before stopping.
            class_weight: Dictionary mapping class indices to a weight
                for the class.
            max_queue_size: Integer. Maximum size for the generator queue.
                If unspecified, `max_queue_size` will default to 10.
            workers: Integer. Maximum number of processes to spin up
                when using process based threading.
                If unspecified, `workers` will default to 1.
            use_multiprocessing: Boolean. If True, use process based threading.
                If unspecified, `workers` will default to False.
                Note that because
                this implementation relies on multiprocessing,
                you should not pass
                non picklable arguments to the generator
                as they can't be passed
                easily to children processes.
            shuffle: Whether to shuffle the order of the batches at
                the beginning of each epoch. Only used with instances
                of `Sequence` (keras.utils.Sequence).
            initial_epoch: Epoch at which to start training
                (useful for resuming a previous training run)
        # Returns
            A `History` object.
        # Example
        ```python
            def generate_arrays_from_file(path):
                while 1:
                    f = open(path)
                    for line in f:
                        # create numpy arrays of input data
                        # and labels, from each line in the file
                        x1, x2, y = process_line(line)
                        yield ({'input_1': x1, 'input_2': x2}, {'output': y})
                    f.close()
            model.fit_generator(generate_arrays_from_file('/my_file.txt'),
                                steps_per_epoch=10000, epochs=10)
        ```
        # Raises
            ValueError: In case the generator yields
                data in an invalid format.
        """"""
        wait_time = 0.01  # in seconds
        epoch = initial_epoch
        do_validation = bool(validation_data)
        self._make_train_function()
        if do_validation:
            self._make_test_function()
        # python 2 has 'next', 3 has '__next__'
        # avoid any explicit version checks
        val_gen = (hasattr(validation_data, 'next') or
                   hasattr(validation_data, '__next__') or
                   isinstance(validation_data, Sequence))
        if val_gen and not validation_steps:","[16, 57, 126, 127]"
"
def format_file_contents(
    src_contents: str, line_length: int, fast: bool
) -> FileContent:
    """"""Reformats a file and returns its contents and encoding.""""""
    if src_contents.strip() == '':
        raise NothingChanged
    dst_contents = format_str(src_contents, line_length=line_length)
    if src_contents == dst_contents:
        raise NothingChanged
    if not fast:
        assert_equivalent(src_contents, dst_contents)
        assert_stable(src_contents, dst_contents, line_length=line_length)
    return dst_contents

def format_str(src_contents: str, line_length: int) -> FileContent:
    """"""Reformats a string and returns new contents.""""""
    src_node = lib2to3_parse(src_contents)
    dst_contents = """"
    lines = LineGenerator()
    elt = EmptyLineTracker()
    py36 = is_python36(src_node)
    empty_line = Line()
    after = 0
    for current_line in lines.visit(src_node):
        for _ in range(after):
            dst_contents += str(empty_line)
        before, after = elt.maybe_empty_lines(current_line)
        for _ in range(before):
            dst_contents += str(empty_line)
        for line in split_line(current_line, line_length=line_length, py36=py36):
            dst_contents += str(line)
    return dst_contents

def lib2to3_parse(src_txt: str) -> Node:
    """"""Given a string with source, return the lib2to3 Node.""""""
    grammar = pygram.python_grammar_no_print_statement
    drv = driver.Driver(grammar, pytree.convert)
    if src_txt[-1] != '\n':
        nl = '\r\n' if '\r\n' in src_txt[:1024] else '\n'
        src_txt += nl
    try:
        result = drv.parse_string(src_txt, True)
    except ParseError as pe:
        lineno, column = pe.context[1]
        lines = src_txt.splitlines()
        try:
            faulty_line = lines[lineno - 1]
        except IndexError:
            faulty_line = ""<line number missing in source>""
        raise ValueError(f""Cannot parse: {lineno}:{column}: {faulty_line}"") from None
    if isinstance(result, Leaf):
        result = Node(syms.file_input, [result])
    return result

def lib2to3_unparse(node: Node) -> str:
    """"""Given a lib2to3 node, return its string representation.""""""
    code = str(node)
    return code

T = TypeVar('T')

class Visitor(Generic[T]):
    """"""Basic lib2to3 visitor that yields things on visiting.""""""
    def visit(self, node: LN) -> Iterator[T]:
        if node.type < 256:
            name = token.tok_name[node.type]
        else:
            name = type_repr(node.type)
        yield from getattr(self, f'visit_{name}', self.visit_default)(node)
    def visit_default(self, node: LN) -> Iterator[T]:
        if isinstance(node, Node):
            for child in node.children:
                yield from self.visit(child)

@dataclass
class DebugVisitor(Visitor[T]):
    tree_depth: int = 0
    def visit_default(self, node: LN) -> Iterator[T]:
        indent = ' ' * (2 * self.tree_depth)
        if isinstance(node, Node):
            _type = type_repr(node.type)
            out(f'{indent}{_type}', fg='yellow')
            self.tree_depth += 1
            for child in node.children:
                yield from self.visit(child)
            self.tree_depth -= 1
            out(f'{indent}/{_type}', fg='yellow', bold=False)
        else:
            _type = token.tok_name.get(node.type, str(node.type))
            out(f'{indent}{_type}', fg='blue', nl=False)
            if node.prefix:
                # We don't have to handle prefixes for `Node` objects since
                # that delegates to the first child anyway.
                out(f' {node.prefix!r}', fg='green', bold=False, nl=False)
            out(f' {node.value!r}', fg='blue', bold=False)

KEYWORDS = set(keyword.kwlist)
WHITESPACE = {token.DEDENT, token.INDENT, token.NEWLINE}
FLOW_CONTROL = {'return', 'raise', 'break', 'continue'}
STATEMENT = {
    syms.if_stmt,
    syms.while_stmt,
    syms.for_stmt,
    syms.try_stmt,
    syms.except_clause,
    syms.with_stmt,
    syms.funcdef,
    syms.classdef,
}
STANDALONE_COMMENT = 153
LOGIC_OPERATORS = {'and', 'or'}","[42, 46, 47, 48, 49, 50, 52, 53, 54, 55]"
"        (?P<key>[a-z_]+)
        \s*(?P<op>%s)(?P<none_inclusive>\s*\?)?\s*
        (?:
            (?P<intval>[0-9.]+(?:[kKmMgGtTpPeEzZyY]i?[Bb]?)?)|
            (?P<strval>(?![0-9.])[a-z0-9A-Z]*)
        )
        \s*$
        ''' % '|'.join(map(re.escape, COMPARISON_OPERATORS.keys())))
    m = operator_rex.search(filter_part)
    if m:
        op = COMPARISON_OPERATORS[m.group('op')]
        if m.group('strval') is not None:
            if m.group('op') not in ('=', '!='):
                raise ValueError(
                    'Operator %s does not support string values!' % m.group('op'))
            comparison_value = m.group('strval')
        else:
            try:
                comparison_value = int(m.group('intval'))
            except ValueError:
                comparison_value = parse_filesize(m.group('intval'))
                if comparison_value is None:
                    comparison_value = parse_filesize(m.group('intval') + 'B')
                if comparison_value is None:
                    raise ValueError(
                        'Invalid integer value %r in filter part %r' % (
                            m.group('intval'), filter_part))
        actual_value = dct.get(m.group('key'))
        if actual_value is None:
            return m.group('none_inclusive')
        return op(actual_value, comparison_value)
    UNARY_OPERATORS = {
        '': lambda v: v is not None,
        '!': lambda v: v is None,
    }
    operator_rex = re.compile(r'''(?x)\s*
        (?P<op>%s)\s*(?P<key>[a-z_]+)
        \s*$
        ''' % '|'.join(map(re.escape, UNARY_OPERATORS.keys())))
    m = operator_rex.search(filter_part)
    if m:
        op = UNARY_OPERATORS[m.group('op')]
        actual_value = dct.get(m.group('key'))
        return op(actual_value)
    raise ValueError('Invalid filter part %r' % filter_part)

def match_str(filter_str, dct):
    """""" Filter a dictionary with a simple string syntax. Returns True (=passes filter) or false """"""
    return all(
        _match_one(filter_part, dct) for filter_part in filter_str.split('&'))

def match_filter_func(filter_str):
    def _match_func(info_dict):
        if match_str(filter_str, info_dict):
            return None
        else:
            video_title = info_dict.get('title', info_dict.get('id', 'video'))
            return '%s does not pass filter %s, skipping ..' % (video_title, filter_str)
    return _match_func

def parse_dfxp_time_expr(time_expr):
    if not time_expr:
        return
    mobj = re.match(r'^(?P<time_offset>\d+(?:\.\d+)?)s?$', time_expr)
    if mobj:
        return float(mobj.group('time_offset'))
    mobj = re.match(r'^(\d+):(\d\d):(\d\d(?:(?:\.|:)\d+)?)$', time_expr)
    if mobj:
        return 3600 * int(mobj.group(1)) + 60 * int(mobj.group(2)) + float(mobj.group(3).replace(':', '.'))

def srt_subtitles_timecode(seconds):
    return '%02d:%02d:%02d,%03d' % (seconds / 3600, (seconds % 3600) / 60, seconds % 60, (seconds % 1) * 1000)

def dfxp2srt(dfxp_data):
    _x = functools.partial(xpath_with_ns, ns_map={
        'ttml': 'http://www.w3.org/ns/ttml',
        'ttaf1': 'http://www.w3.org/2006/10/ttaf1',
        'ttaf1_0604': 'http://www.w3.org/2006/04/ttaf1',
    })
    class TTMLPElementParser(object):
        out = ''
        def start(self, tag, attrib):
            if tag in (_x('ttml:br'), _x('ttaf1:br'), 'br'):
                self.out += '\n'
        def end(self, tag):
            pass
        def data(self, data):
            self.out += data
        def close(self):
            return self.out.strip()
    def parse_node(node):
        target = TTMLPElementParser()
        parser = xml.etree.ElementTree.XMLParser(target=target)
        parser.feed(xml.etree.ElementTree.tostring(node))
        return parser.close()
    dfxp = compat_etree_fromstring(dfxp_data.encode('utf-8'))
    out = []
    paras = dfxp.findall(_x('.//ttml:p')) or dfxp.findall(_x('.//ttaf1:p')) or dfxp.findall(_x('.//ttaf1_0604:p')) or dfxp.findall('.//p')
    if not paras:
        raise ValueError('Invalid dfxp/TTML subtitle')
    for para, index in zip(paras, itertools.count(1)):
        begin_time = parse_dfxp_time_expr(para.attrib.get('begin'))
        end_time = parse_dfxp_time_expr(para.attrib.get('end'))
        dur = parse_dfxp_time_expr(para.attrib.get('dur'))
        if begin_time is None:
            continue
        if not end_time:
            if not dur:","[11, 15, 27]"
"        for additional_status_code, response in self.responses.items():
            assert isinstance(response, dict), ""An additional response must be a dict""
            model = response.get(""model"")
            if model:
                assert lenient_issubclass(
                    model, BaseModel
                ), ""A response model must be a Pydantic model""
                response_name = f""Response_{additional_status_code}_{self.unique_id}""
                response_field = Field(
                    name=response_name,
                    type_=model,
                    class_validators=None,
                    default=None,
                    required=False,
                    model_config=BaseConfig,
                    schema=Schema(None),
                )
                response_fields[additional_status_code] = response_field
        if response_fields:
            self.response_fields: Dict[Union[int, str], Field] = response_fields
        else:
            self.response_fields = {}
        self.deprecated = deprecated
        self.operation_id = operation_id
        self.response_model_include = response_model_include
        self.response_model_exclude = response_model_exclude
        self.response_model_by_alias = response_model_by_alias
        self.response_model_skip_defaults = response_model_skip_defaults
        self.include_in_schema = include_in_schema
        self.response_class = response_class
        assert inspect.isfunction(endpoint) or inspect.ismethod(
            endpoint
        ), f""An endpoint must be a function or method""
        self.dependant = get_dependant(path=self.path_format, call=self.endpoint)
        for depends in self.dependencies[::-1]:
            self.dependant.dependencies.insert(
                0,
                get_parameterless_sub_dependant(depends=depends, path=self.path_format),
            )
        self.body_field = get_body_field(dependant=self.dependant, name=self.unique_id)
        self.dependency_overrides_provider = dependency_overrides_provider
        self.app = request_response(
            get_app(
                dependant=self.dependant,
                body_field=self.body_field,
                status_code=self.status_code,
                response_class=self.response_class or JSONResponse,
                response_field=self.secure_cloned_response_field,
                response_model_include=self.response_model_include,
                response_model_exclude=self.response_model_exclude,
                response_model_by_alias=self.response_model_by_alias,
                response_model_skip_defaults=self.response_model_skip_defaults,
                dependency_overrides_provider=self.dependency_overrides_provider,
            )
        )

class APIRouter(routing.Router):
    def __init__(
        self,
        routes: List[routing.BaseRoute] = None,
        redirect_slashes: bool = True,
        default: ASGIApp = None,
        dependency_overrides_provider: Any = None,
        route_class: Type[APIRoute] = APIRoute,
    ) -> None:
        super().__init__(
            routes=routes, redirect_slashes=redirect_slashes, default=default
        )
        self.dependency_overrides_provider = dependency_overrides_provider
        self.route_class = route_class
    def add_api_route(
        self,
        path: str,
        endpoint: Callable,
        *,
        response_model: Type[Any] = None,
        status_code: int = 200,
        tags: List[str] = None,
        dependencies: Sequence[params.Depends] = None,
        summary: str = None,
        description: str = None,
        response_description: str = ""Successful Response"",
        responses: Dict[Union[int, str], Dict[str, Any]] = None,
        deprecated: bool = None,
        methods: Optional[Union[Set[str], List[str]]] = None,
        operation_id: str = None,
        response_model_include: Union[SetIntStr, DictIntStrAny] = None,
        response_model_exclude: Union[SetIntStr, DictIntStrAny] = set(),
        response_model_by_alias: bool = True,
        response_model_skip_defaults: bool = False,
        include_in_schema: bool = True,
        response_class: Type[Response] = None,
        name: str = None,
    ) -> None:
        route = self.route_class(
            path,
            endpoint=endpoint,
            response_model=response_model,
            status_code=status_code,
            tags=tags or [],
            dependencies=dependencies,
            summary=summary,
            description=description,
            response_description=response_description,
            responses=responses or {},
            deprecated=deprecated,
            methods=methods,
            operation_id=operation_id,
            response_model_include=response_model_include,
            response_model_exclude=response_model_exclude,
            response_model_by_alias=response_model_by_alias,
            response_model_skip_defaults=response_model_skip_defaults,
            include_in_schema=include_in_schema,
            response_class=response_class,
            name=name,
            dependency_overrides_provider=self.dependency_overrides_provider,
        )
        self.routes.append(route)
    def api_route(
        self,
        path: str,
        *,
        response_model: Type[Any] = None,",[97]
"    for c in components:
        if len(c) == 1:
            replaced.append(c[0])
        else:
            ns, tag = c
            replaced.append('{%s}%s' % (ns_map[ns], tag))
    return '/'.join(replaced)

def xpath_element(node, xpath, name=None, fatal=False, default=NO_DEFAULT):
    def _find_xpath(xpath):
        return node.find(compat_xpath(xpath))
    if isinstance(xpath, (str, compat_str)):
        n = _find_xpath(xpath)
    else:
        for xp in xpath:
            n = _find_xpath(xp)
            if n is not None:
                break
    if n is None:
        if default is not NO_DEFAULT:
            return default
        elif fatal:
            name = xpath if name is None else name
            raise ExtractorError('Could not find XML element %s' % name)
        else:
            return None
    return n

def xpath_text(node, xpath, name=None, fatal=False, default=NO_DEFAULT):
    n = xpath_element(node, xpath, name, fatal=fatal, default=default)
    if n is None or n == default:
        return n
    if n.text is None:
        if default is not NO_DEFAULT:
            return default
        elif fatal:
            name = xpath if name is None else name
            raise ExtractorError('Could not find XML element\'s text %s' % name)
        else:
            return None
    return n.text

def xpath_attr(node, xpath, key, name=None, fatal=False, default=NO_DEFAULT):
    n = find_xpath_attr(node, xpath, key)
    if n is None:
        if default is not NO_DEFAULT:
            return default
        elif fatal:
            name = '%s[@%s]' % (xpath, key) if name is None else name
            raise ExtractorError('Could not find XML attribute %s' % name)
        else:
            return None
    return n.attrib[key]

def get_element_by_id(id, html):
    """"""Return the content of the tag with the specified ID in the passed HTML document""""""
    return get_element_by_attribute('id', id, html)

def get_element_by_class(class_name, html):
    """"""Return the content of the first tag with the specified class in the passed HTML document""""""
    retval = get_elements_by_class(class_name, html)
    return retval[0] if retval else None

def get_element_by_attribute(attribute, value, html, escape_value=True):
    retval = get_elements_by_attribute(attribute, value, html, escape_value)
    return retval[0] if retval else None

def get_elements_by_class(class_name, html):
    """"""Return the content of all tags with the specified class in the passed HTML document as a list""""""
    return get_elements_by_attribute(
        'class', r'[^\'""]*\b%s\b[^\'""]*' % re.escape(class_name),
        html, escape_value=False)

def get_elements_by_attribute(attribute, value, html, escape_value=True):
    """"""Return the content of the tag with the specified attribute in the passed HTML document""""""
    value = re.escape(value) if escape_value else value
    retlist = []
    for m in re.finditer(r'''(?xs)
        <([a-zA-Z0-9:._-]+)
         (?:\s+[a-zA-Z0-9:._-]+(?:=[a-zA-Z0-9:._-]*|=""[^""]*""|='[^']*'))*?
         \s+%s=['""]?%s['""]?
         (?:\s+[a-zA-Z0-9:._-]+(?:=[a-zA-Z0-9:._-]*|=""[^""]*""|='[^']*'))*?
        \s*>
        (?P<content>.*?)
        </\1>
    ''' % (re.escape(attribute), value), html):
        res = m.group('content')
        if res.startswith('""') or res.startswith(""'""):
            res = res[1:-1]
        retlist.append(unescapeHTML(res))
    return retlist

class HTMLAttributeParser(compat_HTMLParser):
    """"""Trivial HTML parser to gather the attributes for a single element""""""
    def __init__(self):
        self.attrs = {}
        compat_HTMLParser.__init__(self)
    def handle_starttag(self, tag, attrs):
        self.attrs = dict(attrs)

def extract_attributes(html_element):
    """"""Given a string for an HTML element such as
    <el
         a=""foo"" B=""bar"" c=""&98;az"" d=boz
         empty= noval entity=""&amp;""
         sq='""' dq=""'""
    >
    Decode and return a dictionary of attributes.
    {","[91, 93]"
"from difflib import get_close_matches
from thefuck.utils import get_all_executables, \
    get_valid_history_without_current, get_closest
from thefuck.specific.sudo import sudo_support

@sudo_support
def match(command):
    return (command.script_parts
            and 'not found' in command.stderr
            and bool(get_close_matches(command.script_parts[0],
                                       get_all_executables())))

def _get_used_executables(command):
    for script in get_valid_history_without_current(command):
        yield script.split(' ')[0]

@sudo_support
def get_new_command(command):
    old_command = command.script_parts[0]
    # One from history:
    already_used = get_closest(
        old_command, _get_used_executables(command),
        fallback_to_first=False)
    if already_used:
        new_cmds = [already_used]
    else:
        new_cmds = []
    # Other from all executables:
    new_cmds += [cmd for cmd in get_close_matches(old_command,
                                                  get_all_executables())
                 if cmd not in new_cmds]
    return [' '.join([new_command] + command.script_parts[1:])
            for new_command in new_cmds]

priority = 3000","[2, 8]"
"            if x is None or len(x) == 0:
                # Handle data tensors support when no input given
                # step-size = 1 for data tensors
                batch_size = 1
            elif isinstance(x, list):
                batch_size = x[0].shape[0]
            elif isinstance(x, dict):
                batch_size = list(x.values())[0].shape[0]
            else:
                batch_size = x.shape[0]
            if batch_size == 0:
                raise ValueError('Received an empty batch. '
                                 'Batches should contain '
                                 'at least one item.')
            steps_done += 1
            batch_sizes.append(batch_size)
            if verbose == 1:
                progbar.update(steps_done)
    finally:
        if enqueuer is not None:
            enqueuer.stop()
    averages = []
    for i in range(len(outs)):
        if i not in stateful_metric_indices:
            averages.append(np.average([out[i] for out in outs_per_batch],
                                       weights=batch_sizes))
        else:
            averages.append(np.float64(outs_per_batch[-1][i]))
    return unpack_singleton(averages)

def predict_generator(model, generator,
                      steps=None,
                      max_queue_size=10,
                      workers=1,
                      use_multiprocessing=False,
                      verbose=0):
    """"""See docstring for `Model.predict_generator`.""""""
    model._make_predict_function()
    steps_done = 0
    all_outs = []
    is_sequence = isinstance(generator, Sequence)
    if not is_sequence and use_multiprocessing and workers > 1:
        warnings.warn(
            UserWarning('Using a generator with `use_multiprocessing=True`'
                        ' and multiple workers may duplicate your data.'
                        ' Please consider using the`keras.utils.Sequence'
                        ' class.'))
    if steps is None:
        if is_sequence:
            steps = len(generator)
        else:
            raise ValueError('`steps=None` is only valid for a generator'
                             ' based on the `keras.utils.Sequence` class.'
                             ' Please specify `steps` or use the'
                             ' `keras.utils.Sequence` class.')
    enqueuer = None
    try:
        if workers > 0:
            if is_sequence:
                enqueuer = OrderedEnqueuer(
                    generator,
                    use_multiprocessing=use_multiprocessing)
            else:
                enqueuer = GeneratorEnqueuer(
                    generator,
                    use_multiprocessing=use_multiprocessing)
            enqueuer.start(workers=workers, max_queue_size=max_queue_size)
            output_generator = enqueuer.get()
        else:
            if is_sequence:
                output_generator = iter_sequence_infinite(generator)
            else:
                output_generator = generator
        if verbose == 1:
            progbar = Progbar(target=steps)
        while steps_done < steps:
            generator_output = next(output_generator)
            if isinstance(generator_output, tuple):
                # Compatibility with the generators
                # used for training.
                if len(generator_output) == 2:
                    x, _ = generator_output
                elif len(generator_output) == 3:
                    x, _, _ = generator_output
                else:
                    raise ValueError('Output of generator should be '
                                     'a tuple `(x, y, sample_weight)` '
                                     'or `(x, y)`. Found: ' +
                                     str(generator_output))
            else:
                # Assumes a generator that only
                # yields inputs (not targets and sample weights).
                x = generator_output
            outs = model.predict_on_batch(x)
            outs = to_list(outs)
            if not all_outs:
                for out in outs:
                    all_outs.append([])
            for i, out in enumerate(outs):
                all_outs[i].append(out)
            steps_done += 1
            if verbose == 1:
                progbar.update(steps_done)
    finally:
        if enqueuer is not None:
            enqueuer.stop()
    if len(all_outs) == 1:
        if steps_done == 1:
            return all_outs[0][0]
        else:
            return np.concatenate(all_outs[0])
    if steps_done == 1:
        return [out[0] for out in all_outs]
    else:","[44, 45, 52, 63, 74]"
"        for case in cases:
            result = first.intersection(case)
            assert tm.equalContents(result, second)
        if isinstance(indices, MultiIndex):
            msg = ""other must be a MultiIndex or a list of tuples""
            with pytest.raises(TypeError, match=msg):
                first.intersection([1, 2, 3])
    def test_union_base(self, indices):
        first = indices[3:]
        second = indices[:5]
        everything = indices
        union = first.union(second)
        assert tm.equalContents(union, everything)
        if is_datetime64tz_dtype(indices.dtype):
            # The second.values below will drop tz, so the rest of this test
            #  is not applicable.
            return
        # GH 10149
        cases = [klass(second.values) for klass in [np.array, Series, list]]
        for case in cases:
            if not isinstance(indices, CategoricalIndex):
                result = first.union(case)
                assert tm.equalContents(result, everything)
        if isinstance(indices, MultiIndex):
            msg = ""other must be a MultiIndex or a list of tuples""
            with pytest.raises(TypeError, match=msg):
                first.union([1, 2, 3])
    @pytest.mark.parametrize(""sort"", [None, False])
    def test_difference_base(self, sort, indices):
        if isinstance(indices, CategoricalIndex):
            return
        first = indices[2:]
        second = indices[:4]
        answer = indices[4:]
        result = first.difference(second, sort)
        assert tm.equalContents(result, answer)
        # GH 10149
        cases = [klass(second.values) for klass in [np.array, Series, list]]
        for case in cases:
            if isinstance(indices, (DatetimeIndex, TimedeltaIndex)):
                assert type(result) == type(answer)
                tm.assert_numpy_array_equal(
                    result.sort_values().asi8, answer.sort_values().asi8
                )
            else:
                result = first.difference(case, sort)
                assert tm.equalContents(result, answer)
        if isinstance(indices, MultiIndex):
            msg = ""other must be a MultiIndex or a list of tuples""
            with pytest.raises(TypeError, match=msg):
                first.difference([1, 2, 3], sort)
    def test_symmetric_difference(self, indices):
        if isinstance(indices, CategoricalIndex):
            return
        first = indices[1:]
        second = indices[:-1]
        answer = indices[[0, -1]]
        result = first.symmetric_difference(second)
        assert tm.equalContents(result, answer)
        # GH 10149
        cases = [klass(second.values) for klass in [np.array, Series, list]]
        for case in cases:
            result = first.symmetric_difference(case)
            assert tm.equalContents(result, answer)
        if isinstance(indices, MultiIndex):
            msg = ""other must be a MultiIndex or a list of tuples""
            with pytest.raises(TypeError, match=msg):
                first.symmetric_difference([1, 2, 3])
    def test_insert_base(self, indices):
        result = indices[1:4]
        if not len(indices):
            return
        # test 0th element
        assert indices[0:4].equals(result.insert(0, indices[0]))
    def test_delete_base(self, indices):
        if not len(indices):
            return
        if isinstance(indices, RangeIndex):
            # tested in class
            return
        expected = indices[1:]
        result = indices.delete(0)
        assert result.equals(expected)
        assert result.name == expected.name
        expected = indices[:-1]
        result = indices.delete(-1)
        assert result.equals(expected)
        assert result.name == expected.name
        with pytest.raises((IndexError, ValueError)):
            # either depending on numpy version
            indices.delete(len(indices))
    def test_equals(self, indices):
        if isinstance(indices, IntervalIndex):
            # IntervalIndex tested separately
            return
        assert indices.equals(indices)
        assert indices.equals(indices.copy())
        assert indices.equals(indices.astype(object))
        assert not indices.equals(list(indices))
        assert not indices.equals(np.array(indices))
        # Cannot pass in non-int64 dtype to RangeIndex
        if not isinstance(indices, RangeIndex):",[126]
"            grouper, _, _ = get_grouper(
                dropped,
                key=self.keys,
                axis=self.axis,
                level=self.level,
                sort=self.sort,
                mutated=self.mutated,
            )
        grb = dropped.groupby(grouper, as_index=self.as_index, sort=self.sort)
        sizes, result = grb.size(), grb.nth(n)
        mask = (sizes < max_len).values
        # set the results which don't meet the criteria
        if len(result) and mask.any():
            result.loc[mask] = np.nan
        # reset/reindex to the original groups
        if len(self.obj) == len(dropped) or len(result) == len(
            self.grouper.result_index
        ):
            result.index = self.grouper.result_index
        else:
            result = result.reindex(self.grouper.result_index)
        return result
    def quantile(self, q=0.5, interpolation: str = ""linear""):
        """"""
        Return group values at the given quantile, a la numpy.percentile.
        Parameters
        ----------
        q : float or array-like, default 0.5 (50% quantile)
            Value(s) between 0 and 1 providing the quantile(s) to compute.
        interpolation : {'linear', 'lower', 'higher', 'midpoint', 'nearest'}
            Method to use when the desired quantile falls between two points.
        Returns
        -------
        Series or DataFrame
            Return type determined by caller of GroupBy object.
        See Also
        --------
        Series.quantile : Similar method for Series.
        DataFrame.quantile : Similar method for DataFrame.
        numpy.percentile : NumPy method to compute qth percentile.
        Examples
        --------
        >>> df = pd.DataFrame([
        ...     ['a', 1], ['a', 2], ['a', 3],
        ...     ['b', 1], ['b', 3], ['b', 5]
        ... ], columns=['key', 'val'])
        >>> df.groupby('key').quantile()
            val
        key
        a    2.0
        b    3.0
        """"""
        from pandas import concat
        def pre_processor(vals: np.ndarray) -> Tuple[np.ndarray, Optional[Type]]:
            if is_object_dtype(vals):
                raise TypeError(
                    ""'quantile' cannot be performed against 'object' dtypes!""
                )
            inference = None
            if is_integer_dtype(vals):
                inference = np.int64
            elif is_datetime64_dtype(vals):
                inference = ""datetime64[ns]""
                vals = np.asarray(vals).astype(np.float)
            return vals, inference
        def post_processor(vals: np.ndarray, inference: Optional[Type]) -> np.ndarray:
            if inference:
                # Check for edge case
                if not (
                    is_integer_dtype(inference)
                    and interpolation in {""linear"", ""midpoint""}
                ):
                    vals = vals.astype(inference)
            return vals
        if is_scalar(q):
            return self._get_cythonized_result(
                ""group_quantile"",
                aggregate=True,
                needs_values=True,
                needs_mask=True,
                cython_dtype=np.dtype(np.float64),
                pre_processing=pre_processor,
                post_processing=post_processor,
                q=q,
                interpolation=interpolation,
            )
        else:
            results = [
                self._get_cythonized_result(
                    ""group_quantile"",
                    aggregate=True,
                    needs_values=True,
                    needs_mask=True,
                    cython_dtype=np.dtype(np.float64),
                    pre_processing=pre_processor,
                    post_processing=post_processor,
                    q=qi,
                    interpolation=interpolation,
                )
                for qi in q
            ]
            result = concat(results, axis=0, keys=q)
            # fix levels to place quantiles on the inside
            # TODO(GH-10710): Ideally, we could write this as
            #  >>> result.stack(0).loc[pd.IndexSlice[:, ..., q], :]
            #  but this hits https://github.com/pandas-dev/pandas/issues/10710
            #  which doesn't reorder the list-like `q` on the inner level.
            order = list(range(1, result.index.nlevels)) + [0]
            # temporarily saves the index names
            index_names = np.array(result.index.names)","[71, 73]"
"    def sum(self, *args, **kwargs):
        nv.validate_window_func(""sum"", args, kwargs)
        window_func = self._get_roll_func(""roll_weighted_sum"")
        window_func = get_weighted_roll_func(window_func)
        return self._apply(
            window_func, center=self.center, is_weighted=True, name=""sum"", **kwargs
        )
    @Substitution(name=""window"")
    @Appender(_shared_docs[""mean""])
    def mean(self, *args, **kwargs):
        nv.validate_window_func(""mean"", args, kwargs)
        window_func = self._get_roll_func(""roll_weighted_mean"")
        window_func = get_weighted_roll_func(window_func)
        return self._apply(
            window_func, center=self.center, is_weighted=True, name=""mean"", **kwargs
        )
    @Substitution(name=""window"", versionadded=""\n.. versionadded:: 1.0.0\n"")
    @Appender(_shared_docs[""var""])
    def var(self, ddof=1, *args, **kwargs):
        nv.validate_window_func(""var"", args, kwargs)
        window_func = partial(self._get_roll_func(""roll_weighted_var""), ddof=ddof)
        window_func = get_weighted_roll_func(window_func)
        kwargs.pop(""name"", None)
        return self._apply(
            window_func, center=self.center, is_weighted=True, name=""var"", **kwargs
        )
    @Substitution(name=""window"", versionadded=""\n.. versionadded:: 1.0.0\n"")
    @Appender(_shared_docs[""std""])
    def std(self, ddof=1, *args, **kwargs):
        nv.validate_window_func(""std"", args, kwargs)
        return zsqrt(self.var(ddof=ddof, name=""std"", **kwargs))

class _Rolling(_Window):
    @property
    def _constructor(self):
        return Rolling

class _Rolling_and_Expanding(_Rolling):
    _shared_docs[""count""] = dedent(
        r""""""
    The %(name)s count of any non-NaN observations inside the window.
    Returns
    -------
    Series or DataFrame
        Returned object type is determined by the caller of the %(name)s
        calculation.
    See Also
    --------
    Series.%(name)s : Calling object with Series data.
    DataFrame.%(name)s : Calling object with DataFrames.
    DataFrame.count : Count of the full DataFrame.
    Examples
    --------
    >>> s = pd.Series([2, 3, np.nan, 10])
    >>> s.rolling(2).count()
    0    1.0
    1    2.0
    2    1.0
    3    1.0
    dtype: float64
    >>> s.rolling(3).count()
    0    1.0
    1    2.0
    2    2.0
    3    2.0
    dtype: float64
    >>> s.rolling(4).count()
    0    1.0
    1    2.0
    2    2.0
    3    3.0
    dtype: float64
    """"""
    )
    def count(self):
        if isinstance(self.window, BaseIndexer):
            validate_baseindexer_support(""count"")
        blocks, obj = self._create_blocks()
        results = []
        for b in blocks:
            result = b.notna().astype(int)
            result = self._constructor(
                result,
                window=self._get_window(),
                min_periods=self.min_periods or 0,
                center=self.center,
                axis=self.axis,
                closed=self.closed,
            ).sum()
            results.append(result)
        return self._wrap_results(results, blocks, obj)
    _shared_docs[""apply""] = dedent(
        r""""""
    Apply an arbitrary function to each %(name)s window.
    Parameters
    ----------
    func : function
        Must produce a single value from an ndarray input if ``raw=True``
        or a single value from a Series if ``raw=False``. Can also accept a
        Numba JIT function with ``engine='numba'`` specified.
        .. versionchanged:: 1.0.0
    raw : bool, default None
        * ``False`` : passes each row or column as a Series to the
          function.
        * ``True`` : the passed function will receive ndarray
          objects instead.
          If you are just applying a NumPy reduction function this will
          achieve much better performance.
    engine : str, default 'cython'
        * ``'cython'`` : Runs rolling apply through C-extensions from cython.
        * ``'numba'`` : Runs rolling apply through JIT compiled code from numba.","[85, 86]"
"        # it's possible to callback a different model than self:
        if hasattr(self, 'callback_model') and self.callback_model:
            callback_model = self.callback_model
        else:
            callback_model = self
        callbacks.set_model(callback_model)
        callbacks.set_params({
            'epochs': epochs,
            'steps': steps_per_epoch,
            'verbose': verbose,
            'do_validation': do_validation,
            'metrics': callback_metrics,
        })
        callbacks.on_train_begin()
        enqueuer = None
        val_enqueuer = None
        try:
            if do_validation:
                if val_gen:
                    if workers > 0:
                        if isinstance(validation_data, Sequence):
                            val_enqueuer = OrderedEnqueuer(
                                validation_data,
                                use_multiprocessing=use_multiprocessing)
                            if validation_steps is None:
                                validation_steps = len(validation_data)
                        else:
                            val_enqueuer = GeneratorEnqueuer(
                                validation_data,
                                use_multiprocessing=use_multiprocessing,
                                wait_time=wait_time)
                        val_enqueuer.start(workers=workers, max_queue_size=max_queue_size)
                        validation_generator = val_enqueuer.get()
                    else:
                        validation_generator = validation_data
                else:
                    if len(validation_data) == 2:
                        val_x, val_y = validation_data
                        val_sample_weight = None
                    elif len(validation_data) == 3:
                        val_x, val_y, val_sample_weight = validation_data
                    else:
                        raise ValueError('`validation_data` should be a tuple '
                                         '`(val_x, val_y, val_sample_weight)` '
                                         'or `(val_x, val_y)`. Found: ' +
                                         str(validation_data))
                    val_x, val_y, val_sample_weights = self._standardize_user_data(
                        val_x, val_y, val_sample_weight)
                    val_data = val_x + val_y + val_sample_weights
                    if self.uses_learning_phase and not isinstance(K.learning_phase(), int):
                        val_data += [0.]
                    for cbk in callbacks:
                        cbk.validation_data = val_data
            if workers > 0:
                if is_sequence:
                    enqueuer = OrderedEnqueuer(generator,
                                               use_multiprocessing=use_multiprocessing,
                                               shuffle=shuffle)
                else:
                    enqueuer = GeneratorEnqueuer(generator,
                                                 use_multiprocessing=use_multiprocessing,
                                                 wait_time=wait_time)
                enqueuer.start(workers=workers, max_queue_size=max_queue_size)
                output_generator = enqueuer.get()
            else:
                output_generator = generator
            callback_model.stop_training = False
            # Construct epoch logs.
            epoch_logs = {}
            while epoch < epochs:
                callbacks.on_epoch_begin(epoch)
                steps_done = 0
                batch_index = 0
                while steps_done < steps_per_epoch:
                    generator_output = next(output_generator)
                    if not hasattr(generator_output, '__len__'):
                        raise ValueError('Output of generator should be '
                                         'a tuple `(x, y, sample_weight)` '
                                         'or `(x, y)`. Found: ' +
                                         str(generator_output))
                    if len(generator_output) == 2:
                        x, y = generator_output
                        sample_weight = None
                    elif len(generator_output) == 3:
                        x, y, sample_weight = generator_output
                    else:
                        raise ValueError('Output of generator should be '
                                         'a tuple `(x, y, sample_weight)` '
                                         'or `(x, y)`. Found: ' +
                                         str(generator_output))
                    # build batch logs
                    batch_logs = {}
                    if isinstance(x, list):
                        batch_size = x[0].shape[0]
                    elif isinstance(x, dict):
                        batch_size = list(x.values())[0].shape[0]
                    else:
                        batch_size = x.shape[0]
                    batch_logs['batch'] = batch_index
                    batch_logs['size'] = batch_size
                    callbacks.on_batch_begin(batch_index, batch_logs)
                    outs = self.train_on_batch(x, y,
                                               sample_weight=sample_weight,
                                               class_weight=class_weight)
                    if not isinstance(outs, list):
                        outs = [outs]
                    for l, o in zip(out_labels, outs):
                        batch_logs[l] = o
                    callbacks.on_batch_end(batch_index, batch_logs)
                    batch_index += 1
                    steps_done += 1
                    # Epoch finished.
                    if steps_done >= steps_per_epoch and do_validation:
                        if val_gen:
                            val_outs = self.evaluate_generator(","[37, 69]"
"    # ------------------------------------------------------------------------
    # Constructors
    def __init__(self, values, copy=False):
        if isinstance(values, type(self)):
            values = values._ndarray
        if not isinstance(values, np.ndarray):
            raise ValueError(
                ""'values' must be a NumPy array, not {typ}"".format(
                    typ=type(values).__name__
                )
            )
        if values.ndim != 1:
            raise ValueError(""PandasArray must be 1-dimensional."")
        if copy:
            values = values.copy()
        self._ndarray = values
        self._dtype = PandasDtype(values.dtype)
    @classmethod
    def _from_sequence(cls, scalars, dtype=None, copy=False):
        if isinstance(dtype, PandasDtype):
            dtype = dtype._dtype
        result = np.asarray(scalars, dtype=dtype)
        if copy and result is scalars:
            result = result.copy()
        return cls(result)
    @classmethod
    def _from_factorized(cls, values, original):
        return cls(values)
    @classmethod
    def _concat_same_type(cls, to_concat):
        return cls(np.concatenate(to_concat))
    # ------------------------------------------------------------------------
    # Data
    @property
    def dtype(self):
        return self._dtype
    # ------------------------------------------------------------------------
    # NumPy Array Interface
    def __array__(self, dtype=None):
        return np.asarray(self._ndarray, dtype=dtype)
    _HANDLED_TYPES = (np.ndarray, numbers.Number)
    def __array_ufunc__(self, ufunc, method, *inputs, **kwargs):
        # Lightly modified version of
        # https://docs.scipy.org/doc/numpy-1.15.1/reference/generated/\
        # numpy.lib.mixins.NDArrayOperatorsMixin.html
        # The primary modification is not boxing scalar return values
        # in PandasArray, since pandas' ExtensionArrays are 1-d.
        out = kwargs.get(""out"", ())
        for x in inputs + out:
            # Only support operations with instances of _HANDLED_TYPES.
            # Use PandasArray instead of type(self) for isinstance to
            # allow subclasses that don't override __array_ufunc__ to
            # handle PandasArray objects.
            if not isinstance(x, self._HANDLED_TYPES + (PandasArray,)):
                return NotImplemented
        # Defer to the implementation of the ufunc on unwrapped values.
        inputs = tuple(x._ndarray if isinstance(x, PandasArray) else x for x in inputs)
        if out:
            kwargs[""out""] = tuple(
                x._ndarray if isinstance(x, PandasArray) else x for x in out
            )
        result = getattr(ufunc, method)(*inputs, **kwargs)
        if type(result) is tuple and len(result):
            # multiple return values
            if not lib.is_scalar(result[0]):
                # re-box array-like results
                return tuple(type(self)(x) for x in result)
            else:
                # but not scalar reductions
                return result
        elif method == ""at"":
            # no return value
            return None
        else:
            # one return value
            if not lib.is_scalar(result):
                # re-box array-like results, but not scalar reductions
                result = type(self)(result)
            return result
    # ------------------------------------------------------------------------
    # Pandas ExtensionArray Interface
    def __getitem__(self, item):
        if isinstance(item, type(self)):
            item = item._ndarray
        result = self._ndarray[item]
        if not lib.is_scalar(item):
            result = type(self)(result)
        return result
    def __setitem__(self, key, value):
        value = extract_array(value, extract_numpy=True)
        if not lib.is_scalar(key) and is_list_like(key):
            key = np.asarray(key)
        if not lib.is_scalar(value):
            value = np.asarray(value)
        values = self._ndarray
        t = np.result_type(value, values)
        if t != self._ndarray.dtype:
            values = values.astype(t, casting=""safe"")
            values[key] = value
            self._dtype = PandasDtype(t)
            self._ndarray = values
        else:
            self._ndarray[key] = value","[118, 119, 120, 121, 122, 123, 124, 125, 126]"
"        """"""
        x, y, sample_weights = self._standardize_user_data(
            x, y,
            sample_weight=sample_weight,
            check_batch_axis=True)
        if self.uses_learning_phase and not isinstance(K.learning_phase(), int):
            ins = x + y + sample_weights + [0.]
        else:
            ins = x + y + sample_weights
        self._make_test_function()
        outputs = self.test_function(ins)
        if len(outputs) == 1:
            return outputs[0]
        return outputs
    def predict_on_batch(self, x):
        """"""Returns predictions for a single batch of samples.
        # Arguments
            x: Input samples, as a Numpy array.
        # Returns
            Numpy array(s) of predictions.
        """"""
        x = _standardize_input_data(x, self._feed_input_names,
                                    self._feed_input_shapes)
        if self.uses_learning_phase and not isinstance(K.learning_phase(), int):
            ins = x + [0.]
        else:
            ins = x
        self._make_predict_function()
        outputs = self.predict_function(ins)
        if len(outputs) == 1:
            return outputs[0]
        return outputs
    @interfaces.legacy_generator_methods_support
    def fit_generator(self,
                      generator,
                      steps_per_epoch,
                      epochs=1,
                      verbose=1,
                      callbacks=None,
                      validation_data=None,
                      validation_steps=None,
                      class_weight=None,
                      max_queue_size=10,
                      workers=1,
                      use_multiprocessing=False,
                      shuffle=True,
                      initial_epoch=0):
        """"""Fits the model on data yielded batch-by-batch by a Python generator.
        The generator is run in parallel to the model, for efficiency.
        For instance, this allows you to do real-time data augmentation
        on images on CPU in parallel to training your model on GPU.
        The use of `keras.utils.Sequence` guarantees the ordering
        and guarantees the single use of every input per epoch when
        using `use_multiprocessing=True`.
        # Arguments
            generator: A generator or an instance of `Sequence` (`keras.utils.Sequence`)
                    object in order to avoid duplicate data
                    when using multiprocessing.
                The output of the generator must be either
                - a tuple `(inputs, targets)`
                - a tuple `(inputs, targets, sample_weights)`.
                This tuple (a single output of the generator) makes a single batch.
                Therefore, all arrays in this tuple must have the same length (equal
                to the size of this batch). Different batches may have different sizes.
                For example, the last batch of the epoch is commonly smaller than the
                others, if the size of the dataset is not divisible by the batch size.
                The generator is expected to loop over its data
                indefinitely. An epoch finishes when `steps_per_epoch`
                batches have been seen by the model.
            steps_per_epoch: Total number of steps (batches of samples)
                to yield from `generator` before declaring one epoch
                finished and starting the next epoch. It should typically
                be equal to the number of samples of your dataset
                divided by the batch size. Not used if using `Sequence`.
            epochs: Integer, total number of iterations on the data.
            verbose: Verbosity mode, 0, 1, or 2.
            callbacks: List of callbacks to be called during training.
            validation_data: This can be either
                - a generator for the validation data
                - a tuple (inputs, targets)
                - a tuple (inputs, targets, sample_weights).
            validation_steps: Only relevant if `validation_data`
                is a generator. Total number of steps (batches of samples)
                to yield from `generator` before stopping.
            class_weight: Dictionary mapping class indices to a weight
                for the class.
            max_queue_size: Integer. Maximum size for the generator queue.
                If unspecified, `max_queue_size` will default to 10.
            workers: Integer. Maximum number of processes to spin up
                when using process based threading.
                If unspecified, `workers` will default to 1.
            use_multiprocessing: Boolean. If True, use process based threading.
                If unspecified, `workers` will default to False.
                Note that because
                this implementation relies on multiprocessing,
                you should not pass
                non picklable arguments to the generator
                as they can't be passed
                easily to children processes.
            shuffle: Whether to shuffle the order of the batches at
                the beginning of each epoch. Only used with instances
                of `Sequence` (keras.utils.Sequence).
            initial_epoch: Epoch at which to start training
                (useful for resuming a previous training run)
        # Returns
            A `History` object.
        # Example
        ```python
            def generate_arrays_from_file(path):
                while 1:
                    f = open(path)
                    for line in f:
                        # create numpy arrays of input data
                        # and labels, from each line in the file
                        x1, x2, y = process_line(line)
                        yield ({'input_1': x1, 'input_2': x2}, {'output': y})
                    f.close()","[39, 80]"
"    x, tf_data_format = _preprocess_conv2d_input(x, data_format)
    padding = _preprocess_padding(padding)
    x = tf.nn.convolution(
        input=x,
        filter=kernel,
        dilation_rate=dilation_rate,
        strides=strides,
        padding=padding,
        data_format=tf_data_format)
    if data_format == 'channels_first' and tf_data_format == 'NHWC':
        x = tf.transpose(x, (0, 3, 1, 2))  # NHWC -> NCHW
    return x

def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),
                     padding='valid', data_format=None):
    """"""2D deconvolution (i.e. transposed convolution).
    # Arguments
        x: Tensor or variable.
        kernel: kernel tensor.
        output_shape: 1D int tensor for the output shape.
        strides: strides tuple.
        padding: string, `""same""` or `""valid""`.
        data_format: string, `""channels_last""` or `""channels_first""`.
            Whether to use Theano or TensorFlow/CNTK data format
            for inputs/kernels/outputs.
    # Returns
        A tensor, result of transposed 2D convolution.
    # Raises
        ValueError: if `data_format` is neither `channels_last` or `channels_first`.
    """"""
    if data_format is None:
        data_format = image_data_format()
    if data_format not in {'channels_first', 'channels_last'}:
        raise ValueError('Unknown data_format: ' + str(data_format))
    if isinstance(output_shape, (tuple, list)):
        output_shape = tf.stack(output_shape)
    x, tf_data_format = _preprocess_conv2d_input(x, data_format)
    if data_format == 'channels_first' and tf_data_format == 'NHWC':
        output_shape = (output_shape[0],
                        output_shape[2],
                        output_shape[3],
                        output_shape[1])
    if output_shape[0] is None:
        output_shape = (tf.shape(x)[0],) + tuple(output_shape[1:])
        output_shape = tf.stack(list(output_shape))
    padding = _preprocess_padding(padding)
    if tf_data_format == 'NHWC':
        strides = (1,) + strides + (1,)
    else:
        strides = (1, 1) + strides
    x = tf.nn.conv2d_transpose(x, kernel, output_shape, strides,
                               padding=padding,
                               data_format=tf_data_format)
    if data_format == 'channels_first' and tf_data_format == 'NHWC':
        x = tf.transpose(x, (0, 3, 1, 2))  # NHWC -> NCHW
    return x

def separable_conv1d(x, depthwise_kernel, pointwise_kernel, strides=1,
                     padding='valid', data_format=None, dilation_rate=1):
    """"""1D convolution with separable filters.
    # Arguments
        x: input tensor
        depthwise_kernel: convolution kernel for the depthwise convolution.
        pointwise_kernel: kernel for the 1x1 convolution.
        strides: stride integer.
        padding: string, `""same""` or `""valid""`.
        data_format: string, `""channels_last""` or `""channels_first""`.
        dilation_rate: integer dilation rate.
    # Returns
        Output tensor.
    # Raises
        ValueError: if `data_format` is neither `channels_last` or `channels_first`.
    """"""
    if data_format is None:
        data_format = image_data_format()
    if data_format not in {'channels_first', 'channels_last'}:
        raise ValueError('Unknown data_format: ' + str(data_format))
    x, tf_data_format = _preprocess_conv1d_input(x, data_format)
    padding = _preprocess_padding(padding)
    if tf_data_format == 'NHWC':
        spatial_start_dim = 1
        strides = (1, 1) + strides + (1,)
    else:
        spatial_start_dim = 2
        strides = (1, 1, 1) + strides
    x = tf.expand_dims(x, spatial_start_dim)
    depthwise_kernel = tf.expand_dims(depthwise_kernel, 0)
    pointwise_kernel = tf.expand_dims(pointwise_kernel, 0)
    dilation_rate = (1,) + dilation_rate
    x = tf.nn.separable_conv2d(x, depthwise_kernel, pointwise_kernel,
                               strides=strides,
                               padding=padding,
                               rate=dilation_rate,
                               data_format=tf_data_format)
    x = tf.squeeze(x, [spatial_start_dim])
    if data_format == 'channels_first' and tf_data_format == 'NHWC':
        x = tf.transpose(x, (0, 2, 1))  # NWC -> NCW
    return x

def separable_conv2d(x, depthwise_kernel, pointwise_kernel, strides=(1, 1),
                     padding='valid', data_format=None, dilation_rate=(1, 1)):
    """"""2D convolution with separable filters.
    # Arguments
        x: input tensor
        depthwise_kernel: convolution kernel for the depthwise convolution.
        pointwise_kernel: kernel for the 1x1 convolution.","[96, 99]"
"            # be better to use the sort parameter passed into join
            join_index = self.union(other)
        if sort:
            join_index = join_index.sort_values()
        if return_indexers:
            if join_index is self:
                lindexer = None
            else:
                lindexer = self.get_indexer(join_index)
            if join_index is other:
                rindexer = None
            else:
                rindexer = other.get_indexer(join_index)
            return join_index, lindexer, rindexer
        else:
            return join_index
    def _join_multi(self, other, how, return_indexers=True):
        from pandas.core.indexes.multi import MultiIndex
        from pandas.core.reshape.merge import _restore_dropped_levels_multijoin
        # figure out join names
        self_names = set(com.not_none(*self.names))
        other_names = set(com.not_none(*other.names))
        overlap = self_names & other_names
        # need at least 1 in common
        if not overlap:
            raise ValueError(""cannot join with no overlapping index names"")
        self_is_mi = isinstance(self, ABCMultiIndex)
        other_is_mi = isinstance(other, ABCMultiIndex)
        if self_is_mi and other_is_mi:
            # Drop the non-matching levels from left and right respectively
            ldrop_names = list(self_names - overlap)
            rdrop_names = list(other_names - overlap)
            # if only the order differs
            if not len(ldrop_names + rdrop_names):
                self_jnlevels = self
                other_jnlevels = other.reorder_levels(self.names)
            else:
                self_jnlevels = self.droplevel(ldrop_names)
                other_jnlevels = other.droplevel(rdrop_names)
            # Join left and right
            # Join on same leveled multi-index frames is supported
            join_idx, lidx, ridx = self_jnlevels.join(
                other_jnlevels, how, return_indexers=True
            )
            # Restore the dropped levels
            # Returned index level order is
            # common levels, ldrop_names, rdrop_names
            dropped_names = ldrop_names + rdrop_names
            levels, codes, names = _restore_dropped_levels_multijoin(
                self, other, dropped_names, join_idx, lidx, ridx
            )
            # Re-create the multi-index
            multi_join_idx = MultiIndex(
                levels=levels, codes=codes, names=names, verify_integrity=False
            )
            multi_join_idx = multi_join_idx.remove_unused_levels()
            return multi_join_idx, lidx, ridx
        jl = list(overlap)[0]
        # Case where only one index is multi
        # make the indices into mi's that match
        flip_order = False
        if self_is_mi:
            self, other = other, self
            flip_order = True
            # flip if join method is right or left
            how = {""right"": ""left"", ""left"": ""right""}.get(how, how)
        level = other.names.index(jl)
        result = self._join_level(
            other, level, how=how, return_indexers=return_indexers
        )
        if flip_order:
            if isinstance(result, tuple):
                return result[0], result[2], result[1]
        return result
    def _join_non_unique(self, other, how=""left"", return_indexers=False):
        from pandas.core.reshape.merge import _get_join_indexers
        # We only get here if dtypes match
        assert self.dtype == other.dtype
        lvalues = self._get_engine_target()
        rvalues = other._get_engine_target()
        left_idx, right_idx = _get_join_indexers(
            [lvalues], [rvalues], how=how, sort=True
        )
        left_idx = ensure_platform_int(left_idx)
        right_idx = ensure_platform_int(right_idx)
        join_index = np.asarray(lvalues.take(left_idx))
        mask = left_idx == -1
        np.putmask(join_index, mask, rvalues.take(right_idx))
        join_index = self._wrap_joined_index(join_index, other)
        if return_indexers:
            return join_index, left_idx, right_idx
        else:
            return join_index
    def _join_level(
        self, other, level, how=""left"", return_indexers=False, keep_order=True
    ):
        """"""
        The join method *only* affects the level of the resulting
        MultiIndex. Otherwise it just exactly aligns the Index data to the",[71]
"                    self.to_stdout(json.dumps(ie_result))
                return ie_result
        if result_type == 'video':
            self.add_extra_info(ie_result, extra_info)
            return self.process_video_result(ie_result, download=download)
        elif result_type == 'url':
            # We have to add extra_info to the results because it may be
            # contained in a playlist
            return self.extract_info(ie_result['url'],
                                     download,
                                     ie_key=ie_result.get('ie_key'),
                                     extra_info=extra_info)
        elif result_type == 'url_transparent':
            # Use the information from the embedding page
            info = self.extract_info(
                ie_result['url'], ie_key=ie_result.get('ie_key'),
                extra_info=extra_info, download=False, process=False)
            # extract_info may return None when ignoreerrors is enabled and
            # extraction failed with an error, don't crash and return early
            # in this case
            if not info:
                return info
            force_properties = dict(
                (k, v) for k, v in ie_result.items() if v is not None)
            for f in ('_type', 'url', 'ie_key'):
                if f in force_properties:
                    del force_properties[f]
            new_result = info.copy()
            new_result.update(force_properties)
            # Extracted info may not be a video result (i.e.
            # info.get('_type', 'video') != video) but rather an url or
            # url_transparent. In such cases outer metadata (from ie_result)
            # should be propagated to inner one (info). For this to happen
            # _type of info should be overridden with url_transparent. This
            # fixes issue from https://github.com/rg3/youtube-dl/pull/11163.
            if new_result.get('_type') == 'url':
                new_result['_type'] = 'url_transparent'
            return self.process_ie_result(
                new_result, download=download, extra_info=extra_info)
        elif result_type in ('playlist', 'multi_video'):
            # We process each entry in the playlist
            playlist = ie_result.get('title') or ie_result.get('id')
            self.to_screen('[download] Downloading playlist: %s' % playlist)
            playlist_results = []
            playliststart = self.params.get('playliststart', 1) - 1
            playlistend = self.params.get('playlistend')
            # For backwards compatibility, interpret -1 as whole list
            if playlistend == -1:
                playlistend = None
            playlistitems_str = self.params.get('playlist_items')
            playlistitems = None
            if playlistitems_str is not None:
                def iter_playlistitems(format):
                    for string_segment in format.split(','):
                        if '-' in string_segment:
                            start, end = string_segment.split('-')
                            for item in range(int(start), int(end) + 1):
                                yield int(item)
                        else:
                            yield int(string_segment)
                playlistitems = iter_playlistitems(playlistitems_str)
            ie_entries = ie_result['entries']
            if isinstance(ie_entries, list):
                n_all_entries = len(ie_entries)
                if playlistitems:
                    entries = [
                        ie_entries[i - 1] for i in playlistitems
                        if -n_all_entries <= i - 1 < n_all_entries]
                else:
                    entries = ie_entries[playliststart:playlistend]
                n_entries = len(entries)
                self.to_screen(
                    '[%s] playlist %s: Collected %d video ids (downloading %d of them)' %
                    (ie_result['extractor'], playlist, n_all_entries, n_entries))
            elif isinstance(ie_entries, PagedList):
                if playlistitems:
                    entries = []
                    for item in playlistitems:
                        entries.extend(ie_entries.getslice(
                            item - 1, item
                        ))
                else:
                    entries = ie_entries.getslice(
                        playliststart, playlistend)
                n_entries = len(entries)
                self.to_screen(
                    '[%s] playlist %s: Downloading %d videos' %
                    (ie_result['extractor'], playlist, n_entries))
            else:  # iterable
                if playlistitems:
                    entry_list = list(ie_entries)
                    entries = [entry_list[i - 1] for i in playlistitems]
                else:
                    entries = list(itertools.islice(
                        ie_entries, playliststart, playlistend))
                n_entries = len(entries)
                self.to_screen(
                    '[%s] playlist %s: Downloading %d videos' %
                    (ie_result['extractor'], playlist, n_entries))
            if self.params.get('playlistreverse', False):
                entries = entries[::-1]
            if self.params.get('playlistrandom', False):
                random.shuffle(entries)
            x_forwarded_for = ie_result.get('__x_forwarded_for_ip')
            for i, entry in enumerate(entries, 1):
                self.to_screen('[download] Downloading video %s of %s' % (i, n_entries))
                # This __x_forwarded_for_ip thing is a bit ugly but requires
                # minimal changes
                if x_forwarded_for:
                    entry['__x_forwarded_for_ip'] = x_forwarded_for
                extra = {
                    'n_entries': n_entries,
                    'playlist': playlist,
                    'playlist_id': ie_result.get('id'),",[27]
"        num_train_samples = self._check_num_samples(ins, batch_size,
                                                    steps_per_epoch,
                                                    'steps_per_epoch')
        if num_train_samples is not None:
            index_array = np.arange(num_train_samples)
        self.history = cbks.History()
        _callbacks = [cbks.BaseLogger(
            stateful_metrics=self.stateful_metric_names)]
        if verbose:
            if steps_per_epoch is not None:
                count_mode = 'steps'
            else:
                count_mode = 'samples'
            _callbacks.append(
                cbks.ProgbarLogger(
                    count_mode,
                    stateful_metrics=self.stateful_metric_names))
        _callbacks += (callbacks or []) + [self.history]
        callbacks = cbks.CallbackList(_callbacks)
        out_labels = out_labels or []
        # it's possible to callback a different model than self
        # (used by Sequential models)
        if hasattr(self, 'callback_model') and self.callback_model:
            callback_model = self.callback_model
        else:
            callback_model = self
        callbacks.set_model(callback_model)
        callbacks.set_params({
            'batch_size': batch_size,
            'epochs': epochs,
            'steps': steps_per_epoch,
            'samples': num_train_samples,
            'verbose': verbose,
            'do_validation': do_validation,
            'metrics': callback_metrics or [],
        })
        callbacks.on_train_begin()
        callback_model.stop_training = False
        for cbk in callbacks:
            cbk.validation_data = val_ins
        # To prevent a slowdown, we find beforehand the arrays that need conversion.
        feed = self._feed_inputs + self._feed_targets + self._feed_sample_weights
        indices_for_conversion_to_dense = []
        for i in range(len(feed)):
            if issparse(ins[i]) and not K.is_sparse(feed[i]):
                indices_for_conversion_to_dense.append(i)
        for epoch in range(initial_epoch, epochs):
            # Reset stateful metrics
            for m in self.metrics:
                if isinstance(m, Layer) and m.stateful:
                    m.reset_states()
            callbacks.on_epoch_begin(epoch)
            epoch_logs = {}
            if steps_per_epoch is not None:
                for step_index in range(steps_per_epoch):
                    batch_logs = {}
                    batch_logs['batch'] = step_index
                    batch_logs['size'] = 1
                    callbacks.on_batch_begin(step_index, batch_logs)
                    outs = f(ins)
                    if not isinstance(outs, list):
                        outs = [outs]
                    for l, o in zip(out_labels, outs):
                        batch_logs[l] = o
                    callbacks.on_batch_end(step_index, batch_logs)
                    if callback_model.stop_training:
                        break
                if do_validation:
                    val_outs = self._test_loop(val_f, val_ins,
                                               batch_size=batch_size,
                                               steps=validation_steps,
                                               verbose=0)
                    if not isinstance(val_outs, list):
                        val_outs = [val_outs]
                    # Same labels assumed.
                    for l, o in zip(out_labels, val_outs):
                        epoch_logs['val_' + l] = o
            else:
                if shuffle == 'batch':
                    index_array = _batch_shuffle(index_array, batch_size)
                elif shuffle:
                    np.random.shuffle(index_array)
                batches = _make_batches(num_train_samples, batch_size)
                for batch_index, (batch_start, batch_end) in enumerate(batches):
                    batch_ids = index_array[batch_start:batch_end]
                    try:
                        if isinstance(ins[-1], float):
                            # Do not slice the training phase flag.
                            ins_batch = _slice_arrays(ins[:-1], batch_ids) + [ins[-1]]
                        else:
                            ins_batch = _slice_arrays(ins, batch_ids)
                    except TypeError:
                        raise TypeError('TypeError while preparing batch. '
                                        'If using HDF5 input data, '
                                        'pass shuffle=""batch"".')
                    batch_logs = {}
                    batch_logs['batch'] = batch_index
                    batch_logs['size'] = len(batch_ids)
                    callbacks.on_batch_begin(batch_index, batch_logs)
                    for i in indices_for_conversion_to_dense:
                        ins_batch[i] = ins_batch[i].toarray()
                    outs = f(ins_batch)
                    if not isinstance(outs, list):
                        outs = [outs]
                    for l, o in zip(out_labels, outs):
                        batch_logs[l] = o
                    callbacks.on_batch_end(batch_index, batch_logs)
                    if callback_model.stop_training:
                        break
                    if batch_index == len(batches) - 1:  # Last batch.
                        if do_validation:
                            val_outs = self._test_loop(val_f, val_ins,
                                                       batch_size=batch_size,
                                                       verbose=0)
                            if not isinstance(val_outs, list):","[53, 54, 55]"
"import operator
from typing import TYPE_CHECKING, Type, Union
import numpy as np
from pandas._libs import lib, missing as libmissing
from pandas.core.dtypes.base import ExtensionDtype
from pandas.core.dtypes.common import pandas_dtype
from pandas.core.dtypes.dtypes import register_extension_dtype
from pandas.core.dtypes.generic import ABCDataFrame, ABCIndexClass, ABCSeries
from pandas.core.dtypes.inference import is_array_like
from pandas import compat
from pandas.core import ops
from pandas.core.arrays import PandasArray
from pandas.core.construction import extract_array
from pandas.core.indexers import check_array_indexer
from pandas.core.missing import isna
if TYPE_CHECKING:
    import pyarrow  # noqa: F401

@register_extension_dtype
class StringDtype(ExtensionDtype):
    """"""
    Extension dtype for string data.
    .. versionadded:: 1.0.0
    .. warning::
       StringDtype is considered experimental. The implementation and
       parts of the API may change without warning.
       In particular, StringDtype.na_value may change to no longer be
       ``numpy.nan``.
    Attributes
    ----------
    None
    Methods
    -------
    None
    Examples
    --------
    >>> pd.StringDtype()
    StringDtype
    """"""
    name = ""string""
    #: StringDtype.na_value uses pandas.NA
    na_value = libmissing.NA
    @property
    def type(self) -> Type[str]:
        return str
    @classmethod
    def construct_array_type(cls) -> Type[""StringArray""]:
        """"""
        Return the array type associated with this dtype.
        Returns
        -------
        type
        """"""
        return StringArray
    def __repr__(self) -> str:
        return ""StringDtype""
    def __from_arrow__(
        self, array: Union[""pyarrow.Array"", ""pyarrow.ChunkedArray""]
    ) -> ""StringArray"":
        """"""
        Construct StringArray from pyarrow Array/ChunkedArray.
        """"""
        import pyarrow  # noqa: F811
        if isinstance(array, pyarrow.Array):
            chunks = [array]
        else:
            # pyarrow.ChunkedArray
            chunks = array.chunks
        results = []
        for arr in chunks:
            # using _from_sequence to ensure None is converted to NA
            str_arr = StringArray._from_sequence(np.array(arr))
            results.append(str_arr)
        return StringArray._concat_same_type(results)

class StringArray(PandasArray):
    """"""
    Extension array for string data.
    .. versionadded:: 1.0.0
    .. warning::
       StringArray is considered experimental. The implementation and
       parts of the API may change without warning.
    Parameters
    ----------
    values : array-like
        The array of data.
        .. warning::
           Currently, this expects an object-dtype ndarray
           where the elements are Python strings or :attr:`pandas.NA`.
           This may change without warning in the future. Use
           :meth:`pandas.array` with ``dtype=""string""`` for a stable way of
           creating a `StringArray` from any sequence.
    copy : bool, default False
        Whether to copy the array of data.
    Attributes",[15]
")
@click.option(
    ""--py36"",
    is_flag=True,
    help=(
        ""Allow using Python 3.6-only syntax on all input files.  This will put ""
        ""trailing commas in function signatures and calls also after *args and ""
        ""**kwargs. Deprecated; use --target-version instead. ""
        ""[default: per-file auto-detection]""
    ),
)
@click.option(
    ""--pyi"",
    is_flag=True,
    help=(
        ""Format all input files like typing stubs regardless of file extension ""
        ""(useful when piping source on standard input).""
    ),
)
@click.option(
    ""-S"",
    ""--skip-string-normalization"",
    is_flag=True,
    help=""Don't normalize string quotes or prefixes."",
)
@click.option(
    ""--check"",
    is_flag=True,
    help=(
        ""Don't write the files back, just return the status.  Return code 0 ""
        ""means nothing would change.  Return code 1 means some files would be ""
        ""reformatted.  Return code 123 means there was an internal error.""
    ),
)
@click.option(
    ""--diff"",
    is_flag=True,
    help=""Don't write the files back, just output a diff for each file on stdout."",
)
@click.option(
    ""--fast/--safe"",
    is_flag=True,
    help=""If --fast given, skip temporary sanity checks. [default: --safe]"",
)
@click.option(
    ""--include"",
    type=str,
    default=DEFAULT_INCLUDES,
    help=(
        ""A regular expression that matches files and directories that should be ""
        ""included on recursive searches.  An empty value means all files are ""
        ""included regardless of the name.  Use forward slashes for directories on ""
        ""all platforms (Windows, too).  Exclusions are calculated first, inclusions ""
        ""later.""
    ),
    show_default=True,
)
@click.option(
    ""--exclude"",
    type=str,
    default=DEFAULT_EXCLUDES,
    help=(
        ""A regular expression that matches files and directories that should be ""
        ""excluded on recursive searches.  An empty value means no paths are excluded. ""
        ""Use forward slashes for directories on all platforms (Windows, too).  ""
        ""Exclusions are calculated first, inclusions later.""
    ),
    show_default=True,
)
@click.option(
    ""-q"",
    ""--quiet"",
    is_flag=True,
    help=(
        ""Don't emit non-error messages to stderr. Errors are still emitted; ""
        ""silence those with 2>/dev/null.""
    ),
)
@click.option(
    ""-v"",
    ""--verbose"",
    is_flag=True,
    help=(
        ""Also emit messages to stderr about files that were not changed or were ""
        ""ignored due to --exclude=.""
    ),
)
@click.version_option(version=__version__)
@click.argument(
    ""src"",
    nargs=-1,
    type=click.Path(
        exists=True, file_okay=True, dir_okay=True, readable=True, allow_dash=True
    ),
    is_eager=True,
)
@click.option(
    ""--config"",
    type=click.Path(
        exists=False, file_okay=True, dir_okay=False, readable=True, allow_dash=False
    ),
    is_eager=True,
    callback=read_pyproject_toml,
    help=""Read configuration from PATH."",
)
@click.pass_context
def main(
    ctx: click.Context,
    code: Optional[str],
    line_length: int,
    target_version: List[TargetVersion],
    check: bool,
    diff: bool,
    fast: bool,
    pyi: bool,
    py36: bool,
    skip_string_normalization: bool,
    quiet: bool,
    verbose: bool,
    include: str,
    exclude: str,
    src: Tuple[str, ...],
    config: Optional[str],
) -> None:
    """"""The uncompromising code formatter.""""""
    write_back = WriteBack.from_configuration(check=check, diff=diff)
    if target_version:",[99]
"from twisted.python.failure import Failure
from scrapy.utils.defer import mustbe_deferred, defer_result
from scrapy.utils.request import request_fingerprint
from scrapy.utils.misc import arg_to_iter
logger = logging.getLogger(__name__)

class MediaPipeline(object):
    LOG_FAILED_RESULTS = True
    class SpiderInfo(object):
        def __init__(self, spider):
            self.spider = spider
            self.downloading = set()
            self.downloaded = {}
            self.waiting = defaultdict(list)
    def __init__(self, download_func=None):
        self.download_func = download_func
    @classmethod
    def from_crawler(cls, crawler):
        try:
            pipe = cls.from_settings(crawler.settings)
        except AttributeError:
            pipe = cls()
        pipe.crawler = crawler
        return pipe
    def open_spider(self, spider):
        self.spiderinfo = self.SpiderInfo(spider)
    def process_item(self, item, spider):
        info = self.spiderinfo
        requests = arg_to_iter(self.get_media_requests(item, info))
        dlist = [self._process_request(r, info) for r in requests]
        dfd = DeferredList(dlist, consumeErrors=1)
        return dfd.addCallback(self.item_completed, item, info)
    def _process_request(self, request, info):
        fp = request_fingerprint(request)
        cb = request.callback or (lambda _: _)
        eb = request.errback
        request.callback = None
        request.errback = None
        # Return cached result if request was already seen
        if fp in info.downloaded:
            return defer_result(info.downloaded[fp]).addCallbacks(cb, eb)
        # Otherwise, wait for result
        wad = Deferred().addCallbacks(cb, eb)
        info.waiting[fp].append(wad)
        # Check if request is downloading right now to avoid doing it twice
        if fp in info.downloading:
            return wad
        # Download request checking media_to_download hook output first
        info.downloading.add(fp)
        dfd = mustbe_deferred(self.media_to_download, request, info)
        dfd.addCallback(self._check_media_to_download, request, info)
        dfd.addBoth(self._cache_result_and_execute_waiters, fp, info)
        dfd.addErrback(lambda f: logger.error(
            f.value, extra={'spider': info.spider, 'failure': f})
        )
        return dfd.addBoth(lambda _: wad)  # it must return wad at last
    def _check_media_to_download(self, result, request, info):
        if result is not None:
            return result
        if self.download_func:
            # this ugly code was left only to support tests. TODO: remove
            dfd = mustbe_deferred(self.download_func, request, info.spider)
            dfd.addCallbacks(
                callback=self.media_downloaded, callbackArgs=(request, info),
                errback=self.media_failed, errbackArgs=(request, info))
        else:
            request.meta['handle_httpstatus_all'] = True
            dfd = self.crawler.engine.download(request, info.spider)
            dfd.addCallbacks(
                callback=self.media_downloaded, callbackArgs=(request, info),
                errback=self.media_failed, errbackArgs=(request, info))
        return dfd
    def _cache_result_and_execute_waiters(self, result, fp, info):
        if isinstance(result, Failure):
            # minimize cached information for failure
            result.cleanFailure()
            result.frames = []
            result.stack = None
        info.downloading.remove(fp)
        info.downloaded[fp] = result  # cache result
        for wad in info.waiting.pop(fp):
            defer_result(result).chainDeferred(wad)
    ### Overridable Interface
    def media_to_download(self, request, info):
        """"""Check request before starting download""""""
        pass
    def get_media_requests(self, item, info):
        """"""Returns the media requests to download""""""
        pass
    def media_downloaded(self, response, request, info):
        """"""Handler for success downloads""""""
        return response
    def media_failed(self, failure, request, info):
        """"""Handler for failed downloads""""""
        return failure
    def item_completed(self, results, item, info):
        """"""Called per item when all media requests has been processed""""""
        if self.LOG_FAILED_RESULTS:
            for ok, value in results:
                if not ok:
                    logger.error(
                        '%(class)s found errors processing %(item)s',
                        {'class': self.__class__.__name__, 'item': item},
                        extra={'spider': info.spider, 'failure': value}
                    )","[67, 124]"
"            A dataframe with frequency and counts by category.
        """"""
        counts = self.value_counts(dropna=False)
        freqs = counts / float(counts.sum())
        from pandas.core.reshape.concat import concat
        result = concat([counts, freqs], axis=1)
        result.columns = [""counts"", ""freqs""]
        result.index.name = ""categories""
        return result
    @Substitution(klass=""Categorical"")
    @Appender(_extension_array_shared_docs[""repeat""])
    def repeat(self, repeats, axis=None):
        nv.validate_repeat(tuple(), dict(axis=axis))
        codes = self._codes.repeat(repeats)
        return self._constructor(values=codes, dtype=self.dtype, fastpath=True)
    # Implement the ExtensionArray interface
    @property
    def _can_hold_na(self):
        return True
    @classmethod
    def _concat_same_type(self, to_concat):
        from pandas.core.dtypes.concat import concat_categorical
        return concat_categorical(to_concat)
    def isin(self, values):
        """"""
        Check whether `values` are contained in Categorical.
        Return a boolean NumPy Array showing whether each element in
        the Categorical matches an element in the passed sequence of
        `values` exactly.
        Parameters
        ----------
        values : set or list-like
            The sequence of values to test. Passing in a single string will
            raise a ``TypeError``. Instead, turn a single string into a
            list of one element.
        Returns
        -------
        isin : numpy.ndarray (bool dtype)
        Raises
        ------
        TypeError
          * If `values` is not a set or list-like
        See Also
        --------
        pandas.Series.isin : Equivalent method on Series.
        Examples
        --------
        >>> s = pd.Categorical(['lama', 'cow', 'lama', 'beetle', 'lama',
        ...                'hippo'])
        >>> s.isin(['cow', 'lama'])
        array([ True,  True,  True, False,  True, False])
        Passing a single string as ``s.isin('lama')`` will raise an error. Use
        a list of one element instead:
        >>> s.isin(['lama'])
        array([ True, False,  True, False,  True, False])
        """"""
        if not is_list_like(values):
            values_type = type(values).__name__
            raise TypeError(
                ""only list-like objects are allowed to be passed ""
                f""to isin(), you passed a [{values_type}]""
            )
        values = sanitize_array(values, None, None)
        null_mask = np.asarray(isna(values))
        code_values = self.categories.get_indexer(values)
        code_values = code_values[null_mask | (code_values >= 0)]
        return algorithms.isin(self.codes, code_values)
    def replace(self, to_replace, value, inplace: bool = False):
        """"""
        Replaces all instances of one value with another
        Parameters
        ----------
        to_replace: object
            The value to be replaced
        value: object
            The value to replace it with
        inplace: bool
            Whether the operation is done in-place
        Returns
        -------
        None if inplace is True, otherwise the new Categorical after replacement

        Examples
        --------
        >>> s = pd.Categorical([1, 2, 1, 3])
        >>> s.replace(1, 3)
        [3, 3, 2, 3]
        Categories (2, int64): [2, 3]
        """"""
        inplace = validate_bool_kwarg(inplace, ""inplace"")
        cat = self if inplace else self.copy()
        if to_replace in cat.categories:
            if isna(value):
                cat.remove_categories(to_replace, inplace=True)
            else:
                categories = cat.categories.tolist()
                index = categories.index(to_replace)
                if value in cat.categories:
                    value_index = categories.index(value)
                    cat._codes[cat._codes == index] = value_index
                    cat.remove_categories(to_replace, inplace=True)
                else:
                    categories[index] = value
                    cat.rename_categories(categories, inplace=True)
        if not inplace:","[113, 114, 115, 116, 118, 119, 120, 122, 124]"
"    def set(self, locs, values, check=False):
        assert locs.tolist() == [0]
        self.values = values
    def putmask(
        self, mask, new, align=True, inplace=False, axis=0, transpose=False,
    ):
        """"""
        putmask the data to the block; we must be a single block and not
        generate other blocks
        return the resulting block
        Parameters
        ----------
        mask  : the condition to respect
        new : a ndarray/object
        align : boolean, perform alignment on other/cond, default is True
        inplace : perform inplace modification, default is False
        Returns
        -------
        a new block, the result of the putmask
        """"""
        inplace = validate_bool_kwarg(inplace, ""inplace"")
        # use block's copy logic.
        # .values may be an Index which does shallow copy by default
        new_values = self.values if inplace else self.copy().values
        if isinstance(new, np.ndarray) and len(new) == len(mask):
            new = new[mask]
        mask = _safe_reshape(mask, new_values.shape)
        new_values[mask] = new
        return [self.make_block(values=new_values)]
    def _get_unstack_items(self, unstacker, new_columns):
        """"""
        Get the placement, values, and mask for a Block unstack.
        This is shared between ObjectBlock and ExtensionBlock. They
        differ in that ObjectBlock passes the values, while ExtensionBlock
        passes the dummy ndarray of positions to be used by a take
        later.
        Parameters
        ----------
        unstacker : pandas.core.reshape.reshape._Unstacker
        new_columns : Index
            All columns of the unstacked BlockManager.
        Returns
        -------
        new_placement : ndarray[int]
            The placement of the new columns in `new_columns`.
        new_values : Union[ndarray, ExtensionArray]
            The first return value from _Unstacker.get_new_values.
        mask : ndarray[bool]
            The second return value from _Unstacker.get_new_values.
        """"""
        # shared with ExtensionBlock
        new_items = unstacker.get_new_columns()
        new_placement = new_columns.get_indexer(new_items)
        new_values, mask = unstacker.get_new_values()
        mask = mask.any(0)
        return new_placement, new_values, mask
    def _maybe_coerce_values(self, values):
        """"""
        Unbox to an extension array.
        This will unbox an ExtensionArray stored in an Index or Series.
        ExtensionArrays pass through. No dtype coercion is done.
        Parameters
        ----------
        values : Index, Series, ExtensionArray
        Returns
        -------
        ExtensionArray
        """"""
        return extract_array(values)
    @property
    def _holder(self):
        # For extension blocks, the holder is values-dependent.
        return type(self.values)
    @property
    def fill_value(self):
        # Used in reindex_indexer
        return self.values.dtype.na_value
    @property
    def _can_hold_na(self):
        # The default ExtensionArray._can_hold_na is True
        return self._holder._can_hold_na
    @property
    def is_view(self) -> bool:
        """"""Extension arrays are never treated as views.""""""
        return False
    @property
    def is_numeric(self):
        return self.values.dtype._is_numeric
    def setitem(self, indexer, value):
        """"""
        Set the value inplace, returning a same-typed block.
        This differs from Block.setitem by not allowing setitem to change
        the dtype of the Block.
        Parameters
        ----------
        indexer : tuple, list-like, array-like, slice
            The subset of self.values to set
        value : object
            The value being set
        Returns","[1, 3, 114]"
"    na_mask = isna(x) | (ids == len(bins)) | (ids == 0)
    has_nas = na_mask.any()
    if labels is not False:
        if labels is None:
            labels = _format_labels(
                bins, precision, right=right, include_lowest=include_lowest, dtype=dtype
            )
        else:
            if len(labels) != len(bins) - 1:
                raise ValueError(
                    ""Bin labels must be one fewer than the number of bin edges""
                )
        if not is_categorical_dtype(labels):
            labels = Categorical(labels, categories=labels, ordered=True)
        np.putmask(ids, na_mask, 0)
        result = algos.take_nd(labels, ids - 1)
    else:
        result = ids - 1
        if has_nas:
            result = result.astype(np.float64)
            np.putmask(result, na_mask, np.nan)
    return result, bins

def _coerce_to_type(x):
    """"""
    if the passed data is of datetime/timedelta type,
    this method converts it to numeric so that cut method can
    handle it
    """"""
    dtype = None
    if is_datetime64tz_dtype(x):
        dtype = x.dtype
    elif is_datetime64_dtype(x):
        x = to_datetime(x)
        dtype = np.dtype(""datetime64[ns]"")
    elif is_timedelta64_dtype(x):
        x = to_timedelta(x)
        dtype = np.dtype(""timedelta64[ns]"")
    if dtype is not None:
        # GH 19768: force NaT to NaN during integer conversion
        x = np.where(x.notna(), x.view(np.int64), np.nan)
    return x, dtype

def _convert_bin_to_numeric_type(bins, dtype):
    """"""
    if the passed bin is of datetime/timedelta type,
    this method converts it to integer
    Parameters
    ----------
    bins : list-like of bins
    dtype : dtype of data
    Raises
    ------
    ValueError if bins are not of a compat dtype to dtype
    """"""
    bins_dtype = infer_dtype(bins, skipna=False)
    if is_timedelta64_dtype(dtype):
        if bins_dtype in [""timedelta"", ""timedelta64""]:
            bins = to_timedelta(bins).view(np.int64)
        else:
            raise ValueError(""bins must be of timedelta64 dtype"")
    elif is_datetime64_dtype(dtype) or is_datetime64tz_dtype(dtype):
        if bins_dtype in [""datetime"", ""datetime64""]:
            bins = to_datetime(bins).view(np.int64)
        else:
            raise ValueError(""bins must be of datetime64 dtype"")
    return bins

def _convert_bin_to_datelike_type(bins, dtype):
    """"""
    Convert bins to a DatetimeIndex or TimedeltaIndex if the original dtype is
    datelike
    Parameters
    ----------
    bins : list-like of bins
    dtype : dtype of data
    Returns
    -------
    bins : Array-like of bins, DatetimeIndex or TimedeltaIndex if dtype is
           datelike
    """"""
    if is_datetime64tz_dtype(dtype):
        bins = to_datetime(bins.astype(np.int64), utc=True).tz_convert(dtype.tz)
    elif is_datetime_or_timedelta_dtype(dtype):
        bins = Index(bins.astype(np.int64), dtype=dtype)
    return bins

def _format_labels(bins, precision, right=True, include_lowest=False, dtype=None):
    """""" based on the dtype, return our labels """"""
    closed = ""right"" if right else ""left""
    if is_datetime64tz_dtype(dtype):
        formatter = partial(Timestamp, tz=dtype.tz)
        adjust = lambda x: x - Timedelta(""1ns"")
    elif is_datetime64_dtype(dtype):
        formatter = Timestamp
        adjust = lambda x: x - Timedelta(""1ns"")
    elif is_timedelta64_dtype(dtype):
        formatter = Timedelta
        adjust = lambda x: x - Timedelta(""1ns"")
    else:
        precision = _infer_precision(precision, bins)
        formatter = lambda x: _round_frac(x, precision)
        adjust = lambda x: x - 10 ** (-precision)
    breaks = [formatter(b) for b in bins]
    labels = IntervalIndex.from_breaks(breaks, closed=closed)
    if right and include_lowest:
        # we will adjust the left hand side by precision to","[30, 31]"
"    def values(self):
        # Note: PeriodArray overrides this to return an ndarray of objects.
        return self._data._data
    def __array_wrap__(self, result, context=None):
        """"""
        Gets called after a ufunc.
        """"""
        result = lib.item_from_zerodim(result)
        if is_bool_dtype(result) or lib.is_scalar(result):
            return result
        attrs = self._get_attributes_dict()
        if not is_period_dtype(self) and attrs[""freq""]:
            # no need to infer if freq is None
            attrs[""freq""] = ""infer""
        return Index(result, **attrs)
    # ------------------------------------------------------------------------
    def equals(self, other) -> bool:
        """"""
        Determines if two Index objects contain the same elements.
        """"""
        if self.is_(other):
            return True
        if not isinstance(other, ABCIndexClass):
            return False
        elif not isinstance(other, type(self)):
            try:
                other = type(self)(other)
            except (ValueError, TypeError, OverflowError):
                # e.g.
                #  ValueError -> cannot parse str entry, or OutOfBoundsDatetime
                #  TypeError  -> trying to convert IntervalIndex to DatetimeIndex
                #  OverflowError -> Index([very_large_timedeltas])
                return False
        if not is_dtype_equal(self.dtype, other.dtype):
            # have different timezone
            return False
        return np.array_equal(self.asi8, other.asi8)
    def _ensure_localized(
        self, arg, ambiguous=""raise"", nonexistent=""raise"", from_utc=False
    ):
        # See DatetimeLikeArrayMixin._ensure_localized.__doc__
        if getattr(self, ""tz"", None):
            # ensure_localized is only relevant for tz-aware DTI
            result = self._data._ensure_localized(
                arg, ambiguous=ambiguous, nonexistent=nonexistent, from_utc=from_utc
            )
            return type(self)._simple_new(result, name=self.name)
        return arg
    @Appender(_index_shared_docs[""contains""] % _index_doc_kwargs)
    def __contains__(self, key):
        try:
            res = self.get_loc(key)
            return (
                is_scalar(res)
                or isinstance(res, slice)
                or (is_list_like(res) and len(res))
            )
        except (KeyError, TypeError, ValueError):
            return False
    # Try to run function on index first, and then on elements of index
    # Especially important for group-by functionality
    def map(self, mapper, na_action=None):
        try:
            result = mapper(self)
            # Try to use this result if we can
            if isinstance(result, np.ndarray):
                result = Index(result)
            if not isinstance(result, Index):
                raise TypeError(""The map function must return an Index object"")
            return result
        except Exception:
            return self.astype(object).map(mapper)
    def sort_values(self, return_indexer=False, ascending=True):
        """"""
        Return sorted copy of Index.
        """"""
        if return_indexer:
            _as = self.argsort()
            if not ascending:
                _as = _as[::-1]
            sorted_index = self.take(_as)
            return sorted_index, _as
        else:
            # NB: using asi8 instead of _ndarray_values matters in numpy 1.18
            #  because the treatment of NaT has been changed to put NaT last
            #  instead of first.
            sorted_values = np.sort(self.asi8)
            attribs = self._get_attributes_dict()
            freq = attribs[""freq""]
            if freq is not None and not is_period_dtype(self):
                if freq.n > 0 and not ascending:
                    freq = freq * -1
                elif freq.n < 0 and ascending:
                    freq = freq * -1
            attribs[""freq""] = freq
            if not ascending:
                sorted_values = sorted_values[::-1]
            return self._simple_new(sorted_values, **attribs)
    @Appender(_index_shared_docs[""take""] % _index_doc_kwargs)
    def take(self, indices, axis=0, allow_fill=True, fill_value=None, **kwargs):
        nv.validate_take(tuple(), kwargs)
        indices = ensure_int64(indices)
        maybe_slice = lib.maybe_indices_to_slice(indices, len(self))
        if isinstance(maybe_slice, slice):
            return self[maybe_slice]
        return ExtensionIndex.take(
            self, indices, axis, allow_fill, fill_value, **kwargs
        )","[45, 46, 47, 49, 51, 52, 53, 54, 55, 56]"
"        # Addresses GH #10530
        if self.base > 0:
            labels += type(self.freq)(self.base)
        return binner, bins, labels
    def _get_time_period_bins(self, ax):
        if not isinstance(ax, DatetimeIndex):
            raise TypeError(
                ""axis must be a DatetimeIndex, but got ""
                f""an instance of {type(ax).__name__}""
            )
        freq = self.freq
        if not len(ax):
            binner = labels = PeriodIndex(data=[], freq=freq, name=ax.name)
            return binner, [], labels
        labels = binner = period_range(start=ax[0], end=ax[-1], freq=freq, name=ax.name)
        end_stamps = (labels + freq).asfreq(freq, ""s"").to_timestamp()
        if ax.tzinfo:
            end_stamps = end_stamps.tz_localize(ax.tzinfo)
        bins = ax.searchsorted(end_stamps, side=""left"")
        return binner, bins, labels
    def _get_period_bins(self, ax):
        if not isinstance(ax, PeriodIndex):
            raise TypeError(
                ""axis must be a PeriodIndex, but got ""
                f""an instance of {type(ax).__name__}""
            )
        memb = ax.asfreq(self.freq, how=self.convention)
        # NaT handling as in pandas._lib.lib.generate_bins_dt64()
        nat_count = 0
        if memb.hasnans:
            nat_count = np.sum(memb._isnan)
            memb = memb[~memb._isnan]
        # if index contains no valid (non-NaT) values, return empty index
        if not len(memb):
            binner = labels = PeriodIndex(data=[], freq=self.freq, name=ax.name)
            return binner, [], labels
        freq_mult = self.freq.n
        start = ax.min().asfreq(self.freq, how=self.convention)
        end = ax.max().asfreq(self.freq, how=""end"")
        bin_shift = 0
        # GH 23882
        if self.base:
            # get base adjusted bin edge labels
            p_start, end = _get_period_range_edges(
                start, end, self.freq, closed=self.closed, base=self.base
            )
            # Get offset for bin edge (not label edge) adjustment
            start_offset = Period(start, self.freq) - Period(p_start, self.freq)
            bin_shift = start_offset.n % freq_mult
            start = p_start
        labels = binner = period_range(
            start=start, end=end, freq=self.freq, name=ax.name
        )
        i8 = memb.asi8
        # when upsampling to subperiods, we need to generate enough bins
        expected_bins_count = len(binner) * freq_mult
        i8_extend = expected_bins_count - (i8[-1] - i8[0])
        rng = np.arange(i8[0], i8[-1] + i8_extend, freq_mult)
        rng += freq_mult
        # adjust bin edge indexes to account for base
        rng -= bin_shift
        bins = memb.searchsorted(rng, side=""left"")
        if nat_count > 0:
            # NaT handling as in pandas._lib.lib.generate_bins_dt64()
            # shift bins by the number of NaT
            bins += nat_count
            bins = np.insert(bins, 0, nat_count)
            binner = binner.insert(0, NaT)
            labels = labels.insert(0, NaT)
        return binner, bins, labels

def _take_new_index(obj, indexer, new_index, axis=0):
    if isinstance(obj, ABCSeries):
        new_values = algos.take_1d(obj.values, indexer)
        return obj._constructor(new_values, index=new_index, name=obj.name)
    elif isinstance(obj, ABCDataFrame):
        if axis == 1:
            raise NotImplementedError(""axis 1 is not supported"")
        return obj._constructor(
            obj._data.reindex_indexer(new_axis=new_index, indexer=indexer, axis=1)
        )
    else:
        raise ValueError(""'obj' should be either a Series or a DataFrame"")

def _get_timestamp_range_edges(first, last, offset, closed=""left"", base=0):
    """"""
    Adjust the `first` Timestamp to the preceding Timestamp that resides on
    the provided offset. Adjust the `last` Timestamp to the following
    Timestamp that resides on the provided offset. Input Timestamps that
    already reside on the offset will be adjusted depending on the type of
    offset and the `closed` parameter.
    Parameters
    ----------
    first : pd.Timestamp
        The beginning Timestamp of the range to be adjusted.
    last : pd.Timestamp
        The ending Timestamp of the range to be adjusted.
    offset : pd.DateOffset
        The dateoffset to which the Timestamps will be adjusted.
    closed : {'right', 'left'}, default None
        Which side of bin interval is closed.
    base : int, default 0
        The ""origin"" of the adjusted Timestamps.",[79]
"# Copyright (c) 2014, Chris Church <chris@ninemoreminutes.com>
# Copyright (c) 2017 Ansible Project
# GNU General Public License v3.0+ (see COPYING or https://www.gnu.org/licenses/gpl-3.0.txt)
from __future__ import (absolute_import, division, print_function)
__metaclass__ = type
DOCUMENTATION = '''
name: powershell
plugin_type: shell
version_added: historical
short_description: Windows PowerShell
description:
- The only option when using 'winrm' or 'psrp' as a connection plugin.
- Can also be used when using 'ssh' as a connection plugin and the C(DefaultShell) has been configured to PowerShell.
extends_documentation_fragment:
- shell_windows
'''
import base64
import os
import re
import shlex
import pkgutil
import xml.etree.ElementTree as ET
from ansible.errors import AnsibleError
from ansible.module_utils._text import to_bytes, to_text
from ansible.plugins.shell import ShellBase

_common_args = ['PowerShell', '-NoProfile', '-NonInteractive', '-ExecutionPolicy', 'Unrestricted']
# Primarily for testing, allow explicitly specifying PowerShell version via
# an environment variable.
_powershell_version = os.environ.get('POWERSHELL_VERSION', None)
if _powershell_version:
    _common_args = ['PowerShell', '-Version', _powershell_version] + _common_args[1:]

def _parse_clixml(data, stream=""Error""):
    """"""
    Takes a byte string like '#< CLIXML\r\n<Objs...' and extracts the stream
    message encoded in the XML data. CLIXML is used by PowerShell to encode
    multiple objects in stderr.
    """"""
    clixml = ET.fromstring(data.split(b""\r\n"", 1)[-1])
    namespace_match = re.match(r'{(.*)}', clixml.tag)
    namespace = ""{%s}"" % namespace_match.group(1) if namespace_match else """"
    strings = clixml.findall(""./%sS"" % namespace)
    lines = [e.text.replace('_x000D__x000A_', '') for e in strings if e.attrib.get('S') == stream]
    return to_bytes('\r\n'.join(lines))

class ShellModule(ShellBase):
    # Common shell filenames that this plugin handles
    # Powershell is handled differently.  It's selected when winrm is the
    # connection
    COMPATIBLE_SHELLS = frozenset()
    # Family of shells this has.  Must match the filename without extension
    SHELL_FAMILY = 'powershell'
    _SHELL_REDIRECT_ALLNULL = '> $null'
    _SHELL_AND = ';'
    # Used by various parts of Ansible to do Windows specific changes
    _IS_WINDOWS = True
    env = dict()
    # We're being overly cautious about which keys to accept (more so than
    # the Windows environment is capable of doing), since the powershell
    # env provider's limitations don't appear to be documented.
    safe_envkey = re.compile(r'^[\d\w_]{1,255}$')
    # TODO: add binary module support
    def assert_safe_env_key(self, key):
        if not self.safe_envkey.match(key):
            raise AnsibleError(""Invalid PowerShell environment key: %s"" % key)
        return key
    def safe_env_value(self, key, value):
        if len(value) > 32767:
            raise AnsibleError(""PowerShell environment value for key '%s' exceeds 32767 characters in length"" % key)
        # powershell single quoted literals need single-quote doubling as their only escaping
        value = value.replace(""'"", ""''"")
        return to_text(value, errors='surrogate_or_strict')
    def env_prefix(self, **kwargs):
        # powershell/winrm env handling is handled in the exec wrapper
        return """"
    def join_path(self, *args):
        parts = []
        for arg in args:
            arg = self._unquote(arg).replace('/', '\\')
            parts.extend([a for a in arg.split('\\') if a])
        path = '\\'.join(parts)
        if path.startswith('~'):
            return path
        return path
    def get_remote_filename(self, pathname):
        # powershell requires that script files end with .ps1
        base_name = os.path.basename(pathname.strip())
        name, ext = os.path.splitext(base_name.strip())
        if ext.lower() not in ['.ps1', '.exe']:
            return name + '.ps1'
        return base_name.strip()
    def path_has_trailing_slash(self, path):
        # Allow Windows paths to be specified using either slash.
        path = self._unquote(path)
        return path.endswith('/') or path.endswith('\\')
    def chmod(self, paths, mode):
        raise NotImplementedError('chmod is not implemented for Powershell')
    def chown(self, paths, user):
        raise NotImplementedError('chown is not implemented for Powershell')
    def set_user_facl(self, paths, user, mode):
        raise NotImplementedError('set_user_facl is not implemented for Powershell')
","[95, 96, 97, 98, 99, 100, 101, 102]"
"    def __new__(cls, force_instance: bool = False, **kwargs: Any) -> ""AsyncHTTPClient"":
        io_loop = IOLoop.current()
        if force_instance:
            instance_cache = None
        else:
            instance_cache = cls._async_clients()
        if instance_cache is not None and io_loop in instance_cache:
            return instance_cache[io_loop]
        instance = super(AsyncHTTPClient, cls).__new__(cls, **kwargs)  # type: ignore
        # Make sure the instance knows which cache to remove itself from.
        # It can't simply call _async_clients() because we may be in
        # __new__(AsyncHTTPClient) but instance.__class__ may be
        # SimpleAsyncHTTPClient.
        instance._instance_cache = instance_cache
        if instance_cache is not None:
            instance_cache[instance.io_loop] = instance
        return instance
    def initialize(self, defaults: Dict[str, Any] = None) -> None:
        self.io_loop = IOLoop.current()
        self.defaults = dict(HTTPRequest._DEFAULTS)
        if defaults is not None:
            self.defaults.update(defaults)
        self._closed = False
    def close(self) -> None:
        """"""Destroys this HTTP client, freeing any file descriptors used.
        This method is **not needed in normal use** due to the way
        that `AsyncHTTPClient` objects are transparently reused.
        ``close()`` is generally only necessary when either the
        `.IOLoop` is also being closed, or the ``force_instance=True``
        argument was used when creating the `AsyncHTTPClient`.
        No other methods may be called on the `AsyncHTTPClient` after
        ``close()``.
        """"""
        if self._closed:
            return
        self._closed = True
        if self._instance_cache is not None:
            if self._instance_cache.get(self.io_loop) is not self:
                raise RuntimeError(""inconsistent AsyncHTTPClient cache"")
            del self._instance_cache[self.io_loop]
    def fetch(
        self,
        request: Union[str, ""HTTPRequest""],
        raise_error: bool = True,
        **kwargs: Any
    ) -> ""Future[HTTPResponse]"":
        """"""Executes a request, asynchronously returning an `HTTPResponse`.
        The request may be either a string URL or an `HTTPRequest` object.
        If it is a string, we construct an `HTTPRequest` using any additional
        kwargs: ``HTTPRequest(request, **kwargs)``
        This method returns a `.Future` whose result is an
        `HTTPResponse`. By default, the ``Future`` will raise an
        `HTTPError` if the request returned a non-200 response code
        (other errors may also be raised if the server could not be
        contacted). Instead, if ``raise_error`` is set to False, the
        response will always be returned regardless of the response
        code.
        If a ``callback`` is given, it will be invoked with the `HTTPResponse`.
        In the callback interface, `HTTPError` is not automatically raised.
        Instead, you must check the response's ``error`` attribute or
        call its `~HTTPResponse.rethrow` method.
        .. versionchanged:: 6.0
           The ``callback`` argument was removed. Use the returned
           `.Future` instead.
           The ``raise_error=False`` argument only affects the
           `HTTPError` raised when a non-200 response code is used,
           instead of suppressing all errors.
        """"""
        if self._closed:
            raise RuntimeError(""fetch() called on closed AsyncHTTPClient"")
        if not isinstance(request, HTTPRequest):
            request = HTTPRequest(url=request, **kwargs)
        else:
            if kwargs:
                raise ValueError(
                    ""kwargs can't be used if request is an HTTPRequest object""
                )
        # We may modify this (to add Host, Accept-Encoding, etc),
        # so make sure we don't modify the caller's object.  This is also
        # where normal dicts get converted to HTTPHeaders objects.
        request.headers = httputil.HTTPHeaders(request.headers)
        request_proxy = _RequestProxy(request, self.defaults)
        future = Future()  # type: Future[HTTPResponse]
        def handle_response(response: ""HTTPResponse"") -> None:
            if response.error:
                if raise_error or not response._error_is_response_code:
                    future.set_exception(response.error)
                    return
            future_set_result_unless_cancelled(future, response)
        self.fetch_impl(cast(HTTPRequest, request_proxy), handle_response)
        return future
    def fetch_impl(
        self, request: ""HTTPRequest"", callback: Callable[[""HTTPResponse""], None]
    ) -> None:
        raise NotImplementedError()
    @classmethod
    def configure(
        cls, impl: Union[None, str, Type[Configurable]], **kwargs: Any
    ) -> None:
        """"""Configures the `AsyncHTTPClient` subclass to use.
        ``AsyncHTTPClient()`` actually creates an instance of a subclass.
        This method may be called with either a class object or the
        fully-qualified name of such a class (or ``None`` to use the default,
        ``SimpleAsyncHTTPClient``)
        If additional keyword arguments are given, they will be passed
        to the constructor of each subclass instance created.  The
        keyword argument ``max_clients`` determines the maximum number
        of simultaneous `~AsyncHTTPClient.fetch()` operations that can
        execute in parallel on each `.IOLoop`.  Additional arguments","[42, 44]"
"                UserWarning,
                stacklevel=3,
            )
        elif len(key) != len(self.index):
            raise ValueError(
                f""Item wrong length {len(key)} instead of {len(self.index)}.""
            )
        # check_bool_indexer will throw exception if Series key cannot
        # be reindexed to match DataFrame rows
        key = check_bool_indexer(self.index, key)
        indexer = key.nonzero()[0]
        return self._take_with_is_copy(indexer, axis=0)
    def _getitem_multilevel(self, key):
        # self.columns is a MultiIndex
        loc = self.columns.get_loc(key)
        if isinstance(loc, (slice, Series, np.ndarray, Index)):
            new_columns = self.columns[loc]
            result_columns = maybe_droplevels(new_columns, key)
            if self._is_mixed_type:
                result = self.reindex(columns=new_columns)
                result.columns = result_columns
            else:
                new_values = self.values[:, loc]
                result = self._constructor(
                    new_values, index=self.index, columns=result_columns
                )
                result = result.__finalize__(self)
            # If there is only one column being returned, and its name is
            # either an empty string, or a tuple with an empty string as its
            # first element, then treat the empty string as a placeholder
            # and return the column as if the user had provided that empty
            # string in the key. If the result is a Series, exclude the
            # implied empty string from its name.
            if len(result.columns) == 1:
                top = result.columns[0]
                if isinstance(top, tuple):
                    top = top[0]
                if top == """":
                    result = result[""""]
                    if isinstance(result, Series):
                        result = self._constructor_sliced(
                            result, index=self.index, name=key
                        )
            result._set_is_copy(self)
            return result
        else:
            return self._get_item_cache(key)
    def _get_value(self, index, col, takeable: bool = False):
        """"""
        Quickly retrieve single value at passed column and index.
        Parameters
        ----------
        index : row label
        col : column label
        takeable : interpret the index/col as indexers, default False
        Returns
        -------
        scalar
        """"""
        if takeable:
            series = self._iget_item_cache(col)
            return com.maybe_box_datetimelike(series._values[index])
        series = self._get_item_cache(col)
        engine = self.index._engine
        try:
            loc = engine.get_loc(index)
            return series._values[loc]
        except KeyError:
            # GH 20629
            if self.index.nlevels > 1:
                # partial indexing forbidden
                raise
        # we cannot handle direct indexing
        # use positional
        col = self.columns.get_loc(col)
        index = self.index.get_loc(index)
        return self._get_value(index, col, takeable=True)
    def __setitem__(self, key, value):
        key = com.apply_if_callable(key, self)
        # see if we can slice the rows
        indexer = convert_to_index_sliceable(self, key)
        if indexer is not None:
            # either we have a slice or we have a string that can be converted
            #  to a slice for partial-string date indexing
            return self._setitem_slice(indexer, value)
        if isinstance(key, DataFrame) or getattr(key, ""ndim"", None) == 2:
            self._setitem_frame(key, value)
        elif isinstance(key, (Series, np.ndarray, list, Index)):
            self._setitem_array(key, value)
        else:
            # set column
            self._set_item(key, value)
    def _setitem_slice(self, key: slice, value):
        # NB: we can't just use self.loc[key] = value because that
        #  operates on labels and we need to operate positional for
        #  backwards-compat, xref GH#31469
        self._check_setitem_copy()
        self.iloc._setitem_with_indexer(key, value)
    def _setitem_array(self, key, value):
        # also raises Exception if object array with NA values
        if com.is_bool_indexer(key):
            if len(key) != len(self.index):
                raise ValueError(
                    f""Item wrong length {len(key)} instead of {len(self.index)}!""
                )
            key = check_bool_indexer(self.index, key)
            indexer = key.nonzero()[0]
            self._check_setitem_copy()
            self.iloc._setitem_with_indexer(indexer, value)
        else:
            if isinstance(value, DataFrame):
                if len(value.columns) != len(key):","[67, 68]"
"            self._reset_orphaned_batch_running_tasks(worker_id)
        locally_pending_tasks = 0
        running_tasks = []
        upstream_table = {}
        greedy_resources = collections.defaultdict(int)
        n_unique_pending = 0
        worker = self._state.get_worker(worker_id)
        if worker.is_trivial_worker(self._state):
            relevant_tasks = worker.get_pending_tasks(self._state)
            used_resources = collections.defaultdict(int)
            greedy_workers = dict()  # If there's no resources, then they can grab any task
        else:
            relevant_tasks = self._state.get_pending_tasks()
            used_resources = self._used_resources()
            activity_limit = time.time() - self._config.worker_disconnect_delay
            active_workers = self._state.get_active_workers(last_get_work_gt=activity_limit)
            greedy_workers = dict((worker.id, worker.info.get('workers', 1))
                                  for worker in active_workers)
        tasks = list(relevant_tasks)
        tasks.sort(key=self._rank, reverse=True)
        for task in tasks:
            in_workers = (assistant and getattr(task, 'runnable', bool(task.workers))) or worker_id in task.workers
            if task.status == RUNNING and in_workers:
                # Return a list of currently running tasks to the client,
                # makes it easier to troubleshoot
                other_worker = self._state.get_worker(task.worker_running)
                more_info = {'task_id': task.id, 'worker': str(other_worker)}
                if other_worker is not None:
                    more_info.update(other_worker.info)
                    running_tasks.append(more_info)
            if task.status == PENDING and in_workers:
                upstream_status = self._upstream_status(task.id, upstream_table)
                if upstream_status != UPSTREAM_DISABLED:
                    locally_pending_tasks += 1
                    if len(task.workers) == 1 and not assistant:
                        n_unique_pending += 1
            if (best_task and batched_params and task.family == best_task.family and
                    len(batched_tasks) < max_batch_size and task.is_batchable() and all(
                    task.params.get(name) == value for name, value in unbatched_params.items())):
                for name, params in batched_params.items():
                    params.append(task.params.get(name))
                batched_tasks.append(task)
            if best_task:
                continue
            if task.status == RUNNING and (task.worker_running in greedy_workers):
                greedy_workers[task.worker_running] -= 1
                for resource, amount in six.iteritems((task.resources or {})):
                    greedy_resources[resource] += amount
            if self._schedulable(task) and self._has_resources(task.resources, greedy_resources):
                if in_workers and self._has_resources(task.resources, used_resources):
                    best_task = task
                    batch_param_names, max_batch_size = self._state.get_batcher(
                        worker_id, task.family)
                    if batch_param_names and task.is_batchable():
                        try:
                            batched_params = {
                                name: [task.params[name]] for name in batch_param_names
                            }
                            unbatched_params = {
                                name: value for name, value in task.params.items()
                                if name not in batched_params
                            }
                            batched_tasks.append(task)
                        except KeyError:
                            batched_params, unbatched_params = None, None
                else:
                    workers = itertools.chain(task.workers, [worker_id]) if assistant else task.workers
                    for task_worker in workers:
                        if greedy_workers.get(task_worker, 0) > 0:
                            # use up a worker
                            greedy_workers[task_worker] -= 1
                            # keep track of the resources used in greedy scheduling
                            for resource, amount in six.iteritems((task.resources or {})):
                                greedy_resources[resource] += amount
                            break
        reply = {'n_pending_tasks': locally_pending_tasks,
                 'running_tasks': running_tasks,
                 'task_id': None,
                 'n_unique_pending': n_unique_pending}
        if len(batched_tasks) > 1:
            batch_string = '|'.join(task.id for task in batched_tasks)
            batch_id = hashlib.md5(batch_string.encode('utf-8')).hexdigest()
            for task in batched_tasks:
                self._state.set_batch_running(task, batch_id, worker_id)
            combined_params = best_task.params.copy()
            combined_params.update(batched_params)
            reply['task_id'] = None
            reply['task_family'] = best_task.family
            reply['task_module'] = getattr(best_task, 'module', None)
            reply['task_params'] = combined_params
            reply['batch_id'] = batch_id
            reply['batch_task_ids'] = [task.id for task in batched_tasks]
        elif best_task:
            self._state.set_status(best_task, RUNNING, self._config)
            best_task.worker_running = worker_id
            best_task.time_running = time.time()
            self._update_task_history(best_task, RUNNING, host=host)
            reply['task_id'] = best_task.id
            reply['task_family'] = best_task.family
            reply['task_module'] = getattr(best_task, 'module', None)
            reply['task_params'] = best_task.params
        return reply
    @rpc_method(attempts=1)
    def ping(self, **kwargs):
        worker_id = kwargs['worker']
        self.update(worker_id)
    def _upstream_status(self, task_id, upstream_status_table):
        if task_id in upstream_status_table:",[44]
"    def _parsed_string_to_bounds(self, reso: str, parsed: datetime):
        raise NotImplementedError
    def _partial_date_slice(
        self, reso: str, parsed: datetime, use_lhs: bool = True, use_rhs: bool = True
    ):
        """"""
        Parameters
        ----------
        reso : str
        parsed : datetime
        use_lhs : bool, default True
        use_rhs : bool, default True
        Returns
        -------
        slice or ndarray[intp]
        """"""
        self._validate_partial_date_slice(reso)
        t1, t2 = self._parsed_string_to_bounds(reso, parsed)
        i8vals = self.asi8
        unbox = self._data._unbox_scalar
        if self.is_monotonic:
            if len(self) and (
                (use_lhs and t1 < self[0] and t2 < self[0])
                or ((use_rhs and t1 > self[-1] and t2 > self[-1]))
            ):
                # we are out of range
                raise KeyError
            # TODO: does this depend on being monotonic _increasing_?
            # a monotonic (sorted) series can be sliced
            # Use asi8.searchsorted to avoid re-validating Periods/Timestamps
            left = i8vals.searchsorted(unbox(t1), side=""left"") if use_lhs else None
            right = i8vals.searchsorted(unbox(t2), side=""right"") if use_rhs else None
            return slice(left, right)
        else:
            lhs_mask = (i8vals >= unbox(t1)) if use_lhs else True
            rhs_mask = (i8vals <= unbox(t2)) if use_rhs else True
            # try to find the dates
            return (lhs_mask & rhs_mask).nonzero()[0]
    # --------------------------------------------------------------------
    # Arithmetic Methods
    def _get_addsub_freq(self, other) -> Optional[DateOffset]:
        """"""
        Find the freq we expect the result of an addition/subtraction operation
        to have.
        """"""
        if is_period_dtype(self.dtype):
            # Only used for ops that stay PeriodDtype
            return self.freq
        elif self.freq is None:
            return None
        elif lib.is_scalar(other) and isna(other):
            return None
        elif isinstance(other, (Tick, timedelta, np.timedelta64)):
            new_freq = None
            if isinstance(self.freq, Tick):
                new_freq = self.freq
            return new_freq
        elif isinstance(other, DateOffset):
            # otherwise just DatetimeArray
            return None  # TODO: Should we infer if it matches self.freq * n?
        elif isinstance(other, (datetime, np.datetime64)):
            return self.freq
        elif is_timedelta64_dtype(other):
            return None  # TODO: shouldnt we be able to do self.freq + other.freq?
        elif is_object_dtype(other):
            return None  # TODO: is this quite right?  sometimes we unpack singletons
        elif is_datetime64_any_dtype(other):
            return None  # TODO: shouldnt we be able to do self.freq + other.freq?
        else:
            raise NotImplementedError
    __add__ = _make_wrapped_arith_op_with_freq(""__add__"")
    __sub__ = _make_wrapped_arith_op_with_freq(""__sub__"")
    __radd__ = make_wrapped_arith_op(""__radd__"")
    __rsub__ = make_wrapped_arith_op(""__rsub__"")
    __pow__ = make_wrapped_arith_op(""__pow__"")
    __rpow__ = make_wrapped_arith_op(""__rpow__"")
    __mul__ = make_wrapped_arith_op(""__mul__"")
    __rmul__ = make_wrapped_arith_op(""__rmul__"")
    __floordiv__ = make_wrapped_arith_op(""__floordiv__"")
    __rfloordiv__ = make_wrapped_arith_op(""__rfloordiv__"")
    __mod__ = make_wrapped_arith_op(""__mod__"")
    __rmod__ = make_wrapped_arith_op(""__rmod__"")
    __divmod__ = make_wrapped_arith_op(""__divmod__"")
    __rdivmod__ = make_wrapped_arith_op(""__rdivmod__"")
    __truediv__ = make_wrapped_arith_op(""__truediv__"")
    __rtruediv__ = make_wrapped_arith_op(""__rtruediv__"")
    def isin(self, values, level=None):
        """"""
        Compute boolean array of whether each index value is found in the
        passed set of values.
        Parameters
        ----------
        values : set or sequence of values
        Returns
        -------
        is_contained : ndarray (boolean dtype)
        """"""
        if level is not None:
            self._validate_index_level(level)
        if not isinstance(values, type(self)):
            try:
                values = type(self)(values)
            except ValueError:
                return self.astype(object).isin(values)
        return algorithms.isin(self.asi8, values.asi8)
    @Appender(Index.where.__doc__)","[51, 58]"
"        >>> df.nsmallest(3, 'population', keep='all')
                  population  GDP alpha-2
        Nauru          11300  182      NR
        Tuvalu         11300   38      TV
        Anguilla       11300  311      AI
        To order by the largest values in column ""a"" and then ""c"", we can
        specify multiple columns like in the next example.
        >>> df.nsmallest(3, ['population', 'GDP'])
                  population  GDP alpha-2
        Tuvalu         11300   38      TV
        Nauru          11300  182      NR
        Anguilla       11300  311      AI
        """"""
        return algorithms.SelectNFrame(
            self, n=n, keep=keep, columns=columns
        ).nsmallest()
    def swaplevel(self, i=-2, j=-1, axis=0) -> ""DataFrame"":
        """"""
        Swap levels i and j in a MultiIndex on a particular axis.
        Parameters
        ----------
        i, j : int or str
            Levels of the indices to be swapped. Can pass level name as string.
        Returns
        -------
        DataFrame
        """"""
        result = self.copy()
        axis = self._get_axis_number(axis)
        if not isinstance(result._get_axis(axis), ABCMultiIndex):  # pragma: no cover
            raise TypeError(""Can only swap levels on a hierarchical axis."")
        if axis == 0:
            assert isinstance(result.index, ABCMultiIndex)
            result.index = result.index.swaplevel(i, j)
        else:
            assert isinstance(result.columns, ABCMultiIndex)
            result.columns = result.columns.swaplevel(i, j)
        return result
    def reorder_levels(self, order, axis=0) -> ""DataFrame"":
        """"""
        Rearrange index levels using input order. May not drop or duplicate levels.
        Parameters
        ----------
        order : list of int or list of str
            List representing new level order. Reference level by number
            (position) or by key (label).
        axis : int
            Where to reorder levels.
        Returns
        -------
        DataFrame
        """"""
        axis = self._get_axis_number(axis)
        if not isinstance(self._get_axis(axis), ABCMultiIndex):  # pragma: no cover
            raise TypeError(""Can only reorder levels on a hierarchical axis."")
        result = self.copy()
        if axis == 0:
            assert isinstance(result.index, ABCMultiIndex)
            result.index = result.index.reorder_levels(order)
        else:
            assert isinstance(result.columns, ABCMultiIndex)
            result.columns = result.columns.reorder_levels(order)
        return result
    # ----------------------------------------------------------------------
    # Arithmetic / combination related
    def _combine_frame(self, other, func, fill_value=None, level=None):
        # at this point we have `self._indexed_same(other)`
        if fill_value is None:
            # since _arith_op may be called in a loop, avoid function call
            #  overhead if possible by doing this check once
            _arith_op = func
        else:
            def _arith_op(left, right):
                # for the mixed_type case where we iterate over columns,
                # _arith_op(left, right) is equivalent to
                # left._binop(right, func, fill_value=fill_value)
                left, right = ops.fill_binop(left, right, fill_value)
                return func(left, right)
        if ops.should_series_dispatch(self, other, func):
            # iterate over columns
            new_data = ops.dispatch_to_series(self, other, _arith_op)
        else:
            with np.errstate(all=""ignore""):
                res_values = _arith_op(self.values, other.values)
            new_data = dispatch_fill_zeros(func, self.values, other.values, res_values)
        return new_data
    def _combine_match_index(self, other, func):
        # at this point we have `self.index.equals(other.index)`
        if ops.should_series_dispatch(self, other, func):
            # operate column-wise; avoid costly object-casting in `.values`
            new_data = ops.dispatch_to_series(self, other, func)
        else:
            # fastpath --> operate directly on values
            with np.errstate(all=""ignore""):
                new_data = func(self.values.T, other.values).T
        return new_data
    def _construct_result(self, result) -> ""DataFrame"":
        """"""
        Wrap the result of an arithmetic, comparison, or logical operation.
        Parameters
        ----------
        result : DataFrame","[81, 108, 117]"
"    def rule_code(self) -> str:
        weekday = ccalendar.int_to_weekday.get(self.weekday, """")
        return f""{self._prefix}-{weekday}""
    @classmethod
    def _from_name(cls, suffix=None):
        if not suffix:
            raise ValueError(f""Prefix {repr(cls._prefix)} requires a suffix."")
        # TODO: handle n here...
        weekday = ccalendar.weekday_to_int[suffix]
        return cls(weekday=weekday)

# ---------------------------------------------------------------------
# Quarter-Based Offset Classes

class QuarterOffset(DateOffset):
    """"""
    Quarter representation - doesn't call super.
    """"""
    _default_startingMonth: Optional[int] = None
    _from_name_startingMonth: Optional[int] = None
    _adjust_dst = True
    _attributes = frozenset([""n"", ""normalize"", ""startingMonth""])
    # TODO: Consider combining QuarterOffset and YearOffset __init__ at some
    #       point.  Also apply_index, is_on_offset, rule_code if
    #       startingMonth vs month attr names are resolved
    def __init__(self, n=1, normalize=False, startingMonth=None):
        BaseOffset.__init__(self, n, normalize)
        if startingMonth is None:
            startingMonth = self._default_startingMonth
        object.__setattr__(self, ""startingMonth"", startingMonth)
    def is_anchored(self) -> bool:
        return self.n == 1 and self.startingMonth is not None
    @classmethod
    def _from_name(cls, suffix=None):
        kwargs = {}
        if suffix:
            kwargs[""startingMonth""] = ccalendar.MONTH_TO_CAL_NUM[suffix]
        else:
            if cls._from_name_startingMonth is not None:
                kwargs[""startingMonth""] = cls._from_name_startingMonth
        return cls(**kwargs)
    @property
    def rule_code(self) -> str:
        month = ccalendar.MONTH_ALIASES[self.startingMonth]
        return f""{self._prefix}-{month}""
    @apply_wraps
    def apply(self, other):
        # months_since: find the calendar quarter containing other.month,
        # e.g. if other.month == 8, the calendar quarter is [Jul, Aug, Sep].
        # Then find the month in that quarter containing an is_on_offset date for
        # self.  `months_since` is the number of months to shift other.month
        # to get to this on-offset month.
        months_since = other.month % 3 - self.startingMonth % 3
        qtrs = liboffsets.roll_qtrday(
            other, self.n, self.startingMonth, day_opt=self._day_opt, modby=3
        )
        months = qtrs * 3 - months_since
        return shift_month(other, months, self._day_opt)
    def is_on_offset(self, dt: datetime) -> bool:
        if self.normalize and not _is_normalized(dt):
            return False
        mod_month = (dt.month - self.startingMonth) % 3
        return mod_month == 0 and dt.day == self._get_offset_day(dt)
    @apply_index_wraps
    def apply_index(self, dtindex):
        shifted = liboffsets.shift_quarters(
            dtindex.asi8, self.n, self.startingMonth, self._day_opt
        )
        # TODO: going through __new__ raises on call to _validate_frequency;
        #  are we passing incorrect freq?
        return type(dtindex)._simple_new(
            shifted, freq=dtindex.freq, dtype=dtindex.dtype
        )

class BQuarterEnd(QuarterOffset):
    """"""
    DateOffset increments between business Quarter dates.
    startingMonth = 1 corresponds to dates like 1/31/2007, 4/30/2007, ...
    startingMonth = 2 corresponds to dates like 2/28/2007, 5/31/2007, ...
    startingMonth = 3 corresponds to dates like 3/30/2007, 6/29/2007, ...
    """"""
    _outputName = ""BusinessQuarterEnd""
    _default_startingMonth = 3
    _from_name_startingMonth = 12
    _prefix = ""BQ""
    _day_opt = ""business_end""

# TODO: This is basically the same as BQuarterEnd
class BQuarterBegin(QuarterOffset):
    _outputName = ""BusinessQuarterBegin""
    # I suspect this is wrong for *all* of them.
    _default_startingMonth = 3
    _from_name_startingMonth = 1
    _prefix = ""BQS""
    _day_opt = ""business_start""

class QuarterEnd(QuarterOffset):
    """"""
    DateOffset increments between business Quarter dates.
    startingMonth = 1 corresponds to dates like 1/31/2007, 4/30/2007, ...
    startingMonth = 2 corresponds to dates like 2/28/2007, 5/31/2007, ...
    startingMonth = 3 corresponds to dates like 3/31/2007, 6/30/2007, ...
    """"""
    _outputName = ""QuarterEnd""
    _default_startingMonth = 3
    _prefix = ""Q""
    _day_opt = ""end""
","[82, 83, 84]"
"""""""
Module contains tools for processing files into DataFrames or other objects
""""""
from collections import abc, defaultdict
import csv
import datetime
from io import BufferedIOBase, StringIO, TextIOWrapper
import re
import sys
from textwrap import fill
from typing import Any, Dict, Set
import warnings
import numpy as np
import pandas._libs.lib as lib
import pandas._libs.ops as libops
import pandas._libs.parsers as parsers
from pandas._libs.parsers import STR_NA_VALUES
from pandas._libs.tslibs import parsing
from pandas._typing import FilePathOrBuffer
from pandas.errors import (
    AbstractMethodError,
    EmptyDataError,
    ParserError,
    ParserWarning,
)
from pandas.util._decorators import Appender
from pandas.core.dtypes.cast import astype_nansafe
from pandas.core.dtypes.common import (
    ensure_object,
    ensure_str,
    is_bool_dtype,
    is_categorical_dtype,
    is_dtype_equal,
    is_extension_array_dtype,
    is_file_like,
    is_float,
    is_integer,
    is_integer_dtype,
    is_list_like,
    is_object_dtype,
    is_scalar,
    is_string_dtype,
    pandas_dtype,
)
from pandas.core.dtypes.dtypes import CategoricalDtype
from pandas.core.dtypes.missing import isna
from pandas.core import algorithms
from pandas.core.arrays import Categorical
from pandas.core.frame import DataFrame
from pandas.core.indexes.api import (
    Index,
    MultiIndex,
    RangeIndex,
    ensure_index_from_sequences,
)
from pandas.core.series import Series
from pandas.core.tools import datetimes as tools
from pandas.io.common import (
    get_filepath_or_buffer,
    get_handle,
    infer_compression,
    validate_header_arg,
)
from pandas.io.date_converters import generic_parser
# BOM character (byte order mark)
# This exists at the beginning of a file to indicate endianness
# of a file (stream). Unfortunately, this marker screws up parsing,
# so we need to remove it if we see it.
_BOM = ""\ufeff""
_doc_read_csv_and_table = (
    r""""""
{summary}
Also supports optionally iterating or breaking of the file
into chunks.
Additional help can be found in the online docs for
`IO Tools <https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html>`_.
Parameters
----------
filepath_or_buffer : str, path object or file-like object
    Any valid string path is acceptable. The string could be a URL. Valid
    URL schemes include http, ftp, s3, and file. For file URLs, a host is
    expected. A local file could be: file://localhost/path/to/table.csv.
    If you want to pass in a path object, pandas accepts any ``os.PathLike``.
    By file-like object, we refer to objects with a ``read()`` method, such as
    a file handler (e.g. via builtin ``open`` function) or ``StringIO``.
sep : str, default {_default_sep}
    Delimiter to use. If sep is None, the C engine cannot automatically detect
    the separator, but the Python parsing engine can, meaning the latter will
    be used and automatically detect the separator by Python's builtin sniffer
    tool, ``csv.Sniffer``. In addition, separators longer than 1 character and
    different from ``'\s+'`` will be interpreted as regular expressions and
    will also force the use of the Python parsing engine. Note that regex
    delimiters are prone to ignoring quoted data. Regex example: ``'\r\t'``.
delimiter : str, default ``None``
    Alias for sep.
header : int, list of int, default 'infer'
    Row number(s) to use as the column names, and the start of the
    data.  Default behavior is to infer the column names: if no names
    are passed the behavior is identical to ``header=0`` and column
    names are inferred from the first line of the file, if column
    names are passed explicitly then the behavior is identical to
    ``header=None``. Explicitly pass ``header=0`` to be able to
    replace existing names. The header can be a list of integers that
    specify row locations for a multi-index on the columns
    e.g. [0,1,3]. Intervening rows that are not specified will be
    skipped (e.g. 2 in this example is skipped). Note that this
    parameter ignores commented lines and empty lines if
    ``skip_blank_lines=True``, so ``header=0`` denotes the first line of
    data rather than the first line of the file.
names : array-like, optional
    List of column names to use. If the file contains a header row,
    then you should explicitly pass ``header=0`` to override the column names.
    Duplicates in this list are not allowed.
index_col : int, str, sequence of int / str, or False, default ``None``",[7]
"    Note that if the string is longer than a line, then in-place updating
    may not work (it will print a new line at each refresh).
    """"""
    fp = file
    last_printed_len = [0]  # closure over mutable variable (fast)
    def print_status(s):
        len_s = len(s)
        fp.write('\r' + s + ' '*max(last_printed_len[0] - len_s, 0))
        fp.flush()
        last_printed_len[0] = len_s
    return print_status

class tqdm(object):
    """"""
    Decorate an iterable object, returning an iterator which acts exactly
    like the orignal iterable, but prints a dynamically updating
    progressbar every time a value is requested.
    """"""
    def __init__(self, iterable=None, desc=None, total=None, leave=False,
                 file=sys.stderr, ncols=None, mininterval=0.1,
                 miniters=None, ascii=None, disable=False,
                 unit='it', unit_scale=False, gui=False):
        """"""
        Parameters
        ----------
        iterable  : iterable, optional
            Iterable to decorate with a progressbar.
            Leave blank [default: None] to manually manage the updates.
        desc  : str, optional
            Prefix for the progressbar [default: None].
        total  : int, optional
            The number of expected iterations. If not given, len(iterable) is
            used if possible. As a last resort, only basic progress
            statistics are displayed (no ETA, no progressbar). If `gui` is
            True and this parameter needs subsequent updating, specify an
            initial arbitrary large positive integer, e.g. int(9e9).
        leave  : bool, optional
            If [default: False], removes all traces of the progressbar
            upon termination of iteration.
        file  : `io.TextIOWrapper` or `io.StringIO`, optional
            Specifies where to output the progress messages
            [default: sys.stderr]. Uses `file.write(str)` and `file.flush()`
            methods.
        ncols  : int, optional
            The width of the entire output message. If specified, dynamically
            resizes the progressbar to stay within this bound
            [default: None]. The fallback is a meter width of 10 and no
            limit for the counter and statistics. If 0, will not print any
            meter (only stats).
        mininterval  : float, optional
            Minimum progress update interval, in seconds [default: 0.1].
        miniters  : int, optional
            Minimum progress update interval, in iterations [default: None].
            If specified, will set `mininterval` to 0.
        ascii  : bool, optional
            If [default: None] or false, use unicode (smooth blocks) to fill
            the meter. The fallback is to use ASCII characters `1-9 #`.
        disable : bool
            Whether to disable the entire progressbar wrapper [default: False].
        unit  : str, optional
            String that will be used to define the unit of each iteration
            [default: 'it'].
        unit_scale  : bool, optional
            If set, the number of iterations will be reduced/scaled
            automatically and a metric prefix following the
            International System of Units standard will be added
            (kilo, mega, etc.) [default: False].
        gui  : bool, optional
            If set, will attempt to use matplotlib animations for a
            graphical output [default: false].
        Returns
        -------
        out  : decorated iterator.
        """"""
        # Preprocess the arguments
        if total is None and iterable is not None:
            try:
                total = len(iterable)
            except (TypeError, AttributeError):
                total = None
        if (ncols is None) and (file in (sys.stderr, sys.stdout)):
            ncols = _environ_cols(file)
        if miniters is None:
            miniters = 0
            dynamic_miniters = True
        else:
            dynamic_miniters = False
            mininterval = 0
        if ascii is None:
            ascii = not _supports_unicode(file)
        if gui: # pragma: no cover
            try:
                import matplotlib as mpl
                import matplotlib.pyplot as plt
                from collections import deque
            except ImportError:
                gui = False
            else:
                self.mpl = mpl
                self.plt = plt
        # Store the arguments
        self.iterable = iterable
        self.desc = desc+': ' if desc else ''
        self.total = total
        self.leave = leave
        self.file = file
        self.ncols = ncols
        self.mininterval = mininterval
        self.miniters = miniters
        self.dynamic_miniters = dynamic_miniters
        self.ascii = ascii
        self.disable = disable
        self.unit = unit
        self.unit_scale = unit_scale
        self.gui = gui
        if gui: # pragma: no cover
            # Initialize the GUI display","[98, 125]"
"    )
    # is this an index replacement?
    if (
        not any_callable
        and not any_arraylike
        and not any_groupers
        and match_axis_length
        and level is None
    ):
        if isinstance(obj, DataFrame):
            all_in_columns_index = all(
                g in obj.columns or g in obj.index.names for g in keys
            )
        elif isinstance(obj, Series):
            all_in_columns_index = all(g in obj.index.names for g in keys)
        if not all_in_columns_index:
            keys = [com.asarray_tuplesafe(keys)]
    if isinstance(level, (tuple, list)):
        if key is None:
            keys = [None] * len(level)
        levels = level
    else:
        levels = [level] * len(keys)
    groupings = []
    exclusions = []
    # if the actual grouper should be obj[key]
    def is_in_axis(key):
        if not _is_label_like(key):
            try:
                obj._data.items.get_loc(key)
            except Exception:
                return False
        return True
    # if the grouper is obj[name]
    def is_in_obj(gpr):
        try:
            return id(gpr) == id(obj[gpr.name])
        except Exception:
            return False
    for i, (gpr, level) in enumerate(zip(keys, levels)):
        if is_in_obj(gpr):  # df.groupby(df['name'])
            in_axis, name = True, gpr.name
            exclusions.append(name)
        elif is_in_axis(gpr):  # df.groupby('name')
            if gpr in obj:
                if validate:
                    obj._check_label_or_level_ambiguity(gpr)
                in_axis, name, gpr = True, gpr, obj[gpr]
                exclusions.append(name)
            elif obj._is_level_reference(gpr):
                in_axis, name, level, gpr = False, None, gpr, None
            else:
                raise KeyError(gpr)
        elif isinstance(gpr, Grouper) and gpr.key is not None:
            # Add key to exclusions
            exclusions.append(gpr.key)
            in_axis, name = False, None
        else:
            in_axis, name = False, None
        if is_categorical_dtype(gpr) and len(gpr) != obj.shape[axis]:
            raise ValueError(
                (
                    ""Length of grouper ({len_gpr}) and axis ({len_axis})""
                    "" must be same length"".format(
                        len_gpr=len(gpr), len_axis=obj.shape[axis]
                    )
                )
            )
        # create the Grouping
        # allow us to passing the actual Grouping as the gpr
        ping = (
            Grouping(
                group_axis,
                gpr,
                obj=obj,
                name=name,
                level=level,
                sort=sort,
                observed=observed,
                in_axis=in_axis,
            )
            if not isinstance(gpr, Grouping)
            else gpr
        )
        groupings.append(ping)
    if len(groupings) == 0 and len(obj):
        raise ValueError(""No group keys passed!"")
    elif len(groupings) == 0:
        groupings.append(Grouping(Index([], dtype=""int""), np.array([], dtype=np.intp)))
    # create the internals grouper
    grouper = BaseGrouper(group_axis, groupings, sort=sort, mutated=mutated)
    return grouper, exclusions, obj

def _is_label_like(val):
    return isinstance(val, (str, tuple)) or (val is not None and is_scalar(val))

def _convert_grouper(axis, grouper):
    if isinstance(grouper, dict):
        return grouper.get
    elif isinstance(grouper, Series):
        if grouper.index.equals(axis):
            return grouper._values
        else:
            return grouper.reindex(axis)._values
    elif isinstance(grouper, (list, Series, Index, np.ndarray)):
        if len(grouper) != len(axis):
            raise ValueError(""Grouper and axis must be same length"")
        return grouper
    else:","[56, 59]"
"    if not query:
        return url
    parsed_url = compat_urlparse.urlparse(url)
    qs = compat_parse_qs(parsed_url.query)
    qs.update(query)
    return compat_urlparse.urlunparse(parsed_url._replace(
        query=compat_urllib_parse_urlencode(qs, True)))

def update_Request(req, url=None, data=None, headers={}, query={}):
    req_headers = req.headers.copy()
    req_headers.update(headers)
    req_data = data or req.data
    req_url = update_url_query(url or req.get_full_url(), query)
    req_get_method = req.get_method()
    if req_get_method == 'HEAD':
        req_type = HEADRequest
    elif req_get_method == 'PUT':
        req_type = PUTRequest
    else:
        req_type = compat_urllib_request.Request
    new_req = req_type(
        req_url, data=req_data, headers=req_headers,
        origin_req_host=req.origin_req_host, unverifiable=req.unverifiable)
    if hasattr(req, 'timeout'):
        new_req.timeout = req.timeout
    return new_req

def dict_get(d, key_or_keys, default=None, skip_false_values=True):
    if isinstance(key_or_keys, (list, tuple)):
        for key in key_or_keys:
            if key not in d or d[key] is None or skip_false_values and not d[key]:
                continue
            return d[key]
        return default
    return d.get(key_or_keys, default)

def try_get(src, getter, expected_type=None):
    try:
        v = getter(src)
    except (AttributeError, KeyError, TypeError, IndexError):
        pass
    else:
        if expected_type is None or isinstance(v, expected_type):
            return v

def encode_compat_str(string, encoding=preferredencoding(), errors='strict'):
    return string if isinstance(string, compat_str) else compat_str(string, encoding, errors)

US_RATINGS = {
    'G': 0,
    'PG': 10,
    'PG-13': 13,
    'R': 16,
    'NC': 18,
}

TV_PARENTAL_GUIDELINES = {
    'TV-Y': 0,
    'TV-Y7': 7,
    'TV-G': 0,
    'TV-PG': 0,
    'TV-14': 14,
    'TV-MA': 17,
}

def parse_age_limit(s):
    if type(s) == int:
        return s if 0 <= s <= 21 else None
    if not isinstance(s, compat_basestring):
        return None
    m = re.match(r'^(?P<age>\d{1,2})\+?$', s)
    if m:
        return int(m.group('age'))
    if s in US_RATINGS:
        return US_RATINGS[s]
    return TV_PARENTAL_GUIDELINES.get(s)

def strip_jsonp(code):
    return re.sub(
        r'(?s)^[a-zA-Z0-9_.$]+\s*\(\s*(.*)\);?\s*?(?://[^\n]*)*$', r'\1', code)

def js_to_json(code):
    def fix_kv(m):
        v = m.group(0)
        if v in ('true', 'false', 'null'):
            return v
        elif v.startswith('/*') or v == ',':
            return """"
        if v[0] in (""'"", '""'):
            v = re.sub(r'(?s)\\.|""', lambda m: {
                '""': '\\""',
                ""\\'"": ""'"",
                '\\\n': '',
                '\\x': '\\u00',
            }.get(m.group(0), m.group(0)), v[1:-1])
        INTEGER_TABLE = (
            (r'^(0[xX][0-9a-fA-F]+)\s*:?$', 16),
            (r'^(0+[0-7]+)\s*:?$', 8),
        )
        for regex, base in INTEGER_TABLE:
            im = re.match(regex, v)
            if im:
                i = int(im.group(1), base)
                return '""%d"":' % i if v.endswith(':') else '%d' % i
        return '""%s""' % v
    return re.sub(r'''(?sx)
        ""(?:[^""\\]*(?:\\\\|\\['""nurtbfx/\n]))*[^""\\]*""|
        '(?:[^'\\]*(?:\\\\|\\['""nurtbfx/\n]))*[^'\\]*'|
        /\*.*?\*/|,(?=\s*[\]}])|
        [a-zA-Z_][.a-zA-Z_0-9]*|
        \b(?:0[xX][0-9a-fA-F]+|0+[0-7]+)(?:\s*:)?|
        [0-9]+(?=\s*:)
        ''', fix_kv, code)","[95, 122]"
"        # this is easier
        index = value_counts.index.astype(object)
        # if we want nans, count the mask
        if not dropna:
            # TODO(extension)
            # appending to an Index *always* infers
            # w/o passing the dtype
            array = np.append(array, [self._mask.sum()])
            index = Index(
                np.concatenate([index.values, np.array([np.nan], dtype=object)]),
                dtype=object,
            )
        return Series(array, index=index)
    def _values_for_argsort(self) -> np.ndarray:
        """"""Return values for sorting.
        Returns
        -------
        ndarray
            The transformed values should maintain the ordering between values
            within the array.
        See Also
        --------
        ExtensionArray.argsort
        """"""
        data = self._data.copy()
        data[self._mask] = data.min() - 1
        return data
    @classmethod
    def _create_comparison_method(cls, op):
        op_name = op.__name__
        @unpack_zerodim_and_defer(op.__name__)
        def cmp_method(self, other):
            mask = None
            if isinstance(other, IntegerArray):
                other, mask = other._data, other._mask
            elif is_list_like(other):
                other = np.asarray(other)
                if other.ndim > 1:
                    raise NotImplementedError(
                        ""can only perform ops with 1-d structures""
                    )
                if len(self) != len(other):
                    raise ValueError(""Lengths must match to compare"")
            # numpy will show a DeprecationWarning on invalid elementwise
            # comparisons, this will raise in the future
            with warnings.catch_warnings():
                warnings.filterwarnings(""ignore"", ""elementwise"", FutureWarning)
                with np.errstate(all=""ignore""):
                    result = op(self._data, other)
            # nans propagate
            if mask is None:
                mask = self._mask
            else:
                mask = self._mask | mask
            result[mask] = op_name == ""ne""
            return result
        name = f""__{op.__name__}__""
        return set_function_name(cmp_method, name, cls)
    def _reduce(self, name, skipna=True, **kwargs):
        data = self._data
        mask = self._mask
        # coerce to a nan-aware float if needed
        if mask.any():
            data = self._data.astype(""float64"")
            data[mask] = self._na_value
        op = getattr(nanops, ""nan"" + name)
        result = op(data, axis=0, skipna=skipna, mask=mask, **kwargs)
        # if we have a boolean op, don't coerce
        if name in [""any"", ""all""]:
            pass
        # if we have a preservable numeric op,
        # provide coercion back to an integer type if possible
        elif name in [""sum"", ""min"", ""max"", ""prod""] and notna(result):
            int_result = int(result)
            if int_result == result:
                result = int_result
        return result
    def _maybe_mask_result(self, result, mask, other, op_name):
        """"""
        Parameters
        ----------
        result : array-like
        mask : array-like bool
        other : scalar or array-like
        op_name : str
        """"""
        # may need to fill infs
        # and mask wraparound
        if is_float_dtype(result):
            mask |= (result == np.inf) | (result == -np.inf)
        # if we have a float operand we are by-definition
        # a float result
        # or our op is a divide
        if (is_float_dtype(other) or is_float(other)) or (
            op_name in [""rtruediv"", ""truediv""]
        ):
            result[mask] = np.nan
            return result
        return type(self)(result, mask, copy=False)
    @classmethod
    def _create_arithmetic_method(cls, op):
        op_name = op.__name__",[59]
"                    y,
                    self.bar_width,
                    start=start,
                    label=label,
                    log=self.log,
                    **kwds
                )
                pos_prior = pos_prior + np.where(mask, y, 0)
                neg_prior = neg_prior + np.where(mask, 0, y)
            else:
                w = self.bar_width / K
                rect = self._plot(
                    ax,
                    self.ax_pos + (i + 0.5) * w,
                    y,
                    w,
                    start=start,
                    label=label,
                    log=self.log,
                    **kwds
                )
            self._add_legend_handle(rect, label, index=i)
    def _post_plot_logic(self, ax, data):
        if self.use_index:
            str_index = [pprint_thing(key) for key in data.index]
        else:
            str_index = [pprint_thing(key) for key in range(data.shape[0])]
        name = self._get_index_name()
        s_edge = self.ax_pos[0] - 0.25 + self.lim_offset
        e_edge = self.ax_pos[-1] + 0.25 + self.bar_width + self.lim_offset
        self._decorate_ticks(ax, name, str_index, s_edge, e_edge)
    def _decorate_ticks(self, ax, name, ticklabels, start_edge, end_edge):
        ax.set_xlim((start_edge, end_edge))
        ax.set_xticks(self.tick_pos)
        ax.set_xticklabels(ticklabels)
        if name is not None and self.use_index:
            ax.set_xlabel(name)

class BarhPlot(BarPlot):
    _kind = ""barh""
    _default_rot = 0
    orientation = ""horizontal""
    @property
    def _start_base(self):
        return self.left
    @classmethod
    def _plot(cls, ax, x, y, w, start=0, log=False, **kwds):
        return ax.barh(x, y, w, left=start, log=log, **kwds)
    def _decorate_ticks(self, ax, name, ticklabels, start_edge, end_edge):
        # horizontal bars
        ax.set_ylim((start_edge, end_edge))
        ax.set_yticks(self.tick_pos)
        ax.set_yticklabels(ticklabels)
        if name is not None and self.use_index:
            ax.set_ylabel(name)

class PiePlot(MPLPlot):
    _kind = ""pie""
    _layout_type = ""horizontal""
    def __init__(self, data, kind=None, **kwargs):
        data = data.fillna(value=0)
        if (data < 0).any().any():
            raise ValueError(""{0} doesn't allow negative values"".format(kind))
        MPLPlot.__init__(self, data, kind=kind, **kwargs)
    def _args_adjust(self):
        self.grid = False
        self.logy = False
        self.logx = False
        self.loglog = False
    def _validate_color_args(self):
        pass
    def _make_plot(self):
        colors = self._get_colors(num_colors=len(self.data), color_kwds=""colors"")
        self.kwds.setdefault(""colors"", colors)
        for i, (label, y) in enumerate(self._iter_data()):
            ax = self._get_ax(i)
            if label is not None:
                label = pprint_thing(label)
                ax.set_ylabel(label)
            kwds = self.kwds.copy()
            def blank_labeler(label, value):
                if value == 0:
                    return """"
                else:
                    return label
            idx = [pprint_thing(v) for v in self.data.index]
            labels = kwds.pop(""labels"", idx)
            # labels is used for each wedge's labels
            # Blank out labels for values of 0 so they don't overlap
            # with nonzero wedges
            if labels is not None:
                blabels = [blank_labeler(l, value) for l, value in zip(labels, y)]
            else:
                blabels = None
            results = ax.pie(y, labels=blabels, **kwds)
            if kwds.get(""autopct"", None) is not None:
                patches, texts, autotexts = results
            else:
                patches, texts = results
                autotexts = []
            if self.fontsize is not None:
                for t in texts + autotexts:
                    t.set_fontsize(self.fontsize)
            # leglabels is used for legend labels
            leglabels = labels if labels is not None else idx
            for p, l in zip(patches, leglabels):","[37, 38]"
"def unified_timestamp(date_str, day_first=True):
    if date_str is None:
        return None
    date_str = date_str.replace(',', ' ')
    pm_delta = datetime.timedelta(hours=12 if re.search(r'(?i)PM', date_str) else 0)
    timezone, date_str = extract_timezone(date_str)
    # Remove AM/PM + timezone
    date_str = re.sub(r'(?i)\s*(?:AM|PM)(?:\s+[A-Z]+)?', '', date_str)
    for expression in date_formats(day_first):
        try:
            dt = datetime.datetime.strptime(date_str, expression) - timezone + pm_delta
            return calendar.timegm(dt.timetuple())
        except ValueError:
            pass
    timetuple = email.utils.parsedate_tz(date_str)
    if timetuple:
        return calendar.timegm(timetuple.timetuple())

def determine_ext(url, default_ext='unknown_video'):
    if url is None:
        return default_ext
    guess = url.partition('?')[0].rpartition('.')[2]
    if re.match(r'^[A-Za-z0-9]+$', guess):
        return guess
    # Try extract ext from URLs like http://example.com/foo/bar.mp4/?download
    elif guess.rstrip('/') in KNOWN_EXTENSIONS:
        return guess.rstrip('/')
    else:
        return default_ext

def subtitles_filename(filename, sub_lang, sub_format):
    return filename.rsplit('.', 1)[0] + '.' + sub_lang + '.' + sub_format

def date_from_str(date_str):
    """"""
    Return a datetime object from a string in the format YYYYMMDD or
    (now|today)[+-][0-9](day|week|month|year)(s)?""""""
    today = datetime.date.today()
    if date_str in ('now', 'today'):
        return today
    if date_str == 'yesterday':
        return today - datetime.timedelta(days=1)
    match = re.match('(now|today)(?P<sign>[+-])(?P<time>\d+)(?P<unit>day|week|month|year)(s)?', date_str)
    if match is not None:
        sign = match.group('sign')
        time = int(match.group('time'))
        if sign == '-':
            time = -time
        unit = match.group('unit')
        # A bad approximation?
        if unit == 'month':
            unit = 'day'
            time *= 30
        elif unit == 'year':
            unit = 'day'
            time *= 365
        unit += 's'
        delta = datetime.timedelta(**{unit: time})
        return today + delta
    return datetime.datetime.strptime(date_str, '%Y%m%d').date()

def hyphenate_date(date_str):
    """"""
    Convert a date in 'YYYYMMDD' format to 'YYYY-MM-DD' format""""""
    match = re.match(r'^(\d\d\d\d)(\d\d)(\d\d)$', date_str)
    if match is not None:
        return '-'.join(match.groups())
    else:
        return date_str

class DateRange(object):
    """"""Represents a time interval between two dates""""""
    def __init__(self, start=None, end=None):
        """"""start and end must be strings in the format accepted by date""""""
        if start is not None:
            self.start = date_from_str(start)
        else:
            self.start = datetime.datetime.min.date()
        if end is not None:
            self.end = date_from_str(end)
        else:
            self.end = datetime.datetime.max.date()
        if self.start > self.end:
            raise ValueError('Date range: ""%s"" , the start date must be before the end date' % self)
    @classmethod
    def day(cls, day):
        """"""Returns a range that only contains the given day""""""
        return cls(day, day)
    def __contains__(self, date):
        """"""Check if the date is in the range""""""
        if not isinstance(date, datetime.date):
            date = date_from_str(date)
        return self.start <= date <= self.end
    def __str__(self):
        return '%s - %s' % (self.start.isoformat(), self.end.isoformat())

def platform_name():
    """""" Returns the platform name as a compat_str """"""
    res = platform.platform()
    if isinstance(res, bytes):
        res = res.decode(preferredencoding())
    assert isinstance(res, compat_str)
    return res

def _windows_write_string(s, out):
    """""" Returns True if the string was written using special methods,
    False if it has yet to be written out.""""""
    # Adapted from http://stackoverflow.com/a/3259271/35070
    import ctypes","[7, 15, 21]"
"        if isinstance(key, str):
            try:
                loc = self._get_string_slice(key)
                return series[loc]
            except (TypeError, ValueError):
                pass
            asdt, reso = parse_time_string(key, self.freq)
            grp = resolution.Resolution.get_freq_group(reso)
            freqn = resolution.get_freq_group(self.freq)
            # _get_string_slice will handle cases where grp < freqn
            assert grp >= freqn
            if grp == freqn:
                key = Period(asdt, freq=self.freq)
                loc = self.get_loc(key)
                return series.iloc[loc]
            else:
                raise KeyError(key)
        elif isinstance(key, Period) or key is NaT:
            ordinal = key.ordinal if key is not NaT else NaT.value
            loc = self._engine.get_loc(ordinal)
            return series[loc]
        # slice, PeriodIndex, np.ndarray, List[Period]
        value = Index.get_value(self, series, key)
        return com.maybe_box(self, value, series, key)
    @Appender(_index_shared_docs[""get_indexer""] % _index_doc_kwargs)
    def get_indexer(self, target, method=None, limit=None, tolerance=None):
        target = ensure_index(target)
        if isinstance(target, PeriodIndex):
            if target.freq != self.freq:
                # No matches
                no_matches = -1 * np.ones(self.shape, dtype=np.intp)
                return no_matches
            target = target.asi8
            self_index = self._int64index
        else:
            self_index = self
        if tolerance is not None:
            tolerance = self._convert_tolerance(tolerance, target)
        return Index.get_indexer(self_index, target, method, limit, tolerance)
    @Appender(_index_shared_docs[""get_indexer_non_unique""] % _index_doc_kwargs)
    def get_indexer_non_unique(self, target):
        target = ensure_index(target)
        if isinstance(target, PeriodIndex):
            if target.freq != self.freq:
                no_matches = -1 * np.ones(self.shape, dtype=np.intp)
                return no_matches, no_matches
            target = target.asi8
        indexer, missing = self._int64index.get_indexer_non_unique(target)
        return ensure_platform_int(indexer), missing
    def get_loc(self, key, method=None, tolerance=None):
        """"""
        Get integer location for requested label.
        Parameters
        ----------
        key : Period, NaT, str, or datetime
            String or datetime key must be parseable as Period.
        Returns
        -------
        loc : int or ndarray[int64]
        Raises
        ------
        KeyError
            Key is not present in the index.
        TypeError
            If key is listlike or otherwise not hashable.
        """"""
        if isinstance(key, str):
            try:
                return self._get_string_slice(key)
            except (TypeError, KeyError, ValueError, OverflowError):
                pass
            try:
                asdt, reso = parse_time_string(key, self.freq)
                key = asdt
            except DateParseError:
                # A string with invalid format
                raise KeyError(f""Cannot interpret '{key}' as period"")
        elif is_integer(key):
            # Period constructor will cast to string, which we dont want
            raise KeyError(key)
        try:
            key = Period(key, freq=self.freq)
        except ValueError:
            # we cannot construct the Period
            # as we have an invalid type
            if is_list_like(key):
                raise TypeError(f""'{key}' is an invalid key"")
            raise KeyError(key)
        ordinal = key.ordinal if key is not NaT else key.value
        try:
            return self._engine.get_loc(ordinal)
        except KeyError:
            try:
                if tolerance is not None:
                    tolerance = self._convert_tolerance(tolerance, np.asarray(key))
                return self._int64index.get_loc(ordinal, method, tolerance)
            except KeyError:
                raise KeyError(key)
    def _maybe_cast_slice_bound(self, label, side, kind):
        """"""
        If label is a string or a datetime, cast it to Period.ordinal according","[5, 87, 88, 93]"
"
def get_flat_dependant(dependant: Dependant) -> Dependant:
    flat_dependant = Dependant(
        path_params=dependant.path_params.copy(),
        query_params=dependant.query_params.copy(),
        header_params=dependant.header_params.copy(),
        cookie_params=dependant.cookie_params.copy(),
        body_params=dependant.body_params.copy(),
        security_schemes=dependant.security_requirements.copy(),
        use_cache=dependant.use_cache,
        path=dependant.path,
    )
    for sub_dependant in dependant.dependencies:
        flat_sub = get_flat_dependant(sub_dependant)
        flat_dependant.path_params.extend(flat_sub.path_params)
        flat_dependant.query_params.extend(flat_sub.query_params)
        flat_dependant.header_params.extend(flat_sub.header_params)
        flat_dependant.cookie_params.extend(flat_sub.cookie_params)
        flat_dependant.body_params.extend(flat_sub.body_params)
        flat_dependant.security_requirements.extend(flat_sub.security_requirements)
    return flat_dependant

def is_scalar_field(field: Field) -> bool:
    return (
        field.shape == Shape.SINGLETON
        and not lenient_issubclass(field.type_, BaseModel)
        and not lenient_issubclass(field.type_, sequence_types + (dict,))
        and not isinstance(field.schema, params.Body)
    )

def is_scalar_sequence_field(field: Field) -> bool:
    if (field.shape in sequence_shapes) and not lenient_issubclass(
        field.type_, BaseModel
    ):
        if field.sub_fields is not None:
            for sub_field in field.sub_fields:
                if not is_scalar_field(sub_field):
                    return False
        return True
    if lenient_issubclass(field.type_, sequence_types):
        return True
    return False

def get_dependant(
    *,
    path: str,
    call: Callable,
    name: str = None,
    security_scopes: List[str] = None,
    use_cache: bool = True,
) -> Dependant:
    path_param_names = get_path_param_names(path)
    endpoint_signature = inspect.signature(call)
    signature_params = endpoint_signature.parameters
    dependant = Dependant(call=call, name=name, path=path, use_cache=use_cache)
    for param_name, param in signature_params.items():
        if isinstance(param.default, params.Depends):
            sub_dependant = get_param_sub_dependant(
                param=param, path=path, security_scopes=security_scopes
            )
            dependant.dependencies.append(sub_dependant)
    for param_name, param in signature_params.items():
        if isinstance(param.default, params.Depends):
            continue
        if add_non_field_param_to_dependency(param=param, dependant=dependant):
            continue
        param_field = get_param_field(param=param, default_schema=params.Query)
        if param_name in path_param_names:
            assert param.default == param.empty or isinstance(
                param.default, params.Path
            ), ""Path params must have no defaults or use Path(...)""
            assert is_scalar_field(
                field=param_field
            ), f""Path params must be of one of the supported types""
            param_field = get_param_field(
                param=param,
                default_schema=params.Path,
                force_type=params.ParamTypes.path,
            )
            add_param_to_fields(field=param_field, dependant=dependant)
        elif is_scalar_field(field=param_field):
            add_param_to_fields(field=param_field, dependant=dependant)
        elif isinstance(
            param.default, (params.Query, params.Header)
        ) and is_scalar_sequence_field(param_field):
            add_param_to_fields(field=param_field, dependant=dependant)
        else:
            assert isinstance(
                param_field.schema, params.Body
            ), f""Param: {param_field.name} can only be a request body, using Body(...)""
            dependant.body_params.append(param_field)
    return dependant

def add_non_field_param_to_dependency(
    *, param: inspect.Parameter, dependant: Dependant
) -> Optional[bool]:
    if lenient_issubclass(param.annotation, Request):
        dependant.request_param_name = param.name
        return True
    elif lenient_issubclass(param.annotation, WebSocket):
        dependant.websocket_param_name = param.name
        return True
    elif lenient_issubclass(param.annotation, Response):
        dependant.response_param_name = param.name
        return True
    elif lenient_issubclass(param.annotation, BackgroundTasks):
        dependant.background_tasks_param_name = param.name
        return True
    elif lenient_issubclass(param.annotation, SecurityScopes):
        dependant.security_scopes_param_name = param.name
        return True
    return None

def get_param_field(
    *,
    param: inspect.Parameter,
    default_schema: Type[params.Param] = params.Param,
    force_type: params.ParamTypes = None,
) -> Field:
    default_value = Required
    had_schema = False","[25, 30]"
"                async_def = False
                async_def_nl = False
                async_def_indent = 0
        else:                                  # continued statement
            if not line:
                raise TokenError(""EOF in multi-line statement"", (lnum, 0))
            continued = 0
        while pos < max:
            pseudomatch = pseudoprog.match(line, pos)
            if pseudomatch:                                # scan for tokens
                start, end = pseudomatch.span(1)
                spos, epos, pos = (lnum, start), (lnum, end), end
                token, initial = line[start:end], line[start]
                if initial in numchars or \
                   (initial == '.' and token != '.'):      # ordinary number
                    yield (NUMBER, token, spos, epos, line)
                elif initial in '\r\n':
                    newline = NEWLINE
                    if parenlev > 0:
                        newline = NL
                    elif async_def:
                        async_def_nl = True
                    if stashed:
                        yield stashed
                        stashed = None
                    yield (newline, token, spos, epos, line)
                elif initial == '#':
                    assert not token.endswith(""\n"")
                    if stashed:
                        yield stashed
                        stashed = None
                    yield (COMMENT, token, spos, epos, line)
                elif token in triple_quoted:
                    endprog = endprogs[token]
                    endmatch = endprog.match(line, pos)
                    if endmatch:                           # all on one line
                        pos = endmatch.end(0)
                        token = line[start:pos]
                        if stashed:
                            yield stashed
                            stashed = None
                        yield (STRING, token, spos, (lnum, pos), line)
                    else:
                        strstart = (lnum, start)           # multiple lines
                        contstr = line[start:]
                        contline = line
                        break
                elif initial in single_quoted or \
                    token[:2] in single_quoted or \
                    token[:3] in single_quoted:
                    if token[-1] == '\n':                  # continued string
                        strstart = (lnum, start)
                        endprog = (endprogs[initial] or endprogs[token[1]] or
                                   endprogs[token[2]])
                        contstr, needcont = line[start:], 1
                        contline = line
                        break
                    else:                                  # ordinary string
                        if stashed:
                            yield stashed
                            stashed = None
                        yield (STRING, token, spos, epos, line)
                elif initial.isidentifier():               # ordinary name
                    if token in ('async', 'await'):
                        if async_def:
                            yield (ASYNC if token == 'async' else AWAIT,
                                   token, spos, epos, line)
                            continue
                    tok = (NAME, token, spos, epos, line)
                    if token == 'async' and not stashed:
                        stashed = tok
                        continue
                    if token == 'def':
                        if (stashed
                                and stashed[0] == NAME
                                and stashed[1] == 'async'):
                            async_def = True
                            async_def_indent = indents[-1]
                            yield (ASYNC, stashed[1],
                                   stashed[2], stashed[3],
                                   stashed[4])
                            stashed = None
                    if stashed:
                        yield stashed
                        stashed = None
                    yield tok
                elif initial == '\\':                      # continued stmt
                    # This yield is new; needed for better idempotency:
                    if stashed:
                        yield stashed
                        stashed = None
                    yield (NL, token, spos, (lnum, pos), line)
                    continued = 1
                else:
                    if initial in '([{': parenlev = parenlev + 1
                    elif initial in ')]}': parenlev = parenlev - 1
                    if stashed:
                        yield stashed
                        stashed = None
                    yield (OP, token, spos, epos, line)
            else:
                yield (ERRORTOKEN, line[pos],
                           (lnum, pos), (lnum, pos+1), line)
                pos = pos + 1
    if stashed:
        yield stashed
        stashed = None
    for indent in indents[1:]:                 # pop remaining indent levels
        yield (DEDENT, '', (lnum, 0), (lnum, 0), '')
    yield (ENDMARKER, '', (lnum, 0), (lnum, 0), '')
if __name__ == '__main__':                     # testing
    import sys
    if len(sys.argv) > 1: tokenize(open(sys.argv[1]).readline)","[78, 83, 84]"
"    [('a', '<lambda>_0'), ('a', '<lambda>_1'), ('b', '<lambda>')]
    """"""
    return [
        (pair[0], ""_"".join([pair[1], str(seq[:i].count(pair))]))
        if seq.count(pair) > 1
        else pair
        for i, pair in enumerate(seq)
    ]

# TODO: Can't use, because mypy doesn't like us setting __name__
#   error: ""partial[Any]"" has no attribute ""__name__""
# the type is:
#   typing.Sequence[Callable[..., ScalarResult]]
#     -> typing.Sequence[Callable[..., ScalarResult]]:

def _managle_lambda_list(aggfuncs: Sequence[Any]) -> Sequence[Any]:
    """"""
    Possibly mangle a list of aggfuncs.
    Parameters
    ----------
    aggfuncs : Sequence
    Returns
    -------
    mangled: list-like
        A new AggSpec sequence, where lambdas have been converted
        to have unique names.
    Notes
    -----
    If just one aggfunc is passed, the name will not be mangled.
    """"""
    if len(aggfuncs) <= 1:
        # don't mangle for .agg([lambda x: .])
        return aggfuncs
    i = 0
    mangled_aggfuncs = []
    for aggfunc in aggfuncs:
        if com.get_callable_name(aggfunc) == ""<lambda>"":
            aggfunc = functools.partial(aggfunc)
            aggfunc.__name__ = ""<lambda_{}>"".format(i)
            i += 1
        mangled_aggfuncs.append(aggfunc)
    return mangled_aggfuncs

def _maybe_mangle_lambdas(agg_spec: Any) -> Any:
    """"""
    Make new lambdas with unique names.
    Parameters
    ----------
    agg_spec : Any
        An argument to NDFrameGroupBy.agg.
        Non-dict-like `agg_spec` are pass through as is.
        For dict-like `agg_spec` a new spec is returned
        with name-mangled lambdas.
    Returns
    -------
    mangled : Any
        Same type as the input.
    Examples
    --------
    >>> _maybe_mangle_lambdas('sum')
    'sum'
    >>> _maybe_mangle_lambdas([lambda: 1, lambda: 2])  # doctest: +SKIP
    [<function __main__.<lambda_0>,
     <function pandas...._make_lambda.<locals>.f(*args, **kwargs)>]
    """"""
    is_dict = is_dict_like(agg_spec)
    if not (is_dict or is_list_like(agg_spec)):
        return agg_spec
    mangled_aggspec = type(agg_spec)()  # dict or OrderdDict
    if is_dict:
        for key, aggfuncs in agg_spec.items():
            if is_list_like(aggfuncs) and not is_dict_like(aggfuncs):
                mangled_aggfuncs = _managle_lambda_list(aggfuncs)
            else:
                mangled_aggfuncs = aggfuncs
            mangled_aggspec[key] = mangled_aggfuncs
    else:
        mangled_aggspec = _managle_lambda_list(agg_spec)
    return mangled_aggspec

def _recast_datetimelike_result(result: DataFrame) -> DataFrame:
    """"""
    If we have date/time like in the original, then coerce dates
    as we are stacking can easily have object dtypes here.
    Parameters
    ----------
    result : DataFrame
    Returns
    -------
    DataFrame
    Notes
    -----
    - Assumes Groupby._selected_obj has ndim==2 and at least one
    datetimelike column
    """"""
    result = result.copy()
    obj_cols = [
        idx for idx in range(len(result.columns)) if is_object_dtype(result.dtypes[idx])
    ]
    # See GH#26285
    for n in obj_cols:
        converted = maybe_convert_objects(
            result.iloc[:, n].values, convert_numeric=False
        )
        result.iloc[:, n] = converted",[116]
"import re
from thefuck.shells import shell
from thefuck.specific.git import git_support
from thefuck.utils import eager

@git_support
def match(command):
    return (""fatal: A branch named '"" in command.output
            and "" already exists."" in command.output)

@git_support
@eager
def get_new_command(command):
    branch_name = re.findall(
        r""fatal: A branch named '([^']*)' already exists."", command.output)[0]
    new_command_templates = [['git branch -d {0}', 'git branch {0}'],
                             ['git branch -d {0}', 'git checkout -b {0}'],
                             ['git branch -D {0}', 'git branch {0}'],
                             ['git branch -D {0}', 'git checkout -b {0}'],
                             ['git checkout {0}']]
    for new_command_template in new_command_templates:
        yield shell.and_(*new_command_template).format(branch_name)","[9, 16]"
"        self.assertEqual(settings.getfloat('TEST_FLOATx'), 0.0)
        self.assertEqual(settings.getfloat('TEST_FLOATx', 55.0), 55.0)
        self.assertEqual(settings.getlist('TEST_LIST1'), ['one', 'two'])
        self.assertEqual(settings.getlist('TEST_LIST2'), ['one', 'two'])
        self.assertEqual(settings.getlist('TEST_LISTx'), [])
        self.assertEqual(settings.getlist('TEST_LISTx', ['default']), ['default'])
        self.assertEqual(settings['TEST_STR'], 'value')
        self.assertEqual(settings.get('TEST_STR'), 'value')
        self.assertEqual(settings['TEST_STRx'], None)
        self.assertEqual(settings.get('TEST_STRx'), None)
        self.assertEqual(settings.get('TEST_STRx', 'default'), 'default')
        self.assertEqual(settings.getdict('TEST_DICT1'), {'key1': 'val1', 'ke2': 3})
        self.assertEqual(settings.getdict('TEST_DICT2'), {'key1': 'val1', 'ke2': 3})
        self.assertEqual(settings.getdict('TEST_DICT3'), {})
        self.assertEqual(settings.getdict('TEST_DICT3', {'key1': 5}), {'key1': 5})
        self.assertRaises(ValueError, settings.getdict, 'TEST_LIST1')
    def test_getpriority(self):
        settings = BaseSettings({'key': 'value'}, priority=99)
        self.assertEqual(settings.getpriority('key'), 99)
        self.assertEqual(settings.getpriority('nonexistentkey'), None)
    def test_getcomposite(self):
        s = BaseSettings({'TEST_BASE': {1: 1, 2: 2},
                          'TEST': BaseSettings({1: 10}),
                          'HASNOBASE': BaseSettings({1: 1})})
        cs = s._getcomposite('TEST')
        self.assertEqual(len(cs), 2)
        self.assertEqual(cs[1], 10)
        self.assertEqual(cs[2], 2)
        cs = s._getcomposite('HASNOBASE')
        self.assertEqual(len(cs), 1)
        self.assertEqual(cs[1], 1)
        cs = s._getcomposite('NONEXISTENT')
        self.assertIsNone(cs)
    def test_maxpriority(self):
        # Empty settings should return 'default'
        self.assertEqual(self.settings.maxpriority(), 0)
        self.settings.set('A', 0, 10)
        self.settings.set('B', 0, 30)
        self.assertEqual(self.settings.maxpriority(), 30)
    def test_copy(self):
        values = {
            'TEST_BOOL': True,
            'TEST_LIST': ['one', 'two'],
            'TEST_LIST_OF_LISTS': [['first_one', 'first_two'],
                                   ['second_one', 'second_two']]
        }
        self.settings.setdict(values)
        copy = self.settings.copy()
        self.settings.set('TEST_BOOL', False)
        self.assertTrue(copy.get('TEST_BOOL'))
        test_list = self.settings.get('TEST_LIST')
        test_list.append('three')
        self.assertListEqual(copy.get('TEST_LIST'), ['one', 'two'])
        test_list_of_lists = self.settings.get('TEST_LIST_OF_LISTS')
        test_list_of_lists[0].append('first_three')
        self.assertListEqual(copy.get('TEST_LIST_OF_LISTS')[0],
                             ['first_one', 'first_two'])
    def test_freeze(self):
        self.settings.freeze()
        with self.assertRaises(TypeError) as cm:
            self.settings.set('TEST_BOOL', False)
            self.assertEqual(str(cm.exception),
                             ""Trying to modify an immutable Settings object"")
    def test_frozencopy(self):
        frozencopy = self.settings.frozencopy()
        self.assertTrue(frozencopy.frozen)
        self.assertIsNot(frozencopy, self.settings)
    def test_deprecated_attribute_overrides(self):
        self.settings.set('BAR', 'fuz', priority='cmdline')
        with warnings.catch_warnings(record=True) as w:
            self.settings.overrides['BAR'] = 'foo'
            self.assertIn(""Settings.overrides"", str(w[0].message))
            self.assertEqual(self.settings.get('BAR'), 'foo')
            self.assertEqual(self.settings.overrides.get('BAR'), 'foo')
            self.assertIn('BAR', self.settings.overrides)
            self.settings.overrides.update(BAR='bus')
            self.assertEqual(self.settings.get('BAR'), 'bus')
            self.assertEqual(self.settings.overrides.get('BAR'), 'bus')
            self.settings.overrides.setdefault('BAR', 'fez')
            self.assertEqual(self.settings.get('BAR'), 'bus')
            self.settings.overrides.setdefault('FOO', 'fez')
            self.assertEqual(self.settings.get('FOO'), 'fez')
            self.assertEqual(self.settings.overrides.get('FOO'), 'fez')

    def test_deprecated_attribute_defaults(self):
        self.settings.set('BAR', 'fuz', priority='default')
        with warnings.catch_warnings(record=True) as w:
            self.settings.defaults['BAR'] = 'foo'
            self.assertIn(""Settings.defaults"", str(w[0].message))
            self.assertEqual(self.settings.get('BAR'), 'foo')
            self.assertEqual(self.settings.defaults.get('BAR'), 'foo')
            self.assertIn('BAR', self.settings.defaults)

class SettingsTest(unittest.TestCase):
    if six.PY3:
        assertItemsEqual = unittest.TestCase.assertCountEqual
    def setUp(self):
        self.settings = Settings()
    @mock.patch.dict('scrapy.settings.SETTINGS_PRIORITIES', {'default': 10})
    @mock.patch('scrapy.settings.default_settings', default_settings)
    def test_initial_defaults(self):
        settings = Settings()
        self.assertEqual(len(settings.attributes), 2)
        self.assertIn('TEST_DEFAULT', settings.attributes)
        attr = settings.attributes['TEST_DEFAULT']
        self.assertIsInstance(attr, SettingsAttribute)
        self.assertEqual(attr.value, 'defvalue')
        self.assertEqual(attr.priority, 10)
","[24, 25, 27, 28]"
"            if bar_format:
                format_dict.update(percentage=percentage)
                # auto-remove colon for empty `desc`
                if not prefix:
                    bar_format = bar_format.replace(""{desc}: "", '')
            else:
                bar_format = ""{l_bar}{bar}{r_bar}""
            full_bar = FormatReplace()
            try:
                nobar = bar_format.format(bar=full_bar, **format_dict)
            except UnicodeEncodeError:
                bar_format = _unicode(bar_format)
                nobar = bar_format.format(bar=full_bar, **format_dict)
            if not full_bar.format_called:
                # no {bar}, we can just format and return
                return nobar
            # Formatting progress bar space available for bar's display
            full_bar = Bar(
                frac,
                max(1, ncols - disp_len(nobar))
                if ncols else 10,
                charset=Bar.ASCII if ascii is True else ascii or Bar.UTF)
            if not _is_ascii(full_bar.charset) and _is_ascii(bar_format):
                bar_format = _unicode(bar_format)
            res = bar_format.format(bar=full_bar, **format_dict)
            if ncols:
                return disp_trim(res, ncols)
        elif bar_format:
            # user-specified bar_format but no total
            l_bar += '|'
            format_dict.update(l_bar=l_bar, percentage=0)
            full_bar = FormatReplace()
            nobar = bar_format.format(bar=full_bar, **format_dict)
            if not full_bar.format_called:
                return nobar
            full_bar = Bar(
                0,
                max(1, ncols - disp_len(nobar))
                if ncols else 10,
                charset=Bar.BLANK)
            res = bar_format.format(bar=full_bar, **format_dict)
            if ncols:
                return disp_trim(res, ncols)
        else:
            # no total: no progressbar, ETA, just progress stats
            return ((prefix + "": "") if prefix else '') + \
                '{0}{1} [{2}, {3}{4}]'.format(
                    n_fmt, unit, elapsed_str, rate_fmt, postfix)
    def __new__(cls, *args, **kwargs):
        # Create a new instance
        instance = object.__new__(cls)
        # Construct the lock if it does not exist
        with cls.get_lock():
            # Add to the list of instances
            if not hasattr(cls, '_instances'):
                cls._instances = WeakSet()
            cls._instances.add(instance)
            # Create the monitoring thread
            if cls.monitor_interval and (cls.monitor is None or not
                                         cls.monitor.report()):
                try:
                    cls.monitor = TMonitor(cls, cls.monitor_interval)
                except Exception as e:  # pragma: nocover
                    warn(""tqdm:disabling monitor support""
                         "" (monitor_interval = 0) due to:\n"" + str(e),
                         TqdmMonitorWarning, stacklevel=2)
                    cls.monitor_interval = 0
        # Return the instance
        return instance
    @classmethod
    def _get_free_pos(cls, instance=None):
        """"""Skips specified instance.""""""
        positions = set(abs(inst.pos) for inst in cls._instances
                        if inst is not instance and hasattr(inst, ""pos""))
        return min(set(range(len(positions) + 1)).difference(positions))
    @classmethod
    def _decr_instances(cls, instance):
        """"""
        Remove from list and reposition other bars
        so that newer bars won't overlap previous bars
        """"""
        with cls._lock:
            try:
                cls._instances.remove(instance)
            except KeyError:
                # if not instance.gui:  # pragma: no cover
                #     raise
                pass  # py2: maybe magically removed already
            # else:
            if not instance.gui:
                for inst in cls._instances:
                    # negative `pos` means fixed
                    if hasattr(inst, ""pos"") and inst.pos > abs(instance.pos):
                        inst.clear(nolock=True)
                        inst.pos -= 1
                        # TODO: check this doesn't overwrite another fixed bar
            # Kill monitor if no instances are left
            if not cls._instances and cls.monitor:
                try:
                    cls.monitor.exit()
                    del cls.monitor
                except AttributeError:  # pragma: nocover
                    pass
                else:
                    cls.monitor = None
    @classmethod
    def write(cls, s, file=None, end=""\n"", nolock=False):
        """"""Print a message via tqdm (without overlap with bars).""""""
        fp = file if file is not None else sys.stdout
        with cls.external_write_mode(file=file, nolock=nolock):
            # Write the message
            fp.write(s)
            fp.write(end)
    @classmethod
    @contextmanager
    def external_write_mode(cls, file=None, nolock=False):
        """"""
        Disable tqdm within context and refresh tqdm when exits.","[28, 29, 45, 46]"
"""""""
Colormap Normalization
======================
Objects that use colormaps by default linearly map the colors in the
colormap from data values *vmin* to *vmax*.  For example::
    pcm = ax.pcolormesh(x, y, Z, vmin=-1., vmax=1., cmap='RdBu_r')
will map the data in *Z* linearly from -1 to +1, so *Z=0* will
give a color at the center of the colormap *RdBu_r* (white in this
case).
Matplotlib does this mapping in two steps, with a normalization from
the input data to [0, 1] occurring first, and then mapping onto the
indices in the colormap.  Normalizations are classes defined in the
:func:`matplotlib.colors` module.  The default, linear normalization
is :func:`matplotlib.colors.Normalize`.
Artists that map data to color pass the arguments *vmin* and *vmax* to
construct a :func:`matplotlib.colors.Normalize` instance, then call it:
.. ipython::
   In [1]: import matplotlib as mpl
   In [2]: norm = mpl.colors.Normalize(vmin=-1, vmax=1)
   In [3]: norm(0)
   Out[3]: 0.5
However, there are sometimes cases where it is useful to map data to
colormaps in a non-linear fashion.
Logarithmic
-----------
One of the most common transformations is to plot data by taking its logarithm
(to the base-10).  This transformation is useful to display changes across
disparate scales.  Using `.colors.LogNorm` normalizes the data via
:math:`log_{10}`.  In the example below, there are two bumps, one much smaller
than the other. Using `.colors.LogNorm`, the shape and location of each bump
can clearly be seen:
""""""
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.colors as colors
import matplotlib.cbook as cbook
N = 100
X, Y = np.mgrid[-3:3:complex(0, N), -2:2:complex(0, N)]
# A low hump with a spike coming out of the top right.  Needs to have
# z/colour axis on a log scale so we see both hump and spike.  linear
# scale only shows the spike.
Z1 = np.exp(-X**2 - Y**2)
Z2 = np.exp(-(X * 10)**2 - (Y * 10)**2)
Z = Z1 + 50 * Z2
fig, ax = plt.subplots(2, 1)
pcm = ax[0].pcolor(X, Y, Z,
                   norm=colors.LogNorm(vmin=Z.min(), vmax=Z.max()),
                   cmap='PuBu_r')
fig.colorbar(pcm, ax=ax[0], extend='max')
pcm = ax[1].pcolor(X, Y, Z, cmap='PuBu_r')
fig.colorbar(pcm, ax=ax[1], extend='max')
plt.show()
###############################################################################
# Symmetric logarithmic
# ---------------------
#
# Similarly, it sometimes happens that there is data that is positive
# and negative, but we would still like a logarithmic scaling applied to
# both.  In this case, the negative numbers are also scaled
# logarithmically, and mapped to smaller numbers; e.g., if ``vmin=-vmax``,
# then they the negative numbers are mapped from 0 to 0.5 and the
# positive from 0.5 to 1.
#
# Since the logarithm of values close to zero tends toward infinity, a
# small range around zero needs to be mapped linearly.  The parameter
# *linthresh* allows the user to specify the size of this range
# (-*linthresh*, *linthresh*).  The size of this range in the colormap is
# set by *linscale*.  When *linscale* == 1.0 (the default), the space used
# for the positive and negative halves of the linear range will be equal
# to one decade in the logarithmic range.
N = 100
X, Y = np.mgrid[-3:3:complex(0, N), -2:2:complex(0, N)]
Z1 = np.exp(-X**2 - Y**2)
Z2 = np.exp(-(X - 1)**2 - (Y - 1)**2)
Z = (Z1 - Z2) * 2
fig, ax = plt.subplots(2, 1)
pcm = ax[0].pcolormesh(X, Y, Z,
                       norm=colors.SymLogNorm(linthresh=0.03, linscale=0.03,
                                              vmin=-1.0, vmax=1.0),
                       cmap='RdBu_r')
fig.colorbar(pcm, ax=ax[0], extend='both')
pcm = ax[1].pcolormesh(X, Y, Z, cmap='RdBu_r', vmin=-np.max(Z))
fig.colorbar(pcm, ax=ax[1], extend='both')
plt.show()
###############################################################################
# Power-law
# ---------
#
# Sometimes it is useful to remap the colors onto a power-law
# relationship (i.e. :math:`y=x^{\gamma}`, where :math:`\gamma` is the
# power).  For this we use the `.colors.PowerNorm`.  It takes as an
# argument *gamma* (*gamma* == 1.0 will just yield the default linear
# normalization):
#
# .. note::
#
#    There should probably be a good reason for plotting the data using
#    this type of transformation.  Technical viewers are used to linear
#    and logarithmic axes and data transformations.  Power laws are less
#    common, and viewers should explicitly be made aware that they have
#    been used.
N = 100",[100]
